[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StudyLog",
    "section": "",
    "text": "Home\n\n\nProfile\n\n\n\n\n\n\nStudyLog는 학습 과정을 규칙적으로 기록하고 체계화합니다.\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\nAI·ML·DL\n\n\n\n\n\n\n\n\n프로그래밍·개발\n\n\n\n\n\n\n\n\n컴퓨터 CG·HW\n\n\n\n\n\n\n\n\n보안·네트워크\n\n\n\n\n\n\n\n\n학업·연구\n\n\n\n\n\n\n\n\n기타\n\n\n\n\n\n\n\n\n\n\n프로필 탭 내용\n\n\n\n\n\n이 웹사이트는 Quarto를 이용해 제작되었습니다."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "da/ida_02.html",
    "href": "da/ida_02.html",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "",
    "text": "자료를 효과적으로 요약하고 이해하기 위해 표나 그림을 사용하는 방법에 대해 다루고자 한다. 자료 요약은 분석 대상인 자료의 형태와 특성에 따라 다양한 방법으로 이루어진다."
  },
  {
    "objectID": "da/ida_02.html#자-료-의-입-력",
    "href": "da/ida_02.html#자-료-의-입-력",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "da/ida_02.html#도수분포표",
    "href": "da/ida_02.html#도수분포표",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table\n특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1"
  },
  {
    "objectID": "da/ida_02.html#막대-그래프",
    "href": "da/ida_02.html#막대-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart\n데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18284\\2514237286.py:19: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`"
  },
  {
    "objectID": "da/ida_02.html#원형-그래프",
    "href": "da/ida_02.html#원형-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart\n전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "da/ida_02.html#파레토그림",
    "href": "da/ida_02.html#파레토그림",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart\n막대그래프와 누적선그래프를 결합한 형태의 그래프.\n파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "da/ida_02.html#도수다각형",
    "href": "da/ida_02.html#도수다각형",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency Polygon\n도수분포표의 도수를 선으로 연결한 그래프.\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data-Analysis",
    "section": "",
    "text": "두 모집단에 대한 비교\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n주성분 분석(PCA)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 2, 2025\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 18, 2025\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n6장: 감성 분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n6장: 감성 분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n5장: 텍스트 데이터 마이닝\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n3장: 네이버 카페 크롤링\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n4장: 크롤링 데이터 전처리\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n3장: 네이버 블로그 크롤링\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n2장: 네이버 뉴스 기사 제목 크롤링\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n상관분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n카이제곱 통계\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n연관성의 측도\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n오즈비\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n제 9장 회귀분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n다중 회귀\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n분산분석: 일원분류\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n분산분석: 일원분류 실습\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n분산분석: 이원분류 실습\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n제 10장 공분산\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n제 11장 비모수\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n12장: 두 모집단의 비교\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n11장: 정규모집단에서의 추론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n10장: 통계적 추론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n9장: 표집분포\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n8장: 정규분포\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n7장: 이항분포와 그에 관련된 분포들\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n6장: 확률분포\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n5장: 확률\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n4장: 두 변수 자료의 요약\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n3장: 수치를 통한 연속형 자료의 요약\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n2장: 표와 그림을 통한 자료의 요약\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 9, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n13장:\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n14장:\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n15장:\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "da/ida_12.html",
    "href": "da/ida_12.html",
    "title": "12장: 두 모집단의 비교",
    "section": "",
    "text": "두 모집단의 비교를 위한 추론과정은 자료를 어떻게 수집하느냐에 따라 추론 방법이 달라진다. 대표적인 두 종류의 자료수집과정에 따른 추론방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida_12.html#통계용어",
    "href": "da/ida_12.html#통계용어",
    "title": "12장: 두 모집단의 비교",
    "section": "2. 통계용어",
    "text": "2. 통계용어\n비교 연구 시 자주 사용되는 통계용어.\n실험단위 ( Experimental Unit ) : 실험의 대상. 반응값 ( Response ) : 실험 후 얻어지는 수치. 처리 ( Treatment ) : 비교하고자 하는 특성.\n예제12를 위 통계용어로 다음과 같이 설명할 수 있다:\n숲 지역 ⇨ 처리 1 , 도시 지역 ⇨ 처리 2 각각의 스캐너 측정값 ⇨ 실험단위\n그 측정값의 수치 ⇨ 반응값"
  },
  {
    "objectID": "da/ida_12.html#두-개-의-독-립-표-본",
    "href": "da/ida_12.html#두-개-의-독-립-표-본",
    "title": "12장: 두 모집단의 비교",
    "section": "3. 두 개 의 독 립 표 본",
    "text": "3. 두 개 의 독 립 표 본\n독립인 두 개의 표본으로부터 두 모집단, 혹은 두 가지의 처리효과를 비교하는 통계추론의 방법. 다음은 두 모집단으로부터 추출된 표본과 그로부터 계산되는 통계량을 정리한 것이다: 두 모집단으로부터 추출된 표본.\n위 표본으로부터 계산되는 통계량. 여기서 우리의 관심사는 두 모집단의 평균 반응값의 차이다.\n\n3-1. 모평균의 차에 대한 추론\n두 모평균의 차 ( μ 1 – μ 2 ) 에 대한 추론을 위해서는 두 표본평균의 차 ( x̄ – ȳ )를 이용한다. 두 표본의 크기 n 1, n 2 가 모두 큰 경우 ( 30 이상 )중심극한정리에 의해 두 표본평균은 근사적으로 정규분포를 따른다:\n평균이 μ​ 1 ± μ 2​ 이고, 분산이 σ 12 ​+ σ 22 ​인 정규분포를 따른다:\n이때, X 와 Y 는 서로 독립이다. 복호동순 : 식에서 부호를 2 개 이상 사용할 때, 부호를 앞에서부터 같은 순서로 적용하는 것. 따라서, 두 독립적인 정규분포 변수의 차를 표준화하면 표준정규분포를 따르게 된다: 위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다: 추정량 ± (z값) × (추정된 표준오차)\n두 표본의 크기 n 1, n 2 가 모두 30 이상일 때, 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n델타( Δ , δ: 그리스 알파벳의 네번째 글자)\n검정통계량은 H 0 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-2. 모평균의 차에 대한 추론\n표본의 크기가 작을 경우, 두 모집단에 대하여 정규분포와 표준편차에 대한 가정이 필요하다.\n두 모집단이 모두 정규분포를 따른다. 두 모집단의 표준편차가 일치한다.\n값이 ½보다 작거나 2보다 큰 경우, 위 가정이 적절하지 못한다고 판단한다. 대부분의 경우, σ 를 모르므로 이를 추정하여야 한다.\nσ 에 대한 정보는 편차제곱합에 모두 들어 있다. 따라서 이 두 제곱합을 더하여 각각의 자유도의 합으로 나누어 σ 2 추정량으로 사용하게 된다.\n이를 공통분산 σ 2 의 합동추정량 ( Pooled Variance ) 이라 한다:\n위 식을 이용하여, ( μ 1 – μ 2 ) 에 대한 표준화된 확률변수는 다음과 같다:\n자유도가 ( n ₁ + n ₂ – 2 )인 t 분포를 따른다.\n위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다:\n( μ ₁ – μ ₂ ) 에 대한 신뢰구간은 추정량 ± ( t 값 ) × ( 추정된 표준오차 )의 형식에 의해 정리된다.\n두 모집단이 모두 정규분포를 따르고 두 모표준편차가 같은 때 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-3. 모평균의 차에 대한 추론\n표본의 크기가 작고, 두 모집단의 표준편차가 일치하지 않을 경우, 근사적으로 t 분포를 따르며, 자유도는 ( n 1 – 1 )과 ( n 2 – 1 ) 중 작은 값이다.\n이 분포를 이용한 ( μ 1 – μ 2 ) 에 대한 추론방법은 다음과 같다:\n모평균 차에 대한 100 ( 1 − α ) % 신뢰구간은 근사적으로 위 식과 같다.\n신뢰구간의 경우, 그 구간이 넓어지는 경향이 있다. 따라서 실제 신뢰도는 100 ( 1 − α ) % 이상이 된다.\n가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n검정의 경우, 기각역이 좁아지는 경향이 있다. 따라서 실제 유의수준이 α 이하가 되므로 귀무가설을 기각하지 못할 가능성이 높아진다."
  },
  {
    "objectID": "da/ida_12.html#짝비교",
    "href": "da/ida_12.html#짝비교",
    "title": "12장: 두 모집단의 비교",
    "section": "4. 짝비교",
    "text": "4. 짝비교\nMatched Pair Comparisons 실험 단위들이 비슷해야 한다는 점과 다양한 실험 단위들을 비교해야 한다는 점을 절충하는 접근법.\n짝지워서 각각의 쌍으로 만드는 방법:\n같은 쌍의 실험단위들은 서로 비슷해야 한다. 각 쌍 내에서 두 조건이 다르게 설정되어야 한다.\n짝 비교를 시행할 때의 자료의 형태:\n차의 표본평균과 분산은 다음과 같다:\nX i 와 Y i 는 서로 독립이 아니다. 서로 높은 상관관계를 가질 경우, 짝비교의 효과는 크다. 즉, 전체적인 변화의 폭 ( 변동성 ) 을 줄여 처리효과를 알아내기 수월한 짝비교를 할 수 있다.\n모평균 δ 에 대한 100 ( 1 − α ) % 신뢰구간은 다음과 같다:\n귀무가설 H 0 : δ = δ 0 에 대한 검정통계량은 다음과 같다:\nH ₀ 가 맞을 때 자유도가 ( n – 1 )인 t 분포를 따른다.\n자료가 짝지워져 있는 경우,두 개의 처리를 어떻게 배정할 것인가가 전혀 문제되지 않는다. 자료가 짝지워져 있지 않은 경우, 여러 조건들이 확률적으로 같은 정도로 영향이 미치도록 해야 한다. (어느 한쪽의 처리에만 영향을 주지 않아야 한다.)이와 같이 무작위로 배정하는 것을 랜덤화 ( Randomization ) 라고 한다."
  },
  {
    "objectID": "da/ida_12.html#두-모비율의-차에-대한-추론",
    "href": "da/ida_12.html#두-모비율의-차에-대한-추론",
    "title": "12장: 두 모집단의 비교",
    "section": "5. 두 모비율의 차에 대한 추론",
    "text": "5. 두 모비율의 차에 대한 추론\n두 모집단의 비율을 비교하는 추론하는 방법. 관심의 대상이 되는 어떤 특성의 모집단 1 의 비율을 p 1, 모집단 2 의 비율을 p 2 라고 할 때 두 모집단으로부터 크기가 n 1, n 2 인 표본을 추출하여 각각 특성이 A 인 것과 A 가 아닌 것으로 분류하였다고 가정한다.\n이때 A 를 ’ 성공 ‘, A 가 아닌 것을’ 실패 ’ 라고 하고 두 표본의 성공의 개수를 각각 X, Y 표현한다.\n두 모집단의 특성 A 의 비율을 각각 p 1, p 2 라고 하면 그 추정량은 각 표본으로부터 표본의 비율을 사용하게 된다:\n두 모비율의 차 ( p ₁ – p ₂ ) 의 추정량은 ( p̂ ₁ – p̂ ₂ ) 이 된다. ( p 1 – p 2 ) 에 대한 추론을 하기 위해서는 ( p̂ 1 – p̂ 2 ) 의 분포를 알아야 한다.\n표본의 크기 n 1, n 2 가 큰 경우, 아래 식이 근사적으로 성립한다:\n아래 식도 표본이 서로 독립이므로 정규분포로 근사된다:\n따라서 이를 표준화하면:\n두 확률변수 간의 차이를 표준화하는 것이다.\n위 식을 이용하여 ( p 1 – p 2 ) 에 대한 ( 1 − α ) % 신뢰구간은 다음과 같다:\n( 추정량 ) ± ( z 값 ) × ( 표준오차 )의 형식에 의해 정리된다.\n신뢰구간을 계산할 때, ( 제곱근 속의 ) 실제 모평균 차이 p 1, p 2 는 미지수이므로, 이를 표본 비율의 차이 p̂ 1, p̂ 2 로 대체하여 계산한다.\n\n5-1. 두 모비율의 검정\n표본의 크기가 클 때 두 모비율이 같은지를 검정하는 방법은 두 비율의 차이에 대한 검정을 통해 이루어진다.\n귀무가설 H 0 : p = p 0 을 검정하기 위해 (​ p̂ 1 – p̂ 2 ) 을 이용하게 된다. 이 가설이 맞을 경우의 통계량 분포는 다음과 같다:\np 는 모비율이 같은 두 모집단의 공통비율이다.\n통합된 두 표본으로부터 이를 추정하면:\n위 과정을 바탕으로 검정통계량을 정리할 수 있다:\np̂ 는 귀무가설하에서의 공통비율 p 의 추정량이고, 검정통계량은 H 0 가 맞을 때 근사적으로 N ( 0, 1 ) 을 따른다.\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 12의 ( 1 ) 추정량의 표준오차 ( se ) 를 계산하기 위해 두 변수의 요약통계량 ( s 12, s 22 ) 을 구하는 파이썬 함수를 사용할 수 있다.\n\nvar1 = np.var(x, ddof=1);print(var1) # x의 표본 분산 (자유도=1)\nvar2 = np.var(y, ddof=1);print(var2) # y의 표본 분산 (자유도=1)\n\nn1 = len(x);print(n1) # x의 데이터 수\nn2 = len(y);print(n2) # y의 데이터 수\n\nse = math.sqrt(var1 / n1 + var2 / n2);print(se) # 표준오차\n\n48.06374040272343\n24.789102564102567\n118\n40\n1.0134334699544658\n\n\n표준오차 계산 ( 1.0134 )\n예제 12의 ( 2 ) 모평균의 차 μ 1 – μ 2 에 대한 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 z–값\nz_alpha = stats.norm.ppf(1 - 0.05 / 2);print(z_alpha)\ninterval_z = z_alpha * se; print(interval_z) # 신뢰구간 범위\n\n# x와 y의 평균을 계산\nxbar1 = np.mean(x); print(xbar1) \nxbar2 = np.mean(y); print(xbar2) \n\n# 두 평균의 차이를 계산\ndiff = xbar1 - xbar2; print(diff) \n\n# 신뢰구간\nCI_1 = [diff - interval_z, diff + interval_z]; print(CI_1)\n\n\n## 해석: 95%의 신뢰구간이 0을 포함하지 않으므로 (양측검정에서) \n## 유의수준 5%에서 두 수치가 같다는 귀무가설 (H₀ : μ1 = μ2) 을 기각할 수 있다.\n\n1.959963984540054\n1.986293101838208\n92.9322033898305\n82.075\n10.857203389830502\n[np.float64(8.870910287992295), np.float64(12.84349649166871)]\n\n\n스캐너 자료 μ ₁ – μ ₂ 의 신뢰구간은 10.857 ± 1.986 이다.\n예제 12의 ( 3 ) 두 모집단이 모두 정규분포를 따르고, 분산이 같다는 가정 하에서 합동분산추정량 및 추정량의 표준오차를 구하라.\n\n# 합쳐진 분산 계산\nspooled = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2); print(spooled) \n\n# 합쳐진 표준 오차 계산\nse_spooled = math.sqrt(spooled) * math.sqrt(1 / n1 + 1 / n2); print(se_spooled)\n\n42.24508094306821\n1.1891745810061622\n\n\n합동추정량 ( 42.245 ) / 표준오차 ( 1.189 )\n예제 12의 ( 4 ) 위 결과를 바탕으로 t – 분포의 백분위수 함수를 이용하여 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 t–값\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n1 + n2 - 2); print(t_alpha) \n\n# 신뢰 구간의 범위\ninterval_t = t_alpha * se_spooled; print(interval_t) \n\n# 신뢰구간\nCI_2 = [diff - interval_t, diff + interval_t]; print(CI_2)\n\n## 해석: (2)번의 정규분포를 이용한 신뢰구간보다 \n## 오차범위 값인 interval_t가 더 크므로 신뢰구간이 더 넓어졌음을 알 수 있다.\n## 이는 t–분포의 백분위수 값이 (정규분포의 백분위수 값보다) 더 크기 때문이다.\n\n1.9752875076954723\n2.3489616943304696\n[np.float64(8.508241695500033), np.float64(13.206165084160972)]\n\n\n신뢰구간은 10.857 ± 2.348 이다."
  },
  {
    "objectID": "da/ida_10.html",
    "href": "da/ida_10.html",
    "title": "10장: 통계적 추론",
    "section": "",
    "text": "추출된 표본으로부터 모집단의 일반적인 특성을 추론해내는 것을 통계적 추론이라고 하며, 이는 표본의 크기가 클 때 더 정확하게 성립한다."
  },
  {
    "objectID": "da/ida_10.html#자료의-입력",
    "href": "da/ida_10.html#자료의-입력",
    "title": "10장: 통계적 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 13: 예제 1 에 주어진 자료에 근거해서 \n# 중학교 1학년 남학생의 평균키에 대한 95% 신뢰구간을 구하라. (p.311)\n\nimport numpy as np \n\nheight = np.array([163, 161, 168 , 161, 157, 162, 153, 159, 164, 170, \n                   152, 160, 157, 168, 150, 165, 156, 151, 162, 150, \n                   156, 152, 161, 165, 168, 167, 165, 168, 159, 156])\n                   \n\n#_____________________________________________________________________________\n\n# 예제 14: 예제 8의 검정을 시행하고 그 결과를 비교하여라. (p.313)\n\n# 예제 8: 앞의 예제 1에 주어진 중학생의 키 자료로부터 \n# 그 도시의 중학교 1학년 남학생의 평균키(m)가 다른 도시의 중학교 1학년 \n# 남학생의 평균키인 159 cm와 차이가 있다고 할 수 있는지 판단하라. (p.297)\n\n\n#_____________________________________________________________________________\n\n# 예제 15: 3 장의 예제 11 에 주어진 자료로부터 \n# 평균교통소음정도(μ)에 대한 98% 신뢰구간을 구하고, \n\n# 평균교통소음정도가 60 을 초과한다고 주장할 수 있는지 \n# 유의수준 5% 에서 가설검정을 실시하라. (p.314)\n\nnoise = np.array([55.9, 63.8, 57.2, 59.8, 65.7, 62.7, 60.8, 51.3, 61.8, 56.0, \n                  66.9, 56.8, 66.2, 64.6, 59.5, 63.1, 60.6, 62.0, 59.4, 67.2,\n                  63.6, 60.5, 66.8, 61.8, 64.8, 55.8, 55.7, 77.1, 62.1, 61.0, \n                  58.9, 60.0, 66.9, 61.7, 60.3, 51.5, 67.0, 60.2, 56.2, 59.4, \n                  67.9, 64.9, 55.7, 61.4, 62.6, 56.4, 56.4, 69.4, 57.6, 63.8])"
  },
  {
    "objectID": "da/ida_10.html#통계적-추론",
    "href": "da/ida_10.html#통계적-추론",
    "title": "10장: 통계적 추론",
    "section": "2. 통계적 추론",
    "text": "2. 통계적 추론\nStatistical Inference\n표본이 갖고 있는 정보를 분석하여 모수에 관한 결론을 유도하고, 모수에 대한 가설의 옳고 그름을 판단하는 것을 말한다.\n모집단의 일부인 표본으로부터 전체 모집단의 성질을 추론해내는 것이므로 100% 확실하다고 할 수는 없다. 따라서 통계적인 추론을 할 때에는 그 결론의 부정확한 정도를 반드시 언급하여야 하는데 이러한 정도를 수치로 표시할 수 있게 하는 도구로 앞에서 공부한 확률론과 표준분포 등이 이용된다.\n통계적 추론에는 두 가지 주요 방법이 있다:\n모수의 추정 모수에 대한 가설 검증"
  },
  {
    "objectID": "da/ida_10.html#모평균의-추정",
    "href": "da/ida_10.html#모평균의-추정",
    "title": "10장: 통계적 추론",
    "section": "3. 모평균의 추정",
    "text": "3. 모평균의 추정\nEstimation of Parameters\n모수 중 하나로 포함되는, 모집단의 평균에 대한 점추정과 구간추정을 다루고자 한다.\n\n3-1. 점추정\nPoint Estimation\n추정하고자 하는 하나의 모수에 대해, 여러 개의 확률변수를 사용하여 하나의 통계량을 만들고, 주어진 표본으로부터 그 값을 계산하여 하나의 수치를 제시하는 과정.\n모수 ( Parameter ) : 모집단의 실제 값으로, 우리가 알고 싶어하는 대상. 추정량 ( Estimator ) : 모수를 측정하기 위해 만들어진 통계량. 추정치 ( Estimate ) : 주어진 관측값으로부터 계산된 추정량의 실제 값.\n추정량은 하나의 확률변수이므로, 추출된 표본의 따라 그 값이 달라진다. 수치들의 변화의 정도는 추정량의 정확도와 관계가 있다. 이 정확도를 측정하는 도구 중 하나가 표준오차 ( 추정량의 표준편차 ) 이다.\n\n\n3-2. 표준오차\nStandard Error, SE\n추정량의 정확도를 평가하는 데 중요한 지표이며, 값이 작을수록 추정량이 모집단 모수를 더 정확하게 반영한다고 할 수 있다. 표본평균의 기댓값과 표준오차는, 모집단의 평균과 표준편차가 μ, σ 일 때 위와 같이 구할 수 있다.\n표본평균을 가지고 μ 를 추정할 경우, n 이 클수록 표준오차가 작아져 좀 더 정확한 추정이 가능하다. 그러나 표준오차 계산 시 σ 가 주어지지 않는 경우가 있다. 이 경우, σ 를 표본표준편차로 추정하여 사용할 수 있다.\n\n\n3-3. 구간추정\nInterval Estimation\n추정량의 분포를 이용하여 표본으로부터 모수 값을 포함하리라고 예상되는 구간을 제시하는 것. 이때 제시되는 구간을 신뢰구간이라고 한다.\n\n\n3-4. 신뢰구간\nConfidence Interval\n모집단의 어떤 모수를 추정하기 위해 계산된 범위. 신뢰구간은 ( L , U )의 형태로 이루어지며, 여기서 L 과 U 는 표본으로부터 계산된 통계량이다.\nL ( Lower bound ) : 신뢰구간의 하한값 U ( Upper bound ) : 신뢰구간의 상한값 따라서 표본마다 계산되는 신뢰구간은 서로 다를 수 있다.\n가장 확실한 신뢰구간은 항상 모수를 포함하는 구간이다. 그러나 이는 이론적으로는 가능하지만 실질적으로는 불가능하다. 그렇게 되기 위해서는 신뢰구간이 상당히 길어질 수 밖에 없다.\n이 경우, 신뢰구간 CI 는 매우 넓어질 수밖에 없다.\n항상 모수를 포함하는 신뢰구간은 실질적으로 너무 넓어서 유용하지 않다. 모수에 대한 정확한 정보를 얻으려면 신뢰구간을 가능한 한 줄일 필요가 있다.\n신뢰구간이 좁을수록 추정의 정확성이 높아진다.\n실용적인 측면에서 신뢰구간을 적절히 좁히기 위해, ” 모든 표본에서 항상 모수를 포함해야 한다 ” 는 엄격한 조건을 완화하고, 대부분의 경우에서 모수를 포함하도록 설정하는 것이 필요하다.\n신뢰구간이 모수를 포함할 확률을 1 보다는 작은 일정한 수준에 유지하여 구간의 길이를 줄이는 것이 바람직하다.\n이때 모수를 포함할 확률을 신뢰수준 ( Level of Confidence ) 또는 신뢰도라고 한다.\n\n\n3-5. 모평균 μ에 대한 신뢰구간\n여기에서는 신뢰구간을 계산하는 두 가지 경우를 설명한다:\n\n모집단의 표준편차 σ 를 알고 있는 경우:\n\n정규분포 개념을 이용하여 신뢰구간을 계산한다:\n신뢰수준 95 % 에 해당하는 정규분포의 임계값 Z α / 2 ​를 사용하여 신뢰구간의 범위를 설정한다. 예시 : α = 0.05 일 때, Z ₀.₀₅ / ₂ = Z ₀.₀₂₅ = 1.96\n정규분포는 평균 μ 와 표준편차 σ 를 알 때, 그 분포의 형태가 완전히 결정된다. 평균 μ 를 중심으로 좌우 대칭이며, σ 가 클수록 분포가 넓어진다. 이 특성 덕분에, 정규분포는 모수에 대해 많은 정보를 제공할 수 있다. 모집단이 정규분포를 따른다면, 표본 평균의 분포 역시 정규분포를 따른다.\n위의 식에서 괄호 안에 부등식을 풀어 쓰면:\n위 식을 μ 에 대해 정리하면:\n따라서:\n\n모집단의 표준편차 σ를 모르는 경우:\n일반적인 경우에 관심있는 모집단은 그 분포나 표준편차가 알려져 있지 않다. 이런 경우, 9장에서 다룬 중심극한정리를 이용한다.\n\n위 식은 약간의 수정이 필요한데, 이것은 11장을 참고하기를 바란다.\n이 경우에도, 확률 1 − α 는 근사적으로 얻어진다. n 이 클 때는 σ 대신 s 를 사용해도 확률값에 크게 영향을 주지 않는다.\n일반적으로 추정량의 기댓값이 추정하고자하는 모수값을 갖고 그 분포가 정규분포일 때 100 ( 1 − α ) % 신뢰구간은 다음과 같은 형태를 따른다:\n사용 상황에 따라 표준오차의 정의가 달라질 수 있다.\n\n\n3-6. 신뢰구간의 의미\n이 그래프는 주어진 모수에 대해 95% 신뢰구간을 표시하였다. 주어진 모수 (평균 100, 표준편차 10), 표본 크기 15를 사용하여 총 25개의 표본을 추출하였다.\n위 그래프와 같은 방식으로 표본을 계속 추출하고 신뢰구간을 계산하면, 그 신뢰구간들이 모평균을 포함하는 비율이 95% 에 가까워지는 것을 확인할 수 있다.\n\n\n3-7. 표본 크기의 결정\n많은 수의 표본을 추출하여 신뢰구간을 생성하는 것은 시간과 비용이 많이 소모된다. 따라서 우리가 원하는 정확도를 얻을 수 있는 범위 내에서 표본의 크기를 줄이는 것이 바람직하다.\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α ) % 가 되려면:\n표준화된 확률변수의 분포가 표준정규분포를 따르므로:\n위 사실로부터 아래 식이 성립해야 하며:\n이를 n 에 대하여 풀면 아래 식을 만족해야 한다:\n적정한 표본의 크기는 위의 부등식을 만족하는 ’ 최소의 정수 ’ 가 된다. 만약 모집단이 정규분포라는 가정이 없다면 표본의 크기는 중심극한정리를 이용할 수 있도록 30 이상이 되어야 한다."
  },
  {
    "objectID": "da/ida_10.html#모평균에-대한-검정",
    "href": "da/ida_10.html#모평균에-대한-검정",
    "title": "10장: 통계적 추론",
    "section": "4. 모평균에 대한 검정",
    "text": "4. 모평균에 대한 검정\n통계적 추론 중 하나인 모수에 대한 가설 검증 ( Testing Statistical Hypotheses ) 에 대해 다루고자 한다.\n\n4-1. 가설\nHypotheses 가설검증에는 2 개의 가설이 있다.\n대립가설 ( Alternative Hypothesis ; H ₁ ) : 입증하여 주장하고자 하는 가설.\n귀무가설 ( Null Hypothesis ; H ₀ ) : 대립가설을 입증할 수 없을 때, 대립가설을 무효화하면서 받아들이는 가설.\n\n\n4-2. 오류의 종류\n가설검증에서 내리는 판단이란 다음 2 가지 형태 중 하나로 나타난다. 상황에 따라 다르지만, 일반적으로 제1종 오류에 더 주의를 기울이게 된다.\n제1종 오류 ( Type I Error ; α ) : 귀무가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 없는데, 효과가 있다고 결론 내리는 경우.\n제2종 오류 ( Type II Error ; β ) : 대립가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 있는데, 효과가 없다고 결론 내리는 경우.\n\n\n4-3. 검정통계량\nTest Statistic 표본 데이터를 요약하여 귀무가설을 검정하는 데 사용하는 값. 귀무가설이 참이라는 가정 하에 표본에서 계산된 값으로, 이 값이 귀무가설 하에서 얼마나 극단적인지를 평가한다.\n\n\n4-4. 기각역\nCritical Region 귀무가설을 기각할 수 있는 값들의 집합.\n검정 통계량이 기각역에 속할 경우 귀무가설을 기각한다. 아닐 경우 귀무가설을 기각하지 않는다.\nc 이하이면 H₀ 을 기각한다.\n가장 바람직한 기각역이란 아래 두 확률을 최소화하는 것이 될 것이다.\nα : 제 1 종 오류를 범하게 될 확률. β : 제 2 종 오류를 범하게 될 확률.\n위 두 확률은 다음과 같은 특징이 있다:\n( 1 ) α 와 β 는 서로 반비례 관계에 있다.\n( 2 ) α 는 너무 크게 설정하지 않는 것이 좋다. 너무 큰 α 는 제1종 오류의 확률을 높여 잘못된 결론을 내릴 가능성을 높인다.\n이를 방지하기 위해, 유의수준이라는 상한선을 둘 수 있다.\n\n\n4-5. 유의수준\nSignificance Level 일반적으로 사용되는 유의수준은 0.05 ( 5% ) 또는 0.01 ( 1% ) 이다. 이는 연구자가 제1종 오류의 확률을 낮추어 신뢰할 수 있는 결과를 얻기 위함이다.\n0.05 ( 5% ) : 가장 흔히 사용되는 유의수준. 제1종 오류를 5% 로 제한. 0.01 ( 1% ) : 더 엄격한 기준으로, 제1종 오류를 1% 로 제한. 0.10 ( 10% ) : 덜 엄격한 기준으로, 제1종 오류를 10% 로 제한.\n양측 검정 ( Two — tailed Test ) : 유의 수준 α 는 두 방향 ( 양쪽 끝 ) 에 분배되어 α / 2 씩 기각역을 형성한다.\n단측 검정 ( One — tailed Test ) : 유의 수준 α 는 한 방향에 집중되어 기각역을 형성한다.\n\n\n4-6. 모평균 μ에 대한 검 정\n표본의 크기가 클 때 모평균 μ 에 대한 가설 H ₀ : μ = μ₀ 을 검정하기 위한 검정통계량은 다음과 같다: 단 모집단의 표준편차 σ 가 주어져 있을 때 s를 σ 로 대체한다.\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n4-7. 유의확률\nSignificance Probability 주어진 검정통계량의 관측치로부터 H ₀ 을 기각하게 하는 최소의 유의수준.\nP – 값 ( P – Value )\n일반적으로 유의확률은 주어진 관측값을 경계점으로 하는 기각역의 유의수준으로 얻어진다. Z = z 일 때 각 기각역의 형태에 따라 P – 값을 구하는 식을 정리하면 다음과 같다:"
  },
  {
    "objectID": "da/ida_10.html#모비율에-대한-추론",
    "href": "da/ida_10.html#모비율에-대한-추론",
    "title": "10장: 통계적 추론",
    "section": "5. 모비율에 대한 추론",
    "text": "5. 모비율에 대한 추론\n모집단에서 특정 속성을 가진 개체의 비율을 추정하거나 검정하는 것을 의미한다.\n\n5-1. 점추정\n모비율에 대한 추정량으로 표본비율을 사용할 수 있다: 점추정량이 결정되면 그 추정량의 정확도를 알기 위해 표준오차를 계산할 필요가 있다.\n모집단의 크기가 매우 커서 그에 비해 표본의 크기가 작은 경우, X 의 분포는 반복 횟수 n , 성공의 확률 p 인 이항분포가 된다. 따라서, X 의 기댓값과 분산는 아래와 같다:\nE ( X ) = np Var ( X ) = npq\n표본비율의 표준오차는 다음과 같이 계산된다:\n이 식은 이항분포의 분산을 이용한 결과이다. ​​ 표본비율​의 분산 은 아래 식과 같다:\n표본비율의 표준오차는 이 분산의 제곱근으로 표현된다.\n\n\n5-2. 구간추정\n모비율 P 에 대한 구간추정을 하려면 P 의 추정량인 p̂ 의 분포를 알아야 한다.\nX 의 분포는 이항분포를 따르므로, 중심극한정리를 이용하여 표본 비율 p̂​ 의 분포를 근사적으로 정규분포로 취급할 수 있다:\n분모, 분자를 n으로 나누었을 때, 위와 같은 식이 나온다.\n위 구간이 p를 포함할 확률이 ( 1 − α ) 가 됨을 알 수 있다.\n\n\n5-3. 신뢰구간\np 를 추정량인 p̂ 으로 대체하면, 원하는 신뢰구간을 구할 수 있다.\n\n\n5-4. 표본크기의 결정\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α )% 가 되려면:\n표본 크기가 클 경우, 정규분포를 이용하여 위와 같은 식을 구할 수 있다. 따라서, 표본 크기는 아래 식을 만족해야 한다:\n\n\n5-5. 모비율 p에 대한 검정\n표본 크기가 클 때 모비율 p 에 대한 가설 H ₀ : p = p ₀ 을 검정하기 위한 검정통계량은 다음과 같다:\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 13의 ( 1 ) height 에 대한 요약 통계량 계산하기.\n\nxbar_h = np.mean(height);print(xbar_h) # 평균\n\nvar_h = np.var(height, ddof=1);print(var_h) # 분산 (자유도 1 사용)\n\nsd_h = np.std(height, ddof=1);print(sd_h) # 표준편차 (자유도 1 사용)\n\nmedian_h = np.median(height);print(median_h) # 중앙값\n\nmin_h = np.min(height);print(min_h) # 최솟값\n\nmax_h = np.max(height);print(max_h) # 최댓값\n\nsum_h = np.sum(height);print(sum_h) # 합계\n\nn = height.size;print(n) # 데이터 개수\n\n160.2\n35.88965517241378\n5.990797540596225\n161.0\n150\n170\n4806\n30\n\n\n예제 13의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수. 정규분포의 백분위수 함수.\n\nfrom scipy import stats \n\nse_h = stats.sem(height);print(se_h) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.05 / 2); print(z_alpha) # 95% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_h;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_h - interval, xbar_h + interval]; print(CI) # 신뢰구간\n\n1.0937649834770078\n1.959963984540054\n2.1437399751659827\n[np.float64(158.05626002483402), np.float64(162.34373997516596)]\n\n\n이로부터 95% 신뢰구간은 160.2 ± 2.1437 임을 알 수 있다.\n예제 14 검정하고자 하는 가설은 H ₀ : μ = 159 대 H ₀ : μ ≠ 159 이며, 표본의 크기는 30 이상이다.\n\nzval = (xbar_h-159)/se_h;print(zval) # 가설 검정을 위한 z값\n\npval = 2 * (1 - stats.norm.cdf(zval));print(pval) # p값\n\n## 해석: p값이 크므로 귀무가설을 기각할 수 없음\n## → 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\n\n1.097127827392377\n0.27258551722126834\n\n\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고, 이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\n\nxbar_n = np.mean(noise);print(xbar_n) # 평균\n\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\n\nn = noise.size;print(n) # 데이터 개수\n\n61.373999999999995\n4.780137902544961\n50\n\n\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\n\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\n\n0.6760135851792765\n2.3263478740408408\n1.5726427667045366\n[np.float64(59.801357233295455), np.float64(62.946642766704535)]\n\n\n\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\n\npval = stats.norm.sf(np.abs(zval));print(pval) # p값\n\n## 해석: P—값이 0.021로 0.05보다 작게 나왔으므로 \n## 유의수준 5% 에서 귀무가설을 기각할 수 있다.\n\n## 그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\n\n2.032503532655509\n0.021051353256926374"
  },
  {
    "objectID": "da/ida_08.html",
    "href": "da/ida_08.html",
    "title": "8장: 정규분포",
    "section": "",
    "text": "6장에서 언급되었던, 연속적인 값을 가지는 연속확률분포들 중에서 대부분의 통계학 이론의 기본이 되는 정규분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_08.html#자료의-입력",
    "href": "da/ida_08.html#자료의-입력",
    "title": "8장: 정규분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n!pip install numpy\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7: 어느 대학교의 일반수학 중간고사 성적은\n# 분포가 평균이  63 이고, 분산이  100 인 정규분포를 따른다고 가정한다. (p.232)\n\n# (1) 50 점 이하의 학생은 몇 퍼센트나 되겠는가?\n\n# (2) 상위 10 %의 학생에게  A 를 준다고 하면 \n# 몇 점 이상이 되어야  A 를 받을 수 있겠는가?\n\n# ___________________________________________________________\n\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\n\nimport numpy as np \n\ndata1 = np.array([4001, 3927, 3048, 4298, 4000, 3445, \n                 4949, 3530, 3075, 4012, 3797, 3550, \n                 4027, 3571, 3738, 5157, 3598, 4749, \n                 4263, 3894, 4262, 4232, 3852, 4256, \n                 3271, 4315, 3078, 3607, 3889, 3147, \n                 3421, 3531, 3987, 4120, 4349, 4071, \n                 3683, 3332, 3285, 3739, 3544, 4103, \n                 3401, 3601, 3717, 4846, 5005, 3991, \n                 2866, 3561, 4003, 4387, 3510, 2884, \n                 3819, 3173, 3470, 3340, 3214, 3670, \n                 3694])\n                 \n# ___________________________________________________________\n\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\n\ndata2 = np.array([39.3, 14.8, 6.3, 0.9, 6.5, \n                 3.5, 8.3, 10.0, 1.3, 7.1, \n                 6.0, 17.1, 16.8, 0.7, 7,9, \n                 2.7, 26.2, 24.3, 17.7, 3.2, \n                 7.4, 6.6, 5.2, 8.3, 5.9, \n                 3.5, 8.3, 44.8, 8.3, 13.4, \n                 19.4, 19.0, 14.1, 1.9, 12.0, \n                 19.7, 10.3, 3,4, 16.7, 4.3, \n                 1.0, 7.6, 28.33, 26.2, 31.7, \n                 8.7, 18.9, 3.4, 10.0])\n\nCollecting numpy\n  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\nInstalling collected packages: numpy\nSuccessfully installed numpy-2.2.6\n\n\nWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\nYou should consider upgrading via the 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command."
  },
  {
    "objectID": "da/ida_08.html#연속확률분포",
    "href": "da/ida_08.html#연속확률분포",
    "title": "8장: 정규분포",
    "section": "2. 연속확률분포",
    "text": "2. 연속확률분포\nContinuous Probability Distribution\n연속확률변수 X 가 가질 수 있는 값들의 분포를 나타낸다. 연속확률변수: 특정 범위 내에서 모든 실수 값을 가질 수 있는 변수."
  },
  {
    "objectID": "da/ida_08.html#확률밀도함수",
    "href": "da/ida_08.html#확률밀도함수",
    "title": "8장: 정규분포",
    "section": "2-1. 확률밀도함수",
    "text": "2-1. 확률밀도함수\nProbability Density Function, PDF\n연속확률변수 \\(X\\) 의 확률분포는 확률의 밀도를 나타내는 \\(X\\) 의 확률밀도함수에 의해 결정된다. 아래의 조건을 만족하는 함수 \\(f(X)\\) 를 \\(X\\) 의 확률밀도함수라고 한다.\n\n연속확률변수에서 특정 값에 대한 확률은 0 이다. 예를 들어, [ 0, 1 ] 구간에 있는 실수 값은 무한히 많다. 이러한 경우, 특정한 하나의 값을 가질 확률은 무한히 작은 값이 되며, 이는 수학적으로 0 으로 표현된다.\n\n범위 : 확률밀도함수 f ( x ) 의 값은 항상 0 이상이어야 한다. 이는, 어떤 사건의 확률이 음수일 수 없다는 것을 의미한다.\n확률밀도함수는 이산확률분포의 확률함수와는 달리 확률을 나타내지 않는다. 그러므로, f ( x ) 가 1 보다 작아야 된다는 조건 은 필요가 없다.\n\nPDF 를 사용하여 특정 구간 [ a, b ] 에 속할 확률을 계산할 수 있다.\nPDF 를 전체 범위에 대해 적분 하면 1 이 되어야 한다.\n\n위 조건으로부터 다음과 같은 결론을 내릴 수 있다:\n어떤 구간의 확률을 구할 때에는 그 구간의 경계점이 포함되는가 포함되지 않는가에 영향을 받지 않는다."
  },
  {
    "objectID": "da/ida_08.html#정규분포",
    "href": "da/ida_08.html#정규분포",
    "title": "8장: 정규분포",
    "section": "3. 정규분포",
    "text": "3. 정규분포\nNormal Distribution\n평균 \\(μ\\) 와 분산 \\(σ^2\\) 의해서 그 분포가 확정된다. 그 확률밀도함수 ( PDF ) 의 대략적인 특성은 다음과 같이 표현할 수 있다:\n위 정규분포는 N ( μ, σ² )으로 표시할 수 있다. X 가 평균 μ 로부터 ± σ, ± 2σ, ± 3σ, ± 4σ 의 사이의 있을 확률은 다음과 같다.\n이를 통해 ± 4σ 이상 떨어진 데이터는 매우 드문 현상임을 알 수 있다. 그러므로, 정규분포의 ± 4σ 이상의 영역은 실질적인 분석에서 종종 무시할 수 있다."
  },
  {
    "objectID": "da/ida_08.html#정규분포의-특성",
    "href": "da/ida_08.html#정규분포의-특성",
    "title": "8장: 정규분포",
    "section": "3-1. 정규분포의 특성",
    "text": "3-1. 정규분포의 특성\nμ ± 3σ 안에 거의 모든 확률이 집중된다.\n3 - 2 . 표 준 정 규 분 포 ( Standard Normal Distribution ) 평균 ( μ ) 이 0 이고 표준편차 ( σ )가 1 인 특수한 정규분포.\n확률변수 Z 가 N ( 0, 1 ) 이라고 할 때, Z 는 0 을 중심으로 대칭인 분포를 갖게 되며,\n이를 이용해 다음과 같이 나타낼 수 있다:\n누적분포함수 ( CDF ) 는 특정 값 이하의 확률을 나타낸다.\nP ( Z ≤ b ) 는 Z 가 b 이하일 확률이고, P ( Z ≤ a ) 는 Z 가 a 이하일 확률이다. 따라서 a 에서 b 까지의 확률은 다음과 같다:\n\n3-3. 표준정규확률변수\nStandard Normal Random Variable\n표준정규분포 Z 에 관한 확률계산 방법을 일반 정규분포 X 의 확률계산에 적용할 수 있다. 일반 정규분포 X 를 표준정규분포 Z 로의 식변환을 통해 쉽게 계산할 수 있으며, 이를 표준화라고 한다.\n확률변수 X 가 N ( μ, σ² ) 일 때 표준화된 확률변수 Z 는 정규분포 N ( 0, 1 ) 을 따른다. X 를 표준화하여 Z 로 표현한 후 표준정규분포표를 이용하면 된다.\n예제 7의 ( 1 ) 중간고사 성적을 확률변수 X 라고 하면 주어진 정보를 바탕으로 다음과 같이 표현할 수 있다:\n평균 (μ) = 63, 분산 (σ²) = 100 = 10², 표준편차 (σ) = 10 정규분포표의 −1.3행, 0.00열의 값 = 0.0968\n\nfrom scipy.stats import norm\nprint(norm.cdf(x=50, loc=63, scale=10))\n\n## 해석: 50 점 이하의 학생의 비율은  0.0968 = 9.68% 이다.\n\n0.09680048458561036\n\n\n예제 7의 ( 2 ) x 점 이상의 학생들에게 A 를 준다고 하면 x 는 다음을 만족해야 한다: 상위 10 % = 0.10\n먼저, 표준정규분포표로부터 P [Z ≥ z] = 0.10 이 되는 z 값을 찾는다: 이 경우, 0.10 에 가까운 값을 주는 z = 1.28 을 고르면 된다:\n\nfrom scipy.stats import norm\nprint(norm.ppf(q=0.9, loc=63, scale=10))\n\n## 해석: 상위 10 %의 학생에게  A 를 준다고 하면 \n## 75.8 점 이상의 점수를 받은 학생에게 주면 된다.\n\n75.815515655446"
  },
  {
    "objectID": "da/ida_08.html#이항분포의-정규분포근사",
    "href": "da/ida_08.html#이항분포의-정규분포근사",
    "title": "8장: 정규분포",
    "section": "4. 이항분포의 정규분포근사",
    "text": "4. 이항분포의 정규분포근사\n대규모 시행에서 이항 분포를 정규 분포로 근사하는 방법. 7장의 초기하 분포나, 포아송 분포 마찬가지로, 정규 분포로도 근사하여 계산할 수 있다.\n이항 분포는 n 이 커짐에 따라 근사적으로 정규분포를 따르게 된다. 이때 정규분포의 평균과 분산은 이항분포의에서와 일치하여야 한다:"
  },
  {
    "objectID": "da/ida_08.html#연속성수정",
    "href": "da/ida_08.html#연속성수정",
    "title": "8장: 정규분포",
    "section": "4-1. 연속성수정",
    "text": "4-1. 연속성수정\nContinuity Correction\n이항 분포의 이산적인 값을 연속적인 정규 분포의 구간에 맞추는 작업. 보통 ± ½ ​를 가감하여 근삿값을 계산한다.\n\\[\nX ~ Bin (n, p) ≈ Y \\~ N (μ, σ²)\n\\]\n확률변수 X 가 이항분포, 즉 X ~ Bin ( n, p ) 이고, np, nq ( =1 − p ) 가 모두 클 경우 ( 보통 10 이상 ) X 는 근사적으로 평균이 np, 표준편차가 √npq 인 정규분포를 따른다."
  },
  {
    "objectID": "da/ida_08.html#정규분포가정의-조사",
    "href": "da/ida_08.html#정규분포가정의-조사",
    "title": "8장: 정규분포",
    "section": "5. 정규분포가정의 조사",
    "text": "5. 정규분포가정의 조사\n모집단의 분포가 정규 분포를 따른다는 가정을 조사하기 위해 사용할 수 있는 효과적인 그림. 정규점수그림 ( Normal Scores Plot ) 또는 정규확률그림 ( Normal Probability Plot ) 으로 불린다.\n\n5-1. 정규점수\nNormal Scores\n표본 데이터와 표준 정규 분포 ( 평균 0, 표준편차 1 ) 를 비교하여 데이터의 정규성을 평가하는 데 사용된다. 정규성( Normality ) : 데이터가 정규 분포를 따르는지를 의미한다.\n즉, 표준정규분포의 확률밀도함수 ( PDF ) 를 등확률 구간으로 나누어 주는 경곗값 ( z 값 ) 을 의미한다. 그림으로부터 분포의 형태를 알기 위해선 자료의 크기가 적어도 15 이상은 되어야 한다.\n\n\n5-2. 정규확률그림 그리는 순서\n자료를 작은 것부터 크기순으로 나열한다. 각 자료에 해당하는 점수를 계산한다. i 번째 순서의 자료와 i 번째 순서의 정규점수를 하나의 쌍으로 2 차원 공간상에 나타낸다.\n\n\n5-3. 정규그림의 해석\n정규분포를 따른다면, 양쪽의 값은 서로 가까울 것이라고 예상할 수 있다.\n\n\n5-3. 정규확률그림을 이용한 정규성 판정\n직선식일 경우, 정규분포의 가정이 타당하다고 볼 수 있다. (곡선 등) 직선식을 벗어날 경우, 가정이 의문시된다고 할 수 있다.\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsm.qqplot(data1, line = \"s\")\n\n# \"s\"는 \"standardized\"를 의미한다. \n\n# 이 옵션은 플롯에 표준화된 선\n# (평균 0, 표준편차 1의 정규 분포)에 맞추어 선을 그린다.\n\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에 가까우므로 \n## 데이터가 정규분포를 따를 가능성이 높다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n\n5-4. 자료의 변환\n만약 추출된 표본이 정규확률그림 등에서 정규분포와 상당히 벗어난 것으로 판명되면, 자료의 변환을 통해 정규분포의 형태를 갖도록 시도해 볼 수 있다.\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\nimport matplotlib.pyplot as plt\n\nplt.hist(data2, bins=5, range=(0,50)) \nplt.xlabel('data2') \nplt.ylabel('Density') \nplt.title('Historam of data')\n\n## 해석: 아래 히스토그램을 보면 자료의 분포가 왼쪽으로 편중되어 있으므로\n## 정규분포가 아니라는 의심을 할 수 있다.\n\nText(0.5, 1.0, 'Historam of data')\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nsm.qqplot(data2, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에서 크게 벗어나\n## 데이터가 정규 분포를 따르지 않을 가능성이 있음을 확인할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n이 경우, 큰 자룟값을 더 작게 만드는 과정을 수행한다:\n\n# 원 자료를 제곱근(체적^(0.5))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata3 = np.sqrt(data2)\nsm.qqplot(data3, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n# 원 자료를 네제곱근(체적^(0.25))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata4 = np.power(data2, 0.25)\nsm.qqplot(data4, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida_06.html",
    "href": "da/ida_06.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "5장에서 다룬 표본공간의 근원사건들은 특성을 표현하는 형태로 다뤘다. 이제는 확률변수를 중심으로 실험의 수치적 결과에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_06.html#확률변수",
    "href": "da/ida_06.html#확률변수",
    "title": "6장: 확률분포",
    "section": "1. 확률변수",
    "text": "1. 확률변수\nRandom Variable\n각각의 근원사건들에 실수값을 “대응시키는 함수”이며 X, Y, … 등으로 표시한다.\n확률변수가 가질 수 있는 “값의 개수” 가 유한하거나 무한이라도 “셀 수 있는 경우” 에 이를 “이산확률변수” 라고 한다.\n또한, 연속적인 구간에 속하는 모든 값을 다 가질 수 있는 “연속확률변수” 도 있다."
  },
  {
    "objectID": "da/ida_06.html#이산확률분포",
    "href": "da/ida_06.html#이산확률분포",
    "title": "6장: 확률분포",
    "section": "2. 이산확률분포",
    "text": "2. 이산확률분포\nDiscrete Probability Distribution\n확률변수가 갖는 값들과 그에 “대응하는 확률값” 을 나타내는 것으로, 나열된 표나 수식으로 표현되며, 보통은 “확률변수 X 의 분포” 라고 한다.\n\n2-1. 확률질량함수\nProbability Mass Function, PMF\n확률변수 X가 k개의 값 x₁, x₂, …, xk를 가질 때, 그에 대응하는 확률을 f(x₁), f(x₂), …, f(xk)라고 하면 X의 확률분포는 다음과 같다:\nf(x)는 확률변수 X 가 값 x 를 갖게 되는 확률 P ( X = x ) 을 나타낸다:\n이산확률변수 X 의 확률변수는 다음 조건을 만족해야 한다:\n모든 확률은 0 이상 1 이하의 값을 가진다. 확률변수가 가질 수 있는 모든 값에 대한 확률의 합은 1 이다."
  },
  {
    "objectID": "da/ida_06.html#이산확률변수의-평균과-표준편차",
    "href": "da/ida_06.html#이산확률변수의-평균과-표준편차",
    "title": "6장: 확률분포",
    "section": "3. 이산확률변수의 평균과 표준편차",
    "text": "3. 이산확률변수의 평균과 표준편차\n\n3-1. 기댓값\nExpected Value\nE ( X ) 는 확률변수 X 의 “기댓값(평균)” 또는 X 가 갖는 확률분포의 “모평균” 이라고 한다.\n뮤( M , μ: 그리스 알파벳의 열두째 글자)\n3-2. 모분산 Population Variance\n편차 (즉, 각 값이 기대값에서 얼마나 떨어져 있는지)를 제곱하고, 그 제곱된 값을 각 값이 발생할 확률로 가중평균하는 것이다.\nV a r ( X ) = ∑ ( 편차 ) ² × 확률:\n시그마(Σ, σ: 그리스어 알파벳의 열여덟째 글자)\n모분산의 간편식:\n기댓값을 알고 있다면, 직접적인 정의를 사용하지 않고 게산할 수 있다.\n\n\n3-3. 모표준편차\nPopulation Standard Deviation 모분산의 양의 제곱근으로 계산된다:\n모표준편차 ( σ ) 의 단위는 확률변수 X와 “동일” 하다. 반면 모분산 ( σ² ) 의 단위는 X 의 단위를 “제곱” 한 것이므로, 퍼진정도를 측정하는데 적절하지 않다.\n예를 들어, X 의 단위가 센티미터( cm )라면, 모분산 σ² 의 단위는 제곱센티미터( cm² )가 된다."
  },
  {
    "objectID": "da/ida_06.html#두-확률분포의-결합분포",
    "href": "da/ida_06.html#두-확률분포의-결합분포",
    "title": "6장: 확률분포",
    "section": "4. 두 확률분포의 결합분포",
    "text": "4. 두 확률분포의 결합분포\n하나의 실험에서도 2 개 이상의 측면에 대한 관측이 가능하다. 이 경우 그 2 가지 특성 간의 관계 여부 및 그 관계 정도에 대해 분석할 수 있다.\n\n4-1. 결합확률분포\nJoint Probability Distribution\n2개 이상의 확률변수가 동시에 특정한 값을 가질 확률을 나타내는 분포이다.\n2 개의 확률변수가 이산일 경우,\nX 가 취하는 값을 x₁, …, xm Y 가 취하는 값을 y₁, …, yn 이라고 할 때\nX 와 Y 의 결합확률분포는 모든 1 ≤ i ≤ m, 1 ≤ j ≤ n 에 대하여\n위 식을 구하므로써 결정되며, 다음과 같이 표현할 수 있다:\n\n\n4-2. 주변확률분포\nMarginal Probability Distribution\n결합확률분포에서 한 확률변수를 “고정” 하고 다른 변수의 분포를 고려하는 분포이다. 이는 2 개 이상의 확률변수 중 하나에 대한 “단일 확률분포” 를 얻기 위해 사용한다:\n각각의 주변확률을 이용해서 하나의 변수 때와 마찬가지로 구하면 된다:"
  },
  {
    "objectID": "da/ida_06.html#공분산과-상관계수",
    "href": "da/ida_06.html#공분산과-상관계수",
    "title": "6장: 확률분포",
    "section": "5. 공분산과 상관계수",
    "text": "5. 공분산과 상관계수"
  },
  {
    "objectID": "da/ida_06.html#공분산",
    "href": "da/ida_06.html#공분산",
    "title": "6장: 확률분포",
    "section": "5-1. 공분산",
    "text": "5-1. 공분산\nCovariance\n두 확률변수 X 와 Y 가 함께 변하는 정도를 측정한다.\nX, Y가 같은 방향으로 변화할 경우, (즉, 둘 다 증가하거나 둘 다 감소하는 경우) ( X − μX ​), ( Y − μY ​) 의 부호가 일치할 확률이 상대적으로 커진다. 따라서, 이에 대한 기댓값은 양수가 된다.\nX, Y가 다른 방향으로 변화할 경우, (즉, 한 변수가 증가할 때 다른 변수는 감소하는 경우) ( X − μX ​), ( Y − μY ​) 가 서로 다른 부호를 갖게 될 확률이 상대적으로 커진다. 따라서 이에 대한 기댓값은 음수가 될 것이다.\nX, Y 의 공분산은 아래와 같이 정의된다:"
  },
  {
    "objectID": "da/ida_06.html#상관계수",
    "href": "da/ida_06.html#상관계수",
    "title": "6장: 확률분포",
    "section": "5-2. 상관계수",
    "text": "5-2. 상관계수\nCorrelation Coefficient\n두 확률변수 간의 선형 관계의 강도와 방향을 측정한다. 이것은 공분산을 표준화한 형태로, −1 과 1 사이의 값을 가진다.\n상관계수의 성질:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n정확한 선형관계 Y = aX + b 가 성립할 때, 상관계수의 값은 −1 또는 1 이다.\nX, Y 의 상관계수는 각 확률변수에 상수가 더해지거나 감해지는 것에 영향을 받지 않는다. 상수가 곱해진 경우, 그 부호에만 영향을 받는다.\n상수 c 와 d 의 부호가 다르면 상관계수의 부호가 반대가 된다."
  },
  {
    "objectID": "da/ida_06.html#두-확률변수의-독립성",
    "href": "da/ida_06.html#두-확률변수의-독립성",
    "title": "6장: 확률분포",
    "section": "6. 두 확률변수의 독립성",
    "text": "6. 두 확률변수의 독립성\n2 개의 확률변수 X, Y 가 독립이 되기 위해서는 X, Y 가 취하는 모든 쌍의 값 ( xi, yi ) 에 대해 아래 식을 만족해야 한다.\n두 확률변수 X, Y 가 서로 독립일 때, 아래의 식이 성립한다:\n단, 공분산과 상관계수가 0 이라는 사실이 항상 두 변수가 독립이라는 것을 보장하지 않는다. 이는 두 변수 간에 선형 관계가 없음을 의미하지만, 비선형 관계가 존재할 수 있다.\n두 확률변수가 독립일 경우, 공분산이 0 이 되므로 합과 차의 분산을 쉽게 계산할 수 있다.\n분산과 공분산의 정의를 이용하면: 공분산 항을 제외하여 다음과 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida_04.html",
    "href": "da/ida_04.html",
    "title": "4장: 두 변수 자료의 요약",
    "section": "",
    "text": "조사 대상의 각 개체로부터 둘 또는 그 이상의 변수들을 동시에 관측하는 경우가 더 많다. 두 변수에 관한 관측값을 도표로 요약하고 해석하는 방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida_04.html#자료의-입력",
    "href": "da/ida_04.html#자료의-입력",
    "title": "4장: 두 변수 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제5: 통계학과 신입생 51명의 키와 몸무게를 기록한 것이다.\n# 키와 몸무게의 표본상관계수를 구하고, 산점도를 그려라. (p.108)\n\nimport numpy as np\n\n# 키와 몸무게 자료의 입력\nheight = np.array([181,161,170,160,158,168,162,179,183,178,171,177,163,\n                   158,160,160,158,173,160,163,167,165,163,173,178,170,\n                   167,177,175,169,152,158,160,160,159,180,169,162,178,\n                   173,173,171,171,170,160,167,168,166,164,173,180]) \n\nweight = np.array([78,49,52,53,50,57,53,54,71,73,55,73,51,53,65,48,59,\n                   64,48,53,78,45,56,70,68,59,55,64,59,55,38,45,50,46,\n                   50,63,71,52,74,52,61,65,68,57,47,48,58,59,55,74,74])"
  },
  {
    "objectID": "da/ida_04.html#표본상관계수",
    "href": "da/ida_04.html#표본상관계수",
    "title": "4장: 두 변수 자료의 요약",
    "section": "2. 표본상관계수",
    "text": "2. 표본상관계수\nSample Correlation Coefficient\n산점도에서 점들이 얼마나 “직선에 가까운가” 의 정도를 나타내는 데 쓰이는 측도.\n두 변수 ( x, y ) 에 대하여 관측값 n개의 짝 ( x₁, y₁ ), ( x₂, y₂ ), …, (xn, yn) 이 주어진 때, 상관계수는 다음과 같이 계산된다:\n\n2-1. 공분산\nCovariance\n분산(Variance)은 한 변수의 데이터가 평균 주위에서 얼마나 흩어져 있는지를 나타낸다. 공분산은 “두 변수” 가 함께 변하는 정도를 나타낸다.\n공분산이 “양수” 이면, 두 변수는 “같은 방향” 으로 변한다. 공분산이 “음수” 이면, 두 변수는 “반대 방향” 으로 변한다. 공분산이 “0” 에 가까우면, 두 변수 간에 “선형 관계가 거의 없음” 을 의미한다.\n위 값들은 다음과 같은 수식을 통해 계산된다:\nx̄, ȳ 는 변수 x, y 의 표본 평균을 의미한다:\n표본상관계수는 두 변수의 직선관계의 정도(강도, 방향)를 나타내며, 다음과 같은 특징이 있다:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n계수가 “0” 에 가까울수록 두 변수 간의 직선의 관계가 “매우 약함” 을 의미한다. 표본상관계수의 단위는 없다.\n각 변수의 편차들의 곱을 사용해 계산할 때, 이 곱은 원래 변수들의 단위를 곱한 단위이지만, 분모에 각 변수의 분산을 곱한 값의 제곱근이 들어가기 때문에 단위가 사라지게 된다.\n위 연산은 단위를 제곱근 단위로 바꾸는 역할을 하며, 이로 인해 단위가 없는 숫자가 된다. 변수들의 단위에 영향을 받지 않아, 서로 다른 단위를 가진 변수들 간에도 관계를 비교할 수 있다.\n\n# 키와 몸무게의 표본상관계수\nnp.corrcoef(height, weight)[0][1]\n\n## 해석: 표본상관계수 𝑟이 약 0.74로 나왔다는 것은\n## 두 변수 사이에 강한 '양의 선형 관계'가 있음을 의미한다.\n\nnp.float64(0.7362765055636866)\n\n\n아래에서 설명할 산점도를 포함한 대부분의 그림 요약 방법은 “주관적” 일 수 있다. 반면, 표본상관계수는 “객관적” 인 수치 자료로, 이러한 문제를 “보완” 해준다.\n단, 직선이 아닌 다른 관계(곡선 등)가 있을 수 있으며, 이를 표본상관계수가 제대로 나타내지 못할 수 있다는 점을 유념해야 한다."
  },
  {
    "objectID": "da/ida_04.html#산점도",
    "href": "da/ida_04.html#산점도",
    "title": "4장: 두 변수 자료의 요약",
    "section": "3. 산점도",
    "text": "3. 산점도\nScatter Plot\n변수 x 를 “수평축” 에 놓고, 변수 y 를 “수직축” 에 놓은 후에 각 관측값의 찍을 좌표 위에 표시함 으로써 얻게 되는 그림.\n이를 통해, “두 변수 간의 관계” 를 시각적으로 대략 파악할 수 있다.\n\nimport matplotlib.pyplot as plt\n\n# 산점도 작성\nplt.figure(figsize=(8, 6))  # 그래프의 크기 (가로 8, 세로 6)\nplt.scatter(height, weight, color='slateblue')  # 산점도 색상 설정\n\nplt.xlabel('height (cm)')  # x축 레이블\nplt.ylabel('weight (kg)')  # y축 레이블\nplt.title('height(cm) and weight(kg)')  # 그래프의 제목\n\nplt.grid(True)  # 그리드 추가 (경계선)\nplt.show()"
  },
  {
    "objectID": "da/ida_03.html",
    "href": "da/ida_03.html",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "",
    "text": "연속형 자료가 어떤 값을 중심으로 분포되어 있는가를 나타내는 중심위치의 측도, 각 자료가 중심위치의 값으로부터 흩어진 정도를 나타내는 퍼진 정도의 측도 등을 다루고자 한다."
  },
  {
    "objectID": "da/ida_03.html#자료의-입력",
    "href": "da/ida_03.html#자료의-입력",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제13: 정량 100인 음료수 80병을 임의로 추출하여 그 내용물의 실제 측정된 양을 잰 자료이다.(p.42)\n\nimport numpy as np\n\n# 변수 drink에 NumPy 배열을 할당\ndrink = np.array([98, 99, 100, 99, 99.4, 101.7, 98.8, 101.8, 101.5, \n                 101.8, 102.6, 101, 98.8, 101.4, 99.7, 99.7, 99.7, \n                 100.9, 98.6, 101.4, 102.1, 102.9, 100.8, 101.8, \n                 100, 101.2, 100.5, 101.2, 100.1, 101.6, 101.3, 99.9, \n                 99.4, 99.3, 99.4,101.6, 96.1, 100, 99.7, 99.1, 100.7, \n                 100.8, 100.8, 95.5,100.1, 100.5, 98.9, 99.9, 96.8, \n                 102.4, 100, 103.7, 101.4,99.7, 97.4, 99.5, 97.5, \n                 99.9, 100.3, 100.2, 101.5, 99.4, 99.7, 98.2, 100.3, \n                 100.2, 100.5, 100.4, 101.5, 98.4, 101.4, 98.8, 100.9, \n                 101.1, 100.9, 98.1, 98.7, 99.2, 98.1, 97.2])\n\n중심위치의 측도"
  },
  {
    "objectID": "da/ida_03.html#평균-mean",
    "href": "da/ida_03.html#평균-mean",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "2. 평균 (Mean)",
    "text": "2. 평균 (Mean)\n총 자료의 개수 \\(n\\) 을 모든 관측값 \\(x_1, x_2, \\dots , x_n\\) 의 합으로 나눈 값. 이를 “산술 평균” 이라고도 하며, 공식으로 표현하면 다음과 같다:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n표본 평균은 관측값의 산술평균이며, “극단값” 에 영향을 받는다.\n\n표본 평균: 전체 데이터인 “모집단” 에서 추출한 일부 데이터의 평균을 의미한다. (예시1) 예제13 에서 추출된 80 병의 음료수는 표본에 해당된다.\n극단값(Outlier): 데이터 집합에서 다른 값들과 현저히 다른 값들을 의미한다. (예시2) 데이터 집합 { 11, 12, 13, 300 } 에서 극단값은 300 이다.\n\n\n# 2장의 예제 13에서 주어진 음료수 한 병의 부피 데이터를 기반으로 \n# 평균, 중앙값, 분산, 표준편차, 범위, 사분위수범위를 파이썬을 이용하여 계산하라.(p.83)\n\n## numpy 모듈을 이용하여 계산할 수 있음. ##\n\n# 평균\nprint(np.mean(drink))\n\n100.04125"
  },
  {
    "objectID": "da/ida_03.html#중앙값median",
    "href": "da/ida_03.html#중앙값median",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "3. 중앙값(Median)",
    "text": "3. 중앙값(Median)\n전체 관측값을 크기 순서로 배열했을 때, 가운데에 위치한 값.\n데이터의 개수가 홀수 일 경우, 중앙에 위치한 값이, 짝수 일 경우, 중앙에 위치한 두 값의 평균 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, 극단값의 영향을 받지 않는다. 따라서, 평균과 값이 다를 수 있다.\n\n# 중앙값 계산\nprint(np.median(drink))\n\n100.05\n\n\n퍼진 정도의 측도"
  },
  {
    "objectID": "da/ida_03.html#분-산-variance",
    "href": "da/ida_03.html#분-산-variance",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "4. 분 산 (Variance)",
    "text": "4. 분 산 (Variance)\n확률 분포나 데이터 집합의 산포도(분포도)를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 지표로 사용된다.\n관측값이 \\(x_1, x_2, \\dots , x_n\\) 이고, 표본평균이 \\(\\bar{x}\\) 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평균제곱 오차의 평균: 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n\n4-1. 편차(Deviation):\n각 관측값과 평균의 차이\n편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로,  따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n\n\n4-2. 자유도\nDegrees of Freedom\n위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "da/ida_03.html#표준편차",
    "href": "da/ida_03.html#표준편차",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "5. 표준편차",
    "text": "5. 표준편차\nStandard Deviation\n분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다.\n\n# 표준편차 계산 (표본의 표준편차, 자유도를 1로 설정)\nprint(np.std(drink, ddof=1))\n\n# 표준편차 계산 (모집단의 표준편차)\nprint(np.std(drink))\n\n1.521882058505192\n1.5123403841397614\n\n\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다."
  },
  {
    "objectID": "da/ida_03.html#범위",
    "href": "da/ida_03.html#범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "6. 범위",
    "text": "6. 범위\nRange\n관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다.\n\n# 범위 계산 (최대값 - 최소값)\nprint(np.max(drink) - np.min(drink))\n\n8.200000000000003"
  },
  {
    "objectID": "da/ida_03.html#사분위수범위",
    "href": "da/ida_03.html#사분위수범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "7. 사분위수범위",
    "text": "7. 사분위수범위\nQuartile\n전체 관측값을 작은 순서로 배열 하였을 때, 전체를 사등분 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n\n7-1. 사분위수범위\nInterquartile Range, IQR\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 std 는 자동적으로 n – 1 로 나누어져서 계산된다."
  },
  {
    "objectID": "da/ida_05.html",
    "href": "da/ida_05.html",
    "title": "5장: 확률",
    "section": "",
    "text": "통계적인 추론을 통해서도 모집단에 대한 다양한 정보를 얻을 수 있다. 그 통계적 추론의 기초가 되는 확률이론에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_05.html#사건의-확률",
    "href": "da/ida_05.html#사건의-확률",
    "title": "5장: 확률",
    "section": "1. 사건의 확률",
    "text": "1. 사건의 확률\n동일 조건하에서 한 가지 실험을 반복할 때, 전체 실험 횟수에서 그 사건이 일어나리라고 예상되는 횟수의 비율을 말한다. \n사건을 \\(A\\) 라고 하면, 사건 \\(A\\) 의 확률은 \\(P(A)\\) 로 표시한다.\n\n1-1. 표본공간\nSample Space: Ω\n한 실험에서 나올 수 있는 모든 결과들의 모임. 유한표본공간 ( Finite Sample Space )\n주사위 던지기의 표본공간\n연속표본공간 ( Continuous Sample Space )\n0과 1 사이의 모든 실수"
  },
  {
    "objectID": "da/ida_05.html#근원사건",
    "href": "da/ida_05.html#근원사건",
    "title": "5장: 확률",
    "section": "1-2. 근원사건",
    "text": "1-2. 근원사건\nElementary Outcomes: ω ₁ , ω ₂ … \n표본공간을 구성하는 개개의 결과.\n주사위 던지기의 근원사건\n\n1-3. 사건\nEvent: A, B, …\n표본공간의 부분집합으로, 어떤 특성을 갖는 결과들의 모임 (=근원사건들의 집합)\n주사위 던지기의 사건 A, B"
  },
  {
    "objectID": "da/ida_05.html#확률의-법칙",
    "href": "da/ida_05.html#확률의-법칙",
    "title": "5장: 확률",
    "section": "2. 확률의 법칙",
    "text": "2. 확률의 법칙\n위 정의로부터 확률의 특성을 유추할 수 있다.\n사건 A 가 일어날 확률은, 사건 A 에 속하는 근원사건이 일어날 확률의 “합” 과 같다. Ω 를 하나의 사건이라고 하면, 이 사건은 “반드시” 일어나므로 확률은 “1” 이 되어야 한다."
  },
  {
    "objectID": "da/ida_05.html#확률의-계산",
    "href": "da/ida_05.html#확률의-계산",
    "title": "5장: 확률",
    "section": "3. 확률의 계산",
    "text": "3. 확률의 계산\n\n3-1. 균일 확률\n주사위 던지기에서, 각 숫자가 나올 확률 Ω 가 k 개의 원소로 이루어져 있고, 각 근원사건이 일어날 가능성이 “동일” 하다고 가정할 때, 근원사건 중 하나가 일어날 확률은 1 / k 로 주어진다.\n또 사건 A 가 m 개의 근원사건으로 이루어져 있다면, 사건 A 가 일어날 확률은 위와 같다.\n\n\n3-2. 상대도수 수렴치로서의 확률\n주사위를 60번 던져 10번 2가 나왔을 때, 2가 나올 확률 동일한 실험 N 회를 반복할 때, 사건 A 의 상대도수는 위와 같이 표현된다.\nN 이 증가함에 따라 상대도수가 “일정한 값으로 수렴” 한다면, 그 값으로 사건 A 가 일어날 확률 P ( A ) 를 추정한다."
  },
  {
    "objectID": "da/ida_05.html#확률-법칙",
    "href": "da/ida_05.html#확률-법칙",
    "title": "5장: 확률",
    "section": "4. 확률 법칙",
    "text": "4. 확률 법칙\n여 사 건 ( Complementary Event ): 사건 A 가 일어나지 않는 사건 합 사 건 ( Sum Event ): 사건 A 또는 B 가 일어나는 사건 곱 사 건 ( Product Event ): 사건 A 또는 B 가 동시에 일어나는 사건 배 반 사 건 ( Exclusive Event ): 두 사건이 동시에 일어날 수 없는 경우"
  },
  {
    "objectID": "da/ida_05.html#조건부-확률",
    "href": "da/ida_05.html#조건부-확률",
    "title": "5장: 확률",
    "section": "5. 조건부 확률",
    "text": "5. 조건부 확률\nConditional Probability\n한 사건의 결과가 다른 사건의 발생에 영향을 미치는 확률.\n사건 B가 발생했을 때, 사건 A가 발생할 확률:\n곱 사 건 의 확 률 법 칙 ( Multiplication Rule for Probability ) 두 사건이 동시에 발생할 확률을 계산하는 방법. 이 법칙은 두 사건이 독립적인지 여부에 따라 다르게 적용된다:\n\n5-1. 표본공간의 분할\nPartition\n사건 A₁, A₂, …, An 이 서로 배반사건이고, Ω = A₁ ∪ … ∪ An 일 때, 사건 A₁, A₂, …, An 을 Ω 의 분할이라고 한다:\n이때, 각각의 부분집합은 서로 겹치지 않는다:\n예시: 주사위를 던지는 실험에서, Ω 를 두 개의 부분집합으로 나눌 수 있다.\nA₁ ​: 홀수가 나오는 경우 { 1, 3, 5 } A₂ ​: 짝수가 나오는 경우 { 2, 4, 6 }\n이 경우, Ω 는 다음과 같이 표현된다:\n\n\n5-2. 총확률의 법칙\nLaw of Total Probability\n표본공간의 분할 개념을 사용하여 전체 확률을 계산하는 방법.\n사건 A₁, A₂, …, An 이 표본공간의 분할일 때, 임의의 사건 B 의 확률 “P(B)” 는 다음과 같이 계산할 수 있다:\n이를 조건부 확률 ( 종속사건 ) 을 사용하여 다시 쓰면:\n예시: 사건 B 를 “주사위 눈이 4 이하인 사건” 으로 정의하면,\nP(B) = P({ 1, 2, 3, 4 }) = ⅔ P(A₁)​ = P({ 1, 3, 5 }) = ½ P(A₂)​ = P({ 2, 4, 6 }) = ½\n이 경우, P ( B ) 는 다음과 같이 표현된다:\n각 확률 값을 대입해보면:\n\n\n5-3. 베이즈 정리\nBayes’ rule\n사건 A₁, A₂, …, An 이 Ω 의 분할일 때, 임의의 사건 B 에 대하여 다음 식이 성립한다:\n예시: 사건 B 가 발생한 후, 사건 A₁ ​ 이 발생할 확률:"
  },
  {
    "objectID": "da/ida_07.html",
    "href": "da/ida_07.html",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "",
    "text": "모집단의 구성원들이 두 그룹으로 나누어져 있는 경우의 표본추출에서 광범위하게 쓰이는 확률모형과 그의 특징 및 관련된 다른 확률모형들을 다루고자 한다."
  },
  {
    "objectID": "da/ida_07.html#자료의-입력",
    "href": "da/ida_07.html#자료의-입력",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7 :  어떤 초등학교에서  10 년간 조사결과\n# 평균적으로  4 % 의 학생이 색맹인 것으로 나타났다고 한다. (p.213)\n\n# 올해에도 색맹인 학생의 비율이 예년과 같다고 할 때,\n# 임의로 추출된  200 명의 학생 중 색맹인 학생이  10 명 이하일 확률은 얼마인가?"
  },
  {
    "objectID": "da/ida_07.html#베르누이-시행",
    "href": "da/ida_07.html#베르누이-시행",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "2. 베르누이 시행",
    "text": "2. 베르누이 시행\nBernoulli Distribution\n모집단의 각 구성원이 두 그룹 중 하나에 속하는 경우, 각각의 구성원이 특정 그룹에 속할 확률 p 와 속하지 않을 확률 1 − p 를 따르는 이산 확률 분포.\n시 행 ( Trial ) : 매번 반복되는 추출( 실험 )\n2 개의 가능한 결과 중, 하나는 성공 ( Success, S ), 다른 하나는 실패 ( Failure, F ) 로 이름을 붙인다.\n이는 시행의 결과가 2 개 뿐임을 강조하며, 보통 우리가 관심이 있는 결과에 성공이란 이름을 붙인다.\n각 시행은 독립으로, 각 시행의 결과가 다른 시행의 결과에 영향을 미치지 않는다.\n일반적으로 복원 추출 ( Sampling With Replacement )로 간주된다. 각 시행 후 다시 원래의 상태로 복귀하여, 다음 시행에 영향을 주지 않는다.\n예제 7에 경우, 추출되는 학생은 색맹 ( S ) 또는 정상 ( F )으로 나눌 수 있다."
  },
  {
    "objectID": "da/ida_07.html#이항분포",
    "href": "da/ida_07.html#이항분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "3. 이항분포",
    "text": "3. 이항분포\nBinomial Distribution\n각 시도가 성공 또는 실패 두 가지 결과 중 하나를 가지는 독립적인 시행이 n 번 반복될 때, 성공 횟수를 나타내는 확률분포.\n성공할 확률이 p 인 베르누이 시행을 n 번 반복할 때 일어나는 성공의 횟수가 X 라면, 이 확률변수 X 는 모수가 ( n, p )인 이항분포를 따른다.\n모수(Parameter): 우리가 관심을 가지는 수치\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, x = 0, 1, …, n에 대하여 확률질량함수( PMF ) 는 다음과 같다:\n이 항 계 수 ( Binomial Coefficient )\n주어진 수의 집합에서 특정한 수의 원소를 선택하는 방법의 수. 조 합 ( Combination ) 으로도 알려져 있다.\n예제 7에서 주어진 정보를 바탕으로, 아래와 같이 이항 분포의 모수 ( 파라미터 ) 를 설정할 수 있다:\n성공 확률: p = 0.04 ( 학생이 색맹일 확률 ) 시도 횟수: n = 200 ( 추출된 학생 수 ) 성공 횟수: k ≤ 10 ( 색맹인 학생 수 )\n즉, 확률변수 X 를 200 명 중 색맹인 학생의 수 라고 하면, X는 모수가 ( n, p ) = ( 200, 0.04 ) 인 이항분포를 따르게 된다.\n이때, k ≤ 10 인 경우의 확률을 구해야 하므로, 누적 분포 함수 ( CDF ) 를 사용해야 한다:\n원하는 확률을 계산하려면, 다음 식을 계산해야 한다:\n다만, 위 식을 직접 계산하기에는 번거로운 면이 있다. 부록의 이항분포표에서도 n = 25 까지가 최대이기 때문이다.\n이럴 경우, 아래에서 설명할 포아송분포로 근사하여 계산할 수 있다.\n\nfrom scipy import stats\nstats.binom.cdf(10, 200, 0.04)\n\n## 출력된 값 &gt; 0.8199789826230907\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n\nnp.float64(0.8199789826230907)\n\n\n\n3-1. 이항분포의 기댓값과 표준편차\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:"
  },
  {
    "objectID": "da/ida_07.html#초기하분포",
    "href": "da/ida_07.html#초기하분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "4. 초기하분포",
    "text": "4. 초기하분포\nHypergeometric Distribution\n유한한 모집단에서 비복원 추출 ( Sampling Without Replacement )을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인(FPC)이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다. 결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다."
  },
  {
    "objectID": "da/ida_07.html#포아송분포",
    "href": "da/ida_07.html#포아송분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "5. 포아송분포",
    "text": "5. 포아송분포\nPoisson Distribution\n특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포. 주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n\n119 구조대에 시간당 걸려오는 전화횟수\n국내 발생하는 진도 4 이상 지진의 횟수\n\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\n\nfrom scipy import stats\nstats.poisson.cdf(10, 8)\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\nnp.float64(0.8158857925585467)\n\n\n\n참고용: Finite Population Correction Factor FPC"
  },
  {
    "objectID": "da/ida_09.html",
    "href": "da/ida_09.html",
    "title": "9장: 표집분포",
    "section": "",
    "text": "주어진 표본을 통해 모집단의 성격을 알아내는 과정을 추론 ( Inference )이라 한다. 그 통계적 추론에서 모집단의 특성을 추정하거나 가설 검정을 수행할 때, 사용되는 표집분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_09.html#자료의-입력",
    "href": "da/ida_09.html#자료의-입력",
    "title": "9장: 표집분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 4:  0 ~ 9 까지  10 개의 정수값을 균등하게 갖는 모집단이 있다.\n# (위 모집단의 분포를 이산균등분포라고 한다.) \n\n# 예시: 전화번호 끝자리 번호의 분포\n\n# (1) 여기에서 크기가  5 인 표본을  100 번 뽑아서 \n# (2) 매번 추출된 표본에서 표본평균을 구하고, \n# (3) 그 평균들을 가지고 히스토그램을 그려라. (p.268)"
  },
  {
    "objectID": "da/ida_09.html#통계량",
    "href": "da/ida_09.html#통계량",
    "title": "9장: 표집분포",
    "section": "2. 통계량",
    "text": "2. 통계량\nStatistic\n표본의 관측값들에 의하여 결정되는 양. 표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데, 이때 유념하여야 할 3가지 조건이 있다.\n\n표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n통계량의 값은 추출된 표본의 영향을 받는다:\n다른 표본을 추출할 때마다 통계량의 값은 변한다:"
  },
  {
    "objectID": "da/ida_09.html#표집분포",
    "href": "da/ida_09.html#표집분포",
    "title": "9장: 표집분포",
    "section": "3. 표집분포",
    "text": "3. 표집분포\nSampling Distribution\n통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n\n3-1. 불편추정량\nUnbiased Estimator 분포의 평균값이 추정하려는 모수와 일치하는 추정량. 불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n\n\n3-2. 편의추정량\nBiased Estimator\n분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다. 이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다. 따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n\n\n3-3. 임의표본\nRandom Sample 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn. 위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n\n\n3-4. 표본평균\nSample Mean\n모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:"
  },
  {
    "objectID": "da/ida_09.html#중심극한정리",
    "href": "da/ida_09.html#중심극한정리",
    "title": "9장: 표집분포",
    "section": "4. 중심극한정리",
    "text": "4. 중심극한정리\nCentral Limit Theorem, CLT 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이 표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\n\nimport numpy as np \n\na = np.random.randint(0, 100, size=5) \nb = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nc = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nd = np.random.randint(0, 100, size=5) \n\nprint(\"a :\", a) \nprint(\"b :\", b) \nprint(\"c :\", c)\nprint(\"d :\", d)\n\n## 해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n\na : [57  4 57 42 14]\nb : [79 71  3 65 21]\nc : [37 12 72  9 75]\nd : [37 12 72  9 75]\n\n\n( 2 ) 표본평균을 출력한다.\n\nimport numpy as np \n\nm = [] \n\nnp.random.seed(1234) \nfor i in range(100): \n    sample = np.random.randint(0, 10, size = 5) \n    m.append(np.mean(sample))\n    \nm = np.array(m)\nprint(m)\n\n[5.2 6.4 4.4 3.  5.  1.8 3.  3.4 5.4 7.8 3.4 4.8 4.8 4.8 6.4 4.8 4.4 6.\n 2.8 4.8 4.2 4.4 6.8 1.8 6.  4.6 3.2 2.4 2.8 6.2 3.2 6.8 5.8 5.8 4.4 5.\n 4.4 6.  3.6 4.8 4.8 4.  4.4 5.6 5.2 6.2 1.8 3.8 1.4 6.4 3.8 5.2 4.4 4.4\n 4.6 0.4 3.  5.8 2.2 4.  4.4 3.6 5.2 1.6 3.2 5.8 6.8 3.8 6.2 2.6 2.  4.8\n 2.6 6.  7.6 5.6 6.  3.6 4.2 3.8 5.2 3.4 8.2 4.6 6.8 3.8 4.2 3.8 4.6 2.4\n 4.  4.4 6.  4.4 2.6 3.4 6.  4.6 4.6 2.8]\n\n\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\n\nimport matplotlib.pyplot as plt \n\nplt.hist(m, bins=7)\nplt.xlabel('m') \nplt.ylabel('Frquency') \nplt.title('Historam of m')\n\n## 해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해\n## 정규분포에 가까우리라 예상할 수 있다.\n\nText(0.5, 1.0, 'Historam of m')\n\n\n\n\n\n\n\n\n\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8장에서 배운 정규확률그림을 그려본다.\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nsm.qqplot(m, line='s')\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 점들이 거의 직선상의 있으므로\n## 어느 정도 정규분포를 따른다고 할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida_11.html",
    "href": "da/ida_11.html",
    "title": "11장: 정규모집단에서의 추론",
    "section": "",
    "text": "표본의 크기가 작을 경우에는 일반적인 통계적 추론 방법을 적용하기 어려울 수 있다. 이 경우, 정규분포 대신 t – 분포를 이용한 통계적 추론 방법에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_11.html#자료의-입력",
    "href": "da/ida_11.html#자료의-입력",
    "title": "11장: 정규모집단에서의 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 9: 예제 4에 주어져 있는 자료를 가지고 파이썬을 이용하여 \n# μ 에 대한 90% 신뢰구간을 구하고 예제 4에서 실시한 검정도 \n# 파이썬을 이용하여 다시 시행한 후 그 결과를 비교하라. (p.346)\n\n# ------------------------------------------------------------------------------\n\n# 예제 4: 어느 도시의 보건복지과에서는 \n# 그 도시의 상수원인 어느 호수의 수질에 관심이 있다고 한다. \n\n# 수질을 나타내는 하나의 수치로 단위부피당 평균 세균수가 있는데, \n# 그 수가 200 이상이면 상수원으로 적합 하지 않다고 한다. \n\n# 호수의 열 군데에서 물을 떠서 조사한 결과 단위부피당 세균수가 다음과 같이 나타났다. \n# 이 자료로부터 호수의 단위부피당 평균 세균수(μ)가 200보다 적다고 주장할 수 있겠는가? (p.330)\n\nimport numpy as np \nbacteria = np.array([175, 190, 215, 198, 184, 207, 210, 193, 196, 180])\n\n#_______________________________________________________________________________\n\n# 예제 10: 다음에 주어진 자료를 파이썬을 이용하여 분석하고자 한다. (p.348)\n\nx = np.array([31, 35, 37, 38, 38, 38, 39, 40, 40, 41, 42, 43, 44, 44, 46, 48])"
  },
  {
    "objectID": "da/ida_11.html#t-분포",
    "href": "da/ida_11.html#t-분포",
    "title": "11장: 정규모집단에서의 추론",
    "section": "2. t 분포",
    "text": "2. t 분포\nStudent’s t Distribution 통계학에서 모집단의 표본 평균이 정규분포를 따르지 않는 경우에도 사용가능한 분포. 모집단의 분포가 N ( μ, σ2 ) 일 때 크기가 n 인 표본의 평균 x̄ 의 분포는 정확하게 N ( μ, σ2 / n ) 이다.\n이를 표준화한 것이 아래와 같다:\n일반적으로 σ 는 미지수이므로 이를 표본의 표준편차 s 로 추정하여 사용한다. 표본의 크기가 큰 경우, s 로 대체하여도 그 분포가 큰 영향을 받지 않는다.\n그러나 표본의 크기가 작은 경우, 대체하게 되면 표준화된 확률변수의 분포는 표준정규분포와 달라지게 되며, 이를 t 분포 라고 한다.\n정규모집단 N ( μ, σ2 ) 으로부터 임의추출된 표본을 X 1, …, X n 이라고 할 때, 표본 평균과 표본 분산을 아래와 같이 정의한다:\n위 정의가 성립할 때 아래 식이 성립한다:\n자유도가 (n – 1)인 t 분포를 따르고, 이를 기호로써 t (n – 1)로 표현한다.\n표준정규분포와의\n공통점 : 0 을 중심으로 대칭 & 종모양 분포 차이점 : 양 꼬리부분에 상대적으로 많은 확률이 존재 → 더 두꺼운 꼬리를 갖는다. 이때 자유도가 증가하면, t 분포의 꼬리는 표준정규분포의 꼬리에 가까워진다:\ndf = 1 : 자유도가 1인 경우, df = 5 : 자유도가 5인 경우"
  },
  {
    "objectID": "da/ida_11.html#모평균에-대한-추론",
    "href": "da/ida_11.html#모평균에-대한-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3. 모평균에 대한 추론",
    "text": "3. 모평균에 대한 추론\n표본의 크기가 작거나 모집단이 정규분포를 따르지 않는 경우, t – 분포를 사용한다. 이때는 Z α / 2​ 대신 t α / 2 (n − 1) ​을 사용하여 신뢰구간을 계산해야 한다:\n위 식을 μ 에 대해 정리하면:\n10장: 통계적 추론 → 3 - 5 → (2)번의 식은 위와 같이 수정되어야 한다.\n표본 크기가 작을수록\nt – 분포의 임계값이 커지므로, 신뢰구간이 넓어져 표본표준편차의 불확실성을 반영한다. t – 분포를 사용하여 신뢰구간을 더 넓게 잡으므로, 정확한 추정에 도움이 된다. 표본 크기가 충분히 크다면, t – 분포와 정규분포가 거의 같아지므로 이 경우에만, Z – 분포를 사용할 수 있다.\n따라서:\n또는 아래와 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida_11.html#가설-검정",
    "href": "da/ida_11.html#가설-검정",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3-1. 가설 검정",
    "text": "3-1. 가설 검정\n검정통계량은 H 0 가 맞을 때 자유도가 ( n − 1 ) 인 t – 분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n검정통계량이 t 분포를 따르는 경우의 검정을 t 검정이라고 한다."
  },
  {
    "objectID": "da/ida_11.html#신뢰구간과-양측검정의-관계",
    "href": "da/ida_11.html#신뢰구간과-양측검정의-관계",
    "title": "11장: 정규모집단에서의 추론",
    "section": "4. 신뢰구간과 양측검정의 관계",
    "text": "4. 신뢰구간과 양측검정의 관계\nμ 에 대한 100 ( 1 − α ) % 신뢰구간은 아래와 같다:\nH 0 : μ = μ 0 에 대한 양측검정에서의 기각역은 유의수준이 α 일 때 아래와 같다:\n위 기각역의 여집합인 H 0 를 기각하지 못하고 받아들이는 영역을 ’ 채택영역 ’ 이라고 할 때\n이 채택영역은 다음과 같다:\n이를 μ 0 를 중심으로 풀어쓰면 다음과 같다:\n위 과정을 바탕으로 다음과 같은 결론을 내릴 수 있다:\n모수 θ 에 대한 100 ( 1 − α ) % 신뢰구간이 ( L, U ) 로 구해졌을 때, 가설 H 0 : θ = θ 0 대 H 1 : θ ≠ θ 0 에 대하여 유의수준 α 로 검정을 시행할 때의 결론을 의미한다."
  },
  {
    "objectID": "da/ida_11.html#모표준편차의-추론",
    "href": "da/ida_11.html#모표준편차의-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "5. 모표준편차의 추론",
    "text": "5. 모표준편차의 추론\n모표준편차의 추정과 검정에서는 정규성 가정이 중요한 역할을 한다. 이 가정이 충족되지 않으면, 신뢰구간 계산이나 가설 검정의 결과를 신뢰할 수 없다.\n모표준편차 σ 를 추정하는 과정은 모분산 σ 2 에 대한 추정에서 출발하며, 이 모분산 σ 2 에 대한 추정에서 사용되는 것이 표본분산이다:\n\n5-1. 점추정\nPoint Estimation 모집단의 모수를 단일 값으로 추정하는 방법이다. 모표준편차 σ 의 경우, 점추정은 표본표준편차를 사용하는 방식이다:\n\n\n5-2. 구간추정\nInterval Estimation 모집단의 모수를 특정 신뢰수준에서 포함할 것으로 예상되는 구간을 제공하는 방법이다. 모표준편차 σ 에 대한 구간추정은 모분산에 대한 신뢰구간을 기반으로 하여 계산되며, 이때 모분산 s 2 의 신뢰구간을 구하기 위해 카이제곱 분포를 사용한다.\n\n\n5-3. 카이제곱 분포\nChi-Square Distribution 표본 분산을 모집단 분산과 비교하거나 범주형 변수 간의 독립성을 검정할 때 유용한 분포.\n자유도에 따라 그 형태가 달라지며, 자유도가 커질수록 정규 분포와 유사해진다. 아래 수식은 표본 분산을 모집단 분산으로 표준화한 것이다:\n자유도가 (n – 1)인 x2 분포를 따르고, 이를 기호로써 x2 (n – 1)로 표현한다. 모집단 σ 2 의 신뢰구간을 계산하기 위해 카이제곱 통계량을 사용한다: 위 분포로부터 구한 신뢰구간은 아래 식과 같다:\n위 식에서 괄호 안에 있는 부등식을 σ 2 을 중심으로 풀어 쓰면 다음과 같다:\n따라서 이 식으로부터 σ 2 의 100 ( 1 − α ) % 신뢰구간을 구하면 다음과 같다:\n표준편차 σ 는 σ 2 의 양의 제곱근이므로,그의 신뢰구간은 σ 2 의 신뢰구간의 경곗값의 제곱근을 취하여 얻을 수 있다: 결론적으로, σ 에 대한 100 ( 1 − α ) % 신뢰구간은 위와 같다.\n검정통계량은 H 0 이 맞을 때 자유도 df 인 카이제곱분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 9의 ( 1 ) bacteria 에 대한 요약 통계량 계산하기.\n\nxbar_b = np.mean(bacteria);print(xbar_b) # 평균\n\nvar_b = np.var(bacteria, ddof=1);print(var_b) # 분산 (자유도 1 사용)\n\nsd_b = np.std(bacteria, ddof=1);print(sd_b) # 표준편차 (자유도 1 사용)\n\nmedian_b = np.median(bacteria);print(median_b) # 중앙값\n\n194.8\n172.62222222222226\n13.138577633146681\n194.5\n\n\n\nmin_b = np.min(bacteria);print(min_b) # 최솟값\n\nmax_b = np.max(bacteria);print(max_b) # 최댓값\n\nsum_b = np.sum(bacteria);print(sum_b) # 합계\n\nn = bacteria.size;print(n) # 데이터 개수\n\n175\n215\n1948\n10\n\n\n예제 9의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수 t – 분포의 백분위수 함수\n\nfrom scipy import stats \n\nse_b = stats.sem(bacteria); print(se_b) # 표본표준오차\n\n# 유의수준 0.1에 해당하는 t-분포의 임계값 \nt_alpha = stats.t.ppf(1 - 0.1 / 2, n - 1); print(t_alpha)\n\ninterval = t_alpha * se_b;print(interval) # 신뢰구간의 범위를 계산\n\nCI = [xbar_b - interval, xbar_b + interval]; print(CI) # 신뢰구간\n\n4.154783053568769\n1.8331129326536335\n7.6161865478670645\n[np.float64(187.18381345213294), np.float64(202.41618654786708)]\n\n\n90 % 신뢰구간은 194.8 ± 7.616 임을 알 수 있다.\n예제 9의 ( 3 ) 검정하고자 하는 가설은 H 0 : μ = 200 대 H 1 : μ &lt; 200 이며, 표본의 크기는 10 이다.\n\ntval = (xbar_b - 200) / se_b;print(tval) \n\n# 단측검정: 귀무가설 μ=200, 대립가설 μ&lt;200\npval = stats.t.cdf(tval, n - 1);print(pval)\n\n## 해석: P–값이 0.1211 로 유의수준 5% 에서 귀무가설을 기각할 수 없으므로\n## 주어진 10 개의 자료로부터 호수의 단위 부피당 평균세균수가 200 보다 \n## 적다고 안심할 수 없다.\n\n-1.2515695604210733\n0.12113884687382763\n\n\n예제 10의 ( 1 ) x 에 대한 요약 통계량 계산하기.\n\nxbar_x = np.mean(x);print(xbar_x) # 평균\n\nvar_x = np.var(x, ddof=1);print(var_x) # 분산 (자유도 1 사용)\n\nsd_x = np.std(x, ddof=1);print(sd_x) # 표준편차 (자유도 1 사용)\n\nmedian_x = np.median(x);print(median_x) # 중앙값\n\nmin_x = np.min(x);print(min_x) # 최솟값\n\nmax_x = np.max(x);print(max_x) # 최댓값\n\nsum_x = np.sum(x);print(sum_x) # 합계\n\nn = x.size;print(n) # 데이터 개수\n\n40.25\n18.2\n4.266145801540309\n40.0\n31\n48\n644\n16\n\n\n예제 10의 ( 2 ) x 에 대한 정규확률그림 그리기.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 정규확률그림 그리기\nsm.qqplot(x, line='s')\nplt.title(\"Normal Q-Q Plot\")\n\nText(0.5, 1.0, 'Normal Q-Q Plot')\n\n\n\n\n\n\n\n\n\n예제 10의 ( 3 ) 모평균 μ 에 대한 95% 신뢰구간을 구한다.\n\nfrom scipy import stats \n\nse = stats.sem(x);print(se) # 표준편차\n\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n - 1);print(t_alpha) # 95% 신뢰구간을 위한 t값 \n\ninterval = t_alpha * se;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_x - interval, xbar_x + interval];print(CI) # 신뢰구간\n\n1.0665364503850772\n2.131449545559323\n2.2732686324957263\n[np.float64(37.97673136750427), np.float64(42.52326863249573)]\n\n\n95 % 신뢰구간은 40.25 ± 2.273 임을 알 수 있다.\n예제 10의 ( 4 ) 검정하고자 하는 가설은 H 0 : μ = 38 대 H 1 : μ &gt; 38 이며, 표본의 크기는 16 이다.\n\ntval = (xbar_x - 38) / se;print(tval) # 표본표준오차\n\npval = 1 - stats.t.cdf(tval, n - 1);print(pval) # p값\n\n## 해석: P–값이 0.026 이므로 유의수준 5% 에서 귀무가설을 기각하게 된다.\n## 따라서, 평균이 38 보다 크다고 할 수 있다.\n\n2.109632539223229\n0.026050840503660355"
  },
  {
    "objectID": "da/ida/ida_12.html",
    "href": "da/ida/ida_12.html",
    "title": "12장: 두 모집단의 비교",
    "section": "",
    "text": "두 모집단의 비교를 위한 추론과정은 자료를 어떻게 수집하느냐에 따라 추론 방법이 달라진다. 대표적인 두 종류의 자료수집과정에 따른 추론방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_12.html#통계용어",
    "href": "da/ida/ida_12.html#통계용어",
    "title": "12장: 두 모집단의 비교",
    "section": "2. 통계용어",
    "text": "2. 통계용어\n비교 연구 시 자주 사용되는 통계용어.\n실험단위 ( Experimental Unit ) : 실험의 대상. 반응값 ( Response ) : 실험 후 얻어지는 수치. 처리 ( Treatment ) : 비교하고자 하는 특성.\n예제12를 위 통계용어로 다음과 같이 설명할 수 있다:\n숲 지역 ⇨ 처리 1 , 도시 지역 ⇨ 처리 2 각각의 스캐너 측정값 ⇨ 실험단위\n그 측정값의 수치 ⇨ 반응값"
  },
  {
    "objectID": "da/ida/ida_12.html#두-개-의-독-립-표-본",
    "href": "da/ida/ida_12.html#두-개-의-독-립-표-본",
    "title": "12장: 두 모집단의 비교",
    "section": "3. 두 개 의 독 립 표 본",
    "text": "3. 두 개 의 독 립 표 본\n독립인 두 개의 표본으로부터 두 모집단, 혹은 두 가지의 처리효과를 비교하는 통계추론의 방법. 다음은 두 모집단으로부터 추출된 표본과 그로부터 계산되는 통계량을 정리한 것이다: 두 모집단으로부터 추출된 표본.\n위 표본으로부터 계산되는 통계량. 여기서 우리의 관심사는 두 모집단의 평균 반응값의 차이다.\n\n3-1. 모평균의 차에 대한 추론\n두 모평균의 차 ( μ 1 – μ 2 ) 에 대한 추론을 위해서는 두 표본평균의 차 ( x̄ – ȳ )를 이용한다. 두 표본의 크기 n 1, n 2 가 모두 큰 경우 ( 30 이상 )중심극한정리에 의해 두 표본평균은 근사적으로 정규분포를 따른다:\n평균이 μ​ 1 ± μ 2​ 이고, 분산이 σ 12 ​+ σ 22 ​인 정규분포를 따른다:\n이때, X 와 Y 는 서로 독립이다. 복호동순 : 식에서 부호를 2 개 이상 사용할 때, 부호를 앞에서부터 같은 순서로 적용하는 것. 따라서, 두 독립적인 정규분포 변수의 차를 표준화하면 표준정규분포를 따르게 된다: 위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다: 추정량 ± (z값) × (추정된 표준오차)\n두 표본의 크기 n 1, n 2 가 모두 30 이상일 때, 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n델타( Δ , δ: 그리스 알파벳의 네번째 글자)\n검정통계량은 H 0 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-2. 모평균의 차에 대한 추론\n표본의 크기가 작을 경우, 두 모집단에 대하여 정규분포와 표준편차에 대한 가정이 필요하다.\n두 모집단이 모두 정규분포를 따른다. 두 모집단의 표준편차가 일치한다.\n값이 ½보다 작거나 2보다 큰 경우, 위 가정이 적절하지 못한다고 판단한다. 대부분의 경우, σ 를 모르므로 이를 추정하여야 한다.\nσ 에 대한 정보는 편차제곱합에 모두 들어 있다. 따라서 이 두 제곱합을 더하여 각각의 자유도의 합으로 나누어 σ 2 추정량으로 사용하게 된다.\n이를 공통분산 σ 2 의 합동추정량 ( Pooled Variance ) 이라 한다:\n위 식을 이용하여, ( μ 1 – μ 2 ) 에 대한 표준화된 확률변수는 다음과 같다:\n자유도가 ( n ₁ + n ₂ – 2 )인 t 분포를 따른다.\n위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다:\n( μ ₁ – μ ₂ ) 에 대한 신뢰구간은 추정량 ± ( t 값 ) × ( 추정된 표준오차 )의 형식에 의해 정리된다.\n두 모집단이 모두 정규분포를 따르고 두 모표준편차가 같은 때 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-3. 모평균의 차에 대한 추론\n표본의 크기가 작고, 두 모집단의 표준편차가 일치하지 않을 경우, 근사적으로 t 분포를 따르며, 자유도는 ( n 1 – 1 )과 ( n 2 – 1 ) 중 작은 값이다.\n이 분포를 이용한 ( μ 1 – μ 2 ) 에 대한 추론방법은 다음과 같다:\n모평균 차에 대한 100 ( 1 − α ) % 신뢰구간은 근사적으로 위 식과 같다.\n신뢰구간의 경우, 그 구간이 넓어지는 경향이 있다. 따라서 실제 신뢰도는 100 ( 1 − α ) % 이상이 된다.\n가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n검정의 경우, 기각역이 좁아지는 경향이 있다. 따라서 실제 유의수준이 α 이하가 되므로 귀무가설을 기각하지 못할 가능성이 높아진다."
  },
  {
    "objectID": "da/ida/ida_12.html#짝비교",
    "href": "da/ida/ida_12.html#짝비교",
    "title": "12장: 두 모집단의 비교",
    "section": "4. 짝비교",
    "text": "4. 짝비교\nMatched Pair Comparisons 실험 단위들이 비슷해야 한다는 점과 다양한 실험 단위들을 비교해야 한다는 점을 절충하는 접근법.\n짝지워서 각각의 쌍으로 만드는 방법:\n같은 쌍의 실험단위들은 서로 비슷해야 한다. 각 쌍 내에서 두 조건이 다르게 설정되어야 한다.\n짝 비교를 시행할 때의 자료의 형태:\n차의 표본평균과 분산은 다음과 같다:\nX i 와 Y i 는 서로 독립이 아니다. 서로 높은 상관관계를 가질 경우, 짝비교의 효과는 크다. 즉, 전체적인 변화의 폭 ( 변동성 ) 을 줄여 처리효과를 알아내기 수월한 짝비교를 할 수 있다.\n모평균 δ 에 대한 100 ( 1 − α ) % 신뢰구간은 다음과 같다:\n귀무가설 H 0 : δ = δ 0 에 대한 검정통계량은 다음과 같다:\nH ₀ 가 맞을 때 자유도가 ( n – 1 )인 t 분포를 따른다.\n자료가 짝지워져 있는 경우,두 개의 처리를 어떻게 배정할 것인가가 전혀 문제되지 않는다. 자료가 짝지워져 있지 않은 경우, 여러 조건들이 확률적으로 같은 정도로 영향이 미치도록 해야 한다. (어느 한쪽의 처리에만 영향을 주지 않아야 한다.)이와 같이 무작위로 배정하는 것을 랜덤화 ( Randomization ) 라고 한다."
  },
  {
    "objectID": "da/ida/ida_12.html#두-모비율의-차에-대한-추론",
    "href": "da/ida/ida_12.html#두-모비율의-차에-대한-추론",
    "title": "12장: 두 모집단의 비교",
    "section": "5. 두 모비율의 차에 대한 추론",
    "text": "5. 두 모비율의 차에 대한 추론\n두 모집단의 비율을 비교하는 추론하는 방법. 관심의 대상이 되는 어떤 특성의 모집단 1 의 비율을 p 1, 모집단 2 의 비율을 p 2 라고 할 때 두 모집단으로부터 크기가 n 1, n 2 인 표본을 추출하여 각각 특성이 A 인 것과 A 가 아닌 것으로 분류하였다고 가정한다.\n이때 A 를 ’ 성공 ‘, A 가 아닌 것을’ 실패 ’ 라고 하고 두 표본의 성공의 개수를 각각 X, Y 표현한다.\n두 모집단의 특성 A 의 비율을 각각 p 1, p 2 라고 하면 그 추정량은 각 표본으로부터 표본의 비율을 사용하게 된다:\n두 모비율의 차 ( p ₁ – p ₂ ) 의 추정량은 ( p̂ ₁ – p̂ ₂ ) 이 된다. ( p 1 – p 2 ) 에 대한 추론을 하기 위해서는 ( p̂ 1 – p̂ 2 ) 의 분포를 알아야 한다.\n표본의 크기 n 1, n 2 가 큰 경우, 아래 식이 근사적으로 성립한다:\n아래 식도 표본이 서로 독립이므로 정규분포로 근사된다:\n따라서 이를 표준화하면:\n두 확률변수 간의 차이를 표준화하는 것이다.\n위 식을 이용하여 ( p 1 – p 2 ) 에 대한 ( 1 − α ) % 신뢰구간은 다음과 같다:\n( 추정량 ) ± ( z 값 ) × ( 표준오차 )의 형식에 의해 정리된다.\n신뢰구간을 계산할 때, ( 제곱근 속의 ) 실제 모평균 차이 p 1, p 2 는 미지수이므로, 이를 표본 비율의 차이 p̂ 1, p̂ 2 로 대체하여 계산한다.\n\n5-1. 두 모비율의 검정\n표본의 크기가 클 때 두 모비율이 같은지를 검정하는 방법은 두 비율의 차이에 대한 검정을 통해 이루어진다.\n귀무가설 H 0 : p = p 0 을 검정하기 위해 (​ p̂ 1 – p̂ 2 ) 을 이용하게 된다. 이 가설이 맞을 경우의 통계량 분포는 다음과 같다:\np 는 모비율이 같은 두 모집단의 공통비율이다.\n통합된 두 표본으로부터 이를 추정하면:\n위 과정을 바탕으로 검정통계량을 정리할 수 있다:\np̂ 는 귀무가설하에서의 공통비율 p 의 추정량이고, 검정통계량은 H 0 가 맞을 때 근사적으로 N ( 0, 1 ) 을 따른다.\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 12의 ( 1 ) 추정량의 표준오차 ( se ) 를 계산하기 위해 두 변수의 요약통계량 ( s 12, s 22 ) 을 구하는 파이썬 함수를 사용할 수 있다.\n\nvar1 = np.var(x, ddof=1);print(var1) # x의 표본 분산 (자유도=1)\nvar2 = np.var(y, ddof=1);print(var2) # y의 표본 분산 (자유도=1)\n\nn1 = len(x);print(n1) # x의 데이터 수\nn2 = len(y);print(n2) # y의 데이터 수\n\nse = math.sqrt(var1 / n1 + var2 / n2);print(se) # 표준오차\n\n48.06374040272343\n24.789102564102567\n118\n40\n1.0134334699544658\n\n\n표준오차 계산 ( 1.0134 )\n예제 12의 ( 2 ) 모평균의 차 μ 1 – μ 2 에 대한 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 z–값\nz_alpha = stats.norm.ppf(1 - 0.05 / 2);print(z_alpha)\ninterval_z = z_alpha * se; print(interval_z) # 신뢰구간 범위\n\n# x와 y의 평균을 계산\nxbar1 = np.mean(x); print(xbar1) \nxbar2 = np.mean(y); print(xbar2) \n\n# 두 평균의 차이를 계산\ndiff = xbar1 - xbar2; print(diff) \n\n# 신뢰구간\nCI_1 = [diff - interval_z, diff + interval_z]; print(CI_1)\n\n\n## 해석: 95%의 신뢰구간이 0을 포함하지 않으므로 (양측검정에서) \n## 유의수준 5%에서 두 수치가 같다는 귀무가설 (H₀ : μ1 = μ2) 을 기각할 수 있다.\n\n1.959963984540054\n1.986293101838208\n92.9322033898305\n82.075\n10.857203389830502\n[np.float64(8.870910287992295), np.float64(12.84349649166871)]\n\n\n스캐너 자료 μ ₁ – μ ₂ 의 신뢰구간은 10.857 ± 1.986 이다.\n예제 12의 ( 3 ) 두 모집단이 모두 정규분포를 따르고, 분산이 같다는 가정 하에서 합동분산추정량 및 추정량의 표준오차를 구하라.\n\n# 합쳐진 분산 계산\nspooled = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2); print(spooled) \n\n# 합쳐진 표준 오차 계산\nse_spooled = math.sqrt(spooled) * math.sqrt(1 / n1 + 1 / n2); print(se_spooled)\n\n42.24508094306821\n1.1891745810061622\n\n\n합동추정량 ( 42.245 ) / 표준오차 ( 1.189 )\n예제 12의 ( 4 ) 위 결과를 바탕으로 t – 분포의 백분위수 함수를 이용하여 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 t–값\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n1 + n2 - 2); print(t_alpha) \n\n# 신뢰 구간의 범위\ninterval_t = t_alpha * se_spooled; print(interval_t) \n\n# 신뢰구간\nCI_2 = [diff - interval_t, diff + interval_t]; print(CI_2)\n\n## 해석: (2)번의 정규분포를 이용한 신뢰구간보다 \n## 오차범위 값인 interval_t가 더 크므로 신뢰구간이 더 넓어졌음을 알 수 있다.\n## 이는 t–분포의 백분위수 값이 (정규분포의 백분위수 값보다) 더 크기 때문이다.\n\n1.9752875076954723\n2.3489616943304696\n[np.float64(8.508241695500033), np.float64(13.206165084160972)]\n\n\n신뢰구간은 10.857 ± 2.348 이다."
  },
  {
    "objectID": "da/ida/ida_10.html",
    "href": "da/ida/ida_10.html",
    "title": "10장: 통계적 추론",
    "section": "",
    "text": "추출된 표본으로부터 모집단의 일반적인 특성을 추론해내는 것을 통계적 추론이라고 하며, 이는 표본의 크기가 클 때 더 정확하게 성립한다."
  },
  {
    "objectID": "da/ida/ida_10.html#자료의-입력",
    "href": "da/ida/ida_10.html#자료의-입력",
    "title": "10장: 통계적 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 13: 예제 1 에 주어진 자료에 근거해서 \n# 중학교 1학년 남학생의 평균키에 대한 95% 신뢰구간을 구하라. (p.311)\n\nimport numpy as np \n\nheight = np.array([163, 161, 168 , 161, 157, 162, 153, 159, 164, 170, \n                   152, 160, 157, 168, 150, 165, 156, 151, 162, 150, \n                   156, 152, 161, 165, 168, 167, 165, 168, 159, 156])\n                   \n\n#_____________________________________________________________________________\n\n# 예제 14: 예제 8의 검정을 시행하고 그 결과를 비교하여라. (p.313)\n\n# 예제 8: 앞의 예제 1에 주어진 중학생의 키 자료로부터 \n# 그 도시의 중학교 1학년 남학생의 평균키(m)가 다른 도시의 중학교 1학년 \n# 남학생의 평균키인 159 cm와 차이가 있다고 할 수 있는지 판단하라. (p.297)\n\n\n#_____________________________________________________________________________\n\n# 예제 15: 3 장의 예제 11 에 주어진 자료로부터 \n# 평균교통소음정도(μ)에 대한 98% 신뢰구간을 구하고, \n\n# 평균교통소음정도가 60 을 초과한다고 주장할 수 있는지 \n# 유의수준 5% 에서 가설검정을 실시하라. (p.314)\n\nnoise = np.array([55.9, 63.8, 57.2, 59.8, 65.7, 62.7, 60.8, 51.3, 61.8, 56.0, \n                  66.9, 56.8, 66.2, 64.6, 59.5, 63.1, 60.6, 62.0, 59.4, 67.2,\n                  63.6, 60.5, 66.8, 61.8, 64.8, 55.8, 55.7, 77.1, 62.1, 61.0, \n                  58.9, 60.0, 66.9, 61.7, 60.3, 51.5, 67.0, 60.2, 56.2, 59.4, \n                  67.9, 64.9, 55.7, 61.4, 62.6, 56.4, 56.4, 69.4, 57.6, 63.8])"
  },
  {
    "objectID": "da/ida/ida_10.html#통계적-추론",
    "href": "da/ida/ida_10.html#통계적-추론",
    "title": "10장: 통계적 추론",
    "section": "2. 통계적 추론",
    "text": "2. 통계적 추론\nStatistical Inference\n표본이 갖고 있는 정보를 분석하여 모수에 관한 결론을 유도하고, 모수에 대한 가설의 옳고 그름을 판단하는 것을 말한다.\n모집단의 일부인 표본으로부터 전체 모집단의 성질을 추론해내는 것이므로 100% 확실하다고 할 수는 없다. 따라서 통계적인 추론을 할 때에는 그 결론의 부정확한 정도를 반드시 언급하여야 하는데 이러한 정도를 수치로 표시할 수 있게 하는 도구로 앞에서 공부한 확률론과 표준분포 등이 이용된다.\n통계적 추론에는 두 가지 주요 방법이 있다:\n모수의 추정 모수에 대한 가설 검증"
  },
  {
    "objectID": "da/ida/ida_10.html#모평균의-추정",
    "href": "da/ida/ida_10.html#모평균의-추정",
    "title": "10장: 통계적 추론",
    "section": "3. 모평균의 추정",
    "text": "3. 모평균의 추정\nEstimation of Parameters\n모수 중 하나로 포함되는, 모집단의 평균에 대한 점추정과 구간추정을 다루고자 한다.\n\n3-1. 점추정\nPoint Estimation\n추정하고자 하는 하나의 모수에 대해, 여러 개의 확률변수를 사용하여 하나의 통계량을 만들고, 주어진 표본으로부터 그 값을 계산하여 하나의 수치를 제시하는 과정.\n모수 ( Parameter ) : 모집단의 실제 값으로, 우리가 알고 싶어하는 대상. 추정량 ( Estimator ) : 모수를 측정하기 위해 만들어진 통계량. 추정치 ( Estimate ) : 주어진 관측값으로부터 계산된 추정량의 실제 값.\n추정량은 하나의 확률변수이므로, 추출된 표본의 따라 그 값이 달라진다. 수치들의 변화의 정도는 추정량의 정확도와 관계가 있다. 이 정확도를 측정하는 도구 중 하나가 표준오차 ( 추정량의 표준편차 ) 이다.\n\n\n3-2. 표준오차\nStandard Error, SE\n추정량의 정확도를 평가하는 데 중요한 지표이며, 값이 작을수록 추정량이 모집단 모수를 더 정확하게 반영한다고 할 수 있다. 표본평균의 기댓값과 표준오차는, 모집단의 평균과 표준편차가 μ, σ 일 때 위와 같이 구할 수 있다.\n표본평균을 가지고 μ 를 추정할 경우, n 이 클수록 표준오차가 작아져 좀 더 정확한 추정이 가능하다. 그러나 표준오차 계산 시 σ 가 주어지지 않는 경우가 있다. 이 경우, σ 를 표본표준편차로 추정하여 사용할 수 있다.\n\n\n3-3. 구간추정\nInterval Estimation\n추정량의 분포를 이용하여 표본으로부터 모수 값을 포함하리라고 예상되는 구간을 제시하는 것. 이때 제시되는 구간을 신뢰구간이라고 한다.\n\n\n3-4. 신뢰구간\nConfidence Interval\n모집단의 어떤 모수를 추정하기 위해 계산된 범위. 신뢰구간은 ( L , U )의 형태로 이루어지며, 여기서 L 과 U 는 표본으로부터 계산된 통계량이다.\nL ( Lower bound ) : 신뢰구간의 하한값 U ( Upper bound ) : 신뢰구간의 상한값 따라서 표본마다 계산되는 신뢰구간은 서로 다를 수 있다.\n가장 확실한 신뢰구간은 항상 모수를 포함하는 구간이다. 그러나 이는 이론적으로는 가능하지만 실질적으로는 불가능하다. 그렇게 되기 위해서는 신뢰구간이 상당히 길어질 수 밖에 없다.\n이 경우, 신뢰구간 CI 는 매우 넓어질 수밖에 없다.\n항상 모수를 포함하는 신뢰구간은 실질적으로 너무 넓어서 유용하지 않다. 모수에 대한 정확한 정보를 얻으려면 신뢰구간을 가능한 한 줄일 필요가 있다.\n신뢰구간이 좁을수록 추정의 정확성이 높아진다.\n실용적인 측면에서 신뢰구간을 적절히 좁히기 위해, ” 모든 표본에서 항상 모수를 포함해야 한다 ” 는 엄격한 조건을 완화하고, 대부분의 경우에서 모수를 포함하도록 설정하는 것이 필요하다.\n신뢰구간이 모수를 포함할 확률을 1 보다는 작은 일정한 수준에 유지하여 구간의 길이를 줄이는 것이 바람직하다.\n이때 모수를 포함할 확률을 신뢰수준 ( Level of Confidence ) 또는 신뢰도라고 한다.\n\n\n3-5. 모평균 μ에 대한 신뢰구간\n여기에서는 신뢰구간을 계산하는 두 가지 경우를 설명한다:\n\n모집단의 표준편차 σ 를 알고 있는 경우:\n\n정규분포 개념을 이용하여 신뢰구간을 계산한다:\n신뢰수준 95 % 에 해당하는 정규분포의 임계값 Z α / 2 ​를 사용하여 신뢰구간의 범위를 설정한다. 예시 : α = 0.05 일 때, Z ₀.₀₅ / ₂ = Z ₀.₀₂₅ = 1.96\n정규분포는 평균 μ 와 표준편차 σ 를 알 때, 그 분포의 형태가 완전히 결정된다. 평균 μ 를 중심으로 좌우 대칭이며, σ 가 클수록 분포가 넓어진다. 이 특성 덕분에, 정규분포는 모수에 대해 많은 정보를 제공할 수 있다. 모집단이 정규분포를 따른다면, 표본 평균의 분포 역시 정규분포를 따른다.\n위의 식에서 괄호 안에 부등식을 풀어 쓰면:\n위 식을 μ 에 대해 정리하면:\n따라서:\n\n모집단의 표준편차 σ를 모르는 경우:\n일반적인 경우에 관심있는 모집단은 그 분포나 표준편차가 알려져 있지 않다. 이런 경우, 9장에서 다룬 중심극한정리를 이용한다.\n\n위 식은 약간의 수정이 필요한데, 이것은 11장을 참고하기를 바란다.\n이 경우에도, 확률 1 − α 는 근사적으로 얻어진다. n 이 클 때는 σ 대신 s 를 사용해도 확률값에 크게 영향을 주지 않는다.\n일반적으로 추정량의 기댓값이 추정하고자하는 모수값을 갖고 그 분포가 정규분포일 때 100 ( 1 − α ) % 신뢰구간은 다음과 같은 형태를 따른다:\n사용 상황에 따라 표준오차의 정의가 달라질 수 있다.\n\n\n3-6. 신뢰구간의 의미\n이 그래프는 주어진 모수에 대해 95% 신뢰구간을 표시하였다. 주어진 모수 (평균 100, 표준편차 10), 표본 크기 15를 사용하여 총 25개의 표본을 추출하였다.\n위 그래프와 같은 방식으로 표본을 계속 추출하고 신뢰구간을 계산하면, 그 신뢰구간들이 모평균을 포함하는 비율이 95% 에 가까워지는 것을 확인할 수 있다.\n\n\n3-7. 표본 크기의 결정\n많은 수의 표본을 추출하여 신뢰구간을 생성하는 것은 시간과 비용이 많이 소모된다. 따라서 우리가 원하는 정확도를 얻을 수 있는 범위 내에서 표본의 크기를 줄이는 것이 바람직하다.\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α ) % 가 되려면:\n표준화된 확률변수의 분포가 표준정규분포를 따르므로:\n위 사실로부터 아래 식이 성립해야 하며:\n이를 n 에 대하여 풀면 아래 식을 만족해야 한다:\n적정한 표본의 크기는 위의 부등식을 만족하는 ’ 최소의 정수 ’ 가 된다. 만약 모집단이 정규분포라는 가정이 없다면 표본의 크기는 중심극한정리를 이용할 수 있도록 30 이상이 되어야 한다."
  },
  {
    "objectID": "da/ida/ida_10.html#모평균에-대한-검정",
    "href": "da/ida/ida_10.html#모평균에-대한-검정",
    "title": "10장: 통계적 추론",
    "section": "4. 모평균에 대한 검정",
    "text": "4. 모평균에 대한 검정\n통계적 추론 중 하나인 모수에 대한 가설 검증 ( Testing Statistical Hypotheses ) 에 대해 다루고자 한다.\n\n4-1. 가설\nHypotheses 가설검증에는 2 개의 가설이 있다.\n대립가설 ( Alternative Hypothesis ; H ₁ ) : 입증하여 주장하고자 하는 가설.\n귀무가설 ( Null Hypothesis ; H ₀ ) : 대립가설을 입증할 수 없을 때, 대립가설을 무효화하면서 받아들이는 가설.\n\n\n4-2. 오류의 종류\n가설검증에서 내리는 판단이란 다음 2 가지 형태 중 하나로 나타난다. 상황에 따라 다르지만, 일반적으로 제1종 오류에 더 주의를 기울이게 된다.\n제1종 오류 ( Type I Error ; α ) : 귀무가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 없는데, 효과가 있다고 결론 내리는 경우.\n제2종 오류 ( Type II Error ; β ) : 대립가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 있는데, 효과가 없다고 결론 내리는 경우.\n\n\n4-3. 검정통계량\nTest Statistic 표본 데이터를 요약하여 귀무가설을 검정하는 데 사용하는 값. 귀무가설이 참이라는 가정 하에 표본에서 계산된 값으로, 이 값이 귀무가설 하에서 얼마나 극단적인지를 평가한다.\n\n\n4-4. 기각역\nCritical Region 귀무가설을 기각할 수 있는 값들의 집합.\n검정 통계량이 기각역에 속할 경우 귀무가설을 기각한다. 아닐 경우 귀무가설을 기각하지 않는다.\nc 이하이면 H₀ 을 기각한다.\n가장 바람직한 기각역이란 아래 두 확률을 최소화하는 것이 될 것이다.\nα : 제 1 종 오류를 범하게 될 확률. β : 제 2 종 오류를 범하게 될 확률.\n위 두 확률은 다음과 같은 특징이 있다:\n( 1 ) α 와 β 는 서로 반비례 관계에 있다.\n( 2 ) α 는 너무 크게 설정하지 않는 것이 좋다. 너무 큰 α 는 제1종 오류의 확률을 높여 잘못된 결론을 내릴 가능성을 높인다.\n이를 방지하기 위해, 유의수준이라는 상한선을 둘 수 있다.\n\n\n4-5. 유의수준\nSignificance Level 일반적으로 사용되는 유의수준은 0.05 ( 5% ) 또는 0.01 ( 1% ) 이다. 이는 연구자가 제1종 오류의 확률을 낮추어 신뢰할 수 있는 결과를 얻기 위함이다.\n0.05 ( 5% ) : 가장 흔히 사용되는 유의수준. 제1종 오류를 5% 로 제한. 0.01 ( 1% ) : 더 엄격한 기준으로, 제1종 오류를 1% 로 제한. 0.10 ( 10% ) : 덜 엄격한 기준으로, 제1종 오류를 10% 로 제한.\n양측 검정 ( Two — tailed Test ) : 유의 수준 α 는 두 방향 ( 양쪽 끝 ) 에 분배되어 α / 2 씩 기각역을 형성한다.\n단측 검정 ( One — tailed Test ) : 유의 수준 α 는 한 방향에 집중되어 기각역을 형성한다.\n\n\n4-6. 모평균 μ에 대한 검 정\n표본의 크기가 클 때 모평균 μ 에 대한 가설 H ₀ : μ = μ₀ 을 검정하기 위한 검정통계량은 다음과 같다: 단 모집단의 표준편차 σ 가 주어져 있을 때 s를 σ 로 대체한다.\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n4-7. 유의확률\nSignificance Probability 주어진 검정통계량의 관측치로부터 H ₀ 을 기각하게 하는 최소의 유의수준.\nP – 값 ( P – Value )\n일반적으로 유의확률은 주어진 관측값을 경계점으로 하는 기각역의 유의수준으로 얻어진다. Z = z 일 때 각 기각역의 형태에 따라 P – 값을 구하는 식을 정리하면 다음과 같다:"
  },
  {
    "objectID": "da/ida/ida_10.html#모비율에-대한-추론",
    "href": "da/ida/ida_10.html#모비율에-대한-추론",
    "title": "10장: 통계적 추론",
    "section": "5. 모비율에 대한 추론",
    "text": "5. 모비율에 대한 추론\n모집단에서 특정 속성을 가진 개체의 비율을 추정하거나 검정하는 것을 의미한다.\n\n5-1. 점추정\n모비율에 대한 추정량으로 표본비율을 사용할 수 있다: 점추정량이 결정되면 그 추정량의 정확도를 알기 위해 표준오차를 계산할 필요가 있다.\n모집단의 크기가 매우 커서 그에 비해 표본의 크기가 작은 경우, X 의 분포는 반복 횟수 n , 성공의 확률 p 인 이항분포가 된다. 따라서, X 의 기댓값과 분산는 아래와 같다:\nE ( X ) = np Var ( X ) = npq\n표본비율의 표준오차는 다음과 같이 계산된다:\n이 식은 이항분포의 분산을 이용한 결과이다. ​​ 표본비율​의 분산 은 아래 식과 같다:\n표본비율의 표준오차는 이 분산의 제곱근으로 표현된다.\n\n\n5-2. 구간추정\n모비율 P 에 대한 구간추정을 하려면 P 의 추정량인 p̂ 의 분포를 알아야 한다.\nX 의 분포는 이항분포를 따르므로, 중심극한정리를 이용하여 표본 비율 p̂​ 의 분포를 근사적으로 정규분포로 취급할 수 있다:\n분모, 분자를 n으로 나누었을 때, 위와 같은 식이 나온다.\n위 구간이 p를 포함할 확률이 ( 1 − α ) 가 됨을 알 수 있다.\n\n\n5-3. 신뢰구간\np 를 추정량인 p̂ 으로 대체하면, 원하는 신뢰구간을 구할 수 있다.\n\n\n5-4. 표본크기의 결정\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α )% 가 되려면:\n표본 크기가 클 경우, 정규분포를 이용하여 위와 같은 식을 구할 수 있다. 따라서, 표본 크기는 아래 식을 만족해야 한다:\n\n\n5-5. 모비율 p에 대한 검정\n표본 크기가 클 때 모비율 p 에 대한 가설 H ₀ : p = p ₀ 을 검정하기 위한 검정통계량은 다음과 같다:\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 13의 ( 1 ) height 에 대한 요약 통계량 계산하기.\n\nxbar_h = np.mean(height);print(xbar_h) # 평균\n\nvar_h = np.var(height, ddof=1);print(var_h) # 분산 (자유도 1 사용)\n\nsd_h = np.std(height, ddof=1);print(sd_h) # 표준편차 (자유도 1 사용)\n\nmedian_h = np.median(height);print(median_h) # 중앙값\n\nmin_h = np.min(height);print(min_h) # 최솟값\n\nmax_h = np.max(height);print(max_h) # 최댓값\n\nsum_h = np.sum(height);print(sum_h) # 합계\n\nn = height.size;print(n) # 데이터 개수\n\n160.2\n35.88965517241378\n5.990797540596225\n161.0\n150\n170\n4806\n30\n\n\n예제 13의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수. 정규분포의 백분위수 함수.\n\nfrom scipy import stats \n\nse_h = stats.sem(height);print(se_h) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.05 / 2); print(z_alpha) # 95% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_h;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_h - interval, xbar_h + interval]; print(CI) # 신뢰구간\n\n1.0937649834770078\n1.959963984540054\n2.1437399751659827\n[np.float64(158.05626002483402), np.float64(162.34373997516596)]\n\n\n이로부터 95% 신뢰구간은 160.2 ± 2.1437 임을 알 수 있다.\n예제 14 검정하고자 하는 가설은 H ₀ : μ = 159 대 H ₀ : μ ≠ 159 이며, 표본의 크기는 30 이상이다.\n\nzval = (xbar_h-159)/se_h;print(zval) # 가설 검정을 위한 z값\n\npval = 2 * (1 - stats.norm.cdf(zval));print(pval) # p값\n\n## 해석: p값이 크므로 귀무가설을 기각할 수 없음\n## → 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\n\n1.097127827392377\n0.27258551722126834\n\n\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고, 이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\n\nxbar_n = np.mean(noise);print(xbar_n) # 평균\n\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\n\nn = noise.size;print(n) # 데이터 개수\n\n61.373999999999995\n4.780137902544961\n50\n\n\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\n\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\n\n0.6760135851792765\n2.3263478740408408\n1.5726427667045366\n[np.float64(59.801357233295455), np.float64(62.946642766704535)]\n\n\n\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\n\npval = stats.norm.sf(np.abs(zval));print(pval) # p값\n\n## 해석: P—값이 0.021로 0.05보다 작게 나왔으므로 \n## 유의수준 5% 에서 귀무가설을 기각할 수 있다.\n\n## 그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\n\n2.032503532655509\n0.021051353256926374"
  },
  {
    "objectID": "da/ida/ida_08.html",
    "href": "da/ida/ida_08.html",
    "title": "8장: 정규분포",
    "section": "",
    "text": "6장에서 언급되었던, 연속적인 값을 가지는 연속확률분포들 중에서 대부분의 통계학 이론의 기본이 되는 정규분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_08.html#자료의-입력",
    "href": "da/ida/ida_08.html#자료의-입력",
    "title": "8장: 정규분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n!pip install numpy\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7: 어느 대학교의 일반수학 중간고사 성적은\n# 분포가 평균이  63 이고, 분산이  100 인 정규분포를 따른다고 가정한다. (p.232)\n\n# (1) 50 점 이하의 학생은 몇 퍼센트나 되겠는가?\n\n# (2) 상위 10 %의 학생에게  A 를 준다고 하면 \n# 몇 점 이상이 되어야  A 를 받을 수 있겠는가?\n\n# ___________________________________________________________\n\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\n\nimport numpy as np \n\ndata1 = np.array([4001, 3927, 3048, 4298, 4000, 3445, \n                 4949, 3530, 3075, 4012, 3797, 3550, \n                 4027, 3571, 3738, 5157, 3598, 4749, \n                 4263, 3894, 4262, 4232, 3852, 4256, \n                 3271, 4315, 3078, 3607, 3889, 3147, \n                 3421, 3531, 3987, 4120, 4349, 4071, \n                 3683, 3332, 3285, 3739, 3544, 4103, \n                 3401, 3601, 3717, 4846, 5005, 3991, \n                 2866, 3561, 4003, 4387, 3510, 2884, \n                 3819, 3173, 3470, 3340, 3214, 3670, \n                 3694])\n                 \n# ___________________________________________________________\n\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\n\ndata2 = np.array([39.3, 14.8, 6.3, 0.9, 6.5, \n                 3.5, 8.3, 10.0, 1.3, 7.1, \n                 6.0, 17.1, 16.8, 0.7, 7,9, \n                 2.7, 26.2, 24.3, 17.7, 3.2, \n                 7.4, 6.6, 5.2, 8.3, 5.9, \n                 3.5, 8.3, 44.8, 8.3, 13.4, \n                 19.4, 19.0, 14.1, 1.9, 12.0, \n                 19.7, 10.3, 3,4, 16.7, 4.3, \n                 1.0, 7.6, 28.33, 26.2, 31.7, \n                 8.7, 18.9, 3.4, 10.0])\n\nRequirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.6)\n\n\nWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\nYou should consider upgrading via the 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command."
  },
  {
    "objectID": "da/ida/ida_08.html#연속확률분포",
    "href": "da/ida/ida_08.html#연속확률분포",
    "title": "8장: 정규분포",
    "section": "2. 연속확률분포",
    "text": "2. 연속확률분포\nContinuous Probability Distribution\n연속확률변수 X 가 가질 수 있는 값들의 분포를 나타낸다. 연속확률변수: 특정 범위 내에서 모든 실수 값을 가질 수 있는 변수."
  },
  {
    "objectID": "da/ida/ida_08.html#확률밀도함수",
    "href": "da/ida/ida_08.html#확률밀도함수",
    "title": "8장: 정규분포",
    "section": "2-1. 확률밀도함수",
    "text": "2-1. 확률밀도함수\nProbability Density Function, PDF\n연속확률변수 \\(X\\) 의 확률분포는 확률의 밀도를 나타내는 \\(X\\) 의 확률밀도함수에 의해 결정된다. 아래의 조건을 만족하는 함수 \\(f(X)\\) 를 \\(X\\) 의 확률밀도함수라고 한다.\n\n연속확률변수에서 특정 값에 대한 확률은 0 이다. 예를 들어, [ 0, 1 ] 구간에 있는 실수 값은 무한히 많다. 이러한 경우, 특정한 하나의 값을 가질 확률은 무한히 작은 값이 되며, 이는 수학적으로 0 으로 표현된다.\n\n범위 : 확률밀도함수 f ( x ) 의 값은 항상 0 이상이어야 한다. 이는, 어떤 사건의 확률이 음수일 수 없다는 것을 의미한다.\n확률밀도함수는 이산확률분포의 확률함수와는 달리 확률을 나타내지 않는다. 그러므로, f ( x ) 가 1 보다 작아야 된다는 조건 은 필요가 없다.\n\nPDF 를 사용하여 특정 구간 [ a, b ] 에 속할 확률을 계산할 수 있다.\nPDF 를 전체 범위에 대해 적분 하면 1 이 되어야 한다.\n\n위 조건으로부터 다음과 같은 결론을 내릴 수 있다:\n어떤 구간의 확률을 구할 때에는 그 구간의 경계점이 포함되는가 포함되지 않는가에 영향을 받지 않는다."
  },
  {
    "objectID": "da/ida/ida_08.html#정규분포",
    "href": "da/ida/ida_08.html#정규분포",
    "title": "8장: 정규분포",
    "section": "3. 정규분포",
    "text": "3. 정규분포\nNormal Distribution\n평균 \\(μ\\) 와 분산 \\(σ^2\\) 의해서 그 분포가 확정된다. 그 확률밀도함수 ( PDF ) 의 대략적인 특성은 다음과 같이 표현할 수 있다:\n위 정규분포는 N ( μ, σ² )으로 표시할 수 있다. X 가 평균 μ 로부터 ± σ, ± 2σ, ± 3σ, ± 4σ 의 사이의 있을 확률은 다음과 같다.\n이를 통해 ± 4σ 이상 떨어진 데이터는 매우 드문 현상임을 알 수 있다. 그러므로, 정규분포의 ± 4σ 이상의 영역은 실질적인 분석에서 종종 무시할 수 있다."
  },
  {
    "objectID": "da/ida/ida_08.html#정규분포의-특성",
    "href": "da/ida/ida_08.html#정규분포의-특성",
    "title": "8장: 정규분포",
    "section": "3-1. 정규분포의 특성",
    "text": "3-1. 정규분포의 특성\nμ ± 3σ 안에 거의 모든 확률이 집중된다.\n3 - 2 . 표 준 정 규 분 포 ( Standard Normal Distribution ) 평균 ( μ ) 이 0 이고 표준편차 ( σ )가 1 인 특수한 정규분포.\n확률변수 Z 가 N ( 0, 1 ) 이라고 할 때, Z 는 0 을 중심으로 대칭인 분포를 갖게 되며,\n이를 이용해 다음과 같이 나타낼 수 있다:\n누적분포함수 ( CDF ) 는 특정 값 이하의 확률을 나타낸다.\nP ( Z ≤ b ) 는 Z 가 b 이하일 확률이고, P ( Z ≤ a ) 는 Z 가 a 이하일 확률이다. 따라서 a 에서 b 까지의 확률은 다음과 같다:\n\n3-3. 표준정규확률변수\nStandard Normal Random Variable\n표준정규분포 Z 에 관한 확률계산 방법을 일반 정규분포 X 의 확률계산에 적용할 수 있다. 일반 정규분포 X 를 표준정규분포 Z 로의 식변환을 통해 쉽게 계산할 수 있으며, 이를 표준화라고 한다.\n확률변수 X 가 N ( μ, σ² ) 일 때 표준화된 확률변수 Z 는 정규분포 N ( 0, 1 ) 을 따른다. X 를 표준화하여 Z 로 표현한 후 표준정규분포표를 이용하면 된다.\n예제 7의 ( 1 ) 중간고사 성적을 확률변수 X 라고 하면 주어진 정보를 바탕으로 다음과 같이 표현할 수 있다:\n평균 (μ) = 63, 분산 (σ²) = 100 = 10², 표준편차 (σ) = 10 정규분포표의 −1.3행, 0.00열의 값 = 0.0968\n\nfrom scipy.stats import norm\nprint(norm.cdf(x=50, loc=63, scale=10))\n\n## 해석: 50 점 이하의 학생의 비율은  0.0968 = 9.68% 이다.\n\n0.09680048458561036\n\n\n예제 7의 ( 2 ) x 점 이상의 학생들에게 A 를 준다고 하면 x 는 다음을 만족해야 한다: 상위 10 % = 0.10\n먼저, 표준정규분포표로부터 P [Z ≥ z] = 0.10 이 되는 z 값을 찾는다: 이 경우, 0.10 에 가까운 값을 주는 z = 1.28 을 고르면 된다:\n\nfrom scipy.stats import norm\nprint(norm.ppf(q=0.9, loc=63, scale=10))\n\n## 해석: 상위 10 %의 학생에게  A 를 준다고 하면 \n## 75.8 점 이상의 점수를 받은 학생에게 주면 된다.\n\n75.815515655446"
  },
  {
    "objectID": "da/ida/ida_08.html#이항분포의-정규분포근사",
    "href": "da/ida/ida_08.html#이항분포의-정규분포근사",
    "title": "8장: 정규분포",
    "section": "4. 이항분포의 정규분포근사",
    "text": "4. 이항분포의 정규분포근사\n대규모 시행에서 이항 분포를 정규 분포로 근사하는 방법. 7장의 초기하 분포나, 포아송 분포 마찬가지로, 정규 분포로도 근사하여 계산할 수 있다.\n이항 분포는 n 이 커짐에 따라 근사적으로 정규분포를 따르게 된다. 이때 정규분포의 평균과 분산은 이항분포의에서와 일치하여야 한다:"
  },
  {
    "objectID": "da/ida/ida_08.html#연속성수정",
    "href": "da/ida/ida_08.html#연속성수정",
    "title": "8장: 정규분포",
    "section": "4-1. 연속성수정",
    "text": "4-1. 연속성수정\nContinuity Correction\n이항 분포의 이산적인 값을 연속적인 정규 분포의 구간에 맞추는 작업. 보통 ± ½ ​를 가감하여 근삿값을 계산한다.\n\\[\nX ~ Bin (n, p) ≈ Y \\~ N (μ, σ²)\n\\]\n확률변수 X 가 이항분포, 즉 X ~ Bin ( n, p ) 이고, np, nq ( =1 − p ) 가 모두 클 경우 ( 보통 10 이상 ) X 는 근사적으로 평균이 np, 표준편차가 √npq 인 정규분포를 따른다."
  },
  {
    "objectID": "da/ida/ida_08.html#정규분포가정의-조사",
    "href": "da/ida/ida_08.html#정규분포가정의-조사",
    "title": "8장: 정규분포",
    "section": "5. 정규분포가정의 조사",
    "text": "5. 정규분포가정의 조사\n모집단의 분포가 정규 분포를 따른다는 가정을 조사하기 위해 사용할 수 있는 효과적인 그림. 정규점수그림 ( Normal Scores Plot ) 또는 정규확률그림 ( Normal Probability Plot ) 으로 불린다.\n\n5-1. 정규점수\nNormal Scores\n표본 데이터와 표준 정규 분포 ( 평균 0, 표준편차 1 ) 를 비교하여 데이터의 정규성을 평가하는 데 사용된다. 정규성( Normality ) : 데이터가 정규 분포를 따르는지를 의미한다.\n즉, 표준정규분포의 확률밀도함수 ( PDF ) 를 등확률 구간으로 나누어 주는 경곗값 ( z 값 ) 을 의미한다. 그림으로부터 분포의 형태를 알기 위해선 자료의 크기가 적어도 15 이상은 되어야 한다.\n\n\n5-2. 정규확률그림 그리는 순서\n자료를 작은 것부터 크기순으로 나열한다. 각 자료에 해당하는 점수를 계산한다. i 번째 순서의 자료와 i 번째 순서의 정규점수를 하나의 쌍으로 2 차원 공간상에 나타낸다.\n\n\n5-3. 정규그림의 해석\n정규분포를 따른다면, 양쪽의 값은 서로 가까울 것이라고 예상할 수 있다.\n\n\n5-3. 정규확률그림을 이용한 정규성 판정\n직선식일 경우, 정규분포의 가정이 타당하다고 볼 수 있다. (곡선 등) 직선식을 벗어날 경우, 가정이 의문시된다고 할 수 있다.\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsm.qqplot(data1, line = \"s\")\n\n# \"s\"는 \"standardized\"를 의미한다. \n\n# 이 옵션은 플롯에 표준화된 선\n# (평균 0, 표준편차 1의 정규 분포)에 맞추어 선을 그린다.\n\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에 가까우므로 \n## 데이터가 정규분포를 따를 가능성이 높다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n\n5-4. 자료의 변환\n만약 추출된 표본이 정규확률그림 등에서 정규분포와 상당히 벗어난 것으로 판명되면, 자료의 변환을 통해 정규분포의 형태를 갖도록 시도해 볼 수 있다.\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\nimport matplotlib.pyplot as plt\n\nplt.hist(data2, bins=5, range=(0,50)) \nplt.xlabel('data2') \nplt.ylabel('Density') \nplt.title('Historam of data')\n\n## 해석: 아래 히스토그램을 보면 자료의 분포가 왼쪽으로 편중되어 있으므로\n## 정규분포가 아니라는 의심을 할 수 있다.\n\nText(0.5, 1.0, 'Historam of data')\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nsm.qqplot(data2, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에서 크게 벗어나\n## 데이터가 정규 분포를 따르지 않을 가능성이 있음을 확인할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n이 경우, 큰 자룟값을 더 작게 만드는 과정을 수행한다:\n\n# 원 자료를 제곱근(체적^(0.5))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata3 = np.sqrt(data2)\nsm.qqplot(data3, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n# 원 자료를 네제곱근(체적^(0.25))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata4 = np.power(data2, 0.25)\nsm.qqplot(data4, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida/ida_06.html",
    "href": "da/ida/ida_06.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "5장에서 다룬 표본공간의 근원사건들은 특성을 표현하는 형태로 다뤘다. 이제는 확률변수를 중심으로 실험의 수치적 결과에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_06.html#확률변수",
    "href": "da/ida/ida_06.html#확률변수",
    "title": "6장: 확률분포",
    "section": "1. 확률변수",
    "text": "1. 확률변수\nRandom Variable\n각각의 근원사건들에 실수값을 “대응시키는 함수”이며 X, Y, … 등으로 표시한다.\n확률변수가 가질 수 있는 “값의 개수” 가 유한하거나 무한이라도 “셀 수 있는 경우” 에 이를 “이산확률변수” 라고 한다.\n또한, 연속적인 구간에 속하는 모든 값을 다 가질 수 있는 “연속확률변수” 도 있다."
  },
  {
    "objectID": "da/ida/ida_06.html#이산확률분포",
    "href": "da/ida/ida_06.html#이산확률분포",
    "title": "6장: 확률분포",
    "section": "2. 이산확률분포",
    "text": "2. 이산확률분포\nDiscrete Probability Distribution\n확률변수가 갖는 값들과 그에 “대응하는 확률값” 을 나타내는 것으로, 나열된 표나 수식으로 표현되며, 보통은 “확률변수 X 의 분포” 라고 한다.\n\n2-1. 확률질량함수\nProbability Mass Function, PMF\n확률변수 X가 k개의 값 x₁, x₂, …, xk를 가질 때, 그에 대응하는 확률을 f(x₁), f(x₂), …, f(xk)라고 하면 X의 확률분포는 다음과 같다:\nf(x)는 확률변수 X 가 값 x 를 갖게 되는 확률 P ( X = x ) 을 나타낸다:\n이산확률변수 X 의 확률변수는 다음 조건을 만족해야 한다:\n모든 확률은 0 이상 1 이하의 값을 가진다. 확률변수가 가질 수 있는 모든 값에 대한 확률의 합은 1 이다."
  },
  {
    "objectID": "da/ida/ida_06.html#이산확률변수의-평균과-표준편차",
    "href": "da/ida/ida_06.html#이산확률변수의-평균과-표준편차",
    "title": "6장: 확률분포",
    "section": "3. 이산확률변수의 평균과 표준편차",
    "text": "3. 이산확률변수의 평균과 표준편차\n\n3-1. 기댓값\nExpected Value\nE ( X ) 는 확률변수 X 의 “기댓값(평균)” 또는 X 가 갖는 확률분포의 “모평균” 이라고 한다.\n뮤( M , μ: 그리스 알파벳의 열두째 글자)\n3-2. 모분산 Population Variance\n편차 (즉, 각 값이 기대값에서 얼마나 떨어져 있는지)를 제곱하고, 그 제곱된 값을 각 값이 발생할 확률로 가중평균하는 것이다.\nV a r ( X ) = ∑ ( 편차 ) ² × 확률:\n시그마(Σ, σ: 그리스어 알파벳의 열여덟째 글자)\n모분산의 간편식:\n기댓값을 알고 있다면, 직접적인 정의를 사용하지 않고 게산할 수 있다.\n\n\n3-3. 모표준편차\nPopulation Standard Deviation 모분산의 양의 제곱근으로 계산된다:\n모표준편차 ( σ ) 의 단위는 확률변수 X와 “동일” 하다. 반면 모분산 ( σ² ) 의 단위는 X 의 단위를 “제곱” 한 것이므로, 퍼진정도를 측정하는데 적절하지 않다.\n예를 들어, X 의 단위가 센티미터( cm )라면, 모분산 σ² 의 단위는 제곱센티미터( cm² )가 된다."
  },
  {
    "objectID": "da/ida/ida_06.html#두-확률분포의-결합분포",
    "href": "da/ida/ida_06.html#두-확률분포의-결합분포",
    "title": "6장: 확률분포",
    "section": "4. 두 확률분포의 결합분포",
    "text": "4. 두 확률분포의 결합분포\n하나의 실험에서도 2 개 이상의 측면에 대한 관측이 가능하다. 이 경우 그 2 가지 특성 간의 관계 여부 및 그 관계 정도에 대해 분석할 수 있다.\n\n4-1. 결합확률분포\nJoint Probability Distribution\n2개 이상의 확률변수가 동시에 특정한 값을 가질 확률을 나타내는 분포이다.\n2 개의 확률변수가 이산일 경우,\nX 가 취하는 값을 x₁, …, xm Y 가 취하는 값을 y₁, …, yn 이라고 할 때\nX 와 Y 의 결합확률분포는 모든 1 ≤ i ≤ m, 1 ≤ j ≤ n 에 대하여\n위 식을 구하므로써 결정되며, 다음과 같이 표현할 수 있다:\n\n\n4-2. 주변확률분포\nMarginal Probability Distribution\n결합확률분포에서 한 확률변수를 “고정” 하고 다른 변수의 분포를 고려하는 분포이다. 이는 2 개 이상의 확률변수 중 하나에 대한 “단일 확률분포” 를 얻기 위해 사용한다:\n각각의 주변확률을 이용해서 하나의 변수 때와 마찬가지로 구하면 된다:"
  },
  {
    "objectID": "da/ida/ida_06.html#공분산과-상관계수",
    "href": "da/ida/ida_06.html#공분산과-상관계수",
    "title": "6장: 확률분포",
    "section": "5. 공분산과 상관계수",
    "text": "5. 공분산과 상관계수"
  },
  {
    "objectID": "da/ida/ida_06.html#공분산",
    "href": "da/ida/ida_06.html#공분산",
    "title": "6장: 확률분포",
    "section": "5-1. 공분산",
    "text": "5-1. 공분산\nCovariance\n두 확률변수 X 와 Y 가 함께 변하는 정도를 측정한다.\nX, Y가 같은 방향으로 변화할 경우, (즉, 둘 다 증가하거나 둘 다 감소하는 경우) ( X − μX ​), ( Y − μY ​) 의 부호가 일치할 확률이 상대적으로 커진다. 따라서, 이에 대한 기댓값은 양수가 된다.\nX, Y가 다른 방향으로 변화할 경우, (즉, 한 변수가 증가할 때 다른 변수는 감소하는 경우) ( X − μX ​), ( Y − μY ​) 가 서로 다른 부호를 갖게 될 확률이 상대적으로 커진다. 따라서 이에 대한 기댓값은 음수가 될 것이다.\nX, Y 의 공분산은 아래와 같이 정의된다:"
  },
  {
    "objectID": "da/ida/ida_06.html#상관계수",
    "href": "da/ida/ida_06.html#상관계수",
    "title": "6장: 확률분포",
    "section": "5-2. 상관계수",
    "text": "5-2. 상관계수\nCorrelation Coefficient\n두 확률변수 간의 선형 관계의 강도와 방향을 측정한다. 이것은 공분산을 표준화한 형태로, −1 과 1 사이의 값을 가진다.\n상관계수의 성질:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n정확한 선형관계 Y = aX + b 가 성립할 때, 상관계수의 값은 −1 또는 1 이다.\nX, Y 의 상관계수는 각 확률변수에 상수가 더해지거나 감해지는 것에 영향을 받지 않는다. 상수가 곱해진 경우, 그 부호에만 영향을 받는다.\n상수 c 와 d 의 부호가 다르면 상관계수의 부호가 반대가 된다."
  },
  {
    "objectID": "da/ida/ida_06.html#두-확률변수의-독립성",
    "href": "da/ida/ida_06.html#두-확률변수의-독립성",
    "title": "6장: 확률분포",
    "section": "6. 두 확률변수의 독립성",
    "text": "6. 두 확률변수의 독립성\n2 개의 확률변수 X, Y 가 독립이 되기 위해서는 X, Y 가 취하는 모든 쌍의 값 ( xi, yi ) 에 대해 아래 식을 만족해야 한다.\n두 확률변수 X, Y 가 서로 독립일 때, 아래의 식이 성립한다:\n단, 공분산과 상관계수가 0 이라는 사실이 항상 두 변수가 독립이라는 것을 보장하지 않는다. 이는 두 변수 간에 선형 관계가 없음을 의미하지만, 비선형 관계가 존재할 수 있다.\n두 확률변수가 독립일 경우, 공분산이 0 이 되므로 합과 차의 분산을 쉽게 계산할 수 있다.\n분산과 공분산의 정의를 이용하면: 공분산 항을 제외하여 다음과 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida/ida_04.html",
    "href": "da/ida/ida_04.html",
    "title": "4장: 두 변수 자료의 요약",
    "section": "",
    "text": "조사 대상의 각 개체로부터 둘 또는 그 이상의 변수들을 동시에 관측하는 경우가 더 많다. 두 변수에 관한 관측값을 도표로 요약하고 해석하는 방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_04.html#자료의-입력",
    "href": "da/ida/ida_04.html#자료의-입력",
    "title": "4장: 두 변수 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제5: 통계학과 신입생 51명의 키와 몸무게를 기록한 것이다.\n# 키와 몸무게의 표본상관계수를 구하고, 산점도를 그려라. (p.108)\n\nimport numpy as np\n\n# 키와 몸무게 자료의 입력\nheight = np.array([181,161,170,160,158,168,162,179,183,178,171,177,163,\n                   158,160,160,158,173,160,163,167,165,163,173,178,170,\n                   167,177,175,169,152,158,160,160,159,180,169,162,178,\n                   173,173,171,171,170,160,167,168,166,164,173,180]) \n\nweight = np.array([78,49,52,53,50,57,53,54,71,73,55,73,51,53,65,48,59,\n                   64,48,53,78,45,56,70,68,59,55,64,59,55,38,45,50,46,\n                   50,63,71,52,74,52,61,65,68,57,47,48,58,59,55,74,74])"
  },
  {
    "objectID": "da/ida/ida_04.html#표본상관계수",
    "href": "da/ida/ida_04.html#표본상관계수",
    "title": "4장: 두 변수 자료의 요약",
    "section": "2. 표본상관계수",
    "text": "2. 표본상관계수\nSample Correlation Coefficient\n산점도에서 점들이 얼마나 “직선에 가까운가” 의 정도를 나타내는 데 쓰이는 측도.\n두 변수 ( x, y ) 에 대하여 관측값 n개의 짝 ( x₁, y₁ ), ( x₂, y₂ ), …, (xn, yn) 이 주어진 때, 상관계수는 다음과 같이 계산된다:\n\n2-1. 공분산\nCovariance\n분산(Variance)은 한 변수의 데이터가 평균 주위에서 얼마나 흩어져 있는지를 나타낸다. 공분산은 “두 변수” 가 함께 변하는 정도를 나타낸다.\n공분산이 “양수” 이면, 두 변수는 “같은 방향” 으로 변한다. 공분산이 “음수” 이면, 두 변수는 “반대 방향” 으로 변한다. 공분산이 “0” 에 가까우면, 두 변수 간에 “선형 관계가 거의 없음” 을 의미한다.\n위 값들은 다음과 같은 수식을 통해 계산된다:\nx̄, ȳ 는 변수 x, y 의 표본 평균을 의미한다:\n표본상관계수는 두 변수의 직선관계의 정도(강도, 방향)를 나타내며, 다음과 같은 특징이 있다:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n계수가 “0” 에 가까울수록 두 변수 간의 직선의 관계가 “매우 약함” 을 의미한다. 표본상관계수의 단위는 없다.\n각 변수의 편차들의 곱을 사용해 계산할 때, 이 곱은 원래 변수들의 단위를 곱한 단위이지만, 분모에 각 변수의 분산을 곱한 값의 제곱근이 들어가기 때문에 단위가 사라지게 된다.\n위 연산은 단위를 제곱근 단위로 바꾸는 역할을 하며, 이로 인해 단위가 없는 숫자가 된다. 변수들의 단위에 영향을 받지 않아, 서로 다른 단위를 가진 변수들 간에도 관계를 비교할 수 있다.\n\n# 키와 몸무게의 표본상관계수\nnp.corrcoef(height, weight)[0][1]\n\n## 해석: 표본상관계수 𝑟이 약 0.74로 나왔다는 것은\n## 두 변수 사이에 강한 '양의 선형 관계'가 있음을 의미한다.\n\nnp.float64(0.7362765055636866)\n\n\n아래에서 설명할 산점도를 포함한 대부분의 그림 요약 방법은 “주관적” 일 수 있다. 반면, 표본상관계수는 “객관적” 인 수치 자료로, 이러한 문제를 “보완” 해준다.\n단, 직선이 아닌 다른 관계(곡선 등)가 있을 수 있으며, 이를 표본상관계수가 제대로 나타내지 못할 수 있다는 점을 유념해야 한다."
  },
  {
    "objectID": "da/ida/ida_04.html#산점도",
    "href": "da/ida/ida_04.html#산점도",
    "title": "4장: 두 변수 자료의 요약",
    "section": "3. 산점도",
    "text": "3. 산점도\nScatter Plot\n변수 x 를 “수평축” 에 놓고, 변수 y 를 “수직축” 에 놓은 후에 각 관측값의 찍을 좌표 위에 표시함 으로써 얻게 되는 그림.\n이를 통해, “두 변수 간의 관계” 를 시각적으로 대략 파악할 수 있다.\n\nimport matplotlib.pyplot as plt\n\n# 산점도 작성\nplt.figure(figsize=(8, 6))  # 그래프의 크기 (가로 8, 세로 6)\nplt.scatter(height, weight, color='slateblue')  # 산점도 색상 설정\n\nplt.xlabel('height (cm)')  # x축 레이블\nplt.ylabel('weight (kg)')  # y축 레이블\nplt.title('height(cm) and weight(kg)')  # 그래프의 제목\n\nplt.grid(True)  # 그리드 추가 (경계선)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html",
    "href": "da/ida/ida_02.html",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "",
    "text": "자료를 효과적으로 요약하고 이해하기 위해 표나 그림을 사용하는 방법에 대해 다루고자 한다. 자료 요약은 분석 대상인 자료의 형태와 특성에 따라 다양한 방법으로 이루어진다."
  },
  {
    "objectID": "da/ida/ida_02.html#자-료-의-입-력",
    "href": "da/ida/ida_02.html#자-료-의-입-력",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "da/ida/ida_02.html#도수분포표",
    "href": "da/ida/ida_02.html#도수분포표",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table\n특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1"
  },
  {
    "objectID": "da/ida/ida_02.html#막대-그래프",
    "href": "da/ida/ida_02.html#막대-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart\n데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\2514237286.py:19: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`"
  },
  {
    "objectID": "da/ida/ida_02.html#원형-그래프",
    "href": "da/ida/ida_02.html#원형-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart\n전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html#파레토그림",
    "href": "da/ida/ida_02.html#파레토그림",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart\n막대그래프와 누적선그래프를 결합한 형태의 그래프.\n파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html#도수다각형",
    "href": "da/ida/ida_02.html#도수다각형",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency Polygon\n도수분포표의 도수를 선으로 연결한 그래프.\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "da/ada/ada_06_0.html",
    "href": "da/ada/ada_06_0.html",
    "title": "두 모집단에 대한 비교",
    "section": "",
    "text": "두 모집단의 모평균, 모비율, 모분산의 차이에 대한 가설검증 문제를 다루고자 한다. (12장: 두 모집단의 비교와 이어지는 내용이다.)\n표본 평균을 추정하려면, 표본의 크기와 모분산을 고려해야 한다.\n[1] 두 모분산 σ12, σ 22 이 모두 알려져 있는 경우,\n두 모평균 차에 대한 “추정량” ⇨ “두 표본평균의 차” 통계적 추론을 위한 “준비물” ⇨ “추정량의 분포”\n이 분포는 다음과 같은 평균과 분산을 가진 정규분포를 따른다:\n표준화된 확률변수 Z는 표준정규분포 N(0, 1)를 따른다.\n[2] 두 모분산 σ12, σ 22 을 모두 모르는 경우, 표본의 크기를 고려하게 된다. 표본의 크기가 충분히 큰 경우 ( 25 이상 )\n중심극한정리에 의해 모집단의 분포에 관계없이 x̄ 와 ȳ 가 근사적으로 정규분포를 따른다. 두 모분산의 추정치인 표본분산 s₁², s₂² 를 고려한 통계량을 사용하여 검정을 수행한다.\n[3] 두 모집단이 알려져 있지는 않지만, 모분산이 동일한 것으로 가정할 수 있는 경우, 다음과 같은 평균과 분산을 가지는 정규분포를 따르며, [1]과 동일하다.\n공통분산 σ ² 의 합동추정량 (Pooled Variance) 자유도 n ₁ + n ₂ – 2인 t-분포를 따른다.\n[4] 두 모분산이 서로 다른 경우, [3]번 식은 t-분포를 따르지 않는다. 단, 아래와 같이 자유도를 수정할 경우, 근사적으로 t-분포를 따르게 된다. 근사적으로 t-분포를 따르게 된다. 수정된 자유도(df)."
  },
  {
    "objectID": "da/ada/ada_06_0.html#독립표본-t검정",
    "href": "da/ada/ada_06_0.html#독립표본-t검정",
    "title": "두 모집단에 대한 비교",
    "section": "독립표본 t–검정",
    "text": "독립표본 t–검정\n독립표본에 의한 두 모평균의 비교 두 개의 서로 독립적인 집단의 평균을 비교하여 그 차이가 통계적으로 유의한지 판단하는 방법이다.\n사례: 새로운 강의방식이 초등학생 독해력 향상에 도움이 되는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Reading.csv\"\nReading = pd.read_csv(url)\nReading.head()\n\n\n\n\n\n\n\n\nID\nGroup\nScore\n\n\n\n\n0\n1\nNew\n75\n\n\n1\n2\nNew\n80\n\n\n2\n3\nNew\n72\n\n\n3\n4\nNew\n77\n\n\n4\n5\nNew\n69\n\n\n\n\n\n\n\n가설검증을 결정하기 전에 데이터를 시각화한다.\n\nimport seaborn as sns  # 박스 플롯\nsns.boxplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n중위수와 같은 요인을 비교한 결과, 차이가 나타나므로 이를 근거로 검증을 진행할 수 있다.\n\n# 바이올린 플롯\nsns.violinplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n\n# 기술통계량\nReading.groupby('Group').Score.describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nGroup\n\n\n\n\n\n\n\n\n\n\n\n\nNew\n8.0\n75.375\n4.373214\n69.0\n71.75\n76.0\n78.50\n81.0\n\n\nOld\n8.0\n69.125\n4.086126\n63.0\n67.25\n69.0\n71.25\n76.0\n\n\n\n\n\n\n\n양측검정 적용.\n\n# 그룹 나누기\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n# 양측검증:\n# 두 강의 방식에 차이가 있다. vs 차이가 없다.\n\nfrom scipy.stats import ttest_ind  # 독립 t-검정\nttest_ind(New.Score, Old.Score, equal_var = True)\n\n\n# T통계량: 그룹 간 평균 차이가 실제로 존재하는지를 나타내는 통계량.\n# 통계량이 클수록 차이가 있을 가능성이 높다.\n\n# [3]번 통계량: statistic=2.9536127902039953\n\n\n# 두 꼬리 검정에서의 p-값: pvalue=0.010470744188033123\n\n# 통상적으로 p-값이 0.05보다 작으면 귀무가설을 기각할 수 있다. \n# 즉, 두 강의 방식에 차이가 있다고 결론 내릴 수 있다.\n\nTtestResult(statistic=np.float64(2.9536127902039953), pvalue=np.float64(0.010470744188033123), df=np.float64(14.0))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\n# 단측검정:\n# 새로운 학습법이 더 효과적이다. vs 효과적이지 않다.\n\nstat, pval = ttest_ind(New.Score, Old.Score, equal_var = True)\nprint(\"P\", pval/2)\n\n# p-값이 0.0052로 유의수준 0.05보다 작으므로, 대립가설을 채택할 수 있다.\n\nP 0.005235372094016561\n\n\n단측검정과 등분산 가정 적용.\n\n# 단측검정\nfrom statsmodels.stats.weightstats import ttest_ind\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'pooled') # 등분산 가정 적용:\n          # 두 그룹 간의 분산이 동일하다고 가정\n\n(np.float64(2.9536127902039953),\n np.float64(0.005235372094016561),\n np.float64(14.0))\n\n\n단측검정과 이분산 가정 적용.\n\n# 단측검정\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'unequal') # 이분산 가정 적용:\n          # 두 그룹의 분산이 서로 다르다는 가정\n\n# [4]번 통계량: usevar= 'pooled' ⇨ 'unequal'\n# 14 ⇨ 13.935945095796395 (자유도가 실수로 바뀜)\n\n(np.float64(2.9536127902039953),\n np.float64(0.005256688626975243),\n np.float64(13.935945095796395))\n\n\n결론적으로, 새로운 강의방식이 초등학생 독해력 향상에 도움이 된다고 할 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#대응표본-t검정",
    "href": "da/ada/ada_06_0.html#대응표본-t검정",
    "title": "두 모집단에 대한 비교",
    "section": "대응표본 t–검정",
    "text": "대응표본 t–검정\n대응표본에 의한 두 모평균의 비교 어떤 신발의 마모율을 비교할 때, 독립 표본 검정에 경우, 한 그룹의 사람이 왼쪽 신발을 신고, 다른 그룹의 사람이 오른쪽 신발을 신더라도 상관이 없다. 하지만 대응 표본 검정은 동일한 사람이 왼쪽 신발과 오른쪽 신발을 모두 신어야 만 한다. 각 쌍이 서로 연관되어 있으므로 두 신발을 신는 사람이 동일해야 하며, 표본의 수도 일치해야 한다.\n이는 마모율에 영향을 줄 수 있는 교락 요인(confounding factor), 즉 신발을 신는 사람의 특성 등을 배제하기 때문이다.\n그러므로, 대응 표본 검정은 같은 대상에 대한 실험 전후의 결과를 비교할 때 주로 사용된다.\n\n사례: 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있는가?\n\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Paired.csv\"\nPaired = pd.read_csv(url)\nPaired.head()\n\n\n\n\n\n\n\n\nID\nPretest\nPosttest\n\n\n\n\n0\n1\n80\n82\n\n\n1\n2\n73\n71\n\n\n2\n3\n70\n95\n\n\n3\n4\n60\n69\n\n\n4\n5\n88\n100\n\n\n\n\n\n\n\n박스플롯 시각화 및 기술 통계량 출력.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Pretest와 Posttest에 대한 박스플롯 시각화\nsns.boxplot(data = Paired.iloc[:, [1, 2]], \n            orient = 'h') # 수평 방향\n\n# Pretest와 Posttest의 차이 계산 및 새로운 열(Diff) 추가\nPaired[\"Diff\"] = Paired.Pretest - Paired.Posttest \n                # = 교육 전 성적 - 교육 후 성적\n                # 교육이 효과가 있다면 교육 후 성적이 더 높을 것이므로\n                # 결과적으로는 변수 Diff의 값이 음수로 나와야 한다.\n\n\n\n\n\n\n\n\n두 변수에 대한 상자 그림\n\nPaired.iloc[:,1:4].describe()\n\n# 변수 Diff 평균(mean)이 -7.93이며\n# 실제로 그래프 상에서도 대부분의 개체에서 \n# 변수 Diff의 값이 0보다 작음을 볼 수 있다.\n\n\n# 표준편차(std)는 데이터의 산포도(변동성)를 측정하는 지표로, \n# 데이터가 평균으로부터 얼마나 떨어져 있는지를 나타낸다. \n\n# 표준편차는 항상 0 이상의 값을 가지며, 음수가 될 수 없다. \n# 이는 표준편차가 데이터 값의 차이를 제곱하여 계산하기 때문이다.\n\n\n\n\n\n\n\n\nPretest\nPosttest\nDiff\n\n\n\n\ncount\n15.000000\n15.000000\n15.000000\n\n\nmean\n70.266667\n78.200000\n-7.933333\n\n\nstd\n18.041487\n14.313829\n9.931671\n\n\nmin\n37.000000\n60.000000\n-25.000000\n\n\n25%\n59.500000\n67.000000\n-12.500000\n\n\n50%\n73.000000\n75.000000\n-7.000000\n\n\n75%\n82.000000\n90.500000\n-2.500000\n\n\nmax\n98.000000\n100.000000\n13.000000\n\n\n\n\n\n\n\n히스토그램 및 커널 밀도 추정(KDE) 시각화\n\nsns.distplot(Paired.Diff)\n\n# Seaborn의 최신 버전에서는 더 이상 지원되지 않으므로,\n# sns.histplot 또는 sns.kdeplot을 사용하는 것이 권장된다.\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18852\\4004193321.py:1: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 히스토그램 그리기\nsns.histplot(Paired.Diff, \n             stat = 'density')  # y축을 밀도로 변경\n\n# KDE만 수정하기 위해 따로 그리기\nsns.kdeplot(Paired.Diff, \n            fill = True) # 음영 처리\n\nplt.xlim(-40, 30)     # x축 범위 설정\n\n\n\n\n\n\n\n\n양측검정 적용\n\n# ttest_rel에서 rel은 paired 또는 related를 의미한다.\n\n# 이 함수는 대응표본 t-검정을 수행하는 것으로, \n# 두 관련된 표본에 대한 평균의 차이를 비교하는 데 사용된다.\n\n\nfrom scipy.stats import ttest_rel\nttest_rel(Paired.Pretest, Paired.Posttest)\n\n# p-값이 0.0079(0.79%)로 0.05(5%)보다 작기 때문에 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다. \n# 이는 두 표본 간에 유의미한 차이가 있음을 의미한다.\n\nTtestResult(statistic=np.float64(-3.093705670004429), pvalue=np.float64(0.007930923229026533), df=np.int64(14))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\nstat, pval = ttest_rel(Paired.Pretest, Paired.Posttest)\nprint(\"one-sided p-value =\", pval/2)\n\n# 이 경우에도, p-값이 0.05보다 작으므로 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다.\n\none-sided p-value = 0.003965461614513267\n\n\n결론적으로 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있으며, 사후 테스트의 결과가 더 좋다고 할 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#피셔의-정확검정",
    "href": "da/ada/ada_06_0.html#피셔의-정확검정",
    "title": "두 모집단에 대한 비교",
    "section": "피셔의 정확검정",
    "text": "피셔의 정확검정\nFisher's Exact Test\n독립표본에 의한 두 모비율의 비교 두 모비율에 대한 검정을 수행하기 위해 사용할 수 있는 대표적인 검정법은 두 독립된 이항분포의 비율에 대한 z-검정이다.\n사례: 현 정부에 대한 지지율이 성인 남녀별로 차이가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Support.csv\"\nSupport = pd.read_csv(url)\nSupport.head()\n\n\n\n\n\n\n\n\nID\nGender\nYesNo\n\n\n\n\n0\n1\nMale\nNo\n\n\n1\n2\nFemale\nYes\n\n\n2\n3\nFemale\nNo\n\n\n3\n4\nFemale\nNo\n\n\n4\n5\nFemale\nNo\n\n\n\n\n\n\n\n이 데이터에 대한 2차원 분할표(빈도표) 작성하기.\n\nimport pandas as pd\nSupportTable = pd.crosstab(index = Support[\"Gender\"],\n                           columns = Support[\"YesNo\"])\n\nSupportTable\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n96\n104\n\n\nMale\n140\n110\n\n\n\n\n\n\n\n행 백분율 계산하기.\n\npd.crosstab(index=Support[\"Gender\"], columns=Support[\"YesNo\"],\n           normalize = \"index\") # 각 행의 합을 기준으로 비율을 계산\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n0.48\n0.52\n\n\nMale\n0.56\n0.44\n\n\n\n\n\n\n\n다음과 같은 교차 테이블(Cross Table)을 만들 수 있다.\n양측검증 적용.\n\nfrom scipy.stats import fisher_exact\nfisher_exact(SupportTable, \n             alternative = 'two-sided')\n             \n# 이 결과는 검정 통계량이 0.725이고 \n# p-값이 0.106(10.6%)이다.\n\n# 이는 일반적으로 사용되는 유의 수준 0.05(5%)에서 \n# 통계적으로 유의하지 않다는 것을 의미한다. \n\n# 결론적으로, 두 그룹(또는 변수) 간에 유의한 차이 또는 \n# 연관성을 찾지 못했다는 것을 나타낸다.\n\nSignificanceResult(statistic=np.float64(0.7252747252747253), pvalue=np.float64(0.10634531219761142))\n\n\n정규 근사 검정\n이항분포의 표본 크기 n이 충분히 크면, 이항분포는 정규분포로 근사할 수 있으며, 이를 정규 근사라고 한다. 일반적으로 n×p와 n×(1 − p)가 모두 5 이상이면, 정규분포로 근사할 수 있다고 간주한다.\n\n\\(p\\): 성공 확률\n\n이러한 정규화된 변수를 제곱하면, 자유도가 1인 카이제곱 분포를 따른다. 카이제곱검정(Chi-Square Test) 적용.\n\nfrom scipy.stats import chi2_contingency\nchi2_contingency(SupportTable)\n\n# 카이제곱 통계량: 2.54\n# 유의 수준이 일반적으로 0.05(5%)인 경우, \n# p-값이 0.111(11.1%)이므로 귀무가설을 기각할 수 없다.\n\n# 따라서 이 결과는 두 변수 간에 통계적으로 \n# 유의한 연관성이 없다고 결론지을 수 있다. \n\n# 즉, 이 교차표에 따르면 두 변수는 독립적이다.\n\nChi2ContingencyResult(statistic=np.float64(2.5395141968952935), pvalue=np.float64(0.1110289428837834), dof=1, expected_freq=array([[104.88888889,  95.11111111],\n       [131.11111111, 118.88888889]]))\n\n\n결론적으로, 현 정부에 대한 지지율이 성인 남녀별로 차이가 없다고 할 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#맥니머-검정",
    "href": "da/ada/ada_06_0.html#맥니머-검정",
    "title": "두 모집단에 대한 비교",
    "section": "맥니머 검정",
    "text": "맥니머 검정\n대응표본에 의한 두 모비율의 비교\n맥니머 검정은 피셔의 정확검정이나 카이제곱 검정과 달리 대응 표본에 적용할 수 있는 검정이다. 이 검정은 대응 표본 t-검정과 유사하게 교락 효과를 제거하는 것이 중요하다.\n독립 표본의 경우, 한 사람이 A, B 제품 모두를 사용하지 않아도 무방하다. 그러나 대응 표본에서는 한 사람이 반드시 두 제품 모두를 사용해야 한다.\n사례: 정부에서 정책 발표 후 지지율에 변화가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Prepost.csv\"\nPrepost = pd.read_csv(url)\nPrepost.head()\n\n\n\n\n\n\n\n\nID\nPre\nPost\n\n\n\n\n0\n1\nYes\nYes\n\n\n1\n2\nNo\nNo\n\n\n2\n3\nYes\nNo\n\n\n3\n4\nNo\nNo\n\n\n4\n5\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\n\nPrepostTable = pd.crosstab(index = Prepost[\"Pre\"], \n                           columns = Prepost[\"Post\"], \n                           margins = True, # 각 행과 열의 합계 추가\n                           margins_name = \"합계\")\nPrepostTable\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n18\n27\n45\n\n\nYes\n8\n67\n75\n\n\n합계\n26\n94\n120\n\n\n\n\n\n\n\n\npd.crosstab(index=Prepost[\"Pre\"], columns=Prepost[\"Post\"], \n            margins=True, margins_name=\"합계\", \n            normalize=\"all\") # 전체 데이터에 대한 비율 변환\n            \n# 정책 발표 이전 지지율(pre): 62.5%\n# 정책 발표 이후 지지율(post): 78.3%\n# 결과적으로 15.8%p가 상승하였음을 볼 수 있다.\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n0.150000\n0.225000\n0.375\n\n\nYes\n0.066667\n0.558333\n0.625\n\n\n합계\n0.216667\n0.783333\n1.000\n\n\n\n\n\n\n\n\n# pip install statsmodels\n\nfrom statsmodels.stats.contingency_tables import mcnemar\nprint(mcnemar(PrepostTable, \n              exact = True)) # 이항분포 기반의 정확 검정 방법\n              \n# 0.001(0.1%) &lt; 0.05(5%)\n\nprint(mcnemar(PrepostTable, \n              exact=False)) # 카이제곱분포를 사용한 근사 검정 방법\n              \n# 0.002(0.2%) &lt; 0.05(5%)\n\npvalue      0.0018782254774123432\nstatistic   8.0\npvalue      0.0023457869795667934\nstatistic   9.257142857142858\n\n\n결론적으로, 정부에서 정책 발표 전후 지지율에 변화가 있으며, 정책 발표 후에 지지율이 상승한 것으로 볼 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#f검정",
    "href": "da/ada/ada_06_0.html#f검정",
    "title": "두 모집단에 대한 비교",
    "section": "F–검정",
    "text": "F–검정\nF–test\n모분산의 동일성에 대한 검정 가장 일반적인 검정 방법으로, 두 집단의 모분산이 동일한지 평가한다. 두 집단의 분산 비율을 계산하고, 이를 기반으로 F–분포를 사용하여 p–값을 구한다.\nReading 데이터의 모분산이 다른가?\n이전에 다루었던 Reading 데이터에 대해 분산의 동일성 검정을 위한 사용자 정의 함수를 작성하고, 가설검정을 수행하였다.\n\nimport pandas as pd\n\n# file_path = os.path.join('data', 'Reading.csv')\n# Reading = pd.read_csv(file_path)\n\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n\nimport numpy as np\nfrom scipy import stats\n\ndef F_test(x, y):\n    f = np.var(x, ddof = 1)/np.var(y, ddof = 1)\n    df1 = x.size -1 \n    df2 = y.size -1 \n    p = 2*(1-stats.f.cdf(f, df1, df2))\n    return f, p\n\nF_test(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\n(1.1454545454545453, np.float64(0.8624138071371459))\n\n\n\nBartlett’s Test\n\nfrom scipy import stats\nstats.bartlett(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nBartlettResult(statistic=np.float64(1.2110354068328009), pvalue=np.float64(0.27112715913152846))\n\n\n\n\nLevene’s Test\n\nstats.levene(New.Score, Old.Score)\n\n# 0.6(60%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nLeveneResult(statistic=np.float64(0.1978798586572438), pvalue=np.float64(0.6632376240724351))\n\n\n결론적으로, 두 집단의 모분산이 다르다고 말할 수 없다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "da/ida/ida_03.html",
    "href": "da/ida/ida_03.html",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "",
    "text": "연속형 자료가 어떤 값을 중심으로 분포되어 있는가를 나타내는 중심위치의 측도, 각 자료가 중심위치의 값으로부터 흩어진 정도를 나타내는 퍼진 정도의 측도 등을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_03.html#자료의-입력",
    "href": "da/ida/ida_03.html#자료의-입력",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제13: 정량 100인 음료수 80병을 임의로 추출하여 그 내용물의 실제 측정된 양을 잰 자료이다.(p.42)\n\nimport numpy as np\n\n# 변수 drink에 NumPy 배열을 할당\ndrink = np.array([98, 99, 100, 99, 99.4, 101.7, 98.8, 101.8, 101.5, \n                 101.8, 102.6, 101, 98.8, 101.4, 99.7, 99.7, 99.7, \n                 100.9, 98.6, 101.4, 102.1, 102.9, 100.8, 101.8, \n                 100, 101.2, 100.5, 101.2, 100.1, 101.6, 101.3, 99.9, \n                 99.4, 99.3, 99.4,101.6, 96.1, 100, 99.7, 99.1, 100.7, \n                 100.8, 100.8, 95.5,100.1, 100.5, 98.9, 99.9, 96.8, \n                 102.4, 100, 103.7, 101.4,99.7, 97.4, 99.5, 97.5, \n                 99.9, 100.3, 100.2, 101.5, 99.4, 99.7, 98.2, 100.3, \n                 100.2, 100.5, 100.4, 101.5, 98.4, 101.4, 98.8, 100.9, \n                 101.1, 100.9, 98.1, 98.7, 99.2, 98.1, 97.2])\n\n중심위치의 측도"
  },
  {
    "objectID": "da/ida/ida_03.html#평균-mean",
    "href": "da/ida/ida_03.html#평균-mean",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "2. 평균 (Mean)",
    "text": "2. 평균 (Mean)\n총 자료의 개수 \\(n\\) 을 모든 관측값 \\(x_1, x_2, \\dots , x_n\\) 의 합으로 나눈 값. 이를 “산술 평균” 이라고도 하며, 공식으로 표현하면 다음과 같다:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n표본 평균은 관측값의 산술평균이며, “극단값” 에 영향을 받는다.\n\n표본 평균: 전체 데이터인 “모집단” 에서 추출한 일부 데이터의 평균을 의미한다. (예시1) 예제13 에서 추출된 80 병의 음료수는 표본에 해당된다.\n극단값(Outlier): 데이터 집합에서 다른 값들과 현저히 다른 값들을 의미한다. (예시2) 데이터 집합 { 11, 12, 13, 300 } 에서 극단값은 300 이다.\n\n\n# 2장의 예제 13에서 주어진 음료수 한 병의 부피 데이터를 기반으로 \n# 평균, 중앙값, 분산, 표준편차, 범위, 사분위수범위를 파이썬을 이용하여 계산하라.(p.83)\n\n## numpy 모듈을 이용하여 계산할 수 있음. ##\n\n# 평균\nprint(np.mean(drink))\n\n100.04125"
  },
  {
    "objectID": "da/ida/ida_03.html#중앙값median",
    "href": "da/ida/ida_03.html#중앙값median",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "3. 중앙값(Median)",
    "text": "3. 중앙값(Median)\n전체 관측값을 크기 순서로 배열했을 때, 가운데에 위치한 값.\n데이터의 개수가 홀수 일 경우, 중앙에 위치한 값이, 짝수 일 경우, 중앙에 위치한 두 값의 평균 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, 극단값의 영향을 받지 않는다. 따라서, 평균과 값이 다를 수 있다.\n\n# 중앙값 계산\nprint(np.median(drink))\n\n100.05\n\n\n퍼진 정도의 측도"
  },
  {
    "objectID": "da/ida/ida_03.html#분-산-variance",
    "href": "da/ida/ida_03.html#분-산-variance",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "4. 분 산 (Variance)",
    "text": "4. 분 산 (Variance)\n확률 분포나 데이터 집합의 산포도(분포도)를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 지표로 사용된다.\n관측값이 \\(x_1, x_2, \\dots , x_n\\) 이고, 표본평균이 \\(\\bar{x}\\) 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평균제곱 오차의 평균: 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n\n4-1. 편차(Deviation):\n각 관측값과 평균의 차이\n편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로,  따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n\n\n4-2. 자유도\nDegrees of Freedom\n위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "da/ida/ida_03.html#표준편차",
    "href": "da/ida/ida_03.html#표준편차",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "5. 표준편차",
    "text": "5. 표준편차\nStandard Deviation\n분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다.\n\n# 표준편차 계산 (표본의 표준편차, 자유도를 1로 설정)\nprint(np.std(drink, ddof=1))\n\n# 표준편차 계산 (모집단의 표준편차)\nprint(np.std(drink))\n\n1.521882058505192\n1.5123403841397614\n\n\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다."
  },
  {
    "objectID": "da/ida/ida_03.html#범위",
    "href": "da/ida/ida_03.html#범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "6. 범위",
    "text": "6. 범위\nRange\n관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다.\n\n# 범위 계산 (최대값 - 최소값)\nprint(np.max(drink) - np.min(drink))\n\n8.200000000000003"
  },
  {
    "objectID": "da/ida/ida_03.html#사분위수범위",
    "href": "da/ida/ida_03.html#사분위수범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "7. 사분위수범위",
    "text": "7. 사분위수범위\nQuartile\n전체 관측값을 작은 순서로 배열 하였을 때, 전체를 사등분 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n\n7-1. 사분위수범위\nInterquartile Range, IQR\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 std 는 자동적으로 n – 1 로 나누어져서 계산된다."
  },
  {
    "objectID": "da/ida/ida_05.html",
    "href": "da/ida/ida_05.html",
    "title": "5장: 확률",
    "section": "",
    "text": "통계적인 추론을 통해서도 모집단에 대한 다양한 정보를 얻을 수 있다. 그 통계적 추론의 기초가 되는 확률이론에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_05.html#사건의-확률",
    "href": "da/ida/ida_05.html#사건의-확률",
    "title": "5장: 확률",
    "section": "1. 사건의 확률",
    "text": "1. 사건의 확률\n동일 조건하에서 한 가지 실험을 반복할 때, 전체 실험 횟수에서 그 사건이 일어나리라고 예상되는 횟수의 비율을 말한다. \n사건을 \\(A\\) 라고 하면, 사건 \\(A\\) 의 확률은 \\(P(A)\\) 로 표시한다.\n\n1-1. 표본공간\nSample Space: Ω\n한 실험에서 나올 수 있는 모든 결과들의 모임. 유한표본공간 ( Finite Sample Space )\n주사위 던지기의 표본공간\n연속표본공간 ( Continuous Sample Space )\n0과 1 사이의 모든 실수"
  },
  {
    "objectID": "da/ida/ida_05.html#근원사건",
    "href": "da/ida/ida_05.html#근원사건",
    "title": "5장: 확률",
    "section": "1-2. 근원사건",
    "text": "1-2. 근원사건\nElementary Outcomes: ω ₁ , ω ₂ … \n표본공간을 구성하는 개개의 결과.\n주사위 던지기의 근원사건\n\n1-3. 사건\nEvent: A, B, …\n표본공간의 부분집합으로, 어떤 특성을 갖는 결과들의 모임 (=근원사건들의 집합)\n주사위 던지기의 사건 A, B"
  },
  {
    "objectID": "da/ida/ida_05.html#확률의-법칙",
    "href": "da/ida/ida_05.html#확률의-법칙",
    "title": "5장: 확률",
    "section": "2. 확률의 법칙",
    "text": "2. 확률의 법칙\n위 정의로부터 확률의 특성을 유추할 수 있다.\n사건 A 가 일어날 확률은, 사건 A 에 속하는 근원사건이 일어날 확률의 “합” 과 같다. Ω 를 하나의 사건이라고 하면, 이 사건은 “반드시” 일어나므로 확률은 “1” 이 되어야 한다."
  },
  {
    "objectID": "da/ida/ida_05.html#확률의-계산",
    "href": "da/ida/ida_05.html#확률의-계산",
    "title": "5장: 확률",
    "section": "3. 확률의 계산",
    "text": "3. 확률의 계산\n\n3-1. 균일 확률\n주사위 던지기에서, 각 숫자가 나올 확률 Ω 가 k 개의 원소로 이루어져 있고, 각 근원사건이 일어날 가능성이 “동일” 하다고 가정할 때, 근원사건 중 하나가 일어날 확률은 1 / k 로 주어진다.\n또 사건 A 가 m 개의 근원사건으로 이루어져 있다면, 사건 A 가 일어날 확률은 위와 같다.\n\n\n3-2. 상대도수 수렴치로서의 확률\n주사위를 60번 던져 10번 2가 나왔을 때, 2가 나올 확률 동일한 실험 N 회를 반복할 때, 사건 A 의 상대도수는 위와 같이 표현된다.\nN 이 증가함에 따라 상대도수가 “일정한 값으로 수렴” 한다면, 그 값으로 사건 A 가 일어날 확률 P ( A ) 를 추정한다."
  },
  {
    "objectID": "da/ida/ida_05.html#확률-법칙",
    "href": "da/ida/ida_05.html#확률-법칙",
    "title": "5장: 확률",
    "section": "4. 확률 법칙",
    "text": "4. 확률 법칙\n여 사 건 ( Complementary Event ): 사건 A 가 일어나지 않는 사건 합 사 건 ( Sum Event ): 사건 A 또는 B 가 일어나는 사건 곱 사 건 ( Product Event ): 사건 A 또는 B 가 동시에 일어나는 사건 배 반 사 건 ( Exclusive Event ): 두 사건이 동시에 일어날 수 없는 경우"
  },
  {
    "objectID": "da/ida/ida_05.html#조건부-확률",
    "href": "da/ida/ida_05.html#조건부-확률",
    "title": "5장: 확률",
    "section": "5. 조건부 확률",
    "text": "5. 조건부 확률\nConditional Probability\n한 사건의 결과가 다른 사건의 발생에 영향을 미치는 확률.\n사건 B가 발생했을 때, 사건 A가 발생할 확률:\n곱 사 건 의 확 률 법 칙 ( Multiplication Rule for Probability ) 두 사건이 동시에 발생할 확률을 계산하는 방법. 이 법칙은 두 사건이 독립적인지 여부에 따라 다르게 적용된다:\n\n5-1. 표본공간의 분할\nPartition\n사건 A₁, A₂, …, An 이 서로 배반사건이고, Ω = A₁ ∪ … ∪ An 일 때, 사건 A₁, A₂, …, An 을 Ω 의 분할이라고 한다:\n이때, 각각의 부분집합은 서로 겹치지 않는다:\n예시: 주사위를 던지는 실험에서, Ω 를 두 개의 부분집합으로 나눌 수 있다.\nA₁ ​: 홀수가 나오는 경우 { 1, 3, 5 } A₂ ​: 짝수가 나오는 경우 { 2, 4, 6 }\n이 경우, Ω 는 다음과 같이 표현된다:\n\n\n5-2. 총확률의 법칙\nLaw of Total Probability\n표본공간의 분할 개념을 사용하여 전체 확률을 계산하는 방법.\n사건 A₁, A₂, …, An 이 표본공간의 분할일 때, 임의의 사건 B 의 확률 “P(B)” 는 다음과 같이 계산할 수 있다:\n이를 조건부 확률 ( 종속사건 ) 을 사용하여 다시 쓰면:\n예시: 사건 B 를 “주사위 눈이 4 이하인 사건” 으로 정의하면,\nP(B) = P({ 1, 2, 3, 4 }) = ⅔ P(A₁)​ = P({ 1, 3, 5 }) = ½ P(A₂)​ = P({ 2, 4, 6 }) = ½\n이 경우, P ( B ) 는 다음과 같이 표현된다:\n각 확률 값을 대입해보면:\n\n\n5-3. 베이즈 정리\nBayes’ rule\n사건 A₁, A₂, …, An 이 Ω 의 분할일 때, 임의의 사건 B 에 대하여 다음 식이 성립한다:\n예시: 사건 B 가 발생한 후, 사건 A₁ ​ 이 발생할 확률:"
  },
  {
    "objectID": "da/ida/ida_07.html",
    "href": "da/ida/ida_07.html",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "",
    "text": "모집단의 구성원들이 두 그룹으로 나누어져 있는 경우의 표본추출에서 광범위하게 쓰이는 확률모형과 그의 특징 및 관련된 다른 확률모형들을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_07.html#자료의-입력",
    "href": "da/ida/ida_07.html#자료의-입력",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7 :  어떤 초등학교에서  10 년간 조사결과\n# 평균적으로  4 % 의 학생이 색맹인 것으로 나타났다고 한다. (p.213)\n\n# 올해에도 색맹인 학생의 비율이 예년과 같다고 할 때,\n# 임의로 추출된  200 명의 학생 중 색맹인 학생이  10 명 이하일 확률은 얼마인가?"
  },
  {
    "objectID": "da/ida/ida_07.html#베르누이-시행",
    "href": "da/ida/ida_07.html#베르누이-시행",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "2. 베르누이 시행",
    "text": "2. 베르누이 시행\nBernoulli Distribution\n모집단의 각 구성원이 두 그룹 중 하나에 속하는 경우, 각각의 구성원이 특정 그룹에 속할 확률 p 와 속하지 않을 확률 1 − p 를 따르는 이산 확률 분포.\n시 행 ( Trial ) : 매번 반복되는 추출( 실험 )\n2 개의 가능한 결과 중, 하나는 성공 ( Success, S ), 다른 하나는 실패 ( Failure, F ) 로 이름을 붙인다.\n이는 시행의 결과가 2 개 뿐임을 강조하며, 보통 우리가 관심이 있는 결과에 성공이란 이름을 붙인다.\n각 시행은 독립으로, 각 시행의 결과가 다른 시행의 결과에 영향을 미치지 않는다.\n일반적으로 복원 추출 ( Sampling With Replacement )로 간주된다. 각 시행 후 다시 원래의 상태로 복귀하여, 다음 시행에 영향을 주지 않는다.\n예제 7에 경우, 추출되는 학생은 색맹 ( S ) 또는 정상 ( F )으로 나눌 수 있다."
  },
  {
    "objectID": "da/ida/ida_07.html#이항분포",
    "href": "da/ida/ida_07.html#이항분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "3. 이항분포",
    "text": "3. 이항분포\nBinomial Distribution\n각 시도가 성공 또는 실패 두 가지 결과 중 하나를 가지는 독립적인 시행이 n 번 반복될 때, 성공 횟수를 나타내는 확률분포.\n성공할 확률이 p 인 베르누이 시행을 n 번 반복할 때 일어나는 성공의 횟수가 X 라면, 이 확률변수 X 는 모수가 ( n, p )인 이항분포를 따른다.\n모수(Parameter): 우리가 관심을 가지는 수치\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, x = 0, 1, …, n에 대하여 확률질량함수( PMF ) 는 다음과 같다:\n이 항 계 수 ( Binomial Coefficient )\n주어진 수의 집합에서 특정한 수의 원소를 선택하는 방법의 수. 조 합 ( Combination ) 으로도 알려져 있다.\n예제 7에서 주어진 정보를 바탕으로, 아래와 같이 이항 분포의 모수 ( 파라미터 ) 를 설정할 수 있다:\n성공 확률: p = 0.04 ( 학생이 색맹일 확률 ) 시도 횟수: n = 200 ( 추출된 학생 수 ) 성공 횟수: k ≤ 10 ( 색맹인 학생 수 )\n즉, 확률변수 X 를 200 명 중 색맹인 학생의 수 라고 하면, X는 모수가 ( n, p ) = ( 200, 0.04 ) 인 이항분포를 따르게 된다.\n이때, k ≤ 10 인 경우의 확률을 구해야 하므로, 누적 분포 함수 ( CDF ) 를 사용해야 한다:\n원하는 확률을 계산하려면, 다음 식을 계산해야 한다:\n다만, 위 식을 직접 계산하기에는 번거로운 면이 있다. 부록의 이항분포표에서도 n = 25 까지가 최대이기 때문이다.\n이럴 경우, 아래에서 설명할 포아송분포로 근사하여 계산할 수 있다.\n\nfrom scipy import stats\nstats.binom.cdf(10, 200, 0.04)\n\n## 출력된 값 &gt; 0.8199789826230907\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n\nnp.float64(0.8199789826230907)\n\n\n\n3-1. 이항분포의 기댓값과 표준편차\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:"
  },
  {
    "objectID": "da/ida/ida_07.html#초기하분포",
    "href": "da/ida/ida_07.html#초기하분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "4. 초기하분포",
    "text": "4. 초기하분포\nHypergeometric Distribution\n유한한 모집단에서 비복원 추출 ( Sampling Without Replacement )을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인(FPC)이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다. 결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다."
  },
  {
    "objectID": "da/ida/ida_07.html#포아송분포",
    "href": "da/ida/ida_07.html#포아송분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "5. 포아송분포",
    "text": "5. 포아송분포\nPoisson Distribution\n특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포. 주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n\n119 구조대에 시간당 걸려오는 전화횟수\n국내 발생하는 진도 4 이상 지진의 횟수\n\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\n\nfrom scipy import stats\nstats.poisson.cdf(10, 8)\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\nnp.float64(0.8158857925585467)\n\n\n\n참고용: Finite Population Correction Factor FPC"
  },
  {
    "objectID": "da/ida/ida_09.html",
    "href": "da/ida/ida_09.html",
    "title": "9장: 표집분포",
    "section": "",
    "text": "주어진 표본을 통해 모집단의 성격을 알아내는 과정을 추론 ( Inference )이라 한다. 그 통계적 추론에서 모집단의 특성을 추정하거나 가설 검정을 수행할 때, 사용되는 표집분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_09.html#자료의-입력",
    "href": "da/ida/ida_09.html#자료의-입력",
    "title": "9장: 표집분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 4:  0 ~ 9 까지  10 개의 정수값을 균등하게 갖는 모집단이 있다.\n# (위 모집단의 분포를 이산균등분포라고 한다.) \n\n# 예시: 전화번호 끝자리 번호의 분포\n\n# (1) 여기에서 크기가  5 인 표본을  100 번 뽑아서 \n# (2) 매번 추출된 표본에서 표본평균을 구하고, \n# (3) 그 평균들을 가지고 히스토그램을 그려라. (p.268)"
  },
  {
    "objectID": "da/ida/ida_09.html#통계량",
    "href": "da/ida/ida_09.html#통계량",
    "title": "9장: 표집분포",
    "section": "2. 통계량",
    "text": "2. 통계량\nStatistic\n표본의 관측값들에 의하여 결정되는 양. 표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데, 이때 유념하여야 할 3가지 조건이 있다.\n\n표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n통계량의 값은 추출된 표본의 영향을 받는다:\n다른 표본을 추출할 때마다 통계량의 값은 변한다:"
  },
  {
    "objectID": "da/ida/ida_09.html#표집분포",
    "href": "da/ida/ida_09.html#표집분포",
    "title": "9장: 표집분포",
    "section": "3. 표집분포",
    "text": "3. 표집분포\nSampling Distribution\n통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n\n3-1. 불편추정량\nUnbiased Estimator 분포의 평균값이 추정하려는 모수와 일치하는 추정량. 불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n\n\n3-2. 편의추정량\nBiased Estimator\n분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다. 이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다. 따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n\n\n3-3. 임의표본\nRandom Sample 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn. 위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n\n\n3-4. 표본평균\nSample Mean\n모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:"
  },
  {
    "objectID": "da/ida/ida_09.html#중심극한정리",
    "href": "da/ida/ida_09.html#중심극한정리",
    "title": "9장: 표집분포",
    "section": "4. 중심극한정리",
    "text": "4. 중심극한정리\nCentral Limit Theorem, CLT 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이 표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\n\nimport numpy as np \n\na = np.random.randint(0, 100, size=5) \nb = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nc = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nd = np.random.randint(0, 100, size=5) \n\nprint(\"a :\", a) \nprint(\"b :\", b) \nprint(\"c :\", c)\nprint(\"d :\", d)\n\n## 해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n\na : [ 2 17  4 90 42]\nb : [99 73 67 33 12]\nc : [37 12 72  9 75]\nd : [37 12 72  9 75]\n\n\n( 2 ) 표본평균을 출력한다.\n\nimport numpy as np \n\nm = [] \n\nnp.random.seed(1234) \nfor i in range(100): \n    sample = np.random.randint(0, 10, size = 5) \n    m.append(np.mean(sample))\n    \nm = np.array(m)\nprint(m)\n\n[5.2 6.4 4.4 3.  5.  1.8 3.  3.4 5.4 7.8 3.4 4.8 4.8 4.8 6.4 4.8 4.4 6.\n 2.8 4.8 4.2 4.4 6.8 1.8 6.  4.6 3.2 2.4 2.8 6.2 3.2 6.8 5.8 5.8 4.4 5.\n 4.4 6.  3.6 4.8 4.8 4.  4.4 5.6 5.2 6.2 1.8 3.8 1.4 6.4 3.8 5.2 4.4 4.4\n 4.6 0.4 3.  5.8 2.2 4.  4.4 3.6 5.2 1.6 3.2 5.8 6.8 3.8 6.2 2.6 2.  4.8\n 2.6 6.  7.6 5.6 6.  3.6 4.2 3.8 5.2 3.4 8.2 4.6 6.8 3.8 4.2 3.8 4.6 2.4\n 4.  4.4 6.  4.4 2.6 3.4 6.  4.6 4.6 2.8]\n\n\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\n\nimport matplotlib.pyplot as plt \n\nplt.hist(m, bins=7)\nplt.xlabel('m') \nplt.ylabel('Frquency') \nplt.title('Historam of m')\n\n## 해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해\n## 정규분포에 가까우리라 예상할 수 있다.\n\nText(0.5, 1.0, 'Historam of m')\n\n\n\n\n\n\n\n\n\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8장에서 배운 정규확률그림을 그려본다.\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nsm.qqplot(m, line='s')\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 점들이 거의 직선상의 있으므로\n## 어느 정도 정규분포를 따른다고 할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida/ida_11.html",
    "href": "da/ida/ida_11.html",
    "title": "11장: 정규모집단에서의 추론",
    "section": "",
    "text": "표본의 크기가 작을 경우에는 일반적인 통계적 추론 방법을 적용하기 어려울 수 있다. 이 경우, 정규분포 대신 t – 분포를 이용한 통계적 추론 방법에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_11.html#자료의-입력",
    "href": "da/ida/ida_11.html#자료의-입력",
    "title": "11장: 정규모집단에서의 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 9: 예제 4에 주어져 있는 자료를 가지고 파이썬을 이용하여 \n# μ 에 대한 90% 신뢰구간을 구하고 예제 4에서 실시한 검정도 \n# 파이썬을 이용하여 다시 시행한 후 그 결과를 비교하라. (p.346)\n\n# ------------------------------------------------------------------------------\n\n# 예제 4: 어느 도시의 보건복지과에서는 \n# 그 도시의 상수원인 어느 호수의 수질에 관심이 있다고 한다. \n\n# 수질을 나타내는 하나의 수치로 단위부피당 평균 세균수가 있는데, \n# 그 수가 200 이상이면 상수원으로 적합 하지 않다고 한다. \n\n# 호수의 열 군데에서 물을 떠서 조사한 결과 단위부피당 세균수가 다음과 같이 나타났다. \n# 이 자료로부터 호수의 단위부피당 평균 세균수(μ)가 200보다 적다고 주장할 수 있겠는가? (p.330)\n\nimport numpy as np \nbacteria = np.array([175, 190, 215, 198, 184, 207, 210, 193, 196, 180])\n\n#_______________________________________________________________________________\n\n# 예제 10: 다음에 주어진 자료를 파이썬을 이용하여 분석하고자 한다. (p.348)\n\nx = np.array([31, 35, 37, 38, 38, 38, 39, 40, 40, 41, 42, 43, 44, 44, 46, 48])"
  },
  {
    "objectID": "da/ida/ida_11.html#t-분포",
    "href": "da/ida/ida_11.html#t-분포",
    "title": "11장: 정규모집단에서의 추론",
    "section": "2. t 분포",
    "text": "2. t 분포\nStudent’s t Distribution 통계학에서 모집단의 표본 평균이 정규분포를 따르지 않는 경우에도 사용가능한 분포. 모집단의 분포가 N ( μ, σ2 ) 일 때 크기가 n 인 표본의 평균 x̄ 의 분포는 정확하게 N ( μ, σ2 / n ) 이다.\n이를 표준화한 것이 아래와 같다:\n일반적으로 σ 는 미지수이므로 이를 표본의 표준편차 s 로 추정하여 사용한다. 표본의 크기가 큰 경우, s 로 대체하여도 그 분포가 큰 영향을 받지 않는다.\n그러나 표본의 크기가 작은 경우, 대체하게 되면 표준화된 확률변수의 분포는 표준정규분포와 달라지게 되며, 이를 t 분포 라고 한다.\n정규모집단 N ( μ, σ2 ) 으로부터 임의추출된 표본을 X 1, …, X n 이라고 할 때, 표본 평균과 표본 분산을 아래와 같이 정의한다:\n위 정의가 성립할 때 아래 식이 성립한다:\n자유도가 (n – 1)인 t 분포를 따르고, 이를 기호로써 t (n – 1)로 표현한다.\n표준정규분포와의\n공통점 : 0 을 중심으로 대칭 & 종모양 분포 차이점 : 양 꼬리부분에 상대적으로 많은 확률이 존재 → 더 두꺼운 꼬리를 갖는다. 이때 자유도가 증가하면, t 분포의 꼬리는 표준정규분포의 꼬리에 가까워진다:\ndf = 1 : 자유도가 1인 경우, df = 5 : 자유도가 5인 경우"
  },
  {
    "objectID": "da/ida/ida_11.html#모평균에-대한-추론",
    "href": "da/ida/ida_11.html#모평균에-대한-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3. 모평균에 대한 추론",
    "text": "3. 모평균에 대한 추론\n표본의 크기가 작거나 모집단이 정규분포를 따르지 않는 경우, t – 분포를 사용한다. 이때는 Z α / 2​ 대신 t α / 2 (n − 1) ​을 사용하여 신뢰구간을 계산해야 한다:\n위 식을 μ 에 대해 정리하면:\n10장: 통계적 추론 → 3 - 5 → (2)번의 식은 위와 같이 수정되어야 한다.\n표본 크기가 작을수록\nt – 분포의 임계값이 커지므로, 신뢰구간이 넓어져 표본표준편차의 불확실성을 반영한다. t – 분포를 사용하여 신뢰구간을 더 넓게 잡으므로, 정확한 추정에 도움이 된다. 표본 크기가 충분히 크다면, t – 분포와 정규분포가 거의 같아지므로 이 경우에만, Z – 분포를 사용할 수 있다.\n따라서:\n또는 아래와 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida/ida_11.html#가설-검정",
    "href": "da/ida/ida_11.html#가설-검정",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3-1. 가설 검정",
    "text": "3-1. 가설 검정\n검정통계량은 H 0 가 맞을 때 자유도가 ( n − 1 ) 인 t – 분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n검정통계량이 t 분포를 따르는 경우의 검정을 t 검정이라고 한다."
  },
  {
    "objectID": "da/ida/ida_11.html#신뢰구간과-양측검정의-관계",
    "href": "da/ida/ida_11.html#신뢰구간과-양측검정의-관계",
    "title": "11장: 정규모집단에서의 추론",
    "section": "4. 신뢰구간과 양측검정의 관계",
    "text": "4. 신뢰구간과 양측검정의 관계\nμ 에 대한 100 ( 1 − α ) % 신뢰구간은 아래와 같다:\nH 0 : μ = μ 0 에 대한 양측검정에서의 기각역은 유의수준이 α 일 때 아래와 같다:\n위 기각역의 여집합인 H 0 를 기각하지 못하고 받아들이는 영역을 ’ 채택영역 ’ 이라고 할 때\n이 채택영역은 다음과 같다:\n이를 μ 0 를 중심으로 풀어쓰면 다음과 같다:\n위 과정을 바탕으로 다음과 같은 결론을 내릴 수 있다:\n모수 θ 에 대한 100 ( 1 − α ) % 신뢰구간이 ( L, U ) 로 구해졌을 때, 가설 H 0 : θ = θ 0 대 H 1 : θ ≠ θ 0 에 대하여 유의수준 α 로 검정을 시행할 때의 결론을 의미한다."
  },
  {
    "objectID": "da/ida/ida_11.html#모표준편차의-추론",
    "href": "da/ida/ida_11.html#모표준편차의-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "5. 모표준편차의 추론",
    "text": "5. 모표준편차의 추론\n모표준편차의 추정과 검정에서는 정규성 가정이 중요한 역할을 한다. 이 가정이 충족되지 않으면, 신뢰구간 계산이나 가설 검정의 결과를 신뢰할 수 없다.\n모표준편차 σ 를 추정하는 과정은 모분산 σ 2 에 대한 추정에서 출발하며, 이 모분산 σ 2 에 대한 추정에서 사용되는 것이 표본분산이다:\n\n5-1. 점추정\nPoint Estimation 모집단의 모수를 단일 값으로 추정하는 방법이다. 모표준편차 σ 의 경우, 점추정은 표본표준편차를 사용하는 방식이다:\n\n\n5-2. 구간추정\nInterval Estimation 모집단의 모수를 특정 신뢰수준에서 포함할 것으로 예상되는 구간을 제공하는 방법이다. 모표준편차 σ 에 대한 구간추정은 모분산에 대한 신뢰구간을 기반으로 하여 계산되며, 이때 모분산 s 2 의 신뢰구간을 구하기 위해 카이제곱 분포를 사용한다.\n\n\n5-3. 카이제곱 분포\nChi-Square Distribution 표본 분산을 모집단 분산과 비교하거나 범주형 변수 간의 독립성을 검정할 때 유용한 분포.\n자유도에 따라 그 형태가 달라지며, 자유도가 커질수록 정규 분포와 유사해진다. 아래 수식은 표본 분산을 모집단 분산으로 표준화한 것이다:\n자유도가 (n – 1)인 x2 분포를 따르고, 이를 기호로써 x2 (n – 1)로 표현한다. 모집단 σ 2 의 신뢰구간을 계산하기 위해 카이제곱 통계량을 사용한다: 위 분포로부터 구한 신뢰구간은 아래 식과 같다:\n위 식에서 괄호 안에 있는 부등식을 σ 2 을 중심으로 풀어 쓰면 다음과 같다:\n따라서 이 식으로부터 σ 2 의 100 ( 1 − α ) % 신뢰구간을 구하면 다음과 같다:\n표준편차 σ 는 σ 2 의 양의 제곱근이므로,그의 신뢰구간은 σ 2 의 신뢰구간의 경곗값의 제곱근을 취하여 얻을 수 있다: 결론적으로, σ 에 대한 100 ( 1 − α ) % 신뢰구간은 위와 같다.\n검정통계량은 H 0 이 맞을 때 자유도 df 인 카이제곱분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 9의 ( 1 ) bacteria 에 대한 요약 통계량 계산하기.\n\nxbar_b = np.mean(bacteria);print(xbar_b) # 평균\n\nvar_b = np.var(bacteria, ddof=1);print(var_b) # 분산 (자유도 1 사용)\n\nsd_b = np.std(bacteria, ddof=1);print(sd_b) # 표준편차 (자유도 1 사용)\n\nmedian_b = np.median(bacteria);print(median_b) # 중앙값\n\n194.8\n172.62222222222226\n13.138577633146681\n194.5\n\n\n\nmin_b = np.min(bacteria);print(min_b) # 최솟값\n\nmax_b = np.max(bacteria);print(max_b) # 최댓값\n\nsum_b = np.sum(bacteria);print(sum_b) # 합계\n\nn = bacteria.size;print(n) # 데이터 개수\n\n175\n215\n1948\n10\n\n\n예제 9의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수 t – 분포의 백분위수 함수\n\nfrom scipy import stats \n\nse_b = stats.sem(bacteria); print(se_b) # 표본표준오차\n\n# 유의수준 0.1에 해당하는 t-분포의 임계값 \nt_alpha = stats.t.ppf(1 - 0.1 / 2, n - 1); print(t_alpha)\n\ninterval = t_alpha * se_b;print(interval) # 신뢰구간의 범위를 계산\n\nCI = [xbar_b - interval, xbar_b + interval]; print(CI) # 신뢰구간\n\n4.154783053568769\n1.8331129326536335\n7.6161865478670645\n[np.float64(187.18381345213294), np.float64(202.41618654786708)]\n\n\n90 % 신뢰구간은 194.8 ± 7.616 임을 알 수 있다.\n예제 9의 ( 3 ) 검정하고자 하는 가설은 H 0 : μ = 200 대 H 1 : μ &lt; 200 이며, 표본의 크기는 10 이다.\n\ntval = (xbar_b - 200) / se_b;print(tval) \n\n# 단측검정: 귀무가설 μ=200, 대립가설 μ&lt;200\npval = stats.t.cdf(tval, n - 1);print(pval)\n\n## 해석: P–값이 0.1211 로 유의수준 5% 에서 귀무가설을 기각할 수 없으므로\n## 주어진 10 개의 자료로부터 호수의 단위 부피당 평균세균수가 200 보다 \n## 적다고 안심할 수 없다.\n\n-1.2515695604210733\n0.12113884687382763\n\n\n예제 10의 ( 1 ) x 에 대한 요약 통계량 계산하기.\n\nxbar_x = np.mean(x);print(xbar_x) # 평균\n\nvar_x = np.var(x, ddof=1);print(var_x) # 분산 (자유도 1 사용)\n\nsd_x = np.std(x, ddof=1);print(sd_x) # 표준편차 (자유도 1 사용)\n\nmedian_x = np.median(x);print(median_x) # 중앙값\n\nmin_x = np.min(x);print(min_x) # 최솟값\n\nmax_x = np.max(x);print(max_x) # 최댓값\n\nsum_x = np.sum(x);print(sum_x) # 합계\n\nn = x.size;print(n) # 데이터 개수\n\n40.25\n18.2\n4.266145801540309\n40.0\n31\n48\n644\n16\n\n\n예제 10의 ( 2 ) x 에 대한 정규확률그림 그리기.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 정규확률그림 그리기\nsm.qqplot(x, line='s')\nplt.title(\"Normal Q-Q Plot\")\n\nText(0.5, 1.0, 'Normal Q-Q Plot')\n\n\n\n\n\n\n\n\n\n예제 10의 ( 3 ) 모평균 μ 에 대한 95% 신뢰구간을 구한다.\n\nfrom scipy import stats \n\nse = stats.sem(x);print(se) # 표준편차\n\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n - 1);print(t_alpha) # 95% 신뢰구간을 위한 t값 \n\ninterval = t_alpha * se;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_x - interval, xbar_x + interval];print(CI) # 신뢰구간\n\n1.0665364503850772\n2.131449545559323\n2.2732686324957263\n[np.float64(37.97673136750427), np.float64(42.52326863249573)]\n\n\n95 % 신뢰구간은 40.25 ± 2.273 임을 알 수 있다.\n예제 10의 ( 4 ) 검정하고자 하는 가설은 H 0 : μ = 38 대 H 1 : μ &gt; 38 이며, 표본의 크기는 16 이다.\n\ntval = (xbar_x - 38) / se;print(tval) # 표본표준오차\n\npval = 1 - stats.t.cdf(tval, n - 1);print(pval) # p값\n\n## 해석: P–값이 0.026 이므로 유의수준 5% 에서 귀무가설을 기각하게 된다.\n## 따라서, 평균이 38 보다 크다고 할 수 있다.\n\n2.109632539223229\n0.026050840503660355"
  },
  {
    "objectID": "da/dap/dap_02.html",
    "href": "da/dap/dap_02.html",
    "title": "Clustering",
    "section": "",
    "text": "비지도 학습(Unsupervised Learning) 기법의 한 유형이다.\n사전에 정의된 타겟 변수(종속 변수)가 존재하지 않는 데이터로부터 데이터 간 유사성 또는 거리(distance)를 기반으로 군집(cluster)을 형성하는 방법론이다.\n이는 데이터가 어떠한 구조를 내재하고 있을 것으로 가정하되, 그 구조의 형태—군집의 개수, 모양, 분포—가 사전에 알려져 있지 않은 상태에서 적용된다.\n군집 분석의 핵심 목적은 다음 두 가지로 요약된다. 1. 군집 형성(Clustering): 개체들 간 거리 계산을 통해 자연스러운 그룹을 형성 2. 군집 해석(Cluster Interpretation): 형성된 군집의 특성과 군집 간 관계 구조를 분석하여 의미를 도출\n\n\n군집 분석에서 가장 기초적이며 중요한 요소는 데이터 간 거리(distance) 또는 유사성(similarity) 계산 방식이다. 거리 측정 방식에 따라 군집 결과는 크게 달라지므로, 데이터의 특성(연속형/희소벡터/텍스트 등)에 따라 적절한 측도를 선택해야 한다.\n측도의 형태가 다르더라도, 군집 분석에서는 “거리 기반으로 개체 간 유사성을 정의한다는 점”이 공통적이다.\n\n\nEuclidean Distance 연속형 변수에서 가장 일반적으로 사용되는 거리 척도로, L2 노름(Norm)에 해당한다. 두 관측치 \\(x_i, x_j \\in \\mathbb{R}^p\\) 사이의 유클리드 거리는 다음과 같이 정의된다.\n\\[\nd_{\\mathrm{euclid}}(x_i, x_j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}\n\\]\n예: 2차원 데이터 \\(p_1=(x_1, ~y_1), ~~~p_2=(x_2, ~y_2)\\) 의 경우, 다음과 같이 계산할 수 있다.\n\\[\nd(p_1, ~~~p_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\] \n\n\n각 변수의 단위가 상이할 경우, 거리 계산이 왜곡될 수 있으므로 표준화가 필요하다.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\n\\(\\mu\\): 평균\n\\(\\sigma\\): 표준편차\n\n\n\n\n\n모든 개체를 단일 군집(singleton cluster)으로 초기화한다. \\[\nC_1 = {x_1}, C_2 = {x_2}, \\dots, C_n = {x_n}\n\\]\n현재 존재하는 군집들 간 거리 행렬 \\(D_0\\) 를 계산한다.\n\n행렬은 대칭이며, 각 원소 \\(d(C_i,C_j)\\) 는 군집 \\(C_i, C_j\\) 간 거리이다.\n\n거리 행렬에서 가장 가까운 군집 쌍 \\((C_p,C_q)\\) 을 선택하여 병합한다.\n\n행렬 크기는 1줄씩 감소하며, 새로운 군집 \\(C_{new}=C_p \\cup C_q\\) 가 생성된다.\n\n새로운 군집과 나머지 군집 간 거리를 연결법(Linkage Method)에 따라 재계산한다.\n이 과정을 반복하여 최종적으로 모든 개체가 하나의 군집으로 통합될 때까지 진행한다.\n\n\n\n\n\nManhattan Distance L1 노름 기반 거리로, 고차원 데이터에서 유리할 수 있다.\n\\[\nd_{\\text{manhattan}}(x_i, x_j) = \\sum_{k=1}^{p}\\left|x_{ik} - x_{jk}\\right|\n\\]\n\n\n\nCosine Similarity 텍스트 마이닝 분야에서 주로 사용되며, 벡터 방향의 유사성을 측정한다.\n\\[\n\\text{cos}(x_i, x_j) = \\frac{x_i \\cdot x_j}{|x_i||x_j|}\n\\]\n코사인 거리(Cosine Distance)는 다음과 같이 정의된다.\n\\[\nd_{\\text{cosine}} = 1 - \\text{cos}(x_i, x_j)\n\\]\n\n\n\n\n\n\n\nk-means 기반 군집화 모델은 군집을 수학적으로 구형(spherical) 구조로 가정한다. k-means의 목적함수는 다음을 최소화한다.\n\\[\n\\min_{C_1, ..., C_K} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} |x_i - \\mu_k|^2\n\\]\n이는 각 군집 중심(centroid)으로부터의 제곱거리 최소화를 가정하므로, 군집이 타원형이 아닌 구형에 가까울 때 성능이 가장 안정적이다.\n\n\n\n아래의 경우는 k-means 모델에서 성능이 저하되는 대표 사례이다.\n\n군집의 형태가 길고 가는 모양(elongated cluster)인 경우\n\n구형 중심 거리 기준으로는 정확히 분리되지 않는다.\n\n개체 A, B가 서로 다른 군집 사이에서 ‘다리’ 역할을 하는 중간 위치에 존재하는 경우\n\n두 군집이 실제로 분리되어 있어도 k=2 가정에서 중심이 왜곡된다.\n\n\n이와 같은 경우에는 DBSCAN, 계층적 군집 등 모양 제약이 없는 알고리즘이 더 유리하다.\n\n\n\n\n군집 해석과 군집 수 결정에서 다양한 지표가 사용된다.\n\n\nSilhouette Coefficient\n개체 \\(i\\) 에 대해 * \\(a(i)\\): 같은 군집 내 평균 거리 * \\(b(i)\\): 가장 가까운 다른 군집과의 평균 거리\n실루엣 값은 다음과 같다:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max {a(i), b(i)}}\n\\]\n실루엣 계수는 군집의 응집도(cohesion)와 분리도(separation)를 동시에 평가하는 지표이다.\n\n\n\nElbow Method SSE(Sum of Squared Errors)의 감소율을 관찰하여 변곡점(elbow)을 최적 군집 개수로 간주한다. 수식은 k-means 목적함수와 동일하다.\n\n\n\n\n\n실무에서는 단순히 거리를 계산하여 k-means를 적용하는 것이 아니라, 다음과 같은 요소가 필수적으로 고려된다.\n\n정규화/표준화(Scaling):\n\n변수 간 단위 차이로 인한 거리 왜곡 방지\n\n차원 축소(PCA, t-SNE 등):\n\n고차원에서의 거리 희석 현상 해결\n\n거리 측정 방식 선택:\n\n텍스트 → 코사인\n연속형 수치 → 유클리드\n이상치 존재 → 맨해튼\n\n알고리즘 선택\n\n구형 군집 → k-means\n임의 형태의 군집 → DBSCAN\n계층적 구조 중요 → Hierarchical Clustering\n\n\n\n\n\n\n군집 분석은 다양한 산업 분야에서 핵심 기법으로 활용된다.\n\n금융 Finance\n\n신용카드 소비 패턴 분석\n리스크 기반 고객 세그멘테이션\n사기 탐지(비정상 패턴 발견)\n\n마케팅 Marketing\n\n고객 세분화(Customer Segmentation)\n구매 행동 기반 타겟 마케팅\n추천 시스템의 사용자 군집화\n\n헬스케어 Healthcare\n\n환자 유형 분류\n질병 패턴 분석\n개인 맞춤형 치료 전략 개발\n\n제조업 Manufacturing\n\n불량 패턴 탐지\n공정 조건 기반 군집화\n유지보수(Preventive Maintenance) 최적화\n\n\n군집 분석은 특히 세그멘테이션(Segmentation) 분야에서 실무적 가치가 매우 높다."
  },
  {
    "objectID": "da/dap/dap_02.html#거리유사성-측정-방법론",
    "href": "da/dap/dap_02.html#거리유사성-측정-방법론",
    "title": "Clustering",
    "section": "",
    "text": "군집 분석에서 가장 기초적이며 중요한 요소는 데이터 간 거리(distance) 또는 유사성(similarity) 계산 방식이다. 거리 측정 방식에 따라 군집 결과는 크게 달라지므로, 데이터의 특성(연속형/희소벡터/텍스트 등)에 따라 적절한 측도를 선택해야 한다.\n측도의 형태가 다르더라도, 군집 분석에서는 “거리 기반으로 개체 간 유사성을 정의한다는 점”이 공통적이다.\n\n\nEuclidean Distance 연속형 변수에서 가장 일반적으로 사용되는 거리 척도로, L2 노름(Norm)에 해당한다. 두 관측치 \\(x_i, x_j \\in \\mathbb{R}^p\\) 사이의 유클리드 거리는 다음과 같이 정의된다.\n\\[\nd_{\\mathrm{euclid}}(x_i, x_j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}\n\\]\n예: 2차원 데이터 \\(p_1=(x_1, ~y_1), ~~~p_2=(x_2, ~y_2)\\) 의 경우, 다음과 같이 계산할 수 있다.\n\\[\nd(p_1, ~~~p_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\] \n\n\n각 변수의 단위가 상이할 경우, 거리 계산이 왜곡될 수 있으므로 표준화가 필요하다.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\n\\(\\mu\\): 평균\n\\(\\sigma\\): 표준편차\n\n\n\n\n\n모든 개체를 단일 군집(singleton cluster)으로 초기화한다. \\[\nC_1 = {x_1}, C_2 = {x_2}, \\dots, C_n = {x_n}\n\\]\n현재 존재하는 군집들 간 거리 행렬 \\(D_0\\) 를 계산한다.\n\n행렬은 대칭이며, 각 원소 \\(d(C_i,C_j)\\) 는 군집 \\(C_i, C_j\\) 간 거리이다.\n\n거리 행렬에서 가장 가까운 군집 쌍 \\((C_p,C_q)\\) 을 선택하여 병합한다.\n\n행렬 크기는 1줄씩 감소하며, 새로운 군집 \\(C_{new}=C_p \\cup C_q\\) 가 생성된다.\n\n새로운 군집과 나머지 군집 간 거리를 연결법(Linkage Method)에 따라 재계산한다.\n이 과정을 반복하여 최종적으로 모든 개체가 하나의 군집으로 통합될 때까지 진행한다.\n\n\n\n\n\nManhattan Distance L1 노름 기반 거리로, 고차원 데이터에서 유리할 수 있다.\n\\[\nd_{\\text{manhattan}}(x_i, x_j) = \\sum_{k=1}^{p}\\left|x_{ik} - x_{jk}\\right|\n\\]\n\n\n\nCosine Similarity 텍스트 마이닝 분야에서 주로 사용되며, 벡터 방향의 유사성을 측정한다.\n\\[\n\\text{cos}(x_i, x_j) = \\frac{x_i \\cdot x_j}{|x_i||x_j|}\n\\]\n코사인 거리(Cosine Distance)는 다음과 같이 정의된다.\n\\[\nd_{\\text{cosine}} = 1 - \\text{cos}(x_i, x_j)\n\\]"
  },
  {
    "objectID": "da/dap/dap_02.html#군집-형성의-구조적-특징",
    "href": "da/dap/dap_02.html#군집-형성의-구조적-특징",
    "title": "Clustering",
    "section": "",
    "text": "k-means 기반 군집화 모델은 군집을 수학적으로 구형(spherical) 구조로 가정한다. k-means의 목적함수는 다음을 최소화한다.\n\\[\n\\min_{C_1, ..., C_K} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} |x_i - \\mu_k|^2\n\\]\n이는 각 군집 중심(centroid)으로부터의 제곱거리 최소화를 가정하므로, 군집이 타원형이 아닌 구형에 가까울 때 성능이 가장 안정적이다.\n\n\n\n아래의 경우는 k-means 모델에서 성능이 저하되는 대표 사례이다.\n\n군집의 형태가 길고 가는 모양(elongated cluster)인 경우\n\n구형 중심 거리 기준으로는 정확히 분리되지 않는다.\n\n개체 A, B가 서로 다른 군집 사이에서 ‘다리’ 역할을 하는 중간 위치에 존재하는 경우\n\n두 군집이 실제로 분리되어 있어도 k=2 가정에서 중심이 왜곡된다.\n\n\n이와 같은 경우에는 DBSCAN, 계층적 군집 등 모양 제약이 없는 알고리즘이 더 유리하다."
  },
  {
    "objectID": "da/dap/dap_02.html#군집의-품질-평가-지표",
    "href": "da/dap/dap_02.html#군집의-품질-평가-지표",
    "title": "Clustering",
    "section": "",
    "text": "군집 해석과 군집 수 결정에서 다양한 지표가 사용된다.\n\n\nSilhouette Coefficient\n개체 \\(i\\) 에 대해 * \\(a(i)\\): 같은 군집 내 평균 거리 * \\(b(i)\\): 가장 가까운 다른 군집과의 평균 거리\n실루엣 값은 다음과 같다:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max {a(i), b(i)}}\n\\]\n실루엣 계수는 군집의 응집도(cohesion)와 분리도(separation)를 동시에 평가하는 지표이다.\n\n\n\nElbow Method SSE(Sum of Squared Errors)의 감소율을 관찰하여 변곡점(elbow)을 최적 군집 개수로 간주한다. 수식은 k-means 목적함수와 동일하다."
  },
  {
    "objectID": "da/dap/dap_02.html#알고리즘-선택과-실무적-전처리-요건",
    "href": "da/dap/dap_02.html#알고리즘-선택과-실무적-전처리-요건",
    "title": "Clustering",
    "section": "",
    "text": "실무에서는 단순히 거리를 계산하여 k-means를 적용하는 것이 아니라, 다음과 같은 요소가 필수적으로 고려된다.\n\n정규화/표준화(Scaling):\n\n변수 간 단위 차이로 인한 거리 왜곡 방지\n\n차원 축소(PCA, t-SNE 등):\n\n고차원에서의 거리 희석 현상 해결\n\n거리 측정 방식 선택:\n\n텍스트 → 코사인\n연속형 수치 → 유클리드\n이상치 존재 → 맨해튼\n\n알고리즘 선택\n\n구형 군집 → k-means\n임의 형태의 군집 → DBSCAN\n계층적 구조 중요 → Hierarchical Clustering"
  },
  {
    "objectID": "da/dap/dap_02.html#실무-적용-분야",
    "href": "da/dap/dap_02.html#실무-적용-분야",
    "title": "Clustering",
    "section": "",
    "text": "군집 분석은 다양한 산업 분야에서 핵심 기법으로 활용된다.\n\n금융 Finance\n\n신용카드 소비 패턴 분석\n리스크 기반 고객 세그멘테이션\n사기 탐지(비정상 패턴 발견)\n\n마케팅 Marketing\n\n고객 세분화(Customer Segmentation)\n구매 행동 기반 타겟 마케팅\n추천 시스템의 사용자 군집화\n\n헬스케어 Healthcare\n\n환자 유형 분류\n질병 패턴 분석\n개인 맞춤형 치료 전략 개발\n\n제조업 Manufacturing\n\n불량 패턴 탐지\n공정 조건 기반 군집화\n유지보수(Preventive Maintenance) 최적화\n\n\n군집 분석은 특히 세그멘테이션(Segmentation) 분야에서 실무적 가치가 매우 높다."
  },
  {
    "objectID": "da/dap/dap_02.html#병합적-계층-군집-분석의-절차",
    "href": "da/dap/dap_02.html#병합적-계층-군집-분석의-절차",
    "title": "Clustering",
    "section": "1. 병합적 계층 군집 분석의 절차",
    "text": "1. 병합적 계층 군집 분석의 절차\n\n1.1 초기 단계\n분석 대상 개체가 \\(n\\) 개라고 할 때, 초기에는 모든 개체가 단독 군집으로 간주된다.\n\\[\nC_1 = {x_1}, C_2 = {x_2}, \\ldots, C_n = {x_n}\n\\]\n이후 각 군집 간 거리(유사성)가 거리 행렬(distance matrix)로 표현되며, 이 행렬은 군집 병합 과정에서 매 단계 재계산된다.\n\n\n1.2 단계별 병합(algo) 과정\n각 단계에서는 다음 두 규칙이 반복적으로 적용된다.\n\n현재 존재하는 모든 군집 쌍 중 가장 가까운 군집을 찾는다. \\[\n(C_p, C_q)=\\arg\\min_{C_i, C_j} d(C_i,C_j)\n\\]\n해당 두 군집을 하나의 군집으로 병합한다. \\[\nC_{new}=C_p \\cup C_q\n\\]\n병합 후, 새로운 군집과 다른 군집 간의 거리를 ’연결법(Linkage Method)’에 따라 재계산한다. 이 과정이 반복되어 최종적으로 하나의 군집으로 통합된다. \\[\nn \\rightarrow n-1 \\rightarrow n-2 \\rightarrow \\cdots \\rightarrow 1\n\\]\n\n이러한 병합 과정을 시각적으로 나타낸 것이 덴드로그램(Dendrogram)이며, 수평선의 높이(height)는 해당 병합 단계에서의 군집 간 거리 혹은 이질성(Heterogeneity)을 나타낸다."
  },
  {
    "objectID": "da/dap/dap_02.html#연결법",
    "href": "da/dap/dap_02.html#연결법",
    "title": "Clustering",
    "section": "2. 연결법",
    "text": "2. 연결법\nLinkage Methods 군집 간 거리 계산 방식은 계층적 군집 분석의 결과에 직접적으로 영향을 미치는 핵심 요소이다. 아래는 대표적 연결법들의 정의, 수학적 공식, 특징, 구조적 영향을 상세히 정리한 것이다.\n\n2.1 최단 연결법\nSingle Linkage 두 군집 간 최소 거리(minimum pairwise distance)를 사용한다. \\[\nd_{\\text{single}}(C_i,C_j)=\\min_{x \\in C_i,, y \\in C_j} d(x,y)\n\\]\n특징: * Chain Effect(사슬 현상) 발생 가능성이 높음 (길게 늘어지는 패턴이 나타나며, 여러 개체가 얇은 줄처럼 연결되어 있는 구조) * 개별 데이터들이 사슬처럼 연결되어 길게 늘어난 형태의 군집이 형성될 수 있음 * 군집 모양 취약: 좁고 길게 늘어진(Elongated) 군집에서는 적합하지 않음 * 잡음과 이상치 민감: 외곽 점(outlier)에 의해 군집 구조가 쉽게 왜곡됨\n실무적 주의: * 단순 거리만 고려하므로, 실제 데이터의 밀도나 분포를 충분히 반영하지 못할 수 있음 * 군집 결과가 직관적이지 않거나 왜곡될 수 있으므로, 데이터 특성을 고려하여 다른 연결법과 병행 평가 필요\n\n\n2.2 최장 연결법\nComplete Linkage 두 군집 간 최대 거리(maximum pairwise distance)를 사용한다. \\[\nd_{\\text{complete}}(C_i,C_j)=\\max_{x \\in C_i,, y \\in C_j} d(x,y)\n\\]\n특징: * 군집 내부가 조밀(compact)하게 유지됨 (각 군집 내 데이터 간 최대 거리를 고려하기 때문) * 이상치와 잡음의 영향을 Single linkage 대비 상대적으로 덜 받음 (균형 잡힌 군집 구조(Balanced Cluster Structure) 생성) * 덴드로그램 상에서 병합 높이가 일정하게 유지되어 구조가 시각적으로 균형 있게 나타남\n실무적 고려: * 분류 경계가 명확해야 하는 경우 유용 * 군집 간 거리 기준이 엄격하여, 너무 작은 군집이 과도하게 분리되는 경우 주의 필요\n\n\n2.3 평균 연결법\nAverage Linkage / UPGMA 군집 간 모든 개체 쌍의 거리 평균을 사용한다.\n\\[\nd_{\\text{average}}(C_i,C_j)\n= \\frac{1}{|C_i|\\cdot |C_j|}\n\\sum_{x\\in C_i}\\sum_{y\\in C_j} d(x,y)\n\\]\n특징: * 군집 간 전체적 거리 구조를 반영 (단일 연결법의 사슬 현상 + 최장 연결법의 지나친 조밀화를 완화) * 극단적 이상치에 대한 민감도가 단일/최장 연결법보다 낮음 * 평균 기반 병합으로 군집 내 구조를 보다 세밀하게 반영 * 모든 데이터 쌍의 평균 거리 기반으로 병합이 이루어짐 * 병합 높이가 극단적으로 치우치지 않고, 일정한 간격을 유지하며 균형 있는 시각적 구조를 보여줌\n\n\n2.4 중심 연결법\nCentroid Linkage 각 군집의 중심(centroid)을 계산한 뒤 중심 간 거리로 측정.\n\\[\n\\text{군집} C_i \\text{의 중심}: \\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i} x\n\\]\n\\[\n\\text{군집 간 거리}: d_{\\text{centroid}}(C_i,C_j)=|\\mu_i-\\mu_j|\n\\]\n특징: * 중심 계산과 거리 계산이 행렬 연산으로 처리 가능하여 구현 용이 * 군집이 선형적으로 분리되거나 중심 기반 구조가 뚜렷한 경우 성능이 우수 * 중심만 계산하면 되므로, 반복 연산이 많은 대규모 데이터에서 계산이 비교적 효율적 * 중심 이동으로 인해 Single/Complete/Average보다 덴드로그램의 구조가 덜 안정적일 수 있음 * 역병합(Reversal) 발생 가능성 높음 (군집 병합 후 새로운 중심이 기존 거리 구조를 뒤흔들어 덴드로그램 높이가 역전되는 비단조성(non-monotonicity) 문제가 발생하기 쉬움) * 컷(cut) 기준의 주관성 (덴드로그램의 높이가 단조 증가하지 않아, 군집 수 결정 시 절단 시점 판단이 더 주관적일 수 있음)\n\n\n2.5 중위수 연결법\nMedian Linkage 두 군집 중심의 중위값(median)을 기반으로 정의하며, Centroid linkage와 유사하나 중심 계산 방법이 다르다.\n\\[\n\\text{병합 후 새로운 중심}: \\mu_{new}=\\frac{1}{2}(\\mu_i+\\mu_j)\n\\]\n특징: * 중위수 사용으로 극단값의 영향을 평균보다 적게 받음 (단, 전체 군집 구조 안정성 문제를 해결할 수준의 강인성(robustness)은 아님) * 중위수 기반이므로 계산 과정은 상대적으로 단순하고 구현 난이도도 낮음\n\n역병합(Reversal) 발생 가능성 높음 (Centroid와 마찬가지로 병합 후 새 중위수 위치가 기존 거리 구조를 비단조적으로 변화시켜 덴드로그램 높이 역전(Non-monotonicity)이 발생할 수 있음)\n여러 연구에서 Centroid와 유사하게 군집 구조가 불안정하다는 보고가 존재 (특히 군집 분리 기준이 덴드로그램에서 명확하지 않은 경우가 잦음)\n데이터가 비정상적 분포(heavy-tailed)거나 극단값이 많은 경우 평균 기반보다 중위수 기반이 유리할 수 있음 (그러나 덴드로그램 해석의 비단조성 문제와 불안정성 때문에 Single, Complete, Average, Ward 방식보다 권장 빈도가 현저히 낮음)\n따라서 실무·통계 패키지에서 기본 옵션으로 잘 사용되지 않으며, 실증 연구에서도 활용 빈도가 다른 연결법 대비 매우 낮음\n\n\n\n2.6 Ward의 방법\nWard’s Minimum Variance Method 군집 병합 시 전체 군집 내 오류제곱합(SSE, Within-Cluster Sum of Squares)의 증가를 최소화하는 방법.\n\\[\n\\text{군집} C \\text{의} SSE: \\quad SSE(C)=\\sum_{x\\in C}|x-\\mu_C|^2\n\\]\nWard 방식은 다음을 최소화한다: \\[\n\\Delta = SSE(C_i \\cup C_j) - (SSE(C_i)+SSE(C_j))\n\\]\n특징: * 가장 널리 사용되는 연결법 * 군집이 구형(spherical) 형태로 형성됨 (분산 최소화 원리로 인해 밀집되고 균형 잡힌 구형 구조를 만들기 쉬움) * 단일·최장 연결법보다 이상치 영향이 적고 안정적인 군집 구조를 형성 (병합 높이(height)가 비교적 균일하게 증가 → 매우 안정적이고 해석이 쉬운 덴드로그램 구조를 제공 군집 간 병합 폭이 일정해 분기(branch)가 균형적으로 나타남) * 군집 내 제곱합 기반이라 군집 간 차이가 직관적으로 해석 가능 * k-means와 유사한 알고리즘적 성향 (분산 최소화) (둘 다 군집 내 변동을 최소화하는 방향으로 동작 → 결과 군집 형태가 유사해지는 경향.)\n실무적 고려: * 데이터가 연속형이며, 군집이 구형에 가까운 구조일 때 가장 적합 * 고차원 데이터에서도 안정적이지만, 분산 계산 특성상 변수 스케일링(표준화)이 필수 * 비구형 구조(long, chain-like cluster)를 가진 데이터에서는 과도하게 조밀한 군집이 생성될 수 있음"
  },
  {
    "objectID": "da/dap/dap_02.html#실무적-고려-사항",
    "href": "da/dap/dap_02.html#실무적-고려-사항",
    "title": "Clustering",
    "section": "3. 실무적 고려 사항",
    "text": "3. 실무적 고려 사항\n계층적 군집 분석은 다음과 같은 실무적 특성이 존재한다.\n\n연산 복잡도\n\n\n거리 행렬 계산: \\(O(n^2)\\)\n전체 병합 과정: \\(O(n^3)\\) (일반적 구현 기준)\n\n→ 데이터가 많아지면 실무에서 수천 개 이상은 사실상 불가능 → 샘플링, 차원 축소 병행 필요\n\n데이터 전처리 필요성\n\n\n거리 기반이므로 표준화(Standardization) 필수 \\[\nx'=\\frac{x-\\mu}{\\sigma}\n\\]\n이상치(outlier)에 매우 민감 → 사전 처리 필요\n고차원 데이터에서는 차원의 저주로 성능 저하 → PCA 필요\n\n\n군집 수 결정\n\n\n덴드로그램의 컷 높이(cut height)\n비일관성 계수(inconsistency coefficient)\n코페네틱 상관계수(cophenetic correlation coefficient) 등 고려\n\n\n주관성 존재\n\n\n덴드로그램 컷(cut height) 결정, 연결법 선택,\n최종 군집 수 결정은 분석자의 경험과 목적에 크게 의존\n\n\n실무 활용\n\n\n단위가 다른 변수는 반드시 표준화 필요\n통계적 솔루션만 적용하면 데이터의 감성적 패턴 반영 부족\n마케팅/금융에서 고객 세그멘테이션, 1년 단위 갱신 등 경험적 기준 적용\n\n\n알고리즘 관점\n\n\n최단, 최장, 평균, 중심 연결법 결과는 병합 순서나 군집 모양에서 차이가 발생하지만, 최종 그룹화 자체는 모두 유사\n관점 차이일 뿐, 전체적인 군집 구조 탐색에는 유용함"
  },
  {
    "objectID": "da/dap/dap_02.html#k-평균-군집의-목적-함수",
    "href": "da/dap/dap_02.html#k-평균-군집의-목적-함수",
    "title": "Clustering",
    "section": "1. K-평균 군집의 목적 함수",
    "text": "1. K-평균 군집의 목적 함수\nK-평균 알고리즘은 다음의 목적 함수(Objective Function)를 최소화하는 문제로 정의된다.\n\\[\n\\min_{C_1,\\dots,C_K} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} | x_i - \\mu_k |^2\n\\]\n\n\\(C_k\\): k번째 군집\n\\(\\mu_k\\): k번째 군집의 중심(centroid)\n\\(x_i\\): 군집에 속한 데이터 포인트\n\\(|x_i - \\mu_k|^2\\): 유클리드 거리의 제곱\n목적: 군집 내 제곱합(WSS, Within-Cluster Sum of Squares)의 최소화\n\n이 함수가 최소화될 때, 각 군집은 내부적으로 가장 조밀하며, 군집 간 분리는 상대적으로 크다."
  },
  {
    "objectID": "da/dap/dap_02.html#k-평균-알고리즘-절차",
    "href": "da/dap/dap_02.html#k-평균-알고리즘-절차",
    "title": "Clustering",
    "section": "2. K-평균 알고리즘 절차",
    "text": "2. K-평균 알고리즘 절차\nK-평균 알고리즘의 표준 절차는 다음과 같다.\n\n군집 수(K)의 결정 K는 사전에 사용자가 결정해야 하는 하이퍼파라미터이다. 일반적으로 엘보우 기법(Elbow method), 실루엣 계수(Silhouette coefficient), Gap Statistic 등으로 후보값을 선정한다.\n\n특히 엘보우 기법은 실무에서 가장 흔하게 사용된다.\n\n초기 중심 선택 Initial Centroid 초기 중심은 임의로 선택하거나, K-means++ 등 보다 안정적인 방법으로 선정할 수 있다. 초기값에 따라 최종 군집 결과가 달라질 수 있어 초기 중심 선택은 중요한 단계이다.\n개체의 군집 할당 Assignment Step 각 데이터 포인트 \\(x_i\\) 는 가장 가까운 중심 \\(\\mu_k\\) 에 할당된다. 거리 계산은 일반적으로 유클리드 거리로 수행한다: \\[\nd(x_i, \\mu_k) = \\sqrt{\\sum_{j=1}^{p} (x_{ij} - \\mu_{kj})^2}\n\\]\n\n\n\\(p\\): 변수의 개수\n\n\n군집 중심 재계산 Update Step 각 군집에 속한 데이터의 평균 벡터를 이용해 새로운 중심을 계산한다:\n\n\\[\n\\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i\n\\]\n각 개체가 추가될 때마다(혹은 반복적 재계산 과정에서) 중심값이 변동한다.\n\n수렴 조건 충족 시까지 반복 다음 중 하나의 조건이 충족하면 알고리즘은 종료된다.\n\n\n중심 벡터 변화량이 특정 임계값 이하일 때 \\[\n|\\mu_k^{(t+1)} - \\mu_k^{(t)}| &lt; \\epsilon\n\\]\n일정 횟수(T) 이상 반복 수행\nWSS 변화가 미미할 때\n\n이러한 반복 과정으로 K-means는 군집 내 동질성이 가장 높은 상태로 수렴한다."
  },
  {
    "objectID": "da/dap/dap_02.html#엘보우-기법-1",
    "href": "da/dap/dap_02.html#엘보우-기법-1",
    "title": "Clustering",
    "section": "3. 엘보우 기법",
    "text": "3. 엘보우 기법\nElbow 엘보우 기법은 군집 수 K를 선택하기 위한 정형화된 방법이다.\n\nWSS 정의\n\n\\[\nWSS(K) = \\sum_{k=1}^{K}\\sum_{x_i\\in C_k}|x_i - \\mu_k|^2\n\\]\n군집 수 K가 증가할수록 WSS는 감소한다. 이는 군집이 나뉠수록 각 군집이 더 조밀해지기 때문이다.\n\n엘보우의 해석\n\n\nWSS는 K가 증가할수록 급격히 감소하다가,\n어느 지점에서 감소 폭이 완만해지는 시점이 나타난다.\n이 지점이 “엘보우(elbow)”이며 적절한 K의 후보로 간주된다.\n\n단, 엘보우는 절대적인 정답이 아니며, 실루엣 계수 등 다른 지표와 병행해야 한다."
  },
  {
    "objectID": "da/dap/dap_02.html#데이터-전처리와-표준화",
    "href": "da/dap/dap_02.html#데이터-전처리와-표준화",
    "title": "Clustering",
    "section": "4. 데이터 전처리와 표준화",
    "text": "4. 데이터 전처리와 표준화\nK-평균은 거리 기반 모델이므로 변수의 단위 차이가 큰 경우 오차가 발생한다. 예: 나이(20 ~ 80)와 수입(1,000 ~ 1억)이 함께 있을 경우 수입 변수가 거리 계산을 압도한다.\n따라서 다음과 같은 표준화가 필요하다.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n표준화는 거리 측정의 공정성을 확보하지만, “정보 손실”이 있는 것은 아니며 변수의 스케일 해석이 달라지는 것이 정확한 표현이다."
  },
  {
    "objectID": "da/dap/dap_02.html#차원-축소",
    "href": "da/dap/dap_02.html#차원-축소",
    "title": "Clustering",
    "section": "5. 차원 축소",
    "text": "5. 차원 축소\n대부분의 실무 데이터는 3차원 이상의 다변량 구조를 가진다. K-평균으로 군집을 생성한 뒤, 결과를 2차원 또는 3차원 그래프로 시각화하기 위해 차원 축소 기법(PCA, t-SNE 등)을 사용한다.\n\nPCA를 통해 고차원 공간의 분산을 보존하면서 차원을 축약\n축약된 공간에서 각 군집을 색상(라벨)으로 표시\n이는 군집 구조를 탐색하고 설명하는 데 유용\n\n군집 라벨은 비지도 학습의 결과이며 “예측값”이라기보다 “모델이 발견한 데이터 구조에 대한 할당 결과”에 가깝다."
  },
  {
    "objectID": "da/dap/dap_02.html#실무적-장점과-제약",
    "href": "da/dap/dap_02.html#실무적-장점과-제약",
    "title": "Clustering",
    "section": "6. 실무적 장점과 제약",
    "text": "6. 실무적 장점과 제약\n\n장점\n\n\n계산 효율이 매우 높음\n대규모 데이터에 적합\n구현이 단순하고 결과가 직관적\n마케팅·고객 세그멘테이션·금융 데이터 분석에서 표준처럼 사용됨\n\n\n제약\n\n\n구형(spherical) 군집 가정\n비정형, 길쭉한 군집 구조에서는 부정확\n사전에 K를 결정해야 하는 제약이 존재한다.\nK-means++ 등의 기법을 활용해야 초기값이 안정적\n이상치는 중심값을 크게 왜곡시켜 전체 군집 구조를 변형시킨다."
  },
  {
    "objectID": "da/dap/dap_02.html#실루엣-계수-1",
    "href": "da/dap/dap_02.html#실루엣-계수-1",
    "title": "Clustering",
    "section": "1. 실루엣 계수",
    "text": "1. 실루엣 계수\nSilhouette Coefficient\n\n1.1 수학적 정의\n실루엣 계수는 개별 데이터 포인트가 자신이 속한 군집과 얼마나 응집(Cohesion) 되어 있으며, 동시에 다른 군집과는 얼마나 분리(Separation) 되어 있는지를 정량적으로 측정하는 지표이다.\n특정 데이터 포인트 \\(i\\) 에 대해 다음과 같이 정의한다.\n\n동일 군집 내 거리 평균 \\[\na(i) = \\frac{1}{|C_i|-1}\\sum_{j\\in C_i, j\\neq i} d(i,j)\n\\]\n가장 가까운 외군집과의 평균거리 \\[\nb(i) = \\min_{C_k \\ne C_i} \\left( \\frac{1}{|C_k|} \\sum_{j\\in C_k} d(i,j) \\right)\n\\]\n실루엣 계수 공식 \\[\ns(i)=\\frac{b(i)-a(i)}{\\max(a(i),b(i))}, \\quad \\text{범위}: -1 \\le s(i) \\le 1\n\\]\n\n\n\n1.2 해석 기준\n\n\\(s(i) \\approx 1\\): 군집이 매우 잘 형성됨 (높은 응집 + 높은 분리)\n\\(s(i) \\approx 0\\): 군집 간 경계에 위치\n\\(s(i) &lt; 0\\): 잘못된 군집 배정 가능성이 높음\n\n군집 전체의 실루엣 계수는 다음과 같이 평균으로 계산한다.\n\\[\nS(k) = \\frac{1}{n}\\sum_{i=1}^{n}s(i)\n\\]\n이 값이 최대가 되는 k를 선택하는 것이 대표적 접근이다."
  },
  {
    "objectID": "da/dap/dap_02.html#실무-적용성과-한계",
    "href": "da/dap/dap_02.html#실무-적용성과-한계",
    "title": "Clustering",
    "section": "1.3 실무 적용성과 한계",
    "text": "1.3 실무 적용성과 한계\n장점 * 개별 포인트 단위의 군집 품질 평가 가능 * 모델 비교 가능 (예: K=2~10)\n한계 * 비선형 구조(예: 두 개의 링 모양)에서는 K-means와 함께 비적합 * 고차원 데이터에서 거리 기반 접근의 신뢰도 저하 * 실루엣 계수가 높아도 실무 요구와 맞지 않을 수 있음(예: 비즈니스 세분화 목적이 다른 경우)"
  },
  {
    "objectID": "da/dap/dap_02.html#엘보우-기법-2",
    "href": "da/dap/dap_02.html#엘보우-기법-2",
    "title": "Clustering",
    "section": "2. 엘보우 기법",
    "text": "2. 엘보우 기법\nElbow Method, 군집 내 응집도를 나타내는 군집 내 제곱합(WSS, Within-Cluster Sum of Squares)에 기반한다.\nWSS는 다음과 같이 정의한다.\n\\[\nWSS(k)=\\sum_{i=1}^{k}\\sum_{x \\in C_i} | x - \\mu_i |^2\n\\] * \\(C_i\\): i번째 군집 * \\(\\mu_i\\): 해당 군집의 중심(centroid)\n해석 기준 * WSS(k)는 k가 증가할수록 항상 감소한다. * 감소 곡선에서 기울기 변화가 급격 → 완만으로 바뀌는 지점을 “팔꿈치(Elbow)”라고 한다. * 이 지점이 적절한 군집 수 후보가 된다.\n한계 * 엘보우 지점이 명확히 보이지 않는 경우가 매우 많다. * 시각적 판단 의존도가 높아 객관성이 떨어진다. * 실무에서는 “엘보우처럼 보이는 두세 지점”이 나오는 경우가 흔하며, 이를 전문가가 해석해야 한다."
  },
  {
    "objectID": "da/dap/dap_02.html#갭-통계량",
    "href": "da/dap/dap_02.html#갭-통계량",
    "title": "Clustering",
    "section": "3. 갭 통계량",
    "text": "3. 갭 통계량\nGap Statistic 갭 통계량은 Tibshirani(2001)에 의해 제안되었으며, “관측 데이터의 군집 응집도”와 “참조분포(보통 균일분포) 기반 기대치”를 비교하여 군집의 구조가 통계적으로 의미 있는지를 측정한다.\n\\[\nGap(k)=E^*[\\log(W_k)] - \\log(W_k)\n\\] * \\(W_k\\): k개의 군집으로 분할했을 때의 군집 내 분산 * \\(E^*[\\cdot]\\): 참조 분포에서의 몬테카를로(Monte Carlo) 기준 기대값\n값이 클수록 군집이 “랜덤한 분포보다 더 잘 분리되어 있다”는 의미이다.\n최적 k 선택 규칙 Tibshirani는 다음 조건을 만족하는 최초의 k를 선택하도록 제안하였다.\n\\[\nGap(k) \\ge Gap(k+1) - s_{k+1}\n\\]\n\n\\(s_{k+1}\\): 표준오차(SE)에 기반한 보정 항\n\n장점 * 엘보우 기법보다 객관적 * 내재적 군집 구조를 통계적으로 검증 가능\n단점 * 부트스트랩 반복수가 많을수록 시간 비용이 큼 * 대규모 데이터에서는 사용되지 않는 경우가 많음"
  },
  {
    "objectID": "da/dap/dap_02.html#실무-관점의-종합적-의사결정",
    "href": "da/dap/dap_02.html#실무-관점의-종합적-의사결정",
    "title": "Clustering",
    "section": "4. 실무 관점의 종합적 의사결정",
    "text": "4. 실무 관점의 종합적 의사결정\n\n4.1 지표 간 결과 차이의 존재\n엘보우 기법과 실루엣 계수는 서로 다른 관점에서 k를 선택하기 때문에 서로 다른 결과가 나오기 쉽다. 갭 통계량까지 고려하면 결과는 더욱 다양해진다.\n따라서 한 가지 지표만으로 군집 수를 결정하는 것은 통계적으로 부적절하다.\n\n\n4.2 산업 현장에서의 실제 의사결정 방식\n기업의 고객군 세분화(마스터 세그멘테이션), 금융 리스크 분류, 구좌별 소비자 유형 분류 등에서는 다음의 요소가 함께 고려된다.\n\n지표 값의 통계적 안정성\n군집의 해석 가능성(Interpretability)\n비즈니스 목적에 맞는 분류 구조\n실무 담당자 및 도메인 전문가의 판단\n추후 유지·갱신 가능성\n스케일링(표준화) 여부의 영향\n\n특히, * 변수 단위가 모두 다를 경우 표준화(Z-score normalization)는 필수적이다. * 표준화는 “정보 손실”이 아니라 “스케일 기반 의미가 소거된다는 문제”로 해석하는 것이 정확하다."
  },
  {
    "objectID": "da/dap/dap_02.html#학문성과-해석의-주관성",
    "href": "da/dap/dap_02.html#학문성과-해석의-주관성",
    "title": "Clustering",
    "section": "5. 학문성과 해석의 주관성",
    "text": "5. 학문성과 해석의 주관성\n군집 분석은 지도학습이 아니며, 통계적으로 “가장 좋은 군집”이라는 절대적 기준이 존재하지 않는다. 따라서 해석자, 전문가, 비즈니스 목적에 의해 결과가 달라진다.\n이것이 군집 분석을 최근 연구에서 Exploratory Data Analysis(EDA) 기반 기법으로 분류하는 이유이기도 하다.\n즉, 이론적 수학 모델은 제공된다. 그러나 최종 모델 선택은 이론+실무 목적+해석의 융통성이 결합된 의사결정이다."
  },
  {
    "objectID": "da/dap/dap_02.html#gmm",
    "href": "da/dap/dap_02.html#gmm",
    "title": "Clustering",
    "section": "1. GMM",
    "text": "1. GMM\n다변량 정규분포(Multivariate Gaussian Distribution)의 혼합으로 데이터 분포를 나타낸다. 각 데이터 포인트 \\(x_i \\in \\mathbb{R}^d\\) 는 다음과 같이 확률적으로 군집에 속한다.\n\\[\np(x_i) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\n\\]\n\n\\(K\\) : 군집 수\n\\(\\pi_k\\) : k번째 군집의 혼합 비율 ((_{k=1}^{K} _k = 1))\n\\(\\mu_k\\) : k번째 군집의 평균 벡터\n\\(\\Sigma_k\\) : k번째 군집의 공분산 행렬\n\\(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\) : 다변량 정규분포 확률밀도함수"
  },
  {
    "objectID": "da/dap/dap_02.html#bayesian-gmm",
    "href": "da/dap/dap_02.html#bayesian-gmm",
    "title": "Clustering",
    "section": "2 Bayesian GMM",
    "text": "2 Bayesian GMM\n군집 수 K에 대한 불확실성을 모델링하며, 사전 분포(prior)와 데이터로부터 얻은 우도(likelihood)를 결합하여 후행 분포(posterior)를 계산한다.\n\n혼합 비율 \\(\\pi_k\\) : Dirichlet 사전 분포\n평균 \\(\\mu_k\\) : Gaussian 사전 분포\n공분산 \\(\\Sigma_k\\) : Inverse-Wishart 사전 분포\n\n\\[\np(\\pi, \\mu, \\Sigma \\mid X) \\propto p(X \\mid \\pi, \\mu, \\Sigma) , p(\\pi) , p(\\mu) , p(\\Sigma)\n\\]\nBayesian 추정에서는 EM(Expectation-Maximization)과 유사한 반복적 최적화 또는 변분 추정(Variational Inference)을 사용한다.\n특징 1. 데이터 포인트마다 군집 소속 확률 제공 2. 군집 수가 명확하지 않을 때 사전 분포를 통한 자동 결정 가능성 3. 특이값(outlier)에 민감\n\n이유: 공분산 행렬 추정 시 이상치가 분산을 왜곡하기 때문\n단순 거리 기반이 아니며, 확률적 분포 추정 과정에서 민감"
  },
  {
    "objectID": "da/dap/dap_02.html#군집-안정성-검증",
    "href": "da/dap/dap_02.html#군집-안정성-검증",
    "title": "Clustering",
    "section": "3. 군집 안정성 검증",
    "text": "3. 군집 안정성 검증\n군집 결과가 신뢰할 수 있는지 평가하기 위해 여러 방법이 사용된다.\n\n동일 자료에 다양한 군집 기법 적용\n\n\n같은 데이터셋에 K-means, GMM, 계층적 군집 분석 등 다양한 가정 기반 군집 방법을 적용\n결과 군집이 유사하게 나타나는지 비교하여 안정성을 검증\n\n\n데이터 분할 Cross-validation\n\n\n데이터셋을 임의로 두 부분으로 분할\n각 부분을 독립적으로 군집 분석 수행\n군집 구조가 일관되게 나타나는지 확인\n\n\n변수 제거/추가 Sensitivity Analysis\n\n\n일부 변수를 제거하거나 추가하여 군집 분석 수행\n군집 구조가 어떻게 변화하는지 관찰\n특정 변수에 과도하게 의존하는 군집인지 평가 가능\n\n\n실무 적용\n\n\nBayesian GMM은 다중 패턴이 혼합된 데이터 예: 고객 세분화, 금융 리스크 평가, 헬스케어 환자 그룹 분류에 적합\n안정성 검증 방법은 실무 데이터 분석에서 필수적\n\n데이터 분할, 변수 제거/추가, 다른 알고리즘 비교를 통해 신뢰성 확보\n\n모델 복잡도가 높으므로, 분석 목적, 데이터 특성, 해석 가능성을 함께 고려해야 함\n\n\n파셜 주성분 분석.(교수님 박사 논문)"
  },
  {
    "objectID": "ai-ml-dl.html",
    "href": "ai-ml-dl.html",
    "title": "AI·ML·DL",
    "section": "",
    "text": "인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nDec 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n로지스틱 분류\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n현대 AI 연구\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n현대 AI 연구\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n단계별 머신러닝 학습\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSVM: 비확률적 마진 기반 분류기\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n의사결정트리 실습\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n경사하강법 (Gradient Descent)\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝: R2D3\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝: 실무 사례\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝: EnjoySport\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Approval\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAI 산업 전망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝 개론\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ai/ml_14.html",
    "href": "ai/ml_14.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: November. 12, 2025"
  },
  {
    "objectID": "ai/ml_14.html#문제점",
    "href": "ai/ml_14.html#문제점",
    "title": "인공신경망",
    "section": "2.1 문제점",
    "text": "2.1 문제점\n하나의 가중치만을 따로 떼어 분석하면 전체 네트워크의 상호의존적 구조를 반영하지 못한다는 점이 역전파의 핵심적 난점 중 하나이다.\n\n신경망은 공동 기여 시스템이다\n\n신경망의 출력은 단일 가중치의 효과가 아니라, 모든 입력 ( x_i )와 가중치 ( w_i ), 그리고 여러 층을 거친 비선형 합성 함수의 결과이다. 즉, 각 가중치는 다른 가중치들과의 조합을 통해서만 의미 있는 출력을 만들어낸다.\n이 때문에 하나의 가중치만 고립적으로 평가하면, 다른 가중치들과의 상호작용(interaction)을 무시하게 되어 실제 영향도를 정확히 알 수 없다.\n\n역전파는 ’부분 기여’를 계산하는 과정이다\n\n역전파 알고리즘의 목적은 각 가중치가 전체 손실(Loss)에 얼마나 기여했는가를 계산하는 것이다. 이때 체인룰을 이용해, 출력층에서 발생한 오차가 어떻게 각 가중치 방향으로 퍼져나가는지를 추적한다.\n즉, \\(\\frac{\\partial L}{\\partial w_i}\\) 는 “모든 다른 파라미터들이 고정되어 있을 때, ( w_i )를 미세하게 변화시켰을 때 손실이 얼마나 변하는가”를 의미한다.\n다시 말해, 하나의 가중치 변화가 전체 결과에 미치는 ‘국소적 기여도(local contribution)’를 구하는 것이며, 이는 전체 맥락에서의 상호작용을 미분의 형태로 부분적으로 포착한 것입니다.\n\n그러나 ’전체적 상호작용’은 여전히 남는다\n\n역전파는 개별 가중치의 기울기를 구하되, 그 계산 과정에 이미 모든 다른 가중치와 뉴런의 값이 포함됩니다. 즉, 수학적으로는 고립된 것이 아니라, 계산 그래프(computational graph) 전체를 거쳐 영향을 받아 나온 결과입니다. 그럼에도 불구하고 다음과 같은 한계가 존재합니다.\n\n비선형성(Nonlinearity) 때문에, 다른 가중치가 조금만 달라져도 각 기울기의 상대적 영향이 크게 바뀐다.\n따라서 한 시점의 기울기만으로는 “전체 네트워크에서의 근본적 관계”를 완전히 파악할 수 없다.\n이로 인해 실제 학습에서는 “한 번의 역전파 결과”보다 “다수의 반복(iteration)을 통한 평균적 수렴”이 중요하다. \n\n결론적으로 하나의 가중치를 따로 본다면 신경망의 다차원 상호작용을 제대로 볼 수 없다. 그러나 역전파는 바로 그 문제를 부분 미분의 누적 형태로 해결합니다.\n즉, 각 가중치의 기울기를 “다른 파라미터가 고정된 상태에서의 국소적 영향”으로 계산하고, 이를 전체 그래프를 따라 합성함으로써, 전체 시스템이 함께 조정되도록 하는 것입니다.\n결국, 하나의 가중치는 단독으로는 의미가 없고, 오직 “네트워크 전체에서의 미분적 상호작용” 속에서만 의미를 가집니다.\n이 점이 신경망이 선형 모델과 본질적으로 다른 이유이며, 딥러닝 학습이 단순한 회귀(regression)가 아닌 비선형적 협조 최적화(non-linear cooperative optimization)라는 점을 보여준다."
  },
  {
    "objectID": "ai/ml_14.html#해결-방안",
    "href": "ai/ml_14.html#해결-방안",
    "title": "인공신경망",
    "section": "2.2 해결 방안",
    "text": "2.2 해결 방안\n역전파 + 경사하강법 작동원리\n\n역전파의 순서적 기울기 계산\n\n출력층에서 손실이 계산된 후, 그 오차를 뒤로 거슬러 올라가며(backward) 각 층의 가중치가 손실에 어떤 영향을 미치는지를 따져봅니다.\n즉,\n\n마지막 층부터 오차의 영향을 계산하고,\n그 결과를 이전 층의 가중치로 전달하며,\n각 가중치의 변화 방향(∂L/∂w)을 구합니다.\n\n이게 바로 chain rule을 층별로 적용하는 과정입니다.\n\n경사하강법(Gradient Descent)\n\n각 가중치의 기울기(∂L/∂w)는 “이 방향으로 가면 손실이 증가한다”를 의미합니다. 따라서 반대 방향(−∂L/∂w)으로 이동하면 손실이 감소합니다. 이를 수식으로 표현하면: [ w_{new} = w_{old} - ] 여기서 ()는 학습률(learning rate)로, 이동 속도를 조절합니다.\n즉, 각 가중치는 “에러가 줄어드는 방향으로” 조금씩 움직이며, 이 과정이 전체 네트워크에서 동시에 일어납니다.\n\n순차적 파라미터 갱신\n\n하나의 가중치 ( w_i )를 업데이트할 때는, 그 시점의 다른 파라미터를 임시로 고정한 상태로 취급합니다. 즉, 한 번의 역전파에서는 모든 가중치를 동시에 업데이트하되, 각각은 “현재 다른 값들이 고정되어 있다”는 전제 하에서 계산된 기울기에 따라 움직입니다.\n이 점에서 동시적이면서도 독립적인 최적화 단위가 형성됩니다. 그 후, 전체 네트워크의 파라미터가 한 번에 갱신되며 다음 반복(iteration)으로 넘어갑니다."
  },
  {
    "objectID": "ai/ml_14.html#예제-시그모이드의-연산-흐름-구조",
    "href": "ai/ml_14.html#예제-시그모이드의-연산-흐름-구조",
    "title": "인공신경망",
    "section": "2.3. 예제: 시그모이드의 연산 흐름 구조",
    "text": "2.3. 예제: 시그모이드의 연산 흐름 구조\n시그모이드 함수 \\[\ng(z) = \\frac{1}{1 + e^{-z}}\n\\]\n를 계산 그래프로 풀어쓰면, 다음과 같은 일련의 연산 노드로 표현됩니다.\n\\[\nz → ( * -1 ) → exp → ( +1 ) → ( 1/x ) → g\n\\]\n즉,\n\n입력값 ( z )에\n음수를 곱하고( * -1 ),\n지수함수를 취하고( exp ),\n1을 더하고( +1 ),\n역수를 취하면( 1/x ) 결과적으로 시그모이드 출력 ( g )가 나옵니다.\n\n이처럼 단순한 하나의 수식도 여러 개의 연산 노드로 분해되어, 각 노드에서 미분(기울기)이 계산되고 역전파될 수 있도록 구성됩니다.\n\n복잡한 신경망 = 수식의 확장적 노드화\n\n신경망이 복잡해진다는 것은 결국 “이러한 단순한 연산 노드들이 수천, 수만 개로 연결되어 합성된 형태”를 의미합니다.\n즉,\n\n층이 깊을수록 합성 함수의 깊이가 늘어나고,\n뉴런 수가 많을수록 병렬적인 연산 노드가 많아지며,\n전체 네트워크는 거대한 계산 그래프(computational graph)로 확장된다.\n\n이 그래프 상에서 역전파는 체인룰을 적용하여, 출력 노드에서 입력 노드로 기울기를 노드 단위로 전파(backpropagate) 한다.\n\n직관적으로 보면 신경망의 “복잡성”은 사실 수학적 표현의 압축 정도입니다.\n\n즉,\n\n수식으로는 간단히 적힌 ( g(f(h(x))) ) 같은 표현이,\n실제 계산 단계에서는 수십 개의 노드로 세분화되어 구현됩니다.\n\n따라서, 복잡해 보이는 신경망도 결국 “단순한 기본 연산들의 반복적 조합”이며, 그 조합을 노드 그래프 형태로 펼쳐놓은 것이 바로 신경망 구조입니다.\n\n예 — hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) 같은 표현을 TensorFlow 코드·계산 그래프·TensorBoard 이미지로 웹에서 쉽게 찾을 수 있습니다.\n권장 이미지 유형 코드 스니펫과 간단 다이어그램(활성화 함수 흐름). TensorBoard로 그린 연산 그래프(MatMul → Add(b) → Sigmoid). 계산 그래프의 노드(곱셈·덧셈·비선형 연산) 시각화 예시."
  },
  {
    "objectID": "ai/ml_14.html#해결-방법2",
    "href": "ai/ml_14.html#해결-방법2",
    "title": "인공신경망",
    "section": "2.4 해결 방법2",
    "text": "2.4 해결 방법2\n\n순전파(Forward Propagation)\n\n먼저 신경망은 입력 데이터를 받아 층을 따라 순방향으로 연산을 수행합니다.\n즉, \\[\nx \\rightarrow (W_1, b_1) \\rightarrow L_1 \\rightarrow (W_2, b_2) \\rightarrow L_2 \\rightarrow \\cdots \\rightarrow \\hat{y}\n\\]\n\n\\(\\hat{y}\\): 예측값(hypothesis)\n\n이 단계에서는 단지 “현재 가중치로 계산된 결과”를 내는 것뿐입니다.\n\n역전파(Backpropagation)\n\n출력값이 실제 정답 (y)와 다르면 손실 함수 (L(y, ))가 커집니다. 이때 오차를 역방향으로 전파하여 각 가중치 (w)가 오차에 미친 영향을 계산하고, 그에 따라 가중치를 오차가 줄어드는 방향으로 미세하게 수정합니다.\n즉, \\[\nw := w - \\eta \\frac{\\partial L}{\\partial w}\n\\]\n이 과정을 데이터셋의 각 샘플(혹은 배치)에 대해 반복하면서 학습이 진행됩니다.\n\n대규모 데이터가 필요한 이유\n\n오류를 줄이기 위해서는 다양한 입력 상황을 학습해야 합니다. 데이터가 많을수록 네트워크는\n\n특정 패턴에 과적합되지 않고,\n전체 분포의 일반적 경향을 학습할 수 있습니다.\n\n즉, 대규모 데이터는 가중치가 균형 있게 조정되도록 도와주며, 이는 결국 일반화 능력(generalization)을 향상시킵니다.\n\n가중치 변화 폭의 수렴\n\n학습이 진행될수록\n\n손실이 줄어들고,\n기울기(gradient)의 크기도 점점 작아집니다.\n\n이로 인해 가중치의 변동 폭은 점점 감소하며, 결국 오차가 거의 변하지 않는 수렴 상태에 도달합니다.\n이때 각 가중치의 분포는 무작위 초기화 상태에서 점차 안정된 정규분포 형태로 가까워집니다. 이는 확률론적으로 “많은 데이터 샘플에 의해 평균적으로 조정된 결과”이기 때문입니다.\n\n테스트 데이터로 검증\n\n훈련(Training) 과정에서 사용되지 않은 테스트 데이터(Test set)를 이용해 학습된 모델이 새로운 데이터에서도 잘 작동하는지 검증합니다.\n이 과정을 통해 정확도(accuracy), 손실(loss), 과적합 여부(overfitting) 등을 평가할 수 있습니다."
  },
  {
    "objectID": "ai/ml_12.html",
    "href": "ai/ml_12.html",
    "title": "현대 AI 연구",
    "section": "",
    "text": "인공지능 발전과 대규모 언어 모델의 CoT 통합 구조에 대해 다루고자 한다.\n01 AI의 학문적 발전 인공지능(AI)의 발전은 소프트웨어적 진화와 하드웨어적 진화라는 두 축을 중심으로 병행되어 왔다.\n이 두 축은 독립적으로 발전한 것이 아니라, 서로 상호 보완적 관계 속에서 영향을 주고받으며 오늘날의 AI 연구 쳬계를 형성하였다.\n1 . 소프트웨어적 발전 ① 인과관계 중심 접근\n심볼릭 어프로치, Symbolic AI\nAI 연구의 초창기에는 인간의 사고 과정을 모사하기 위해 명시적 인과관계(explicit causality)를 기반으로 한 접근이 시도되었다.\n이 시기의 대표적 연구 흐름은 Symbolic AI로, 지식공학(knowledge engineering)을 중심으로 발전하였다.\n대표적 사례로는 전문가 시스템(expert system)이 있으며, MYCIN과 DENDRAL이 대표적 예이다.\n이러한 시스템은 인간 전문가의 지식을 ’규칙(rule)’과 ’추론(inference)’의 형태로 체계화하여, 명시적으로 저장하고, 이를 이용해 논리적 결론을 도출하는 구조를 가진다.\n장점 원인과 결과의 관계가 명확하므로, 시스템의 판단 근거를 추적할 수 있어 설명 가능성(explainability)이 높다.\n한계 모든 지식을 사람이 사전에 정의해야 하므로, 현실 세계의 복잡성과 불확실성을 충분히 반영하지 못한다.\n② 상관관계 중심 접근\n애니매틱 어프로치, Connectionist / Statistical AI\n이후 연구자들은 인과관계를 직접 모델링하기 보다는, 데이터로부터 패턴을 학습하는 방향으로 나아갔다.\n이것이 바로 연결주의적 또는 통계적 접근으로, 오늘날의 머신러닝 및 딥러닝의 기반이 된다.\n이 접근법에서는 대량의 데이터를 수집·정제한 뒤, 심층신경망(deep neural networks) 등의 모델을 학습시켜 결과를 도출한다.\n장점 명시적 인과모형 없이도 높은 예측 정확도를 달성하며, 이미지 인식·자연어 처리·음성 인식 등 다양한 분야에서 혁신적 성과를 보였다.\n한계 모델 내부 구조가 복잡하고 가중치에 의존하므로, 판단 과정이 불투명한 블랙박스(black box) 문제가 발생한다.\n즉, 상관관계를 통해 결과를 도출할 수는 있지만, 그 관계가 ‘왜’ 성립하는가에 대한 설명이 어렵다. 이로 인해 실세계 응용에서 신뢰성과 윤리성의 한계가 제기되었다.\n③ 하이브리드 접근 및 설명 가능한 AI\nExplainable AI, XAI OR Neuro-Symbolic AI\n이러한 한계를 극복하기 위한 새로운 방향으로 설명 가능한 인공지능 연구가 등장하였다.\n이는 심볼릭 AI의 논리적 해석력과 연결주의 AI의 학습 능력을 통합하는 하이브리드 AI, 혹은 뉴로-심볼릭 AI의 형태로 발전하고 있다.\n이 접근의 핵심은 예측성과 해석성의 통합에 있다.\n하이브리드 AI의 운영 구조는 다음과 같다.\n예측 단계: 대규모 데이터를 활용하여 통계적 분석과 패턴 인식을 수행함으로써 예측 결과를 생성한다.\n설명 단계: 규칙 및 온톨로지 기반의 인과 추론 체계를 활용하여, 예측의 근거를 논리적으로 해석하고 검증한다. 적용 분야: 의료, 금융, 법률 등 설명 가능성이 필수적인 영역. 현재 한계: 완전한 인과적 추론 수준에는 아직 도달하지 못하였으며, 인과적 설명의 내재화(causal reasoning integration)는 여전히 AI 연구의 핵심 과제로 남아 있다.\n2 . 하드웨어적 발전 Physical AI\nAI의 발전은 알고리즘적 진보를 넘어, 물리적 지능(physical intelligence)의 단계로 확장되었다.\n이를 피지컬 AI(Physical AI)라고 불리며, 단순한 연산 능력 향상이 아니라 실제 환경과의 실시간 상호작용을 가능하게 하는 새로운 패러다임을 의미한다.\n① 철학적 배경\n피지컬 AI의 이론적 기반은 ” 지능은 환경과의 상호작용을 통해 완성된다 ” 는 체화된 인지(embodied cognition) 개념에 있다.\n이는 지능을 단순한 계산 능력이 아니라, 신체적 경험과 감각적 피드백을 포함한 총체적 능력으로 본다.\n② 실제 구현\n이 개념은 자율주행차, 로봇 조작, 드론 제어, 스마트 IoT 기기 등에서 실현되고 있다.\nAI 알고리즘이 센서 및 엑추에이터와 결합함으로써, 기계는 외부 자극을 인식하고 판단하며 행동하는 실시간 자율 시스템으로 발전한다.\n③ 학문적 의미\n단순 기술 확장이 아니라, AI가 스스로 감지, 학습, 행동하는 실체적 지능을 실현하려는 연구 축으로 이해할 수 있다.\n하드웨어적 발전은 단순한 기술적 진보가 아닌, AI의 실체적 존재화(realization)로 이해된다.\n즉, AI가 데이터로만 존재하던 비물질적 지능에서 벗어나, 감지(sensing)–학습(learning)–행동(acting)의 순환 구조를 스스로 수행하는 단계로 진입한 것이다.\n따라서 하드웨어적 진화는 소프트웨어 중심 AI의 한계를 보완하며, 현실 세계 속에서 자율적 판단과 행동 능력을 구현하는 핵심 축으로 평가된다. 이는 향후 지능의 통합적 구현(integrated intelligence)을 실현하기 위한 필수적 토대로 자리 잡고 있다.\n02 CoT 통합 구조 Chain-of-Thought Integrated Architecture\n대규모 언어 모델(LLM, Large Language Model)은 최근 몇 년간 두 가지 방향으로 발전해왔다.\n① 하나는 언어 생성 능력(Language Generation)에 초점을 둔 학습 중심형 모델(Learning-Oriented Model)\n② 다른 하나는 논리적 사고(Logical Reasoning)를 내재화한 추론 중심형 모델(Reasoning-Oriented Model)이다.\n이 구분은 모델의 구조적 형태보다는, 모델이 최적화하는 목적 함수(Objective Function)와 훈련 패러다임(Training Paradigm)의 차이에 의해 정의된다.\n1 . 학습 중심형 모델 Learning-Oriented Model\nGPT-3에서 GPT-4.5로 이어지는 계열은 지도 학습(SL)과 자기회귀 언어 모델링(Autoregressive Language Modeling)을 기반으로 한 대표적 학습형 구조이다.\n이 모델들은 입력된 문맥(Context)에 대해 다음 단어의 조건부 확률을 최대화하도록 학습된다.\n\\[P(w_t | w_1, w_2, \\ldots, w_{t-1}; \\theta)\\]\n(:) 모델의 파라미터 손실 함수: 일반적으로 음의 로그 가능도(Negative Log-Likelihood)로 정의된다. \\[\\mathcal{L}(\\theta) = -\\sum_{t=1}^{T} \\log P(w_t | w_{&lt;t}; \\theta)\\]\n이 접근은 모델이 방대한 언어 데이터를 통계적으로 모사하며, 언어의 문맥적 패턴을 효율적으로 학습하게 만든다.\nGPT-4.5 이하의 모델들은 주로 텍스트 생성(Text Completion), 요약(Summarization), 번역(Translation) 등 언어적 유창성이 필요한 과제에 최적화되어 있다.\n그러나 이러한 모델들은 논리적 추론(logical reasoning)과 같은 고차적 사고를 명시적으로 수행하지 못한다는 한계를 지닌다.\n2 . 추론 중심형 모델 Reasoning-Oriented Model\n이 한계를 보완하기 위해 OpenAI는 2024년 이후 O 시리즈(O1, O1-mini, O3 등)를 개발하였다.\n이들 모델은 단순한 언어 예측이 아닌, 사고 과정(CoT)을 내재화하여 논리적·수학적 추론을 수행하도록 설계된 구조이다.\n즉, 결과를 곧바로 산출하는 대신, 모델 내부에서 일련의 사고 단계를 거쳐 중간 논리 과정(intermediate reasoning steps)을 생성한 후 최종 응답을 도출한다.\n이를 수식적으로 표현하면 다음과 같다.\n\\[\\text{Answer} = f_\\theta(\\text{Prompt}) = g_\\theta(\\text{Chain of Thought Steps})\\]\n(g_:)​ 모델 내부의 사고 전개 과정.\n즉, 모델은 단순히 단어를 예측하는 확률기계가 아니라, 내재적 추론 구조를 가진 사고 시스템으로 진화한 것이다.\n이러한 추론 능력은 별도의 규칙 기반이 아닌, 사전학습(Pretraining)과 인간 피드백 강화학습(RLHF)을 통해 점진적으로 강화된다.\n결과적으로 O 시리즈는 언어의 표현 능력보다 사고의 정확성과 합리성을 우선시하는 추론 중심형(reasoning-oriented) 모델로 분류된다.\n3 . GPT-5: CoT 기반 통합형 모델 Integrative Model\n2025년 발표된 GPT-5는 기존 GPT 계열의 학습 중심형 구조와 O 시리즈의 추론 중심형 구조를 통합한 CoT 기반 하이브리드 LLM 아키텍처로 정의된다.\nGPT-5는 입력의 복잡도에 따라 자동으로 두 가지 처리 모드 중 하나를 선택한다.\nFast Mode: 단순 질의에 대한 신속 응답 Deliberative Mode: 복합 문제에 대한 단계적 사고(CoT 기반)로 자동 전환하는 이중 처리 구조를 갖는다. 이 과정을 수식으로 나타내면 다음과 같다.\n\\[\\text{Output} = \\begin{cases} f_\\theta(x) & \\text{if } x \\in \\text{simple query} \\\\ f_\\theta(g_\\theta(x)) & \\text{if } x \\in \\text{complex reasoning task} \\end{cases}\\]\n(g_(x):) CoT 기반 내부 사고 전개 과정. 이를 통해 GPT-5는 단순 질의응답(Q&A)에서는 빠른 응답을 제공하고, 복합적인 문제(계획 수립, 코드 분석, 수학적 추론 등)에서는 단계적 사고 절차를 자동으로 활성화한다.\n4 . 이론적 통합 관점 학술적 관점에서 보면 GPT-5의 구조적 진화를 요약하면 다음과 같다.\n\\[\\text{L-O GPT (≤4.5)} + \\text{R-O O-Series} \\Rightarrow \\text{GPT-5 (Integrated CoT Model)}\\]\n즉, GPT-5는 단순히 매개변수 규모가 확장된 모델이 아니라, 학습 중심적 언어 처리와 추론 중심적 사고 구조를 결합하여 인공지능의 인식 능력(perception)과 사고 능력(reasoning)을 동시에 고도화한 모델로 정의된다.\nLLM 발전 모델 비교 구분 모델 계열 중심 개념 주요 기능 학습 중심형 GPT-3 ~ GPT-4.5 언어 패턴 학습 및 문맥 예측 언어 생성, 번역, 요약 등 추론 중심형 O1 ~ O3 논리적 사고 및 CoT 전개 논리·수학적 문제 해결 통합형 GPT-5 언어 생성 + 사고 통합 자동 모드 전환, 고차원 추론 수행"
  },
  {
    "objectID": "ai/ml_10.html",
    "href": "ai/ml_10.html",
    "title": "단계별 머신러닝 학습",
    "section": "",
    "text": "시그모이드, 소프트맥스, 다층 퍼셉트론 등에 대해 다루고자 한다.\n01 다중 회귀 Multiple Regression\n앞서 살펴본 단일 입력(단변량, single variable) 회귀모델은 하나의 독립 변수만을 고려하였다.\n그러나 실제 데이터 분석에서는 여러 입력값을 동시에 반영하는 다중 회귀 모델이 보다 현실적이고 효과적으로 활용된다.\n이 모델은 여러 독립 변수(feature)를 기반으로 종속 변수(target)를 예측하며, 복잡한 현실 세계의 관계를 선형적으로 근사할 수 있다.\n다중 회귀의 기본 구조는 단일 회귀와 동일하다. 입력 변수의 개수만 증가하고, 이에 대응하는 가중치(weight) 역시 벡터 형태로 확장된다.\n\\[\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\\]\nw_i: 각 특성(feature)에 대한 가중치, x_i: 입력 변수, b: 절편(bias)\n1 . 벡터와 행렬 기반 연산 입력 변수가 증가하더라도 손실함수의 계산 원리는 변하지 않는다. 다만, 계산의 복잡성이 높아지므로, 가중치와 입력값을 벡터 또는 행렬 형태로 표현된다.\n\\[\\mathbf{X} \\in \\mathbb{R}^{m \\times n}, \\quad \\mathbf{w} \\in \\mathbb{R}^{n \\times 1}, \\quad \\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b}\\]\nm: 데이터 포인트(샘플)의 수 n: 특성(feature)의 수 이러한 행렬 표현을 통해 모든 샘플에 대한 예측을 단일 연산으로 수행할 수 있어 모델 학습 과정의 계산 효율성(computational efficiency)이 크게 향상된다.\n2 . 핵심 선형대수 연산 핵심 연산은 다음과 같다.\n① 내적(dot product)\n두 벡터의 방향과 크기를 비교하는 연산으로, 회귀에서는 예측값 계산 및 유사도 측정((cos θ))에 주로 활용된다.\n② 외적(cross product)\n두 벡터에 수직인 새로운 벡터를 생성하며, (sin θ)를 통해 방향성을 표현한다. 다만 외적은 회귀 분석이나 유사도 계산보다는 물리적 벡터 계산에서 주로 사용된다.\n고차원 연산과 병렬 처리 다중 입력 연산은 수많은 곱셈과 덧셈을 포함하므로, 입력 차원이 커질수록 연산 부하가 기하급수적으로 증가한다. 특히 이미지, 영상, 음성 등 대규모 데이터셋에서는 이 문제가 더욱 두드러진다.\n이러한 연산 복잡도를 해결하기 위해 병렬 연산(parallel computation)기법이 적용되며, 대표적으로 GPU가 사용된다.\nGPU는 수천 개의 코어를 이용해 행렬 및 벡터 연산을 동시에 수행하므로, 3차원 좌표 변환이나 2차원 이미지 처리 등 고속 연산 환경에서 탁월한 성능을 발휘한다.\n또한 CUDA와 같은 GPU 전용 병렬처리 라이브러리 사용 시 AI 모델 학습 과정에서 연산 속도를 획기적으로 향상시킬 수 있다.\n이와 같이 확장된 회귀모델의 기반에는 선형대수(linear algebra)가 자리하고 있으며, 모든 계산은 행렬 곱, 내적, 외적 등 선형대수적 연산 원리에 의해 수행된다.\n3 . 스케일링과 정규화 scaling & normalization\n다중 회귀 모델의 학습 과정에서는 입력 데이터의 스케일 차이가 학습 안정성과 예측 정확도에 큰 영향을 미칠 수 있다.\n이를 방지하기 위해, 각 특성(feature)을 동일한 기준으로 조정하는 스케일링과 정규화 과정이 필수적이다.\n① 스케일링\n각 변수의 단위나 값의 범위 차이로 인해 특정 특성이 모델 학습 과정에서 과도하게 영향을 미치는 문제를 방지한다.\n예: 각 값에 대해 최대값 또는 범위를 기준으로 나누어 0~1 범위로 조정\n② 정규화\nZ-점수 정규화를 적용하여, 데이터를 평균 0, 표준편차 1의 정규분포로 변환하면 특성 간 스케일 불균형을 더욱 효과적으로 완화할 수 있다.\n예: 데이터가 타원형 분포를 가진 경우 각 값을 최대값으로 나누어 원형 형태로 정규화할 수 있다.\n이러한 변환을 수행하면 손실 함수의 등고선(contour)이 타원형에서 원형으로 바뀌어, 경사하강법의 수렴 속도와 안정성이 향상된다.\n즉, 모든 입력값을 동일한 기준으로 인식하도록 조정하여 모델이 보다 균형 잡힌 방식으로 최적화를 수행하게 만든다.\n참고로, 변동계수(CV)는 데이터의 상대적 분산을 평가하는 보조 지표로 활용될 수 있으나, 본 장에서는 필수 적용 사항은 아니다.\n02 분류와 비선형 함수의 개념 Classification & Nonlinear Functions\n앞서 다중 및 다항 회귀를 통해 연속적인 수치값을 예측하는 모델 구조를 살펴보았다.\n그러나 실제 문제의 상당수는 단순한 수치 예측이 아닌, “이것이냐, 저것이냐”와 같은 범주형(class) 결과를 요구한다.\n이처럼 출력값이 명확히 구분되는 문제에서는 회귀(regression) 대신 분류(classification) 개념이 사용된다.\n그중에서도 두 가지 범주를 구분하는 가장 기본적인 형태가 이진 분류(binary classification) 이다.\n대표적인 활용 사례는 다음과 같다.\n스팸 메일 탐지: 스팸 / 정상 소셜 미디어 콘텐츠 노출 결정: 보이기 / 숨기기 신용카드 부정 거래 탐지: 정상 / 이상 MRI 영상 판독: 정상 / 비정상 이와 같이, 분류 문제는 연속적 수치 예측과 달리 출력값이 불연속적이고 범주형이라는 점에서 학습 방식과 손실 함수, 모델 설계 측면에서 특수성을 가진다.\n1 . 회귀에서 분류로의 전환 선형 회귀는 입력값의 선형 결합을 통해 연속적인 출력값을 생성한다.\n그러나 분류 문제에서는 출력이 특정한 범위 내에 속해야 하며, 즉 결과값이 “0 또는 1”, “긍정 혹은 부정”처럼 해석 가능한 확률 형태로 제한될 필요가 있다.\n예를 들어, 종양의 크기에 따라 악성(1)과 양성(0)을 분류하는 문제를 생각해보자.\n데이터에서 “크기 40 이상일 때 악성” 으로 정의하면, 단순 선형 모델은 30 ~ 40 사이에 결정 경계선(decision boundary)을 그어 두 범주를 구분할 수 있다.\n그러나 새로운 데이터에서 70 이상의 값이 등장하면, 기존 경계선이 더 이상 유효하지 않아 새로운 기준선 재설정 문제가 발생한다.\n이러한 한계를 해결하기 위해, 선형 출력값을 0 ~ 1 사이로 압축하여 확률적으로 해석 가능한 비선형 변환 함수(activation function)가 도입된다.\n그중 가장 대표적인 예가 시그모이드 함수이다.\n2 . 시그모이드 함수 Sigmoid Function\n시그모이드 함수는 실수 입력값 (z)를 받아 이를 0 ~ 1 사이의 실수 값으로 변환하는 비선형 함수이다.\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\n여기서 (z)는 이전 장에서 다룬 선형 결합 (WX)에 해당한다. 즉, 회귀모델의 출력값을 시그모이드 함수에 통과시켜 확률(probability) 형태로 변환하는 과정이다.\n확률적 해석 가능\n입력값이 매우 크면 1에 근접 입력값이 매우 작으면 0에 근접 이 함수는 ” 연속적인 선형 출력을 상·하한이 존재하는 구간으로 압축하는 변환 ” 으로 이해할 수 있다.\n부드러운 결정 경계\nS자 곡선(S-shaped curve)을 형성하며, 입력값이 증가에 따라 악성일 확률이 점진적으로 증가한다. 선형 출력이 비선형적으로 압축되어, 부드러운 결정 경계를 제공한다.\n미분 연속성\n미분이 연속적이므로, 경사하강법과 같은 최적화 알고리즘에서 기울기 계산이 원활하다.\n이러한 특성 덕분에 시그모이드 함수는 암 진단(양성/음성), 이메일 스팸 분류(스팸/정상) 등 이진 분류 문제에서 확률 기반 의사결정을 수행하는 핵심 요소로 활용된다.\n3 . 하이퍼볼릭 탄젠트 함수 Hyperbolic tangent Function\n시그모이드 함수와 유사하게, 하이퍼볼릭 탄젠트(tanh) 함수도 입력값을 비선형적으로 변환하는 함수이다.\n이 함수는 다음과 같이 정의된다.\n\\[\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\]\n출력값은 -1 ~ 1 사이에 위치한다. 중심이 0 에 있어, 시그모이드보다 학습 과정에서 안정성이 높다. 예: 감정 분석에서 긍정(+1)과 부정(-1)을 구분, 좋아요/싫어요 같은 양방향 선택 구조를 다룰 때 유용하다.\n단, 음의 출력 범위를 포함하므로 연산적으로 시그모이드보다 약간 복잡하며, 데이터 특성에 따라 선택적으로 적용된다.\n비교표 함수 출력 범위 중심값 주 용도 장점 시그모이드 0 ~ 1 0.5 확률 기반 이진 분류 확률적 해석 용이, 단순 하이퍼볼릭 탄젠트 -1 ~ +1 0 양극적 분류(긍정/부정) 중심 0, 학습 안정성 높음 시그모이드와 tanh는 모두 선형 회귀 결과를 비선형적으로 변환하여 확률 또는 분류 경계를 생성하는 역할을 한다.\n이러한 활성화 함수의 도입은 회귀모델이 “연속 예측”에서 “범주 판별”로 확장되는 전환점이며, 이후의 심층신경망(Deep Neural Network) 구조에서도 핵심적 역할을 수행한다.\n03 다중 입력 확장\n1 . 가설 Hypothesis\n단일 입력 로지스틱 회귀에서처럼, 여러 입력(feature)을 동시에 고려할 경우에도 원리는 동일하다.\n입력값의 선형 결합 (z = WX + b)를 시그모이드 함수에 통과시키면 출력값이 0 ~ 1 사이의 확률로 변환된다.\n\\[h(X) = \\sigma(WX + b) = \\frac{1}{1 + e^{-(WX+b)}}\\]​\n이 확률값은 주어진 입력이 특정 범주에 속할 가능성을 의미하며, 이를 통해 이진 분류 문제에서 직관적이고 안정적인 의사결정이 가능하다.\n2 . 다중 입력 확장 Multivariate Input\n현실 세계의 데이터는 단일 입력보다는 여러 독립 변수(feature)를 동시에 고려해야 하는 경우가 많다.\n다중 입력의 경우에도 선형 결합의 구조는 동일하며, 단지 입력 벡터와 가중치 벡터를 확장하여 계산한다.\n\\[z = W_1 x_1 + W_2 x_2 + \\dots + W_n x_n + b = WX + b\\]\n이렇게 계산된 다중 입력 결과는 소프트맥스(Softmax) 함수를 통해 정규화될 수 있다.\n\\[\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\]\n소프트맥스는 각 범주의 확률 합이 1이 되도록 조정하며, 출력값이 여러 클래스 중 하나로 분류될 수 있도록 확률 기반 해석 제공\n다중 클래스 분류 문제에서 필수적인 역할을 수행한다.\n3 . 다중 클래스(Softmax + 원-핫) 확장 다중 클래스 문제에서는 각 클래스 k에 대해 선형 점수(score)를 계산하고 소프트맥스(Softmax)로 확률분포를 얻는다.\n\\[s_k = w_k^\\top x + b_k,\\qquad p_k=\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\]\n손실은 다중 클래스 크로스엔트로피(원-핫 레이블 y 기준):\n\\[J=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k} y^{(i)}_k \\log p_k^{(i)}\\]​\n구조적으로 퍼셉트론과 유사하며, 신경망(NN)의 전단계로 이해할 수 있다.\n4 . 다층 페셉트론 Multi-Layer Perceptron, MLP\n단일 로지스틱 회귀와 다중 입력 확장을 기반으로, 구조를 여러 층으로 확장하면 다층 퍼셉트론이 된다.\n각 층에서는 선형 변환과 비선형 활성화가 반복 적용되며, 입력 간의 복잡한 상호작용과 패턴을 학습할 수 있다.\nMLP는 심층 신경망(DNN)의 기본 단위로, 다층 구조를 통해 비선형적 결정 경계와 복잡한 데이터까지 패턴 학습이 가능하다."
  },
  {
    "objectID": "ai/ml_07.html",
    "href": "ai/ml_07.html",
    "title": "SVM: 비확률적 마진 기반 분류기",
    "section": "",
    "text": "SVM, 즉 비확률적 마진 기반 분류기에 대해 다루고자 한다.\n수준별 이해도 ① 데이터 정제 및 전처리\n대상: 기존 IT 인력 요구 수준: 기본적인 이해로 충분하며, 실무 적용 중심 내용: 결측치 처리, 이상치 제거, 형식 변환 등 데이터의 품질을 높이는 작업 ② 모델링(연계 결합, 파이프라인 구성)\n대상: 석사 및 박사 수준의 연구자 요구 수준: 심층적인 이해와 설계 능력 필요 내용: 모델의 구조 설계, 학습 및 추론 과정의 최적화, 다양한 알고리즘의 조합 및 실험 ③ 알고리즘 설계\n대상: 석사 및 박사 수준의 연구자 요구 수준: 이론적 배경과 수학적 이해가 필수 내용: 새로운 알고리즘의 개발, 기존 알고리즘의 개선, 수학적 모델링 및 분석 이러한 단계별 구분은 일반적으로 데이터 과학 및 AI 분야에서의 역할 분담과 학습 경로를 반영한 것이다.\n실무에서는 데이터 정제 및 전처리가 핵심적인 역할을 하며, 연구 및 개발 단계에서는 모델링과 알고리즘 설계가 중심이 된다.\n우리는 이번 과정에서 모델링(연계 결합 및 파이프라인 구성) 단계에 초점을 맞춰 배워볼 것이다.\n01 서포트 벡터 머신 Support Vector Machine, SVM\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js 기본적으로 선형 분리(linear separator) 찾는 알고리즘.\n데이터가 선형적으로 구분되지 않을 경우, 커널(kernel)을 이용하여 입력 데이터를 더 높은 차원 공간으로 사상(mapping)한 뒤 선형 판별을 수행한다.\n대표적인 예로 다항식 커널(polynomial kernel) 은 다음과 같이 정의된다.\n\\[K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^\\top \\mathbf{y} + c)^d\\]\n이 커널은 입력 벡터의 고차항 조합을 암시적으로 고려한다.\n또한, 명시적으로 모든 고차항을 계산하지 않아도 커널 트릭(kernel trick) 을 통해 고차원 특징 공간에서의 내적을 효율적으로 대체할 수 있다.\n① 특징(feature) / 속성, 변수\n데이터의 각 관측치는 여러 개의 속성으로 구성되며, 이를 (x_1, x_2, , x_n)​으로 표현한다.\n이때 이 (n)개의 속성을 차원(dimension) 이라고 한다.\n② 차수(degree)\n일반적으로 다항식(polynomial)의 최고 지수(exponent)를 의미한다.\n예: 다차원 입력 ((x_1, x_2)) 에 대해 (x_1^2), (x_1 x_2), (x_2^2) 등 고차항(high-order term) 을 추가하면 모델은 비선형(곡선 형태) 관계를 표현할 수 있게 된다.\n02 데이터의 벡터 표현 데이터는 점과 방향성을 갖는 벡터(vector) 로 표현될 수 있다.\n벡터는 스칼라(scalar)와 달리 크기뿐 아니라 방향을 가지며, 이를 표현하기 위해서는 시작 좌표와 끝 좌표가 필요하다.\n한편, 어떤 점 ((x_1, x_2, , x_n))도 원점을 기준으로 하면 원점에서 그 점까지의 벡터로 간주할 수 있다. 즉, 점과 벡터는 서로 밀접한 개념이며, 데이터는 수학적으로 이러한 벡터 형태로 표현된다.\n자연어 처리(NLP)에서는 문장을 단어(토큰) 단위로 나누고, 각 단어를 하나의 임베딩 벡터(embedding vector) 로 나타낸다.\n이를 다음과 같이 표현할 수 있다.\n\\[\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{id}) \\in \\mathbb{R}^d\\]\n(i:) 문장 내 단어의 인덱스 (d:) 임베딩 공간의 차원(dimension) 모든 단어 벡터는 동일한 차원 (d)를 가지며, 유사한 의미를 가진 단어들은 벡터 공간상에서 가까운 위치에 존재한다. 이러한 벡터 표현은 문장 내 의미적 관계를 수치적으로 분석할 수 있게 해 준다.\n03 토큰 수와 표현의 제약\n1 . 언어 모델의 입력 구조 언어 모델은 입력할 수 있는 토큰(token) 의 수에 제한이 있다. 즉, 모델은 정해진 최대 토큰 길이(max token length) 이내의 입력만 처리할 수 있다.\n또한, 각 토큰은 고정된 차원 (d)의 임베딩 벡터(embedding vector) 로 변환되므로, 모델의 입력 공간 역시 차원이 고정되어 있다고 할 수 있다.\n① 고전적 표현 방식\nBag-of-Words 또는 One-hot Encoding 에서는 문장 내 각 단어의 존재 여부를 0과 1로 표시하여 표현하였다.\n예: 특정 단어가 문장에 포함되어 있으면 1, 포함되지 않으면 0으로 나타내는 방식.\n② 현대적 표현 방식\n임베딩 기반 모델(embedding-based model) 에서는 이진값이 아닌 실수(real-valued) 벡터를 사용하며, 단어의 의미 유사성과 문맥 정보를 함께 반영한다.\n이로써 모델은 단순한 단어 포함 여부를 넘어, 단어 간의 의미적 관계와 문맥적 상호작용까지 학습할 수 있다.\n2 . 문장의 벡터 표현과 유사도 계산 문장은 이러한 단어(토큰)들의 벡터로 구성되며, 전체 문장은 다차원 공간 상의 하나의 점(vector) 으로 표현된다.\n문장 간의 유사도는 두 벡터 간의 거리(distance) 나 코사인 유사도(cosine similarity) 를 통해 측정할 수 있으며, 거리가 가까울수록 의미적으로 유사한 문장으로 해석된다.\n그러나 실제 데이터는 차원이 매우 높고, 대부분의 값이 0인 희소(sparse) 형태를 띤다.\n이러한 고차원 희소 표현은 계산 비용은 크고 메모리 효율은 낮아, 이를 더 작은 차원으로 변환하는 차원 축소(dimensionality reduction) 가 필요하다.\n차원 축소는 본래 의미 구조를 최대한 보존 및 데이터 압축 과정이다.\n대표적인 기법: PCA(주성분분석), SVD(특이값 분해), 워드 임베딩(word embedding) 등.\n이 과정을 통해 얻어진 밀집 벡터(dense vector) 는 정보가 효율적으로 압축된 형태로, 계산 효율이 높고 모델이 의미적 관계를 더욱 잘 학습할 수 있다.\n이러한 벡터는 저차원 공간으로 투영(projection) 되어, 이후의 학습, 분류, 또는 유사도 계산 등의 연산에 활용된다.\n04 SVM의 핵심 개념\nSupport Vector Machines (SVM): An Intuitive Explanation\nEverything you always wanted to know about this powerful supervised ML algorithm\nmedium.com\n출처: Tibrewal (2021), Medium – Support Vector Machines (SVM)\n1 . 결정 경계 Decision Boundary\nSVM은 두 클래스를 분리하는 초평면 (^ + b = 0) 을 찾는다.\n쉽게 말해, 데이터를 두 클래스(예: 빨간 점 vs 파란 점)로 나누는 선(2차원) 또는 초평면(3차원 이상) 을 찾는 것이며, 핵심 목표는 두 클래스 사이의 “간격”을 최대화하는 초평면을 찾는 것이다.\n초평면의 수학적 표현과 조건 결정 경계: (^ + b = 0) 마진 경계: (^ + b = +1) 또는 (^ + b = -1) 서포트 벡터 조건: (y_i(^_i + b) = 1)\n2 . 마진 Margin\n결정 경계와 서포트 벡터 사이의 거리. SVM은 이 마진을 최대화하는 초평면을 찾는다.\n마진이 넓을수록 모델의 일반화 능력이 향상되어, 새로운 데이터가 들어와도 결정 경계가 잘못 분류될 가능성이 줄어든다.\n즉, 마진 크기와 분류 오류 확률은 반비례 관계에 있다.\n\\[P_{\\text{error}} \\propto \\frac{1}{\\gamma_2}\\]\nSVM은 “마진 최대화 문제” 를 풀어 초평면을 결정한다. 수학적으로는 다음 문제를 푼다:\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\quad \\text{s.t.} \\; y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1\\]\n(:) 초평면의 법선 벡터(normal vector) ( b ; : ; ) 편향(bias) (y_i:) 클래스 레이블(+1 또는 -1) (_i:) 각 데이터 점 이 문제는 Lagrange 승수법 및 이중 문제(dual problem)를 통해 풀 수 있으며, 서포트 벡터만으로도 최적의 초평면을 결정할 수 있다.\n3 . 서포트 벡터 Support Vectors\n결정 경계 또는 마진 경계에 가장 가까운 데이터 포인트들. 쉽게 말해, “초평면을 밀었을 때 점들이 가장 먼저 닿는 위치”이다.\n이 점들은 초평면의 위치와 방향을 결정하며, 나머지 점들은 보통 결정 경계에 영향을 주지 않는다.\n(단, soft margin SVM에서는 일부 내부 점이 영향을 줄 수도 있다.)\n05 SVM의 최적화 문제 앞서 설명한 마진 최대화 문제에서, 이상치가 존재하거나 데이터가 완전히 선형적으로 분리되지 않는 경우 슬랙 변수 (_i)를 도입한다.\n이는 약간의 분류 오류를 허용하면서 마진 최대화가 가능해, 각 데이터 포인트의 결정 경계에서의 거리를 고려하는 힌지 손실 함수로 최적화 문제를 정의한다.\nSVM의 최적화 문제는 다음과 같이 표현된다:\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i\\]\n단, 제약 조건은 다음과 같다:\n\\[y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\quad \\forall i\\]\n(_i:) 슬랙 변수(허용 오차) (C:) 정규화 파라미터\n1 . 슬랙 변수 & 힌지 손실 Slack Variable & Hinge Loss\n슬랙 변수 (_i)는 각 데이터 포인트가 결정 경계에서 얼마나 벗어났는지를 나타낸다.\n이는 힌지 손실 함수와 동일한 역할을 한다:\n\\[L(y_i, f(\\mathbf{x}_i)) = \\max(0, 1 - y_i f(\\mathbf{x}_i))\\]\n여기서 (f(_i) = ^_i + b)이다. 이 함수는 다음과 같은 경우에 대해 다르게 동작한다:\n(_i)가 정확하게 분류됨, 마진을 만족하는 경우: 손실 0 (_i)가 정확하게 분류됨, 마진 내에 있는 경우: 양의 손실 (_i)가 잘못 분류됨: 큰 손실 이러한 손실은 모델이 마진을 넓히고 오차를 최소화하도록 유도한다.\n2 . 비선형 분류를 위한 커널 방법 비선형 데이터의 경우, SVM은 커널 함수를 사용하여 고차원 특성 공간으로 데이터를 매핑한다. 이는 선형적으로 분리 불가능한 데이터를 선형적으로 분리할 수 있게 된다.\n커널 함수는 다음과 같은 형태를 가진다:\n\\[K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\\]\n(())는 특성 공간으로의 매핑 함수(basis function, kernel)로, 커널 트릭을 사용하면 내적 계산만으로 고차원 공간에서의 계산을 효율적으로 수행할 수 있다.\n현실 데이터에서는 선형 관계보다는 비선형 관계가 더 흔하다.\n예: AND, OR 게이트는 선형적으로 분리 가능하지만 XOR 게이트는 선형 경계로 구분할 수 없으며, 이를 해결하려면 2차원 이상의 고차원 공간으로 매핑해야 직선(초평면)으로 구분 가능하다.\n따라서 XOR 문제와 같이 선형 분리가 어려운 문제를 표현하기 위해 AI 연구자들은 원 공간(original space)에서 고차원 매핑 공간(mapping space)으로 데이터를 변환하는 개념을 도입했다.\n이때 사용되는 함수들을 커널 함수(kernel function)라 부르며, 매핑된 공간(mapping space)에서는 비선형 데이터가 선형적으로 분리 가능해진다.\n대표적인 커널 함수: 선형(linear), Gaussian (RBF), 다항식(polynomial), sigmoid 등.\n한편, 데이터 처리 과정에서 발생하는 노이즈는 측정 오류나 무작위 변동에 의해 생기는 것으로 일반적으로 제거를 고려한다.\n반면 이상치는 데이터 패턴에서 벗어난 극단적인 값으로 유의미한 정보를 포함할 수 있어 분석에 포함되기도 한다.\n노이즈와 이상치를 구분하는 기준은 보통 도메인 전문가의 판단에 따라 결정된다.\n4 . 최적화 문제의 이중 문제 Dual Problem\nSVM의 최적화 문제는 이중 문제로 변환할 수 있으며, Lagrange 승수 (_i)를 도입하여 다음과 같이 표현된다:\n\\[\\max_{\\alpha} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\\]\n단, 제약 조건은 다음과 같다:\n\\[0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0\\]\n이 이중 문제를 풀면 최적의 (_i)를 얻을 수 있으며, 이를 통해 초평면의 계수 ()와 (b)를 계산할 수 있다.\n06 모델 일반화와 조정\n1 . 하이퍼파라미터 상수 (C)는 슬랙 변수에 부여되는 패널티 강도(regularization strength)를 조절한다.\n(C)가 크면: 오차를 최소화하려는 경향이 강해져 과적합 위험 증가 (C)가 작으면: 마진 증가 및 일부 오차를 허용하여 일반화 성능 향상 즉, (C) 값은 결정 경계의 유연성을 결정하는 핵심 요인이다. 이 값을 조정하는 과정을 하이퍼파라미터 튜닝(Hyperparameter Tuning) 또는 파인튜닝(Fine-Tuning)이라 한다.\n2 . 전이학습 Transfer Learning\n모델이 학습 데이터에는 잘 맞지만, 새로운 데이터(테스트 데이터)에서는 성능이 저하되는 경우를 과적합이라 한다.\n이를 방지하려면 다양한 데이터에 대해 잘 작동하는 일반화 능력이 필요하다.\n딥러닝에서는 이러한 일반화를 위해 이미 대규모 데이터(예: 기사, 위키피디아 등)로 학습된 사전 학습 모델(Pre-trained Model)을 기반으로 하는 전이학습(Transfer Learning)이 활용된다.\n전이학습은 기존 지식을 새로운 도메인에 적용하는 과정이며, 특정 분야의 데이터로 추가 학습하여 세부 조정을 수행하는 단계를 파인튜닝이라 한다.\n결국, SVM에서의 (C) 조정과 같이 전이학습 및 파인튜닝 역시 “기존 모델의 일반화 능력을 유지하면서 특정 목적에 맞게 조정하는 과정”이라는 공통 원리를 가진다."
  },
  {
    "objectID": "ai/ml_04.html",
    "href": "ai/ml_04.html",
    "title": "머신러닝: 실무 사례",
    "section": "",
    "text": "반도체 HBM 공정과 스마트 CCTV를 중심으로 머신러닝의 실제 활용에 대해 다루고자 한다.\n01 제조 공정 혁신\n1 . 과거 제조업 현장 오랜 기간 아날로그 방식의 공정 관리에 의존해 왔으나 글로벌 경쟁 심화와 공정 복잡성 증가로 인해 단순 수작업이나 경험 중심의 운영만으로는 품질과 효율성을 확보하기 어려워졌다.\n이에 따라 디지털 전환(DX) 과 인공지능 전환(AX) 은 제조업 경쟁력 강화를 위한 핵심 과제로 부상하였다.\nDX와 AI 트랜스포메이션의 개념을 정리하고, 실제 사례로서 반도체 HBM 적층 공정의 기술적 차이를 분석한다.\n2 . Digital Transformation (DX) 기존의 아날로그 및 비체계적 공정을 시스템화된 디지털 관리 체계와 데이터베이스 기반 운영으로 전환하는 과정이다.\n공정, 설비, 품질 관리 정보를 실시간으로 수집·저장 운영 데이터의 추적성(traceability) 확보 표준화된 관리 체계 구축을 통해 오류 및 낭비 최소화 제조업에서 DX는 스마트 팩토리로의 진입을 위한 기초 단계라 할 수 있다.\n3 . AI Transformation (AX) DX 위에 구축된 데이터 인프라는 AI 분석을 통한 고도화로 확장된다. 이것은 다음 단계들을 포함한다.\n센서 및 IoT 장치를 통한 빅데이터 수집 실시간 데이터 시각화 및 공정 모니터링 이상치(anomaly) 탐지 및 자동 알림 데이터 기반 원인 분석 및 해결 방안 제시 공정 최적화 및 예측 유지보수 이러한 체계는 설비 수준이 낮은 공장에서도 점진적 도입이 가능하며, 데이터 기반 의사결정을 통해 전사적 품질 및 생산성 개선 효과를 제공한다.\n4 . 사례 분석: HBM 적층 공정 고대역폭 메모리(HBM)는 여러 개의 DRAM 다이를 수직으로 적층하여 초고속 데이터 전송을 가능케 하는 핵심 기술이다. 하지만 적층 단수가 증가할수록 수율(yield) 저하가 주요 문제로 지적된다.\n① SK하이닉스\nMR-MUF 공정을 활용하여 적층 단수 증가에도 수율 저하를 최소화한 것으로 평가된다.\n일부 보고에 따르면 12단 적층에서도 비교적 안정적인 수율 확보가 가능하다.\n② 삼성전자\nTC-NCF 기반 공정을 채택했으며, 접합 안정성 확보 및 void 제어 문제로 인해 수율 측면에서 난제를 겪고 있는 것으로 알려져 있다. 다만, 최근에는 Cu-to-Cu 본딩 등 새로운 기술을 병행 도입하고 있다.\n공개된 산업 보고 및 전문가 분석에 따르면, “삼성은 수율이 70% 미만, SK는 상대적으로 높은 수율 확보”라는 평가가 존재하나, 이는 공식 수치가 아닌 시장 리서치 기반 추정에 가깝다.\n‘HBM’ 골든타임 노리는 엇갈린 시선···삼전 ‘포괄적’ vs SK하닉 ‘트렌드’\n[이뉴스투데이 김진영 기자] AI 반도체 시장이 급속도로 확대되는 가운데 고대역폭메모리(HBM)의 중요성이 커지고 있다. 삼성전자와 SK하이닉스는 모두 AI 생태계 확장을 목표로 하고 있지만 HBM을\nwww.enewstoday.co.kr\nAI 시대의 새로운 심장: 삼성전자와 SK하이닉스, HBM과 차세대 메모리 기술로 펼치는 반도체 패권\nAI 시대의 HBM 기술 혁명! 삼성전자와 SK하이닉스의 패권 경쟁, 차세대 메모리의 미래를 탐구합니다. 한국 반도체의 도약을 확인하세요!\nskywork.ai 따라서 기술적 차이와 경향성은 확인되지만, 구체적인 수치는 신뢰도 높은 자료가 부족하므로 주의가 필요하다.\n5 . 결론 DX 와 AX 는 제조업 혁신의 핵심 경로로, 단순한 디지털화에서 나아가 AI 기반 예측·최적화 체계로의 전환을 의미한다.\n이는 설비 수준과 무관하게 단계적으로 도입이 가능하며, 스마트 팩토리 구현의 기반이 된다.\nHBM 사례는 AX 의 필요성을 잘 보여준다. 고난도의 적층 공정에서는 공정 변수와 결함 데이터를 정밀하게 수집·분석해야 수율을 안정적으로 확보할 수 있다. SK와 삼성의 공정 차이는 이러한 데이터 기반 접근의 중요성을 방증한다.\n따라서 제조업의 경쟁력 확보를 위해서는 DX → AX → 스마트 팩토리로 이어지는 단계적 전략이 필수적이며, 이는 단순한 시스템 도입을 넘어 데이터와 AI 중심의 운영 철학 전환을 요구한다.\n02 서울시 스마트 CCTV 시스템 서울시는 2020년부터 지능형 CCTV를 도입하여 교통 흐름 분석, 범죄 예방, 실시간 교통 신호 제어 등을 시도하였으나, 초기 도입 단계에서 AI 모델이 실제 현장 환경에서 기대 이하의 성능을 보였다.\n이는 모델이 주로 실내 환경이나 정형화된 데이터에만 학습되어, 다양한 날씨, 시간대, 사람의 행동 패턴 등 실제 환경의 변수를 충분히 반영하지 못했기 때문이다.\n서울시는 이러한 문제를 해결하기 위해 산·학·연과 자치구가 참여하는 ‘지능형 CCTV 활성화 계획’ 을 수립하고, 맞춤형 이벤트 설정, 오탐 데이터 학습, 사물·사람 구분 학습 등을 추진하였다.\n지능형CCTV로 만드는 디지털 안전도시 서울\n서울시가 시민 안전 강화 및 범죄 등 예방을 위해 올해 AI기반 지능형 CCTV를 대폭 늘리고, 시민들의 정보접근성을 높여줄 공공와이파이를 확대할 계획입니다. 서울시는 또 유동인구가 많은 곳 등\nscpm.seoul.go.kr\n서울시, ’지능형 CCTV’로 지자체 ICT 우수사례 대통령상 수상\n서울시가 ’제30회 지방자치단체 정보통신 우수사례 발표대회’에서 지능형 CCTV 오탐지 문제 해결로 대통령상인 최우수상을 수상했다. 관제 효율과 시민 안전망 강화가 높이 평가됐다. 서울시는\nwww.etnews.com 그 결과, 지능형 CCTV의 상황 판별 정확도는 36% → 71%로 향상되었고, 관제요원의 이벤트 확인률도 37% → 82%로 높아졌다.\n불필요한 탐지 건수는 월 454만 → 53만 건으로 줄어들며 약 8.8배 감소하였다. 이러한 개선은 사건 처리 건수를 이전보다 6배 이상 증가된 성과를 가져왔다.\n또한, 서울시는 2026년부터 지능형 CCTV에 생성형 AI를 접목하는 시범사업을 추진하여, 기존 CCTV가 단순히 ‘이상 유무’ 만을 판별하는 수준에서 ‘왜 이상한지, 어떤 맥락인지’ 를 설명할 수 있는 단계로 진화할 계획이다.\n이러한 사례는 AI 기반 CCTV 시스템의 효용성을 높이기 위해서는 다양한 상황을 반영한 데이터 수집과 모델 훈련이 필수적임을 보여준다.\n정상 상황뿐 아니라 예외적 상황까지 포함하는 데이터 기반의 모델 훈련이 가장 중요한 요소로 고려되어야 한다."
  },
  {
    "objectID": "ai/ml_02.html",
    "href": "ai/ml_02.html",
    "title": "머신러닝 개론",
    "section": "",
    "text": "머신러닝의 전반적인 개론에 대해 다루고자 한다.\n\n01 머신러닝\nMachine Learning\n데이터로부터 패턴을 학습하여 예측이나 의사결정을 수행하는 인공지능의 한 분야이다.\n머신러닝은 데이터의 라벨(Label) 존재 여부에 따라 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)으로 구분된다. 이를 이해하기 위해 대표적인 예시인 분류와 군집화를 비교하도록 한다.\n1 . 분류(Classification) 지도학습의 대표적인 기법으로, 라벨이 주어진 데이터를 학습하여 새로운 데이터의 정답을 예측한다.\n결과값은 이산형 범주(예: 스팸/일반메일)이며, 대표 알고리즘으로 로지스틱 회귀, 의사결정나무, SVM 등이 있다.\n2 . 군집화(Clustering) 비지도학습에 속하며, 라벨이 없는 데이터를 유사도에 따라 그룹화하여 내재된 구조를 발견한다.\n실제 라벨이 없으므로 손실함수 대신 클러스터 내의 응집도(cohesion)와 클러스터 간 분리도(separation)와 같은 목적함수를 사용하여 데이터의 구조를 평가한다. [출처]\n대표 알고리즘으로는 K-Means, DBSCAN, 계층적 군집화가 있으며, 고객 세분화나 이미지 분석 등에 활용된다. [출처1], [출처2]\n반지도학습(Semi-supervised Learning)\n일부 데이터에만 라벨이 존재하는 경우, 라벨이 없는 데이터를 함께 활용해 학습 성능을 향상시키는 방법이다. 이는 라벨링 비용이 높은 실제 환경에서 자주 사용된다.\n02 손실함수 Loss function\n앞서 살펴본 지도학습은 라벨이 존재하는 데이터를 기반으로 예측 모델을 학습한다. 이러한 학습의 핵심은 예측값과 실제값의 차이를 최소화하는 것으로, 이를 위해 손실함수를 정의한다.\n예측과 정답의 오차(error)를 수치화하며, 학습의 목표는 이 손실을 최소화하는 것이다. [출처]\n이 과정에서 사용되는 가중치(weight) 는 입력 변수의 중요도를 나타내는 모델의 매개변수이며, 학습은 손실함수를 최소화하는 방향으로 가중치를 조정하는 절차이다.\n기울기(gradient)는 손실함수에 대한 가중치의 변화율을 의미하며, 이를 이용해 가중치를 반복적으로 업데이트한다.\n최종적으로 학습이 완료되면 성능 지표(evaluation metric)를 통해 모델의 예측력이 평가된다. [출처]\n강화학습, Reinforcement Learning\n앞서 지도학습이 정답 데이터를 통해 모델을 학습했다면, 강화학습은 보상을 통해 스스로 최적의 행동을 학습한다.\n예를 들어, 로봇이 공의 궤적을 예측하고 어떤 타격 동작을 해야 가장 좋은 보상을 얻는지를 학습하는 방식이다.\n계층적 구조를 적용해, 상위 수준에서는 언제 어떤 자세로 칠지 등의 전략을 결정하고, 하위 수준에서는 실제 관절 제어와 동작 실행을 담당한다.\n결과적으로 로봇이 사람처럼 스핀·속도에 대응하며 탁구를 치는 것이 가능해진다.\n03 차원축소 Dimensionality reduction\n고차원 데이터에서 중요한 정보를 보존한 채 저차원으로 표현하는 기법이다.\n숨겨진 구조나 패턴을 더 명확히 드러내고 데이터의 시각화 및 분석 효율을 높이는 데 활용된다. [출처]\n이들은 차원을 줄이면서도 정보 손실을 최소화하는 것을 목표로 한다.\n이러한 차원 축소 알고리즘들은 학습 방식에 따라 선형과 비선형으로 구분된다.\n1 . 선형 기법 데이터의 분산이나 경계선을 직선(또는 평면) 형태로 설명하는 방식이다.\n공분산 행렬의 고유벡터나 선형 결합 계수를 구하며, 경우에 따라 가중치나 파라미터를 학습하지만 반드시 절편(intercept)이 포함되지는 않는다.\n대표적인 기법으로는 PCA(주성분분석), LDA(선형판별분석)이 있다.\n2 . 비선형 기법 복잡하게 휘어진 데이터 구조를 그대로 보존하려는 방식이다.\n고차원 공간에서의 유사성 확률을 저차원에서도 유지하도록 하며, 전통적인 가중치나 절편 개념을 사용하지 않는다.\n대표적인 기법으로는 t-SNE가 있다.\n3 . 특징의 의미 머신러닝에서 특징(feature)은 관측된 입력 변수로서, 목표값 y를 예측하는 데 사용되는 정보이다. [출처]\n이러한 변수들은 모두 같은 역할을 하지는 않으며, 모델은 학습을 통해 변수의 기여도나 중요도를 반영한다. [출처]\n선형 모델에서는 가중치(coefficient)가 크면 특정 변수의 영향이 크다는 의미가 될 수 있지만, 변수 간 상관관계나 스케일 차이로 단순 비교는 주의가 필요하다. [출처]\n그러나 모든 모델이 가중치를 명확히 제공하는 것은 아니다.\n특히 비선형 구조나 트리 기반 모델에서는 변수의 영향력을 직접 해석하기 어려우므로,\nfeature importance, SHAP, LIME 등의 기법을 이용해 각 변수의 기여도를 시각화한다.\n04 최적화 알고리즘 파라미터 공간(parameter space)을 탐색하여 더 나은 해를 찾는 절차라는 점에서 탐색(search)의 성격을 가진다.\n하지만 전통적인 그래프, 경로 탐색 알고리즘과는 달리, 연속적인 공간에서 목적 함수를 기준으로 한 최적 해를 찾아가는 과정이다.\n비선형(non-convex) 문제나 딥러닝 모델의 경우, 전역 최적해(global optimum)에 도달할 수 있는지,\n아니면 지역 최적해(local optimum)나 안장점(saddle point)에 머무를지에 대한 이론적 증명은 매우 복잡하다.\n특히 Adam, AMSGrad 같은 최적화 기법의 수렴(convergence)을 보장하는 연구에서도, 하이퍼파라미터 설정이나 학습률 조건 등이 중요한 변수가 되며, 수렴 증명 자체가 쉽지 않다는 점이 강조된다.\n또한 최적화 과정은 많은 반복(iteration)과 대규모 데이터, 복잡한 모델 구조가 결합될 경우 계산 비용(computation cost)과 시간적 비용이 매우 크다.\n이러한 이유로 실무에서는 학습 과정이 고비용·고시간 소모적이라는 점이 자주 지적된다.\n05 실무 사용처\n1 . 지자체 분야 지자체는 교통 흐름 예측, 범죄 예측, 환경 모니터링 등 다양한 분야에서 머신러닝 알고리즘을 활용한다.\n예: 교통 혼잡도 예측, 재난 발생 가능성 예측\n2 . 제조 분야 제조업체는 센서 데이터를 분석하여 장비 고장을 예측하고, 이를 통해 유지보수 비용을 절감하는 예지 보수(Predictive Maintenance)를 한다.\n생산 라인에서 발생하는 결함을 실시간으로 감지 및 품질 향상을 위해 머신러닝 알고리즘이 사용된다.\nKAMP, 인공지능제조플랫폼\n스마트 대한민국 구현의 허브!제조 AI 강국으로의 도약 KAMP가 함께합니다.\nwww.kamp-ai.kr\n3 . 금융 분야 금융 기관은 고객의 신용도 평가 및 대출 리스크 관리를 위해 머신러닝 모델을 활용한다.\n거래 패턴을 분석하여 이상 거래를 실시간으로 감지 및 사기 예방에 사용된다.\n06 데이터 분석 파이프라인\n1 . 수집, Collection DB 연계(Repository), 웹 크롤링, 에이전트 설치, 서버 로그, IoT 센서 등 다양한 채널을 통해 데이터를 확보한다.\n이때 수집된 데이터에는 정상치(normal data), 비정상치(outlier 또는 anomaly), 그리고 노이즈(noise) 가 함께 포함될 수 있으며, 이 세 요소의 비율과 품질은 분석 성능에 직접적인 영향을 미친다.\n다만 이러한 비율은 고정된 규칙이 있는 것은 아니며, 데이터의 특성과 도메인(예: 제조, 금융, 의료 등)에 따라 경험적으로 조정·판단되어야 한다.\n핵심은 각 요소를 정확히 구분하고 적절히 처리하는 것이다.\n2 . 정제, Cleaning 수집된 데이터의 품질을 향상시켜 학습 알고리즘이 효율적으로 동작하도록 한다.\n이 단계에서는 먼저 추가·삭제·대체 등의 과정을 통해 누락값(missing values)을 처리하거나 불필요한 데이터를 제거한다.\n또한, 로그 변환이나 스케일 조정 등 변환(Transformation) 작업을 수행하고, 범주형 데이터를 수치형으로 바꾸는 인코딩(Encoding) 과정을 거친다.\n변수 간의 범위를 맞추기 위해 스케일링(Scaling) 또는 정규화(Normalization) 를 적용하며, 이상치(outlier)는 탐지 후 제거하거나 조정하여 데이터의 왜곡을 방지한다.\n한편, 이상치 또는 희소 클래스(sparse class)에 대한 증강(augmentation) 은 전통적 전처리(cleaning) 절차에는 포함되지 않지만,\n불균형(class imbalance) 문제를 완화하거나 특정 도메인(예: 이미지, 텍스트)에서 모델의 일반화 능력 향상을 위해 합성(over-sampling), 생성모델 기반 증강, 혼합 확장(mixup) 등 증강 기법으로 활용된다.\n이러한 방식은 소수 클래스의 표현을 늘려 분포의 대표성을 높이는 전략으로 연구되고 있다.\n예를 들어, SMOTE(Synthetic Minority Over-sampling Technique)는 소수 클래스 데이터 간 보간(interpolation) 방식을 통해 합성 샘플을 생성함으로써 클래스 불균형을 완화하는 대표적인 증강 기법이다.\n또한, GAN(Generative Adversarial Network) 기반 증강 기법(BAGAN 등)은 소수 클래스를 생성하여 불균형 문제를 해결하려는 연구도 있다.\n3 . 저장, Storage 수집 및 정제된 데이터를 효율적으로 보관하고, 분석과 학습에 활용할 수 있도록 관리한다.\n데이터는 세 가지 유형으로 나뉘며, 각 유형에 따라 저장 기술이 다르다.\n① 정형 데이터, Structured Data\n고정된 스키마를 가진 테이블 형식 데이터로, 관계형 DB(RDBMS)에 저장된다.\n예: MES(Manufacturing Execution System)에서 생성되는 생산 이력, IoT 센서 데이터 등\n대표 기술: MySQL, MariaDB, PostgreSQL, SAP HANA(인메모리 DB), 데이터 웨어하우스(Amazon Redshift, Google BigQuery)\n② 비정형 데이터, Unstructured Data\n사전 정의된 스키마가 없으며, 텍스트, 이미지, 영상, 오디오 등 다양한 형식을 포함한다.\n예: CCTV 영상, 소셜 미디어 게시물, 고객 피드백\n대표 기술: MongoDB, Cassandra, HBase(NoSQL DB), Hadoop HDFS, 데이터 레이크(Amazon S3, Azure Data Lake)\n③ 반정형 데이터, Semi-structured Data\n일부 구조적 요소를 포함하지만 완전한 스키마는 없는 데이터\n예: JSON, XML, 로그 파일, IoT 센서 데이터\n대표 기술: MongoDB, Couchbase, 데이터 레이크(Amazon S3, Azure Data Lake)\n참고 사항 Schema-on-write: 데이터를 저장할 때 스키마를 정의하는 방식 (RDBMS)\nSchema-on-read: 데이터를 읽을 때 스키마를 적용하는 방식 (NoSQL, 데이터 레이크)\n4 . 시각화, Visualization 분석 결과를 직관적으로 이해 및 전달하기 위한 단계이다. 특히 대시보드(Dashboard)는 여러 시각화 요소를 통합하여 웹이나 애플리케이션 상에서 상호작용 가능하게 제공한다.\n① 주요 시각화 도구\nPlotly: 웹 기반 인터랙티브 시각화 라이브러리로, 2D·3D 플롯과 대시보드 제작에 적합하다. Dash: Plotly를 기반으로 한 Python 웹 프레임워크로, 코드만으로 대시보드를 구현 가능하다. Matplotlib: 기본 2D 시각화에 강점을 가지며, 세밀한 커스터마이징이 가능하다. Seaborn: Matplotlib 기반으로 통계적 시각화에 특화되어 있으며, 간결한 문법과 미려한 디자인을 제공한다. ② 주요 시각화 유형\n2D/3D 플롯: 데이터 분포 및 변수 간 관계를 표현 바 차트, 파이 차트: 범주형 데이터의 분포를 시각화 히트맵(Heatmap): 데이터 간 상관관계 또는 밀도를 색상으로 표현 맵(Map): 지리적 데이터를 시각화하여 공간적 패턴 분석\n5 . 분석, Analysis 수집·정제·저장된 데이터를 기반으로 의미 있는 인사이트를 도출한다. 빅데이터와 AI 환경에서 머신러닝은 핵심적인 역할을 하며, 네 가지의 주요 기법이 있다.\n① 분류, Classification\n지도학습(Supervised Learning)에 속하며, 입력 데이터에 대해 미리 정의된 레이블을 예측한다.\n활용 예: 스팸 이메일 필터링, 질병 진단 등\n② 군집화, Clustering\n비지도학습(Unsupervised Learning)에 속하며, 라벨 없이 데이터 내 유사한 특성을 가진 그룹을 식별한다.\n활용 예: 고객 세분화, 시장 분석\n③ 예측, Prediction/Regression\n연속형 목표값을 예측하며, 과거 데이터 기반으로 미래 값을 추정한다.\n④ 추천, Recommendation\n사용자 행동이나 선호도를 분석하여 개인화된 추천을 제공한다.\n활용 예: 전자상거래, 콘텐츠 플랫폼"
  },
  {
    "objectID": "ai/ml_01.html",
    "href": "ai/ml_01.html",
    "title": "AI 산업 전망",
    "section": "",
    "text": "AI 산업 전망에 대해 다루고자 한다.\n01 팬데믹\n1 . 온라인 전환 및 IT 수요 급증 팬데믹으로 인해 오프라인 중심이던 기업들이 온라인으로 빠르게 전환되었다.\nMS의 CEO인 Satya Nadella는 2020년 4월 30일에 발표된 분기 실적 보고서에서 다음과 같이 언급하였다:\n” 우리는 두 달 만에 2년 분량의 디지털 전환을 경험했다. ” [출처] 재택근무와 원격교육이 보편화됨에 따라 화상회의, 실시간 번역, 맞춤형 배경 등 다양한 비대면 기술이 빠르게 발전했으며, 이로 인해 협업 솔루션 시장은 급격히 확대되었다.\n연도 시장 규모 추정치 출처/비고 2019 약 9.878 십억 달러 Allied Market Research 2020 약 15.25 십억 달러 Fortune Business Insights 2021 추정치 계산 불가 COVID-19 효과가 강해, 일관된 비교가 어렵다는 한계 있음. 2022 약 27.4081 십억 달러 Grand View Research 2023 약 21.79 십억 달러 Fortune Business Insights 2024 약 36.1142 십억 달러 Grand View Research 미국의 온라인 쇼핑 매출은 2019년 5,712억 달러에서 2020년 8,154억 달러로 전년 대비 43% 증가했다. [출처]\n유럽에서도 2020년 4월, 전체 소매 판매는 큰 폭으로 감소했지만, 온라인 구매 비중은 2019년 4월 19.1%에서 30.7%로 급증했다. [출처]\n각국의 락다운이 시행되면서 화상회의, 원격근무, 온라인 교육 등의 증가로 인터넷 트래픽이 1주일 이내에 15 ~ 20% 증가했다. [출처]\n2 . 팬데믹 종료 이후 이전 수준으로 회귀 여부 소비 회복의 불균형 가능성이 언급되었다.\n팬데믹으로 대면 서비스 수요는 급감했지만, 팬데믹 이후 일부 행태는 유지될 가능성이 컸다. [출처]\n또한, 많은 오프라인 매장의 폐업, 상업용 부동산 공실 증가 등은 온라인 전환이 영구적인 변화였음을 시사한다.\n1 . 오프라인 회복, 여전히 주요 비중 유지 2019년 디지털 판매 성장률이 53.5%였던 반면, 2022년 오프라인 매출은 전체 성장의 78.1%를 차지하며 오프라인 매장이 여전히 매출 성장의 주요 원천임을 보여준다. [출처]\n실제 매장 개방 시 디지털 매출이 평균 6.9% 증가하지만, 매장을 닫으면 온라인 매출은 11.5% 줄어든다는 연구도 있다. 이는 오프라인 매장의 존재가 디지털 매출에도 긍정적 영향을 준다는 점을 시사한다. [출처]\n2 . 오프라인이 다시 강세를 보이는 추세 팬데믹 중 급성장한 전자상거래는 이후 이전 예상 수준으로 회귀하는 경향이 있는 반면, 오프라인 소매업은 예상보다 더 높은 회복세를 보였다. 오프라인 소매 비중이 온라인보다 더욱 빠르게 반등했다는 분석도 있다. [출처]\n3 . 장기적인 변화는 지속 중 “다시는 이전으로 돌아가지 않을 것”이라는 표현이 있을 만큼, 디지털화된 경제 구조는 팬데믹 이후에도 지속될 가능성이 크다는 분석도 존재한다. 변화된 생산 및 소비 방식이 장기적으로 유지될 것이라는 견해이다.\n02 GPT\n1 . GPT 탄생, 챗봇 혁명 OpenAI는 2022년 11월 30일, GPT-3.5 기반의 ChatGPT를 공개적인 “연구 미리보기(research preview)” 형태로 출범시켰다.\n이 발표 직후 비약적인 성장세를 보였으며, 출시 두 달 만에 1억 명 이상의 사용자를 확보하는 등 빠른 이용자 확산을 기록했다.\n이 모델은 텍스트 기반 응답뿐만 아니라, 코드 생성 및 코드 완성이 가능한 능력을 갖추고 있어 많은 개발자가 실제로 활용 중이다.\n2 . 저렴한 수준의 AI 코딩 서비스 OpenAI가 제공하는 최상위 구독 요금제인 ChatGPT Pro는 월 $200에 무제한 GPT-4o Pro 접근권, 고급 음성 기능 등을 포함하며, 이는 약 50만 원 수준에 해당한다. [출처]\n구글도 마찬가지로 AI Ultra 요금제를 발표했으며, 가격은 $249.99/월로, Deep Think reasoning 모드 등 고급 기능을 탑재한 모델 및 AI 툴 세트를 제공한다. [출처]\n위 요금제는 고급 모델을 사용하려는 개인 혹은 연구자에게 적합하며, 초급 개발자를 비용 대비 대체하는 수준의 AI 기능을 충분히 포함하고 있다.\n3 . 월 10 ~ 20만 원대의 AI 코딩 도구 보다 현실적인 예시로, 다음과 같은 AI 코딩 어시스턴트도 존재한다: [출처]\n구분 비용 및 특징 GitHub Copilot 약 $10/월 , GPT-4 기반 코드 완성 기능 제공 Cursor Pro 약 $20/월, 다양한 모델 액세스 및 IDE 통합 Tabnine Pro 및 유사 도구들 $12–20/월 이들 도구는 초급 개발자 수준의 코딩 지원에 필요한 기능을 충분히 제공하며, 월 약 10 ~ 20만 원의 비용으로 사용 가능하다.\n03 경제 둔화\n1 . 물가 상승 → 금리 인상 중앙은행은 높은 인플레이션을 억제하기 위해 기준금리를 인상한다. 이는 통화정책의 기본 방향이며, 비교적 일반적인 경제 수단이다.\n테일러 룰(Taylor Rule)에 따르면, 인플레이션이 1% 상승할 때마다 명목금리는 그 이상을 인상해야 실제 금리가 상승하여 경제를 안정시키는 효과가 생긴다.\n2 . 금리 인상 → 고용 감소 물가 상승을 막기 위한 금리 인상은 기업의 비용 부담을 높여 고용 축소로 이어질 수 있으며 일부 계층(예: 여성, 소수민족 등)에 더 큰 영향을 줄 수 있다. [출처] 실제로 금리 인상 후 경기 침체(리세션) 즉 고용 시장이 위축되는 경향이 있다.\n3 . 고용 감소 → 소비 위축 고용이 줄면 소득 및 소비 여력이 감소되어, 결과적으로 경제활동 전반에 부정적 영향을 미친다. 이는 중앙은행이 다시 경기 부양을 위한 금리 인하를 고려하게 만드는 요인 중 하나이다. [출처]\n4 . 소득 감소 및 불확실성 → 투자 여력 축소 고용 불안정과 소비 위축은 기업의 투자 여력도 감소시킨다. 이는 재정의 불안정과 기업 활동 위축으로 이어질 수 있다는 일반적 경제 논리와 맞닿아 있다.\n04 고령화\n1 . 기술직 내 세대별 노동력 변화 대형 테크 기업에서 직원 평균 연령이 약 3년 이상 상승했다. 이는 밀레니얼 세대 이상이 IT 업계 내에서 지배적인 위치를 차지하게 되었음을 보여준다. 동시에, 디지털에 익숙한 젊은 세대(Gen Z)의 비중이 감소하고 있는 것으로 해석된다. [출처]\n이러한 경향은 “신입보다 시니어 인력이 주류가 되는 현상”과 연관시켜 볼 수 있다.\n2 . 고령화된 노동층의 도전 및 기회 AI 도입 과정에서 고령 노동자를 배제하지 않고 포함할 수 있도록 하는 전략 즉 ’age-proofing AI’의 필요성을 강조한다.\n이는 인력 다양성과 포용성을 위한 조치이며, 고령자 중심 구조가 불가피한 사회 변화로 다가올 수 있음을 시사한다. [출처]\n55–64세 연령층의 고용률은 학력 수준에 따라 크게 차이가 있으며, 고학력자 중심으로 고령 노동 시장에서의 참여가 증가하고 있다. [출처]\n3 . 시니어 중심 노동 시장에 대한 사회적 우려 45세 이상의 IT 종사자 중 70%가 연령 차별을 경험하거나 목격한 적이 있다고 응답했다.\n이 사실은 고령 인력이 노동 시장에서 고립될 수 있다는 구조적 위험도 내포한다. [출처]\n중국의 “curse of 35” (35세 저주) 사례도 주목할 만한다. 일부 IT 기업들은 30대 중반 이상 직원을 비용 부담과 에너지 부족을 이유로 선호하지 않는 경향이 있으며, 이는 고령 인력의 지위가 위태로울 수 있음을 나타낸다. [출처]\n4 . 기업 내 핵심 지식과 경험의 상실 U.S. 기업들의 경우, 베이비붐 세대가 은퇴하면서 조직의 핵심 지식이 사라지고 경험이 축적되지 않는 상황에 직면하고 있다. 이는 기업의 효율성과 혁신 역량 저하로 이어질 수 있다. [출처]\n고령 직원의 은퇴는 생산성 및 리더십 공백을 초래할 수 있어, 이를 대비한 체계적인 승계 계획과 멘토링 프로그램 수립이 필수적이라고 강조한다. [출처]\nNASA의 아폴로 사업 경험처럼, 핵심 인력 퇴직 후 해당 지식을 대체하지 못한 사례들도 있다. [출처] 2005년 Accenture 조사에서는 많은 기업들이 은퇴자 지식 이전을 위한 계획조차 없었음을 보고했다. [출처]\n5 . 고령 노동 비중 증가 추세 2022년 대비 2032년까지, 65세 이상 노동자의 노동 참여율이 6.6%에서 8.6%로 상승하며, 전체 노동 인구 증가분 중 57%가 고령층에 해당할 것으로 전망된다. 이는 고령층 노동자 비중이 높아지는 구조적 변화를 가리킨다. [출처]\nベイビーブーマー 세대의 대규모 은퇴로, Gen X, 밀레니얼, Z세대가 노동 시장의 주요 역량으로 부상하며, ’Silver Tsunami’로도 불리는 전환기가 도래하고 있다. [출처]\n05 산업 변곡점 1998년, NIPA(한국 IT산업진흥원)이 설립되어 대한민국의 IT 산업 육성을 위한 중추적 역할을 해 왔다.\n1 . 한국 정부의 AI 전략 집중 2025년 8월, 한국 정부는 AI 및 혁신 프로젝트 30개 추진을 골자로 한 AI 중심 경제 전략을 발표했다. 로봇, 자동차, 반도체, 드론 등 산업 전반을 포함하며, 국가 성장 펀드 규모는 100조 원 규모이다. [출처]\n또한 국가 AI 전략위원회를 설립하여 AI 정책 및 전략을 대통령 직속으로 관리하고 있다.\n“한국 정부는 AI를 국가 전략 산업으로 설정했고, 이제 본격적인 출발점에 서 있다.” 결과적으로, 기존 IT는 계속 존재하지만 정책·투자·혁신 관점에서는 AI가 1순위, 전통 IT는 2순위라는 구조적 변화라고 볼 수 있다.\nAI 산업에서의 경쟁 우위 확보는 주요 국정 과제로 꼽힌다. 이를 위해서는 데이터 확보 및 AI 인프라가 중요한 전략으로 강조된다. [출처]\n2 . 플랫폼을 통한 데이터 확보 한국에서는 네이버가 검색 엔진 시장의 절반 이상을 차지하고 있으며, 외국 Big Tech의 점유율은 상대적으로 낮게 유지되고 있다는 보고가 있다. [출처]\n특히, 한국은 지리정보 데이터와 같은 특정 데이터에 대해 엄격한 규제를 두고 있으며, 이로 인해 Google Maps 같은 서비스 기능이 완전히 작동하지 못하는 경우가 있다.\n또, Google은 한국의 앱스토어 수수료 및 자사 앱 우선 노출 등 관련하여 공정거래위원회 또는 규제 대상이 된 사례가 있으며, 이는 국내 플랫폼 보호와 경쟁 환경의 복합적 모습을 보여준다. [출처]\n한국 내 검색·광고 시장 분석에서는 네이버가 국내 사용자 특성, 언어, 콘텐츠 생태계와의 적합성 덕분에 강한 지위를 유지하고 있다는 언급이 있다. [출처]\n네이버와 카카오가 AI·콘텐츠 강화 전략을 내세우며 “자체 생태계 확장”을 추진 중이라는 보도도 있다. [출처]\n3 . 일본의 플랫폼 부재 일본은 전통적으로 내수 중심 플랫폼 기업이 한국·중국 등에 비해 적다는 평가가 있다.\n언어 및 문화, 법률·규제 측면에서 외국 기술 및 플랫폼 기업에 의존하는 경향이 있었고, 이에 따라 일본 정부나 대기업이 데이터 주권, 플랫폼 통합성 확보에 더 민감할 가능성 있다.\n일본과 유럽 간 데이터 공간(data space) 구현 방식을 비교한 연구에서는, 일본이 거버넌스 체계나 인증 프레임워크, 기술 표준 면에서 유럽보다 상호 운용성 확보에 어려움을 겪고 있다는 분석이 나온다. [출처]\n저작권 및 개인정보 보호 규제 측면에서, 일본은 AI 학습용 데이터 활용에 있어 일부 유연한 접근을 허용한 사례도 있으나, 여전히 데이터 접근과 보안 규제의 균형이 과제로 남아 있다. [출처]\n특허 기반 연구에서도 일본 기업들이 기술 복잡성 측면에서는 대응 가능하지만, 신생 플랫폼 기업의 민첩성과 확장성에서는 경쟁국에 비해 약점이 있다는 해석이 나온다. [출처]\n① 생성형 AI는 대규모 데이터와 학습용 데이터의 접근성이 핵심이다.\n일본이 거버넌스 체계, 인증 프레임워크, 기술 표준 면에서 유럽보다 상호 운용성 확보가 어렵다면, 다양한 출처의 데이터를 통합·활용하는 데 제약이 생긴다.\n결과적으로 AI 모델 학습 속도와 성능 향상에 제한이 발생할 수 있으며, 저작권·개인정보 보호 규제의 균형 문제도 데이터 확보와 활용에 직접적 영향을 준다.\n② AI 시스템 개발에는 소프트웨어 설계, 최적화, 확장성이 필수적이다.\n일본의 소프트웨어 산업이 플랫폼 경쟁력 확보에서 취약하다면, 혁신적 AI 서비스 개발이나 글로벌 경쟁에서 불리할 수 있다.\n특허 기반 연구에서는 기술 복잡성 대응은 가능하지만, 신생 플랫폼 기업의 민첩성 및 확장성 부족은 시장 적응과 혁신 속도에 제한을 줄 수 있다.\n따라서 한국도 국내 플랫폼이 강력한 우위를 가진다고 방심할 수 없다. 데이터 확보, 기술 표준화, 소프트웨어 역량 강화, 규제 환경의 유연화 등 다각적인 대비가 필요하며, 지속적인 혁신과 인프라 확충을 통해 경쟁 우위를 유지해야 한다.\n06 AI의 전망\n1 . AI 산업의 급성장 MS는 AI 인프라 공급업체인 Nebius와 최대 200억 달러 규모의 5년 계약을 체결하였다. 이는 AI 모델 학습과 추론을 위한 GPU 리소스 확보를 위한 전략의 일환이다. [출처]\nNVIDIA는 차세대 AI 칩인 ’Rubin CPX’를 2026년 말 출시할 예정이며, 이는 고해상도 비디오 생성 및 AI 기반 SW 개발에 최적화되어 있다. [출처]\n데이터 센터의 전력 수요가 2027년까지 50%, 2030년까지 165% 증가할 것으로 전망하고 있다. [출처]\n2 . 데이터와 GPU의 중요성 IDTechEx는 2025년 GPU 배치량이 기하급수적으로 증가할 것으로 예상하며, 이는 AI 모델 훈련과 추론에 필수적인 요소로 작용하고 있다. [출처]\n데이터 센터 GPU 시장이 2025년 1,199억 7,000만 달러에서 2030년까지 2,280억 4,000만 달러에 이를 것으로 예상하고 있다. [출처]\n3 . AI 관련 인력 수요 Databricks는 AI 제품에 대한 수요 증가로 연간 수익이 40억 달러에 이를 것으로 예상하며, 이는 AI 관련 인력의 수요 증가를 시사한다. [출처]\n4 . AI 산업의 지속 가능성 NVIDIA는 AI 시장이 연평균 성장률(CAGR) 26.6%로 성장할 것으로 전망하며, 이는 AI 산업의 지속 가능성을 뒷받침한다. [출처]\n현재 엔비디아(NVDA)의 주가는 170.76달러로, 전일 대비 2.58달러(1.53%) 상승하였다. 이는 AI 산업의 성장과 엔비디아의 시장 지배력을 반영하는 지표로 볼 수 있다."
  },
  {
    "objectID": "ai/ml_03.html",
    "href": "ai/ml_03.html",
    "title": "머신러닝: R2D3",
    "section": "",
    "text": "모델 학습과 R2D3 를 활용한 실무 적용 방법에 대해 다루고자 한다.\n01 신경망 Neural Network, NN\n현대 딥러닝 모델의 대부분은 신경망 구조에서 발전하였으며, 데이터의 특성(공간적·순차적 구조)과 문제 유형에 따라 다양한 형태로 변형되어 사용된다.\n1 . 인공 신경망 Artificial Neural Network\n인공 신경망은 딥러닝의 기본 구조로, 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 구성된다.\n각 뉴런은 입력값에 가중치(weight)와 편향(bias)을 적용한 뒤, 활성화 함수(activation function)를 통해 비선형 출력을 생성한다.\n딥러닝(DL)은 이러한 인공 신경망을 다층화(심층화)하여 복잡한 패턴과 비선형 관계를 학습할 수 있도록 확장한 구조이다.\n2 . 합성곱 신경망 Convolutional Neural Network, CNN\n이미지와 같이 격자형 구조를 가진 데이터의 공간적 패턴(spatial pattern)을 학습하는 데 특화되어 있다.\n합성곱(convolution) 연산을 통해 국소적인 특징(local feature)을 추출하며, 계층이 깊어질수록 점차 추상적인 특징을 학습한다.\n최근에는 Vision Transformer(ViT) 등과 같은 트랜스포머 기반 모델이 이미지 분석에도 적용되며, CNN과 함께 시각 분야의 핵심 구조로 활용되고 있다.\n3 . 순환 신경망 Recurrent Neural Network, RNN\n시간적 또는 순차적 데이터를 처리하기 위해 설계된 구조로, 이전 단계의 은닉 상태(hidden state)를 현재 입력과 함께 사용하여 시점 간 의존성(temporal dependency)을 학습한다.\n다만, 기본 RNN은 긴 시퀀스 처리 시 기울기 소실(vanishing gradient) 문제가 발생할 수 있다.\n이를 개선하기 위해 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit)와 같은 변형 구조가 널리 사용된다.\n4 . 트랜스포머 Transformer\n순환 구조 없이 시퀀스 데이터를 병렬적으로 처리할 수 있도록 고안된 모델로, 자기어텐션(Self-Attention) 메커니즘을 통해 입력 간 상관관계를 학습한다.\n이 구조는 RNN보다 장거리 의존성(Long-range dependency)을 효과적으로 포착하며, 자연어처리(NLP)를 넘어 이미지, 음성, 멀티모달 학습 등 다양한 분야로 확장되고 있다.\n요약하자면,\nCNN은 “공간적 패턴 추출”에, RNN은 “순차적 패턴 학습”에, Transformer는 “병렬적·전역적 패턴 학습”에 강점을 지닌다. 이들은 모두 “신경망”이라는 공통 기반 위에서 발전해 온 핵심적인 딥러닝 구조이다.\n02 사이킷런 이러한 신경망 기반 접근 외에도, 통계적 학습 이론을 바탕으로 한 전통적 머신러닝 기법이 존재한다.\n그 대표적인 라이브러리가 Scikit-learn이며, 파이썬 기반의 대표적인 머신러닝 라이브러리로 회귀, 분류, 클러스터링, 차원 축소 등 다양한 알고리즘을 일관된 인터페이스로 제공한다.\n이 라이브러리는 심층 신경망 구조인 CNN, RNN 등을 기본적으로 포함하지 않으며, 이러한 구조는 TensorFlow, Keras, PyTorch 와 같은 딥러닝 프레임워크에서 주로 구현된다.\n이 알고리즘은 본질적으로 데이터의 구역을 단순 분할이 아닌, 입력 변수(feature), 목표 변수(target) 간의 관계를 함수적 형태로 모델링하는 방식이다.\n일반적으로 데이터는 ” 행(sample) × 열(feature) ” 형태의 구조를 가지며, 각 행은 개별 관측치를, 각 열은 설명 변수를 나타낸다.\n모델은 이러한 특징들을 바탕으로 목표 변수를 예측하는 함수를 학습한다.\n예: 아파트 가격 예측 문제에서 면적, 교통, 교육 환경 등 다양한 요인을 특징으로 사용 시 데이터는 고차원 공간에 분포하게 된다.\n이때 데이터가 3차원에서 도넛 형태와 같이 복잡한 분포를 보일 수 있으며, 단순한 선형 경계로는 구분하기 어려운 경우가 많다.\n이러한 상황에서는 PCA, t-SNE, UMAP 과 같은 차원 축소 기법을 활용하여 고차원 데이터를 저차원 공간으로 투영함으로써 데이터의 패턴과 구조를 보다 명확하게 시각화할 수 있다.\n03 고도와 주택 가격 R2D3의 시각적 기계학습 입문 글에서는 샌프란시스코와 맨해튼의 주택 가격 데이터를 활용하여, 머신러닝 모델이 입력(feature)과 목표(target) 간 관계를 학습하는 과정을 시각적으로 설명한다.\nA visual introduction to machine learning\nWhat is machine learning? See how it works with our animated data visualization.\nwww.r2d3.us\n샌프란시스코는 언덕 지형이 많아 고도(elevation)가 주택 가격에 영향을 미치는 중요한 요인이 된다.\n반면 맨해튼은 대체로 평지 구조이므로 고도 요인의 영향이 미미하거나 다르게 나타난다.\n이처럼 고도는 미국 부동산 시장에서 elevation 이라는 변수로 다뤄지며, 조망(view), 프라이버시, 도시 스카이라인 등과 함께 가격 형성에 작용할 수 있다.\n한국의 경우, 언덕 위 아파트는 접근성 저하, 급경사 도로, 기반 시설 확충 비용 등의 이유로 가격이 낮게 평가되는 경향이 있다.\n반대로 바닷가 조망이 있는 주택은 오히려 낮은 가격을 받는 경우도 있다. 이는 고도 자체보다 조망성, 접근성, 지형 조건 등 복합 요인들이 작용한 결과라 볼 수 있다.\n이러한 차이는 머신러닝에서 하나의 입력 변수(feature)가 지역적·사회적 맥락에 따라 서로 다른 패턴을 학습하게 되는 과정을 잘 보여준다.\n입력 변수(feature)를 2D 평면에 점으로 시각화하고, 이 점들을 기준으로 분류 경계(decision boundary)를 학습하는 과정을 설명한다.\n이와 유사하게, 부동산 시장에서 고도(elevation)를 하나의 특징(feature)로 입력하면, 모델은 고도와 가격 패턴 사이의 관계를 학습하여 경계 또는 함수 형태로 변환할 수 있다.\n예를 들어, 한국에서는 고도 증가가 가격 하락으로 이어지는 함수적 경향이, 미국 일부 지역에서는 고도 증가가 가격 상승과 연결된 함수적 경향이 학습될 수 있다.\n즉, 고도(elevation)는 부동산 가격 모델링에서 하나의 중요한 입력 변수이며, 지역별 맥락과 다른 특징들과의 상호작용을 고려해야 한다.\n이러한 지역별 차이를 반영하여, 머신러닝 모델은 학습 데이터 기반으로 최적의 예측 함수를 찾아내게 된다.\n전체 데이터를 학습용(train), 검증용(validation), 테스트용(test) 으로 분할한다.\n예를 들어, 전체의 70 %를 학습용, 30 %를 테스트용으로 나눈 뒤, 학습용 중 일부(예: 10–20 %)를 검증용으로 재분할한다.\n학습 중에 매 epoch마다 학습 정확도와 검증 정확도를 기록하여 학습 상태를 모니터링한다. 검증 정확도가 점진적으로 상승하면 정상적인 학습 경향이다.\n하지만 검증 정확도가 자주 들쭉날쭉하거나, 학습 정확도만 계속 상승하고 검증 정확도는 정체되면 과적합 또는 학습 불안정성을 의심해야 한다. 이때 하이퍼파라미터 조정, 정규화(regularization) 적용, 또는 데이터 분할 재설계 등을 고려한다.\nR2D3 예제에 따르면,\n학습 정확도(training accuracy): 100% 검증 정확도(validation accuracy): 89.7% 구체적으로,\n뉴욕 주택: 112개 중 100개 정확 분류 샌프란시스코 주택: 130개 중 117개 정확 분류 이는 모델이 학습 데이터에 과적합되어 있으며, 현실적 적용에는 제한적임을 의미한다. 실무 적용 기준으로는 최소 95% 이상의 검증 정확도가 요구되므로, 현 시점에서의 모델 적용은 신중을 요한다.\n04 결정 트리 학습과 과적합 관리\n1 . 단일 변수 기반 분류의 한계 의사결정트리는 각 노드에서 조건문(if-then)을 통해 데이터를 두 그룹으로 분할하며, 경계를 설정하여 분류를 수행한다.\n그러나 단일 변수(예: elevation)만으로는 고도와 주택 가격 간의 비선형적 관계, 지역별 특성 등을 충분히 반영할 수 없어 데이터 간 명확한 구분이 어렵다.\nR2D3 시각화 예시에서도 나타나듯, 고도 하나만으로 주택 가격 범주를 정확히 분류하기 어려우므로, 면적, 교통 접근성, 조망 등 여러 변수(feature)를 고려한 다변량 분할이 필요하다.\n2 . 최적 분할과 분할 기준 트리는 각 노드에서 모든 후보 특성(feature)과 임계값(threshold)을 조합하여 가능한 분할(split candidates)을 탐색한다.\n각 후보 분할은 엔트로피(entropy), 정보 이득(information gain), 지니 불순도(Gini impurity) 등 불순도 지표를 통해 평가되며, 가장 데이터가 균질하게 나뉘는 최적 분할(best split)이 선택된다.\n이 과정을 반복하면, 학습 데이터가 점차 균질한 소그룹으로 세분화되어 목표 변수(target)에 대한 예측 성능이 향상된다.\n3 . 과적합의 발생과 영향 트리 깊이가 증가하면 모델이 학습 데이터에 과도하게 적합(overfit)될 수 있다.\n학습 데이터에서는 높은 정확도를 나타내지만, 테스트 데이터에서는 일반화 성능이 저하되는 과적합 현상이 발생한다.\nR2D3 예시에서는 단일 변수 분할을 반복하여 분류 경계를 지나치게 세분화함으로써 일부 데이터 포인트가 잘못 분류되는 사례가 확인된다.\n이러한 과적합은 모델이 학습 데이터의 불필요한 변동과 노이즈까지 학습하게 만들어, 실무에서의 모델 신뢰도를 떨어뜨리는 부작용을 초래한다.\n4 . 과적합 방지 전략 과적합을 방지하기 위해서는 여러 전략을 병행할 수 있다.\n먼저, 모델 제약을 통해 트리의 최대 깊이(max depth)나 최소 샘플 수(min samples split)와 같은 파라미터를 제한하면, 지나치게 세분화되는 것을 방지할 수 있다.\n또한, 가지치기(pruning)를 통해 불필요하게 세분화된 분할을 사후적으로 제거하며 모델 단순화 및 일반화 성능을 향상할 수 있다.\n마지막으로, 교차검증(cross-validation)을 활용하여 학습과 검증을 반복하면, 모델이 특정 데이터셋에만 과적합되는 것을 방지하고 안정적인 성능을 확보할 수 있다."
  },
  {
    "objectID": "ai/ml_06.html",
    "href": "ai/ml_06.html",
    "title": "Credit Approval",
    "section": "",
    "text": "의사결정트리 기반 신용평가 분석에 대해 다루고자 한다\n01 Credit Approval UCI의 Credit Approval 데이터셋의 일부 속성(attribute)들을 보고, 어떤 속성을 학습 가설(feature set)에 포함하는 것이 더 좋은지를 판단하고자 한다.\n참고자료\n\n기계 학습의 기초 2-1~2-5 &gt; Machine Learning in Korean (1) | 카이스트 응용인공지능 연구실\n\naai.kaist.ac.kr 본 장에서는 KAIST 인공지능연구원(AAI)에서 제공한 강의 자료 기계 학습의 기초 3강, 4강을 참고하였다.\nUCI Machine Learning Repository\nA1: b, a. A2: continuous. A3: continuous. A4: u, y, l, t. A5: g, p, gg. A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff. A7: v, h, bb, j, n, z, dd, ff, o. A8: continuous. A9: t, f. A10: t, f. A11: continuous. A12: t, f. A13: g, p, s. A14: continuous. A15:\narchive.ics.uci.edu # pip install ucimlrepo # ucimlrepo 파이썬 패키지를 통해 간편히 불러올 수도 있다.\nfrom ucimlrepo import fetch_ucirepo import pandas as pd\n\n1. 데이터 불러오기 (Credit Approval, ID = 27)\ncredit = fetch_ucirepo(id=27)\n\n\n2. 피처와 타깃 결합\ndf = pd.concat([credit.data.features, credit.data.targets], axis=1) df\n데이터셋 개요\n전체 인스턴스 수: 690개 샘플(instances): 690개 피처(features): 15개 타깃(target, 승인 여부): 1개 일부 피처들이 연속형(continuous), 일부 명목형(categorical).\n결측값(missing values)이 존재함 (UCI 리포지토리에 “Has Missing Values? Yes” 라고 명시됨). [출처]\n결측이 표기된 방식이 ‘?’ 문자열 (특히 categorical 또는 일부 continuous 피처)임. [출처]\n여기서 A1, A9 두 속성이 각각 긍정/부정 클래스를 얼마나 잘 분리할 수 있는지를 비교하고 있고, 어떤 속성이 더 좋은 속성인지 판별하고자 한다.\n\n\n각 컬럼의 고유값이 시리즈 형태로 반환\nunique_values = df[[‘A1’, ‘A9’, ‘A16’]].apply(lambda x: x.unique()) unique_values\nA1에 결측치 확인\nA1, A9 속성 기준 승인/거절 빈도 및 퍼센트 요약표 생성\nimport pandas as pd\n\n\nA1 컬럼의 NaN 값을 문자열 ’?’로 변환 (결측치 표시)\ndf[‘A1_filled’] = df[‘A1’].fillna(‘?’)\n\n\n전체 클래스 분포\nclass_dist = df[‘A16’].value_counts().rename_axis(‘A16’).reset_index(name=‘count’) total_positive = class_dist.loc[class_dist[‘A16’]==‘+’,‘count’].values[0] total_negative = class_dist.loc[class_dist[‘A16’]==‘-’,‘count’].values[0]\n\n\nA1 교차표 (결측치 포함)\ncross_A1 = pd.crosstab(df[‘A1_filled’], df[‘A16’]).reset_index() cross_A1.insert(0, ‘속성’, ‘A1’) cross_A1 = cross_A1.rename(columns={‘A1_filled’: ‘값’, ‘+’: ‘승인(+)’, ‘-’: ‘거절(-)’})\n\n\n퍼센트 컬럼 추가\ncross_A1[‘승인(%)’] = (cross_A1[‘승인(+)’] / total_positive * 100).round(1) cross_A1[‘거절(%)’] = (cross_A1[‘거절(-)’] / total_negative * 100).round(1)\n\n\nA9 교차표 (결측치 없음)\ncross_A9 = pd.crosstab(df[‘A9’], df[‘A16’]).reset_index() cross_A9.insert(0, ‘속성’, ‘A9’) cross_A9 = cross_A9.rename(columns={‘A9’: ‘값’, ‘+’: ‘승인(+)’, ‘-’: ‘거절(-)’}) cross_A9[‘승인(%)’] = (cross_A9[‘승인(+)’] / total_positive * 100).round(1) cross_A9[‘거절(%)’] = (cross_A9[‘거절(-)’] / total_negative * 100).round(1)\n\n\n전체 클래스 분포를 표 형태로 변환\noverall = pd.DataFrame({ ‘속성’: [‘전체’], ‘값’: [‘-’], ‘승인(+)’: [total_positive], ‘거절(-)’: [total_negative], ‘승인(%)’: [100.0], ‘거절(%)’: [100.0] })\n\n\n데이터 병합\ncombined_df = pd.concat([overall, cross_A1, cross_A9], ignore_index=True) combined_df\n1 . 속성 선택 기준 머신러닝 / 개념 학습 이론에서 좋은 속성(feature)을 선택하는 기준은 다음과 같다:\n정보 이득(IG) 혹은 엔트로피 감소 지니 계수 (Gini impurity) 감소 분류 순도 (Purity) 잡음 민감성 — 속성이 얼마나 레이블 노이즈에 영향을 덜 받는가 이 속성 선택 기준들은 결정 트리 알고리즘(C4.5, CART 등)에서 흔히 쓰인다.\n즉, 어떤 속성이 클래스 라벨(+)과 (–)를 더 잘 구분할 수 있을지, 엔트로피(불확실성)를 많이 줄일 수 있을지를 보는 것이다.\n2 . A1 vs A9 비교\n기계 학습의 기초 3강 pdf 자료 A9 속성의 Positive/Negative 비율을 보면, t일 경우 약 79%가 Positive, f일 경우 약 93%가 Negative로 나타나, 클래스 구분력이 매우 높음을 알 수 있다.\n반면 A1 속성은 a와 b 사이의 Positive 비율 차이가 약 3~4% 수준으로 미미하여, 클래스 구분 신호가 약하다.\n따라서 (A9)를 포함하는 학습 가설 모델은 더 높은 성능을 기대할 수 있다.\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n3 . 주의할 점 / 한계 제시된 것은 빈도수 기반 단순 비교일 뿐이고, 실제 속성의 복합 상관관계나 다른 속성과의 상호작용은 고려하지 않는다.\n결측치(?) 처리가 중요하다. 제시된 A1의 경우 “?” 값이 일부 존재하므로, 이를 어떻게 처리하는가가 결과에 영향을 준다.\n노이즈(잘못된 레이블, 입력 오류 등)가 존재할 수 있고, 속성 선택이 그 노이즈에 민감할 수 있다.\n속성 하나만으로 모든 분류가 가능하지 않으므로 여러 속성을 조합하는 가설이 필요하다.\n02 암맹처리 UCI의 Credit Approval 데이터셋에서 결측치(missing values, 암맹처리 또는 ‘?’ 표기됨) 처리를 하는 이유에 대해 설명하고자 한다.\n1 . 분류 모델/알고리즘의 단점 대부분의 ML 라이브러리(예: scikit-learn)의 분류/회귀 알고리즘은 입력 피처(feature)의 값이 모두 수치(numeric), 범주형(categorical) 변환이 완료된 상태여야 한다.\n‘?’ 같은 문자열은 수치 연산, 비교, 통계계산, 분기(split) 등이 불가능하거나 오류를 발생시키기 쉬우며, 평균(mean), 엔트로피 계산, 불순도 계산 시 오류가 발생한다.\n따라서 ’?’를 NaN 또는 동일한 결측치 포맷으로 바꾸고, 이를 채우거나 해당 샘플/속성을 제거해야 한다.\n2 . 모델의 성능 및 일반화 결측치를 처리하는 핵심 목적 중 하나가 편향(bias) 제거이다.\n결측치 미처리 시 모델이 학습 중 일부 피처를 무시하거나, 잘못된 피처 분할(split)로 과적합(overfitting) 또는 편향(bias)가 커질 수 있기 때문이다.\n또한 학습 데이터와 테스트 데이터 간의 일관성이 깨질 수 있다.\n학습에는 특정 방식으로 결측치 처리를 했는데, 실제 예측 시에는 결측치 형태가 다르면 예측이 잘 안 되는 경우\n3 . 통계적 분석의 정확성 확보 결측이 있는 상태로 단순히 평균/분포/상관관계(correlation) 계산 등을 하면, 해당 피처가 가진 정보가 왜곡될 수 있다.\n예: 연속형 피처에 결측치가 많으면 그 피처의 평균/표준편차 등의 계산이 왜곡됨 → 전체 엔트로피나 지니, 정보 이득(IG) 계산 등에 영향을 끼침.\n4 . 데이터의 규모와 손실 최소화 전체 데이터 중 결측이 있는 샘플이 많지 않다면(dropna), 이들을 제거하는 것이 하나의 방법임.\n이 데이터셋에서는 약 37개의 행(samples) — 전체의 약 5% 정도가 결측값을 포함함. [출처]\n하지만 결측치가 많은 피처가 있다면 단순 제거는 정보 손실이 큼. 이런 경우 imputation(대체값 채우기) 방법(평균, 최빈값, 예측 모델 등)이 사용된다.\n5 . 실제 적용 예시 여러 프로젝트/분석에서, 이 데이터셋의 결측치는 ’?’로 표시되어 있어 먼저 이를 NaN으로 변환함. [출처]\n연속형 피처에 대해서는 평균(mean) 또는 중앙값(median)으로 채움(imputation). 명목형(categorical) 피처의 경우 최빈값(mode)으로 채우는 사례가 많음. [출처]\n또는 결측이 있는 샘플을 통째로 제거(drop)하는 방법이 사용됨. 예: 37개의 결측 샘플을 제거한 후 분석 진행한 연구 있음. [출처]\n결과적으로, 암맹처리는 단순히 오류를 피하기 위한 것뿐만 아니라 모델의 공정성(fairness)과 일반화 성능을 높이는 중요한 단계이다.\n03 ID3 의사결정나무 분할 전략 ID3(Iterative Dichotomiser 3) 알고리즘은 의사결정나무 모델에서 데이터를 분할할 특성을 선택할 때 정보 이득(IG)을 기준으로 삼는다.\n정보 이득은 특정 속성으로 데이터를 분할했을 때 엔트로피가 얼마나 감소하는지를 측정하는 지표로, 엔트로피는 데이터의 불순도 또는 무질서를 나타낸다.\n즉, 엔트로피 감소가 클수록 해당 속성은 데이터를 더 잘 분리한다고 판단되어 우선적으로 선택된다. [출처].\n엔트로피 계산 예제\n1 . 전체 엔트로피 H(Y) (P(+) = 307/690 ,P(-) = 383/690 ) (H(Y) = - (0.445 _2 0.445 + 0.555 _2 0.555) )\n2 . A1로 분할 후 엔트로피 H(Y|A1) 각 그룹 엔트로피 계산 후 가중 평균:\n\\[H(Y|A1) = \\frac{210}{690} H(a) + \\frac{468}{690} H(b) + \\frac{12}{690} H(?)\\]\n각 그룹 엔트로피:\n(a: (H(a) = - (98/210 _2 (98/210) + 112/210 _2 (112/210)) ) ) (b: (H(b) = - (206/468 _2 (206/468) + 262/468 _2 (262/468)) ) ) (?: (H(?) = - (3/12 _2 (3/12) + 9/12 _2 (9/12)) ) ) 가중 평균:\n\\[H(Y|A1) \\approx \\frac{210}{690}\\cdot0.998 + \\frac{468}{690}\\cdot0.993 + \\frac{12}{690}\\cdot0.811 \\approx 0.994\\]\n3 . A9로 분할 후 엔트로피 H(Y|A9) ( t: (H(t) = - (284/361 _2(284/361) + 77/361 _2(77/361)) )) ( f: (H(f) = - (23/329 _2(23/329) + 306/329 _2(306/329)) ) ) 가중 평균:\n\\[H(Y|A9) \\approx \\frac{361}{690}\\cdot0.722 + \\frac{329}{690}\\cdot0.164 \\approx 0.455\\]\n4 . 정보이득 계산 \\[IG(Y, A1) = H(Y) - H(Y|A1) \\approx 0.99 - 0.994 = -0.004 \\quad (\\text{사실상 0})\\]\n\\[IG(Y, A9) = H(Y) - H(Y|A9) \\approx 0.99 - 0.455 = 0.535\\]\nA9가 정보이득이 훨씬 크므로, ID3에서는 A9를 루트 노드(최초 분할 기준)로 선택한다. A1은 거의 정보이득이 없어 의미 있는 분할이 되지 않는다.\n따라서, A9가 먼저 선택되어 분할 기준으로 사용되며, 이는 모델의 성능 향상에 기여할 수 있다.\n04 의사결정 트리의 한계 의사결정 트리(Decision Tree)는 구조가 직관적이고 해석이 용이하다는 장점이 있으나, 현실의 데이터가 갖는 노이즈와 불일치성에 매우 민감하다는 단점이 있다.\n데이터에 포함된 잡음이나 이상치가 많을 경우, 트리는 그 불완전한 패턴까지 학습하여 과적합(overfitting) 이 발생하거나 작은 데이터 변화에도 트리 구조가 크게 달라지는 불안정성(variance) 을 보인다.\n이러한 이유로, 단일 트리 모델은 실제 업무 환경에서의 예측 신뢰성이 낮을 수 있다.\n이러한 문제를 보완하기 위해 도입된 방법이 앙상블 기법이다.\n1 . 앙상블 기법 Ensemble\n여러 개의 학습기(base learners)를 조합하여, 단일 모델보다 더 안정적이고 일반화 성능이 높은 예측 결과를 도출하는 방법.\n이는 음악의 ’오케스트라’처럼 개별 모델의 예측을 조화롭게 결합하여 전체적인 조화(화음)를 이루는 방식으로 비유할 수 있다.\n앙상블 기법은 일반적으로 두 가지로 구분된다.\n2 . Bagging Bootstrap Aggregating\n데이터로부터 복원추출(bootstrap sampling)된 여러 하위 표본을 생성하여, 각 표본마다 독립된 트리를 학습시킨 후 결과를 평균(또는 투표)하는 방식이다.\n대표적인 알고리즘은 랜덤 포레스트(Random Forest) 로, 각 트리 학습 시 특성(feature)의 일부만 무작위로 선택하여 트리 간의 상관성 및 분산을 효과적으로 감소한다.\n이러한 무작위성과 집계(aggregation) 과정을 통해 단일 트리에 비해 일반화 능력과 예측 안정성이 향상된다.\n3 . Boosting 약한 학습기(weak learner)를 순차적으로 학습시키는 방식으로, 이전 모델이 잘못 예측한 데이터에 더 큰 가중치를 부여하여 점진적으로 오차를 보정한다.\n이 방식은 편향(bias) 을 줄이는 데 효과적이며, 대표적인 알고리즘으로 AdaBoost, Gradient Boosting, XGBoost 등이 있다.\n다만, 데이터에 노이즈가 많을 경우 과적합 위험이 존재하므로, 적절한 규제(regularization)와 학습률 조절이 필요하다.\n4 . 효과와 한계 일반적으로 단일 트리 모델보다 2 ~ 5% 내외의 성능 향상을 보이는 것으로 보고되지만, 향상 폭은 데이터의 특성과 모델 설계에 따라 달라질 수 있다.\n또한, 앙상블은 여러 모델을 결합하므로 해석력이 낮아지고 계산 비용이 증가하는 단점도 존재한다."
  },
  {
    "objectID": "ai/ml_08.html",
    "href": "ai/ml_08.html",
    "title": "의사결정트리 실습",
    "section": "",
    "text": "의사결정트리 실습에 대해 다루고자 한다.\n01 커널 서포트 벡터 머신 Kernel Support Vector Machine\n1 . 제목2 %matplotlib inline import numpy as np import matplotlib.pyplot as plt\n\n랜덤 시드 설정\nnp.random.seed(0)\n\n\nXOR 데이터 생성\nX_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0) y_xor = np.where(y_xor, 1, 0)\n\n\n클래스별 산점도\nplt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c=‘b’, marker=‘o’, label=‘class 1’, s=50) plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c=‘r’, marker=‘s’, label=‘class 0’, s=50)\n\n\n그래프 옵션\nplt.legend() plt.xlabel(“x1”) plt.ylabel(“x2”) plt.title(“XOR Problem”) plt.show()\n%matplotlib inline import numpy as np import matplotlib.pyplot as plt from sklearn.svm import SVC import matplotlib as mpl\n\n\nXOR 데이터 생성\nnp.random.seed(0) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0) y_xor = np.where(y_xor, 1, 0)\n\n\nXOR plot 함수 정의\ndef plot_xor_flipped(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid( np.arange(xmin, xmax, (xmax-xmin)/100), np.arange(ymin, ymax, (ymax-ymin)/100) ) ZZ = np.reshape( model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape ) # 색상 반전 ZZ = 1 - ZZ\nplt.contourf(XX, YY, ZZ, alpha=0.5, cmap=mpl.cm.Paired)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='class 1', s=50)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='class 0', s=50)\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.title(title)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\n\n\n결정 경계 시각화\nsvc = SVC(kernel=‘linear’) svc.fit(X_xor, y_xor) plot_xor_flipped(X_xor, y_xor, svc, “Classification Result by Linear SVC XOR”) plt.show()\nimport numpy as np from sklearn.preprocessing import FunctionTransformer\n\n\n기저 함수 정의\ndef basis(X): return np.vstack([X[:, 0]2, np.sqrt(2)X[:, 0]X[:, 1], X[:, 1]2]).T\n\n\n테스트용 배열 생성\nx = np.arange(8).reshape(4, 2) x\nFunctionTransformer(basis).fit_transform(x)\nimport matplotlib.pyplot as plt from sklearn.preprocessing import FunctionTransformer\n\n\n기저 함수 변환\nX_xor2 = FunctionTransformer(basis).fit_transform(X_xor)\n\n\n변환된 특징으로 산점도\nplt.scatter(X_xor2[y_xor == 1, 0], X_xor2[y_xor == 1, 1], c=“b”, s=50, label=“class 1”) plt.scatter(X_xor2[y_xor == 0, 0], X_xor2[y_xor == 0, 1], c=“r”, s=50, label=“class 0”) plt.legend() plt.show()\n02 ㄷ SVM with various Kernels\nimport matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import FunctionTransformer from sklearn.svm import SVC\n\n\n— 한글 폰트 설정 —\nplt.rc(‘font’, family=‘Malgun Gothic’) # Windows: ‘Malgun Gothic’, Mac: ‘AppleGothic’, Linux: ‘NanumGothic’ plt.rcParams[‘axes.unicode_minus’] = False # 마이너스 기호 깨짐 방지\ndef plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid( np.arange(xmin, xmax, (xmax-xmin)/100), np.arange(ymin, ymax, (ymax-ymin)/100) ) ZZ = np.reshape( model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape )\n# 색상 반전\nZZ = 1 - ZZ  \n\nplt.contourf(XX, YY, ZZ, alpha=0.5, cmap=plt.cm.Paired)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='class 1', s=50)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='class 0', s=50)\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.title(title)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\n\n\n적용\nplot_xor(X_xor, y_xor, basismodel, “기저함수 SVC 모형을 사용한 XOR 분류 결과”) plt.show()\n\n\n시그모이드 커널 SVC 학습\nsigmoidsvc = SVC(kernel=“sigmoid”, gamma=2, coef0=2) sigmoidsvc.fit(X_xor, y_xor)\n\n\nXOR 데이터 결정 경계 시각화\nplot_xor(X_xor, y_xor, sigmoidsvc, “Sigmoid SVC”) plt.show()\nrbfsvc = SVC(kernel=“rbf”).fit(X_xor, y_xor) plot_xor(X_xor, y_xor, rbfsvc, “RBF SVC”) plt.show()\nplot_xor(X_xor, y_xor, SVC(kernel=“rbf”, gamma=2). fit(X_xor, y_xor), “RBF SVC (gamma=2)”) plt.show()\nplot_xor(X_xor, y_xor, SVC(kernel=“rbf”, gamma=50). fit(X_xor, y_xor), “RBF SVC (gamma=50)”) plt.show()"
  },
  {
    "objectID": "ai/ml_11.html",
    "href": "ai/ml_11.html",
    "title": "로지스틱 분류",
    "section": "",
    "text": "[주제] 에 대해 다루고자 한다.\n01 최적화 알고리즘\nAlec Radford’s animations for optimization algorithms\nAlec Radford has created some great animations comparing optimization algorithms SGD , Momentum , NAG , Adagrad , Adadelta , RMSprop (unfo…\nwww.denizyuret.com 기본 SGD 외에 다음과 같은 변형을 자주 사용한다.\nSGD (확률적 경사하강) — 큰 데이터셋에 효율적 Momentum — 관성 도입으로 수렴 가속 및 진동 완화 AdaGrad — 좌표별 학습률 적응 Adam — 1차·2차 모멘트를 사용한 적응형 방법 (실무에서 널리 사용; 성능·안정성 우수)\n\n모델 검증과 일반화 7.1 K-Fold 교차검증 모델 성능 평가를 위해 데이터를 K개의 폴드(fold)로 나누고, 순차적으로 하나의 폴드를 검증셋으로, 나머지를 학습셋으로 사용한다.\n\n데이터를 K등분 (예: 5-fold → 5등분) 각 iteration마다 서로 다른 폴드를 검증셋으로 지정 K번 반복 후 평균 성능 산출 특수 사례: LOOCV(Leave-One-Out CV)\n데이터 포인트 하나를 검증셋으로, 나머지를 학습셋으로 사용 데이터가 적을 때 모델 평가를 정밀하게 할 수 있음 7.2 학습률(Learning Rate)과 데이터 전처리 학습률은 경사하강법의 파라미터 갱신 속도를 조절하며, 과도하면 발산, 너무 작으면 수렴 속도가 느려진다. 데이터 전처리는 모델 안정성과 성능 향상에 중요하며, 일반적으로 정규화(normalization), 스케일링(scaling), 결측치 처리 등을 포함한다. 7.3 오버피팅 방지 모델이 학습 데이터에만 과적합되는 것을 방지하기 위해 다양한 전략을 사용한다.\n데이터 증강(Data Augmentation): 학습 데이터 변형을 통해 다양성을 확보 특징 수 축소(Feature Selection/Reduction): 불필요한 입력 제거 정규화(Regularization): L1/L2 정규화, 드롭아웃(Dropout) 등 이 과정을 통해 모델은 학습 데이터뿐 아니라 미지의 데이터에서도 일반화 성능을 발휘할 수 있다.\n03 퍼셉트론과 다층 신경망 Perceptron &Multi-Layer Neural Network\n1 . 신경망의 구조와 활성화 함수 인공신경망(NN)은 생물학적 뉴런을 모사하여 입력 신호를 처리한다.\n입력 신호(x0,x1,…x_0, x_1, ​,x1​,…): 다른 뉴런(axon)으로부터 전달되는 신호 가중치(w0,w1,…w_0, w_1, ​,w1​,…): 각 신호의 중요도를 나타내며, 시냅스(synapse)와 유사 가중합(dendrite): 각 입력에 가중치를 곱한 후 합산 \\[z = \\sum_i w_i x_i + b\\]\n활성화 함수(Activation Function, f): 뉴런의 세포체(cell body)가 총합 입력을 해석하여 출력 신호 생성 \\[y = f(z) = f\\Big(\\sum_i w_i x_i + b\\Big)\\]\n출력 y는 다음 뉴런(axon)으로 전달되며, 이 신호가 0인지 1인지 또는 확률값인지 결정한다. 이 구조를 MLP이라고 하며, 다층 구성으로 비선형 문제를 해결할 수 있다.\n2 . 역사적 배경 Frank Rosenblatt (1957): 퍼셉트론(Perceptron) 제안 — 단층 신경망 기반의 이진 분류기 Widrow & Hoff (1960): ADALINE(Adaptive Linear Neuron) / MADALINE — 가중치 적응 학습 규칙 제안 Widrow-Hoff Rule (Delta Rule) 가중치 학습 과정은 다음과 같다.\n가중치 초기화Wi(0)을 임의값으로 설정W_i(0) Wi​(0)을 임의값으로 설정 입력 패턴과 목표 출력 제시 출력 계산 (Hard Limiter) \\[y(t) = f_h\\Big(\\sum_{i=0}^{n-1} w_i(t) X_i(t) - \\epsilon\\Big)\\]\n: 임계치 f_h​: 하드리미터 함수 가중치 갱신 \\[W_i(t+1) = W_i(t) + \\alpha \\,[\\,d(t) - y(t)\\,] X_i(t), \\quad 0 \\le i \\le n-1\\]\n: 학습률, 0 &lt; &lt; 1 d(t): 목표 출력값 실제 출력과 목표 출력이 일치하면 가중치는 변하지 않음 반복 수행 출력이 목표에 도달할 때까지 2~4단계 반복\n3 . 퍼셉트론을 통한 논리 연산 퍼셉트론은 기본적인 논리 게이트 연산을 구현할 수 있다.\nAND, OR 연산: 입력값의 가중합이 임계치(θ)를 초과하면 출력 1, 아니면 0 XOR 연산: 단층 퍼셉트론으로는 직선 결정경계로 구분 불가능 — 다층 퍼셉트론 필요 입력 X_0, X_1 ​AND OR XOR 0, 0 0 0 0 0, 1 0 1 1 1, 0 0 1 1 1, 1 1 1 0 AND: 위쪽 위치, 직선 결정 가능 OR: 아래쪽 위치, 직선 결정 가능 XOR: 직선 결정 불가, 곡선 또는 다층 구조 필요\n03 다층 퍼셉트론과 역전파 Backpropagation\n\n초기 비관과 연구의 침체기 1969년, Marvin Minsky 교수와 Seymour Papert는 저서 Perceptrons에서 다음과 같이 지적했다.\n\n“멀티 레이어 퍼셉트론(Multi-Layer Perceptron)으로 구성하면 문제를 해결할 수는 있지만, 실제로 구현하는 사람은 아무도 없을 것이다.”\n이로 인해 퍼셉트론 연구는 첫 번째 머신러닝 침체기(Artificial Intelligence Winter)를 맞이하게 되었다.\n단층 퍼셉트론으로 XOR와 같은 비선형 문제를 해결할 수 없다는 한계 다층 구조의 학습 방법 부재 2. 역전파(Backpropagation)의 등장 1974년과 1982년, Paul Werbos는 다층 신경망의 학습 문제를 해결할 방법을 제안했으며, 1986년, Geoffrey Hinton 등이 이를 체계화하였다.\n핵심 아이디어: 순방향(forward)으로 출력과 실제값의 차이(오차)를 계산 출력층에서 입력층 방향으로 오차를 역전파(backpropagation) 하여 각 가중치를 조정 이로써 단층 퍼셉트론으로 해결할 수 없었던 XOR 등 비선형 문제도 학습 가능하게 되었다.\n\n수학적 원리 다층 퍼셉트론에서 각 뉴런의 출력은 다음과 같이 정의된다.\n\nf=wx+bf = w x + bf=wx+b\n또는 단계적으로,\ng=wx,f=g+bg = w x, f = g + bg=wx,f=g+b\n체인룰(Chain Rule)과 편미분을 이용하여, 출력 오차를 각 층의 가중치에 대해 분배한다. 미분의 기본 정의: ddxf(x)=lim⁡Δx→0f(x+Δx)−f(x)Δx f(x) = _{x } dxd​f(x)=Δx→0lim​Δxf(x+Δx)−f(x)​\n예제: 2차원 입력 x1,x2x_1, x_2x1​,x2​와 가중치 w=[5,−7,−11]w = [5, -7, -11]w=[5,−7,−11]일 경우 출력 [y1,y2]=[1,0,0][y_1, y_2] = [1,0,0][y1​,y2​]=[1,0,0]을 얻을 수 있으며, 각 가중치에 대한 기울기를 계산해 오차를 감소시키는 방향으로 업데이트한다. 4. 의미와 의의 역전파 알고리즘은 신경망 학습의 근간이 되었으며, 다층 퍼셉트론 구조는 XOR와 같은 비선형 문제 해결 가능 현재의 딥러닝 모델(Convolutional Neural Network, Transformer 등)도 이 원리를 확장한 것 즉, 단층 퍼셉트론에서 시작한 연구가 다층 구조와 역전파를 통해 현대 신경망의 기초로 발전하게 된 역사적 과정이다.\n①②③④⑤⑥⑦⑧⑨ ⋅ ⌎\n₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ⁱ ⁿ / ¹ ² ³ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ₋ / α β δ θ ε π μ σ Ω φ ω\n− ± × ∑ ∴ ≥ ≤ ≒ ≓ ⋯ ⋮ / ⇨ ←→↑↓↔︎↕ / ℉ ℃\n[출처]"
  },
  {
    "objectID": "ai/ml_13.html",
    "href": "ai/ml_13.html",
    "title": "현대 AI 연구",
    "section": "",
    "text": "Reporting Date: November. 5, 2025\n\n엔비디아(NVIDIA)는 인공지능 연산을 위한 GPU 시장의 절대적 선도 기업으로, 최근에는 하드웨어 중심의 성능 향상을 넘어 AI 생태계 전체를 아우르는 플랫폼 전략으로 진화하고 있다. 대표적으로 ‘코스모스(Cosmos) Simulation Model’과 ‘옴니버스(Omniverse)’ 플랫폼은 3D 모델링, 물리 기반 시뮬레이션, 렌더링을 통합하여 현실과 유사한 디지털 트윈(Digital Twin) 환경을 구현하는 기술적 기반을 제공한다. 이러한 시뮬레이션 시스템은 실제 물리 세계의 상호작용을 정밀하게 재현함으로써, AI 에이전트(agent)의 학습, 검증, 최적화 실험을 위한 실증적 환경을 제공한다.\n2025년 기준으로 인공지능 연구의 핵심 개념은 에이전트(Agent)이다. 이는 인간의 인지적 판단과 행동을 모사하거나 대리 수행할 수 있는 자율적 지능 시스템을 의미한다.\n과거의 대화형 AI는 명시적으로 프로그래밍된 규칙에 의존했으나, 현재는 자연어 프롬프트(prompt)를 통해 고차원적 명령을 직접 해석하고 실행할 수 있다. 이러한 고도화된 시스템의 중심에는 대규모 언어모델(LLM, Large Language Model)이 있으며, 이는 언어적 지식뿐 아니라 시각·음성·문서 등 다양한 데이터 모달리티(modality)를 통합한 멀티모달 구조로 발전하고 있다.\nLLM의 기술적 진화는 ‘멀티모달 학습(Multimodal Learning)’ 역량을 중심으로 전개된다. 텍스트, 이미지, 오디오, 비디오 등의 이질적 데이터를 통합적으로 해석하고 상호 연관성을 학습하는 것이 핵심이며, 이는 각 데이터 형식별로 고유한 입출력 구조와 통신 프로토콜을 요구한다.\n현재의 LLM은 외부 데이터를 분석하고 가공할 수 있으나, 인간 수준의 자율적 시각 생성 능력에는 아직 도달하지 못한 상태다.\nAI의 실질적 응용의 종착점은 ’피지컬 AI(Physical AI)’이다. 이는 로봇, 제조, 물류, 스마트 팩토리 등 물리적 환경에서 실시간으로 인지·판단·행동을 수행할 수 있는 지능형 시스템을 의미한다. 미국은 첨단 반도체 설계 능력을 보유하고 있으나, 제조 기반의 약화로 인해 피지컬 AI의 산업적 적용에는 한계가 존재한다. 반면, 중국은 제조 기술과 산업 자동화 인프라 측면에서 강점을 보유하고 있으나, 미·중 기술 패권 경쟁이 양국 간 협력의 제약 요인으로 작용한다.\n이러한 상황 속에서 블랙록(BlackRock), 오픈AI(OpenAI)의 샘 알트먼(Sam Altman), 앤스로픽(Anthropic)의 다리오 아모데이(Dario Amodei), 엔비디아의 젠슨 황(Jensen Huang) 등 AI 생태계를 주도하는 주요 인물들은 한국을 차세대 전략적 거점으로 평가하고 있다. 이는 반도체 제조 분야에서 삼성전자와 SK하이닉스가 가진 기술적 우위와 글로벌 공급망 내 핵심적 위치 때문이다.\n한국의 반도체 산업은 엔비디아와 같은 AI 중심 기업의 연산 가속화를 위한 필수적 기반으로 작용하며, 양측의 기술적 상호의존 관계는 AI 반도체 생산 효율을 높이고, 피지컬 AI 산업화의 촉진을 가능하게 하는 협력 생태계를 형성하고 있다.\n\n인공지능의 이론적 기반은 크게 두 가지로 구분된다.\n첫째, 인과관계 기반의 지식 모델은 인간의 논리적 추론과 유사한 방식으로 동작하며, 둘째, 상관관계 기반의 데이터 모델은 통계적 학습을 통해 패턴을 식별한다.\n즉, 학습 기반 AI의 이해 연결주의(Connectionism) – 데이터에 대한 학습 능력을 이용하여 지능 구현\n후자의 대표적 구현이 머신러닝(ML)과 딥러닝(DL)이며, 인공신경망(ANN) 및 생성형 AI 모델이 이에 포함된다.\n\nANN Artificial Neural Network 인간의 신경세포(뉴런) 구조를 모사한 계산 모델이다.\n뉴런은 수상돌기(다중 입력), 세포핵(통합 중심), 축삭돌기(단일 출력)로 구성되며, 시냅스(synapse)를 통해 연결된다. 인간의 뇌는 약 1000억 개의 뉴런과 100조 개 이상의 시냅스로 이루어져 있으며, 각 뉴런은 평균 1000개 이상의 시냅스를 형성한다.\n입력 신호는 수상돌기를 통해 수집되어 세포핵에서 통합·처리되며, 각 입력의 영향력은 가중치(weight)로 조정된다.\n신호가 일정 임계치를 초과하면 활성화 함수(activation function)에 의해 출력으로 변환된다.\n이때 주로 사용되는 함수는 역치 함수(threshold function)와 시그모이드(sigmoid) 함수이며, 이는 입력 자극의 강도와 출력 반응의 비선형 관계를 수학적으로 모델링한다.\n출력 신호는 축삭돌기를 따라 전도되며, 말단부에서는 아세틸콜린 등의 신경전달물질을 통해 다른 뉴런으로 전파된다. 학습 과정에서 형성되는 시냅스의 가중치 변화가 곧 기억과 학습의 수학적 표현이다.\n시각 정보를 예로 들면, 인간의 눈은 약 700만 개의 원추세포와 1억 2000만 개의 막대세포로 구성되어 색상과 명암을 인식한다. 인공신경망에서는 이러한 시각 입력을 디지털 이미지의 픽셀로 표현하며, 예를 들어 28×28 흑백 필기체 데이터는 784차원 벡터로 변환되어 입력층에 주어진다.\n다층 퍼셉트론(MLP, Multi-Layer Perceptron)은 이 벡터 데이터를 여러 은닉층(hidden layer)을 통해 비선형적으로 변환하며, 각 층의 노드 수와 활성화 방식에 따라 인식 정확도가 달라진다.\n출력층(output layer)에는 가중치 대신 확률 분포를 계산하는 소프트맥스(Softmax) 함수가 사용된다. 이를 통해 예측 결과를 확률적으로 해석할 수 있으며, TOP-1 또는 TOP-3 정확도를 기준으로 분류 성능을 평가한다.\n하지만 모델의 복잡도가 과도하면 과적합(overfitting)이 발생할 수 있으며, 이는 학습 데이터의 특성에 과도하게 종속된 결과를 초래한다. 따라서 모델 구조의 단순화나 규제화(regularization) 기법을 통해 이를 완화한다.\n음성 데이터는 또 다른 모달리티를 구성한다. 파형, 주파수, 진동의 패턴을 학습하여 화자 인식, 음성 인식, 다자 음성 분리 등으로 응용된다. 철도나 엘리베이터의 작동음 분석을 통해 고장 여부를 조기 진단하는 산업적 활용도 이에 해당한다.\n결국 인식(recognition)은 곧 분류(classification)이다.\n신경망이 입력을 해석하고, 그 결과를 특정 클래스에 매핑하는 과정은 본질적으로 확률적 분류 문제이다. 이러한 학습의 핵심은 오차(error)를 줄이는 것이며, 과거에는 복잡한 가중치 구조로 인해 학습이 어려웠으나, 역전파(backpropagation) 알고리즘의 도입으로 이를 해결하였다.\n이 혁신으로 인해 인공신경망은 1980년대 후반의 혹한기를 극복하고, 현대 딥러닝의 토대를 구축하게 되었다."
  },
  {
    "objectID": "ai/ml_15.html",
    "href": "ai/ml_15.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: November. 19, 2025\n머신러닝에서 최적화(optimization)는 곧 고차원 매개변수 공간(parameter space)에서 최소점을 탐색하는 문제이며, 이 관점에서 학습 알고리즘은 본질적으로 탐색 알고리즘이다.\n인공신경망의 맥락에서는 이를 학습 알고리즘이라 부르며, 가장 대표적인 접근이 확률적 경사하강법(Stochastic Gradient Descent, SGD)이다.\nSGD는 손실 함수(loss function)의 기울기를 소량의 미니배치(mini-batch)로 근사하여 계산하기 때문에 계산 비용이 낮고 대규모 데이터셋에서도 효율적이다. 그러나 이러한 근사적 기울기는 통계적 분산이 크기 때문에 갱신 경로가 불안정하게 진동한다.\n이 문제를 완화하기 위해 도입된 개념이 모멘텀(momentum)이다. 이는 물리학적 관점에서 관성(inertia)과 마찰력(friction)을 도입한 것으로, 갱신 벡터를 단순히 현재 기울기만으로 결정하지 않고 이전 갱신의 방향성을 누적해 속도 벡터를 형성한다.\n결과적으로 최적화 경로는 비평면적(local curvature)의 영향을 덜 받고 안정적으로 수렴하며, 협곡형 지형(ravine)에서 진동을 줄이는 데 효과적이다."
  },
  {
    "objectID": "ai/ml_15.html#적응형-학습률",
    "href": "ai/ml_15.html#적응형-학습률",
    "title": "인공신경망",
    "section": "적응형 학습률",
    "text": "적응형 학습률\nadaptive learning rate AdaGrad는 학습 과정에서 각 파라미터의 변화량에 따라 학습률을 자동 조정한다. 기울기의 제곱 누적합을 기반으로 학습률을 축소하기 때문에 자주 변하는 파라미터는 더 빠르게 학습률이 감소하고, 드물게 변하는 파라미터는 학습률이 상대적으로 유지된다. 이는 데이터의 기하학적 구조에 따라 비등방적(anisotropic) 최적화를 수행하는 효과가 있다.\nAdam(Adaptive Moment Estimation)은 모멘텀과 AdaGrad의 장점을 결합한 비선형 최적화 알고리즘으로, 1차 및 2차 모멘트(기울기의 평균과 분산)를 동시에 추정한다.\nAdaGrad처럼 학습률이 급격히 감소하여 학습이 조기 정지되는 문제를 완화하고, 모멘텀 기반 탐색보다 진동이 적으며 빠른 초기 수렴 속도를 갖는다. 실험적으로 Adam은 다양한 비정상적(non-stationary) 목적 함수에서 강건하며, 딥러닝 모델 전반에서 사실상 표준으로 사용된다.\n최적화와 독립적으로, 모델의 일반화 성능을 향상하기 위해 정규화 기법이 도입되며 대표적인 것이 L2 정규화(weight decay)이다. 이는 가중치를 작게 유지함으로써 오버피팅을 방지하고, 매개변수 공간에서 불필요한 자유도를 억제함으로써 더 안정적인 표현 학습을 유도한다. 이는 통계학적 관점에서는 리지 회귀(ridge regression)의 페널티 항과 동일한 역할을 한다."
  },
  {
    "objectID": "ai/ml_15.html#가중치-초기화",
    "href": "ai/ml_15.html#가중치-초기화",
    "title": "인공신경망",
    "section": "가중치 초기화",
    "text": "가중치 초기화\nweight initialization 학습 안정성을 결정하는 중요한 요소.\nXavier 초기화는 시그모이드 함수와 같은 대칭적 활성화 함수에서, 전방/역방 전파 시 분산이 일정하게 유지되도록 설계된 방식으로 표준편차를 (1/)에 비례하도록 설정한다.\nReLU 계열 함수에서는 활성 뉴런 비율이 달라지기 때문에 분산 유지 조건이 다르게 정의되며, 이에 기반한 He 초기화는 표준편차를 ()로 확장하여 표현력을 높이고 초기 활성화의 비선형 왜곡을 방지한다.\n초기화가 중요한 이유는 깊은 신경망에서 발생하는 기울기 소실(vanishing gradient) 때문이다. 이는 활성화 함수의 포화 구간(saturation region)에서 기울기가 거의 0에 가까워지는 현상으로, 특히 시그모이드 함수는 입력이 조금만 크거나 작아도 기울기가 소멸한다. 이로 인해 초기 레이어는 거의 학습되지 않으며, 네트워크 전체가 비효율적으로 수렴한다. ReLU 함수는 양의 영역에서 기울기가 일정하게 유지되기 때문에 기울기 소실 문제를 근본적으로 완화하고, 깊은 모델의 학습을 가능하게 만든 핵심 요인이다.\n딥러닝 분야에서 비교 실험은 필수적이며, 다양한 최적화 알고리즘과 초기화 전략을 CIFAR-10과 같은 벤치마크 데이터셋에서 체계적으로 검증하는 과정은 모델 성능 평가의 표준 절차다. 이러한 데이터셋은 역사적으로 많은 연구자(예: 마빈 민스키와 관련된 초기 신경망 논쟁 이후의 연구 흐름)에 의해 발전해 왔으며, 현대적 딥러닝 모델의 성능 비교와 구조적 혁신을 검증하는 중요한 실험 환경을 제공한다."
  },
  {
    "objectID": "da/dap/dap_03.html",
    "href": "da/dap/dap_03.html",
    "title": "주성분 분석(PCA)",
    "section": "",
    "text": "Principal Component Analysis, PCA 상관관계가 존재할 수 있는 (p)개의 관찰 변수를 선형 변환하여 서로 상관관계가 없는 새로운 인공 변수(주성분)를 생성하는 통계적 기법이다.\n고차원 데이터에서는 변수 간 상관 관계가 분석과 해석을 복잡하게 만들 수 있으므로, PCA를 통해 데이터 구조를 단순화하고, 차원 축소를 수행함으로써 데이터 시각화, 노이즈 제거, 변수 간 다중공선성 문제 해결이 가능하다."
  },
  {
    "objectID": "da/dap/dap_03.html#주성분-생성",
    "href": "da/dap/dap_03.html#주성분-생성",
    "title": "주성분 분석(PCA)",
    "section": "1. 주성분 생성",
    "text": "1. 주성분 생성\n주성분 분석의 목적은 다음과 같이 정의할 수 있다:\n\n상관관계가 존재할 수 있는 \\(p\\) 개의 원본 관찰 변수 \\(X = [x_1, x_2, \\dots, x_p]\\) 를 선형 변환(linear transformation)하여, 서로 상관관계가 없는 새로운 변수 집합 \\(Z = [z_1, z_2, \\dots, z_p]\\) 를 생성한다.\n변환된 변수 \\(Z\\) 는 서로 직교(orthogonal)하므로 상관성이 제거되며, 이를 통해 차원 축소와 데이터 구조 파악을 보다 효과적으로 수행할 수 있다.\n각 주성분 \\(z_k\\) 는 원자료의 분산을 최대한 보존하도록 선택되며, 주성분의 순서는 설명하는 분산의 크기에 따라 결정된다."
  },
  {
    "objectID": "da/dap/dap_03.html#표준화",
    "href": "da/dap/dap_03.html#표준화",
    "title": "주성분 분석(PCA)",
    "section": "2 표준화",
    "text": "2 표준화\nStandardization PCA 수행 시 변수의 단위와 스케일이 다를 경우, 표준화를 먼저 수행해야 한다. 표준화 방법:\n\\[\nx'_i = \\frac{x_i - \\bar{x}}{s_x}, \\quad i = 1,2,\\dots,n\n\\]\n\n\\(x_i\\) : 원 변수 값\n\\(\\bar{x}\\) : 변수의 평균\n\\(s_x\\) : 변수의 표준편차\n\n표준화 후 변수는 평균 0, 분산 1을 갖게 되어 PCA에서 각 변수의 영향력이 균등하게 반영된다.\n데이터 행렬은 다음과 같이 표현한다.\n\n원본 데이터 행렬: \\(X \\in \\mathbb{R}^{p \\times n}\\)\n\n\\(p\\): 관찰 변수의 수\n\\(n\\): 관측치(샘플)의 수"
  },
  {
    "objectID": "da/dap/dap_03.html#선형-변환",
    "href": "da/dap/dap_03.html#선형-변환",
    "title": "주성분 분석(PCA)",
    "section": "3. 선형 변환",
    "text": "3. 선형 변환\n주성분 \\(z_k\\) 는 원본 변수의 선형 결합으로 정의되며 다음과 같이 표현된다.\n\\[\nz_k = a_{1k} x_1 + a_{2k} x_2 + \\dots + a_{pk} x_p\n= \\mathbf{a}_k^T \\mathbf{x}, \\quad k = 1, 2, \\dots, p\n\\]\n\n\\(\\mathbf{x} \\in \\mathbb{R}^p\\): 원본 데이터 벡터\n\\(\\mathbf{a}_k \\in \\mathbb{R}^p\\): k번째 주성분의 로딩(loading) 벡터\n\\(\\mathbf{a}_k^T \\mathbf{a}_j = 0 ( k \\neq j )\\): 주성분 간 직교성\n\n\n예시: 첫 번째 관측치 (n_1)의 첫 번째 주성분 좌표 (z_{1,1} = x_1^T v_1 = -0.2152) → (n_1)이 첫 번째 주성분 축 (v_1) 상에서 어디에 위치하는지를 나타내는 스칼라 값이다."
  },
  {
    "objectID": "da/dap/dap_03.html#분산-최대화",
    "href": "da/dap/dap_03.html#분산-최대화",
    "title": "주성분 분석(PCA)",
    "section": "4. 분산 최대화",
    "text": "4. 분산 최대화\n첫 번째 주성분 \\(z_1\\): 전체 분산을 최대한 설명하는 방향이 되도록 선택. 두 번째 주성분 \\(z_2\\): \\(z_1\\) 과 직교한다는 제약 아래 남아 있는 분산을 최대한 설명하는 방향으로 정의됨. 이러한 방식으로 각 주성분은 계층적으로 결정된다.\n따라서 k번째 주성분의 로딩 벡터 \\(\\mathbf{a}_k\\) 는 다음 최적화 문제를 통해 구해진다.\n일반식 (k번째 주성분)\n\\[\n\\mathbf{a}*k\n= \\arg\\max*{\\mathbf{a}}\n\\mathrm{Var}(\\mathbf{a}^T \\mathbf{x}),\n\\quad\n\\text{subject to }\n\\mathbf{a}^T \\mathbf{a} = 1,;\n\\mathbf{a}^T \\mathbf{a}_j = 0,; j &lt; k\n\\]\n특수식 (첫 번째 주성분) 첫 번째 주성분은 이전 주성분이 없으므로 직교 조건이 필요 없습니다.\n\\[\n\\mathbf{a}_1\n= \\arg\\max_{\\mathbf{a}}\n\\mathrm{Var}(\\mathbf{a}^T \\mathbf{x}),\n\\quad\n\\text{subject to }\n\\mathbf{a}^T \\mathbf{a} = 1\n\\]\n\n\\(|\\mathbf{a}|^2 = \\mathbf{a}^T \\mathbf{a} = 1\\): Lagrange 승수법 적용을 위한 제약 조건"
  },
  {
    "objectID": "da/dap/dap_03.html#공분산-행렬과-고유값-분해",
    "href": "da/dap/dap_03.html#공분산-행렬과-고유값-분해",
    "title": "주성분 분석(PCA)",
    "section": "5. 공분산 행렬과 고유값 분해",
    "text": "5. 공분산 행렬과 고유값 분해\n\nLagrange 승수법 적용 → 고유값 문제 도출\n\n첫 번째 주성분에 대해 Lagrange 함수를 구성한다.\n\\[\nL(\\mathbf{a},\\lambda)\n= \\mathbf{a}^T \\Sigma \\mathbf{a} - \\lambda (\\mathbf{a}^T\\mathbf{a}-1)\n\\]\n편미분하여 최적 조건을 구하면, 다음 고유값 문제가 얻어진다.\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{a}} = 2\\Sigma\\mathbf{a} - 2\\lambda \\mathbf{a} = 0\n\\quad \\Rightarrow \\quad \\Sigma \\mathbf{a} = \\lambda \\mathbf{a}\n\\]\n즉, 주성분 로딩 벡터는 공분산 행렬의 고유벡터이다. * 추가: 여기서 \\(|\\mathbf{a}|^2 = 1\\) 조건은 단위벡터(normalized vector)로 만들어 주성분 크기 비교가 가능하게 하는 역할을 한다.\n\n공분산 행렬 계산\n\n데이터 행렬 \\(X \\in \\mathbb{R}^{p \\times n}\\) 에 대해 공분산 행렬은 다음으로 정의된다. \\(\\Sigma\\)는 실대칭 행렬이므로 고유값 분해가 가능하다.\n\\[\n\\Sigma = \\frac{1}{n-1} XX^T\n\\]\n\n추가\n\n\\(X \\in \\mathbb{R}^{p \\times n}\\) 가 아닌 \\(n \\times p\\) 형태라면 \\(\\Sigma = \\frac{1}{n-1} X^T X\\) 로 계산해야 함. → 구현 시 데이터 행렬 차원 주의\n\\(\\Sigma\\) 는 대칭 행렬(Symmetric matrix) → 고유벡터는 서로 직교(orthogonal)하며 정규화 가능\n\n\n\n고유값·고유벡터 해석 → 주성분의 분산\n\n고유값 분해에서 얻어진 값:\n\\[\n\\Sigma \\mathbf{a}_k = \\lambda_k \\mathbf{a}_k\n\\]\n\n\\(\\mathbf{a}_k\\): k번째 주성분 방향(loading vector)\n\\(\\lambda_k\\): 해당 주성분이 설명하는 분산\n\n주성분 순서: 고유값 크기 순서대로 정렬 → \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p\\)\n다음과 같이 분산과 연결된다: \\[\n\\mathrm{Var}(z_k) = \\lambda_k\n\\]"
  },
  {
    "objectID": "da/dap/dap_03.html#차원-축소",
    "href": "da/dap/dap_03.html#차원-축소",
    "title": "주성분 분석(PCA)",
    "section": "6. 차원 축소",
    "text": "6. 차원 축소\nDimensionality Reduction PCA는 고차원 데이터에서 분산을 많이 설명하는 주성분만 선택하여 차원을 축소하는 기법이다. 원래의 \\(p\\) 차원 데이터 중 정보를 가장 많이 보존하는 상위 \\(m\\) 개의 주성분( \\(m &lt; p\\) )을 선택한다.\n\n설명되는 분산 비율 EVR, Explained Variance Ratio 공분산 행렬의 고유값 \\(\\lambda_k\\) 는 k번째 주성분이 설명하는 분산을 의미한다. 따라서 EVR은 다음과 같이 정의된다.\n\n\\[\n\\text{EVR}_k\n= \\frac{\\lambda_k}{\\sum_{i=1}^p \\lambda_i},\n\\quad k = 1,2,\\dots,p\n\\]\n\n누적 설명 분산 비율 Cumulative EVR 차원 축소의 선택 기준은 누적 EVR이다.\n\n\\[\n\\text{Cumulative EVR}_m\n= \\sum*{k=1}^m \\text{EVR}_k\n\\]\n일반적으로 전체 분산의 80~90%를 설명하는 m개의 주성분을 선택하면 정보 손실 없이 효과적인 차원 축소가 가능하다.\n\n새로운 좌표계로의 사영 projection 선택된 상위 \\(m\\) 개의 주성분 벡터\n\n\\[\nA_m = [\\mathbf{a}_1, \\dots, \\mathbf{a}_m] \\in \\mathbb{R}^{p \\times m}\n\\]\n사용하여 원본 데이터 \\(X \\in \\mathbb{R}^{p \\times n}\\) 를 새로운 좌표계(주성분 축)로 투사(projection)한다. 각 관측치 \\(x_i \\in \\mathbb{R}^p\\) (i번째 열) 는 다음과 같이 \\(m\\) 차원 좌표 \\(z_i \\in \\mathbb{R}^m\\) 로 변환된다:\n\\[\nz_i = A_m^T x_i\n\\]\n\n\\(m\\) : 선택된 주성분 수 (차원 축소 후)\n\\(z_i\\) : 관측치 (i)의 주성분 좌표\n예: 5개의 주성분을 선택하면 각 관측치는 5차원 좌표를 가지게 됨\n\n\\[\nZ_m = A_m^T X \\in \\mathbb{R}^{m \\times n}\n\\]\n\n\\(A_m = [\\mathbf{a}_1, \\dots, \\mathbf{a}_m]\\): 상위 m개의 주성분 로딩 벡터\n\\(Z_m\\): 축소된 차원에서의 데이터 표현\n\n각 주성분은 원본 데이터의 분산을 최대한 유지하며 서로 직교하기 때문에, \\(Z_m\\) 은 상호 독립적인 새로운 좌표계에서의 데이터가 된다.\n\n해석\n\n\n각 주성분은 원본 변수들의 선형 결합으로 구성되므로, 가중치(로딩) 벡터 \\(\\mathbf{a}_k\\) 를 확인하면 해당 주성분이 어떤 변수에 의해 형성되었는지 해석 가능하다.\n예: \\(\\mathbf{a}_1 = [0.5, 0.5, -0.3, \\dots]\\) → 첫 번째 주성분은 첫 두 변수의 기여도가 높음을 의미\n필요 시, 시각화를 통해 각 주성분과 원자료 변수의 관계를 분석 가능.\n(z_{i,k})는 관측치 (i)를 주성분 (k) 축에 사영한 값\n시각화 시, 좌표 값의 크기는 축 방향으로 관측치가 얼마나 멀리 위치하는지를 나타냄\n고유벡터 성분의 부호와 크기는 원 변수들이 주성분에 미치는 영향도를 나타냄\n부호는 방향성을 의미하며, 절대 크기를 통해 상대적 기여도를 판단 가능\n각 관측치의 (z_{i,k}) 값은 주성분 축 상의 위치를 나타냄\n2차원 또는 3차원 시각화 시, 좌표 값은 관측치가 새로운 축 상에서 가지는 위치와 변동성을 직관적으로 보여줌\n예: (z_{1,1})을 x축으로, (z_{1,2})를 y축으로 하여 관측치를 점으로 표현하면, 주성분 상에서의 데이터 분포를 시각화 가능\n\n\n실무 활용\n\n\nPCA는 고차원 데이터의 시각화, 노이즈 제거, 변수 간 다중공선성 해결, 머신러닝 전처리에 활용됨\n실무 적용 시 유의사항:\n\n원자료 변수 단위가 서로 다른 경우 표준화(Standardization) 필요\nPCA는 비지도 학습이므로 목적 변수(y)를 고려하지 않음\n차원 축소 후 선택된 주성분이 실제 업무 의미와 일치하는지 확인 필요\n주성분 해석 시, 원본 변수와의 관계를 반드시 확인하여 의미 있는 인사이트를 확보\n\n\n\n전처리 차원 축소 vs PCA 차원 축소\n\n\n\n\n\n\n\n\n\n구분\n전처리 차원 축소\nPCA 차원 축소\n\n\n\n\n목적\n불필요 변수 제거, 단위 조정 등\n분산 최대 보존, 상관관계 제거, 통계적 차원 축소\n\n\n방식\n경험적/규칙적\n선형 변환 기반, 수학적 최적화\n\n\n결과\n원 변수 일부 제거\n주성분으로 변환된 새로운 좌표\n\n\n활용\n데이터 정제, 단순화\n시각화, 피처 선택, 노이즈 제거, 모델 전처리"
  },
  {
    "objectID": "da/dap/dap_03.html#데이터가-적으므로-데이터-증강을-통해-정규분포를-따르는-노이즈-생성한다.-비율-서로-알맞게-설정.-강건성을-보기-위해서.-여러개-중에-비교하여-월드의-효용성-입증.-2로도-한-번-해보기-어떤-패턴이-나타나는지-가우시안-메소드-베이지안-메소드.약간-효능감이-떨어진다-가중치중심점가-0.5이상인-것만-표시함-k-mean-가우시안-베이지안의-혼동행렬을-출력.-해석-방식이-조금-달라짐.-결과적으로-k-mean-가우시안이-보다-효능감-있게-분리한다.-여기서-차이가-발생하는-데-이-이유는-각각-중심기준으로-원형태-타원-형태로-그-기준이-달라서-차이가-나는-것임.-그러나-베이지안이-너무-디테일하게-한다고-특성을-무시하면서-억지로-그룹을-맞추려고-하는-것은-그러면-안된다.",
    "href": "da/dap/dap_03.html#데이터가-적으므로-데이터-증강을-통해-정규분포를-따르는-노이즈-생성한다.-비율-서로-알맞게-설정.-강건성을-보기-위해서.-여러개-중에-비교하여-월드의-효용성-입증.-2로도-한-번-해보기-어떤-패턴이-나타나는지-가우시안-메소드-베이지안-메소드.약간-효능감이-떨어진다-가중치중심점가-0.5이상인-것만-표시함-k-mean-가우시안-베이지안의-혼동행렬을-출력.-해석-방식이-조금-달라짐.-결과적으로-k-mean-가우시안이-보다-효능감-있게-분리한다.-여기서-차이가-발생하는-데-이-이유는-각각-중심기준으로-원형태-타원-형태로-그-기준이-달라서-차이가-나는-것임.-그러나-베이지안이-너무-디테일하게-한다고-특성을-무시하면서-억지로-그룹을-맞추려고-하는-것은-그러면-안된다.",
    "title": "주성분 분석(PCA)",
    "section": "데이터가 적으므로 데이터 증강을 통해 정규분포를 따르는 노이즈 생성한다. 비율 서로 알맞게 설정. 강건성을 보기 위해서. 여러개 중에 비교하여 월드의 효용성 입증. 2로도 한 번 해보기 어떤 패턴이 나타나는지?? 가우시안 메소드, 베이지안 메소드.(약간 효능감이 떨어진다) 가중치(중심점)가 0.5이상인 것만 표시함 k-mean, 가우시안, 베이지안의 혼동행렬을 출력. 해석 방식이 조금 달라짐. 결과적으로 k-mean, 가우시안이 보다 효능감 있게 분리한다. 여기서 차이가 발생하는 데 이 이유는 각각 중심기준으로 원형태, 타원 형태로 그 기준이 달라서 차이가 나는 것임. 그러나 베이지안이 너무 디테일하게 한다고 특성을 무시하면서 억지로 그룹을 맞추려고 하는 것은 그러면 안된다.",
    "text": "데이터가 적으므로 데이터 증강을 통해 정규분포를 따르는 노이즈 생성한다. 비율 서로 알맞게 설정. 강건성을 보기 위해서. 여러개 중에 비교하여 월드의 효용성 입증. 2로도 한 번 해보기 어떤 패턴이 나타나는지?? 가우시안 메소드, 베이지안 메소드.(약간 효능감이 떨어진다) 가중치(중심점)가 0.5이상인 것만 표시함 k-mean, 가우시안, 베이지안의 혼동행렬을 출력. 해석 방식이 조금 달라짐. 결과적으로 k-mean, 가우시안이 보다 효능감 있게 분리한다. 여기서 차이가 발생하는 데 이 이유는 각각 중심기준으로 원형태, 타원 형태로 그 기준이 달라서 차이가 나는 것임. 그러나 베이지안이 너무 디테일하게 한다고 특성을 무시하면서 억지로 그룹을 맞추려고 하는 것은 그러면 안된다."
  },
  {
    "objectID": "da/dap/dap_03.html#시나리오-할수-있다-없다를-떠나서.-ceo관점에서-뭐가-궁금한가-가설을-여러개-세우고-묶는다.-그리고-단계별로-가지치기하면서-한다.-뭐가-빠져있지-뭘-붙이지-어떤-식을-세워야-하는가",
    "href": "da/dap/dap_03.html#시나리오-할수-있다-없다를-떠나서.-ceo관점에서-뭐가-궁금한가-가설을-여러개-세우고-묶는다.-그리고-단계별로-가지치기하면서-한다.-뭐가-빠져있지-뭘-붙이지-어떤-식을-세워야-하는가",
    "title": "주성분 분석(PCA)",
    "section": "시나리오, 할수 있다 없다를 떠나서. CEO관점에서 뭐가 궁금한가? 가설을 여러개 세우고 묶는다. 그리고 단계별로 가지치기하면서 한다. 뭐가 빠져있지? 뭘 붙이지? 어떤 식을 세워야 하는가?",
    "text": "시나리오, 할수 있다 없다를 떠나서. CEO관점에서 뭐가 궁금한가? 가설을 여러개 세우고 묶는다. 그리고 단계별로 가지치기하면서 한다. 뭐가 빠져있지? 뭘 붙이지? 어떤 식을 세워야 하는가?\n종이나 변수에 대해 자세히 설명하기 갭통계량에서는 2를 할 이유는 없다, 적당히 큰 것이 좋기 때문. 기울기가 기준이 아님.\n미국 대학의 최신의 데이터를 가져와서 쓰기. 증강한 데이터 패턴. 보여주기, 원데이터의 기초통계량과의 차이 등. 미적분학, 행렬, 벡터. 몇 백만, 십만개로 해야 됨.\n아이디어. 데이터 주입. 그럼 분석결과와 시각화 나옴. 클릭하면 파이썬 코드 나옴.\nSVM에서도 라그랑주 승수법으로 풀어주는 기법이 있다. RBF커널도 사용. 가장 성능이 좋다고 나옴."
  },
  {
    "objectID": "cs/bn_01.html",
    "href": "cs/bn_01.html",
    "title": "네트워크 개론",
    "section": "",
    "text": "네트워크 개론에 대해 다루고자 한다.\n01 통신 두 개 이상의 주체 간 정보의 송수신과 이해를 포함하는 상호작용 과정으로 정의된다.\n통신 주체는 인간 또는 기계, IoT 시스템과 같은 장치가 될 수 있으며, 전달되는 정보는 언어, 데이터, 음성, SNS 메시지 등 다양한 형태를 갖는다.\n효율적 통신을 위해서는 일정한 규약과 전송 시점, 데이터 인코딩 방식 등 명확한 방법이 요구되며, 이는 프로토콜의 형태로 구현된다.\n통신 과정은 단순한 정보 전달에 그치지 않고, 수신자가 해당 정보를 흡수하고 의미를 해석하는 과정을 포함한다. 아울러 제한된 자원을 어떻게 효율적으로 활용할 것인가는 통신 설계와 운영에서 핵심적 고려 사항이다.\n1 . 기본요소 통신은 정보를 주고받는 과정에서 다섯 가지 기본 요소로 구성된다. 먼저 송신기는 정보를 생성하고 전송하는 주체로, 사람, 컴퓨터, IoT 장치 등이 될 수 있다.\n메시지는 송신기가 전달하고자 하는 실제 정보로, 언어, 데이터, 음성, 영상 등 다양한 형태를 가진다.\n전송 매체는 메시지가 송신기에서 수신기로 전달되는 물리적 또는 논리적 경로를 의미하며, 유선, 무선, 광섬유 등 다양한 방식이 존재한다.\n수신기는 전달된 메시지를 받아 해석하고 이해하는 주체로, 송신기와 마찬가지로 사람이나 기계가 될 수 있다.\n마지막으로 프로토콜은 송신기와 수신기 간의 통신을 원활하게 수행하기 위해 적용되는 규약이나 약속으로, 메시지 형식, 전송 순서, 오류 처리 방법 등을 정의한다.\n이 다섯 요소는 상호 유기적으로 작용하여 신뢰성 있고 효율적인 통신을 가능하게 한다.\n2 . 유튜브 영상 재생 과정 사용자가 폰에서 영상을 재생하면, 폰은 HTTP나 HTTPS와 같은 프로토콜을 통해 유튜브 서버에 요청 패킷을 전송한다.\n이 과정에서 IP 주소를 기반으로 목적지를 지정하고, TCP 를 통해 전송 신뢰성을 확보하며 데이터가 순서대로 전달되도록 제어한다.\n유튜브 서버는 요청을 수신한 후, 해당 영상 데이터를 폰으로 전송한다. 전송된 영상 데이터는 일반적으로 RTP 와 같은 스트리밍 프로토콜을 통해 재생되며, 폰은 이를 받아 디코딩하고 화면에 출력한다.\n이러한 일련의 과정은 사용자가 요청한 영상이 안정적이고 실시간으로 재생되도록 설계되어 있다.\n3 . 통신의 종류 음성 통신은 사람 간 실시간 대화를 목적으로 하는 통신으로, 전통적인 전화선과 이동통신망을 통해 이루어진다.\n대표적으로 GSM, VoLTE 와 같은 이동통신 기술이 있으며, SIP 과 같은 신호 프로토콜을 통해 통화 연결과 종료, 세션 관리를 수행한다.\n데이터 통신은 문자, 파일, 인터넷 서비스 등 디지털 데이터를 전달하는 통신을 의미한다. Wi-Fi, LTE, 5G 와 같은 네트워크를 통해 TCP/IP 기반의 신뢰성 있는 전송이 이루어지며, IoT 환경에서는 MQTT와 같은 경량 메시지 프로토콜이 사용되기도 한다.\n방송 통신은 라디오, TV, 위성, 케이블과 같이 단방향 전송을 특징으로 하며, 다수의 수신자가 동시에 정보를 받을 수 있도록 설계되어 있다. 사용자는 송신자의 신호를 수신만 할 수 있으며, 피드백은 제한적이다.\n사물 인터넷(IoT) 통신은 다양한 장치와 센서가 네트워크에 연결되어 데이터를 주고받는 통신을 의미한다. Wi-Fi, Zigbee, LoRa와 같은 무선 기술을 사용하며, CoAP, MQTT 등 경량화된 프로토콜을 통해 제한된 자원에서도 효율적인 데이터 전송과 관리가 가능하다.\n4 . 통신의 역사와 발전 과정 고대·중세 초기 벽화, 연기 신호, 문자를 통한 기본적인 정보 전달. 15 ~ 18세기 인쇄술의 발명으로 정보의 대량 복제와 확산 가능. 세마포어 신호기(망루에서 깃발, 팔 신호로 메시지 전달). 19세기 전신기(모스 부호) 발명으로 장거리 신속 통신 실현. 전화기의 등장으로 음성 전달 가능. 무선 통신 기술 개발 시작. 20세기 라디오 방송 보급 → 대중 매체로 자리잡음. 인공위성(스푸트니크, 통신 위성)으로 전 지구적 통신 가능. 현대 인터넷 시대 ARPANET에서 시작 → TCP/IP 프로토콜 확립. WWW(월드 와이드 웹) 등장으로 정보 공유 혁신. 모바일 인터넷 확산으로 휴대 기기 중심 통신 발전. 최신 세대 5G: 초고속, 초저지연, 초연결을 특징으로 하여 IoT, 자율주행, 메타버스 등 새로운 서비스 기반 제공.\n02 인터넷\n1 . 작동 원리 인터넷은 패킷 교환 방식을 기반으로 동작한다.\n전송하고자 하는 데이터는 일정한 크기의 패킷 단위로 분할되며, 각 패킷에는 출발지와 목적지의 주소 정보가 포함된다.\n이 패킷들은 네트워크 상에서 최적의 경로를 따라 독립적으로 전송되고, 목적지에 도착한 후 원래의 데이터로 재조립된다.\n이 과정에서 핵심 역할을 하는 것이 TCP/IP 프로토콜이다.\nTCP 는 데이터의 신뢰성 있는 전송을 보장하고, 패킷이 순서대로 도착하도록 제어한다. IP 는 각 패킷이 정확한 목적지로 전달되도록 주소 지정과 경로 설정을 담당한다.\n또한, 인터넷에서는 사람이 기억하기 쉬운 도메인 이름을 사용한다. 이를 실제 통신에 필요한 IP 주소로 변환하는 과정이 DNS 이며, 사용자가 도메인 이름을 입력하면 DNS 서버가 해당 이름에 대응하는 IP 주소를 찾아 반환한다.\n이 과정을 통해 최종적으로 사용자의 요청이 해당 서버로 전달되고, 서버는 응답 데이터를 다시 패킷 형태로 전송한다.\n03 네트워크 기반 서비스 구조\n1 . 서버-클라이언트 구조 (중앙집중형) 클라이언트(PC, 브라우저, 앱)가 인터넷을 통해 중앙 서버에 요청을 보내고, 서버는 웹 애플리케이션 및 DB를 통해 응답을 제공. 예: AWS, Azure, GCP\n장점:\n데이터와 서비스를 통합적으로 관리 가능 중앙 통제 용이 스케일 업(서버 성능 확장) 또는 스케일 아웃(서버 추가)으로 확장성 확보 단점:\n서버 장애 시 전체 서비스가 중단될 위험 서버 의존도가 높아 운영 비용 및 리스크 집중\n2 . P2P 구조 (분산형) 모든 노드가 동등한 지위를 가지며, 직접 연결을 통해 데이터를 공유하고 처리. 예: IPFS, 토렌트, 블록체인\n장점:\n분산 구조로 인한 고가용성 확보 뛰어난 확장성 중앙 서버 비용 절감 단점:\n보안 관리가 상대적으로 어려움 운영·유지 관리 복잡성 데이터 동기화 지연 발생 가능\n3 . 하이브리드 구조 서버-클라이언트 구조와 P2P 구조를 혼합한 형태.\n특징:\n핵심 데이터나 보안이 중요한 부분은 중앙 서버에서 관리 대용량 파일 전송, 분산 저장, 연산 등은 P2P 네트워크 활용 중앙 집중 관리와 분산 구조의 장점을 모두 살리면서 단점을 보완 가능\n04 네트워크 기본 개념\n1 . 기본 구성 요소 노드(Node): 네트워크에 연결된 모든 장치(PC, 서버, 라우터, 스마트폰 등). 링크(Link): 노드 간 데이터를 전달하는 경로(유선 케이블, 무선 전파 등). 대역폭(Bandwidth): 네트워크가 단위 시간당 전송할 수 있는 데이터의 최대량, 즉 네트워크의 ‘용량’.\n2 . 네트워크 주소 체계 IP 주소: 네트워크에서 장치를 구분하는 논리적 주소 (IPv4, IPv6). MAC 주소: 네트워크 인터페이스 카드(NIC)에 부여된 물리적 주소, 전 세계적으로 유일. DNS: 사람이 이해하기 쉬운 도메인 이름을 IP 주소로 변환해주는 시스템.\n3 . 통신 방향 분류 (비용 증가 순) 단방향(Simplex): 한쪽 방향으로만 데이터 전송 가능 (예: 라디오, TV). 반이중(Half Duplex): 양방향 통신 가능하지만 동시에 불가능 (예: 무전기). 전이중(Full Duplex): 양방향 동시 통신 가능 (예: 전화, 현대 네트워크).\n4 . 거리 기준 네트워크 분류 PAN: 개인 영역 네트워크 (블루투스, 개인 기기 간 연결). LAN: 근거리 네트워크 (가정, 사무실). MAN: 도시 규모 네트워크. WAN: 광역 네트워크, 인터넷 포함.\n5 . 계층화 개념 네트워크는 복잡성을 줄이고 유지보수를 용이하게 하기 위해 계층적으로 설계됨. 각 계층은 독립적으로 동작하며, 모듈화된 구조로 상호 의존성을 최소화.\n비유: 비행기 서비스 과정\n매표소 계층 → 티켓 발권 수화물 계층 → 짐 위탁 게이트 계층 → 탑승 절차 활주로/비행기 계층 → 실제 이동 수행\nOSI 7계층 모델은 네트워크 통신을 7개의 층으로 나누어 각각의 역할과 기능을 정의한 개념 모델이다.\n1 . 물리층 Physical Layer\n역할: 실제 데이터 전송 매체를 통해 0과 1의 비트 신호를 전기적·광학적 신호로 변환하여 전송. 주요 기능: 케이블, 허브, 리피터 등 하드웨어 장치와 비트 전송. 예시: UTP 케이블, 광섬유, RJ-45 커넥터, 전송 속도(100Mbps, 1Gbps 등).\n2 . 데이터 링크층 Data Link Layer\n역할: 물리적 주소(MAC 주소)를 기반으로 신뢰성 있는 프레임 전송. 주요 기능: 에러 검출(CRC), 흐름 제어, 프레임화, 스위치 동작. 예시: Ethernet, Wi-Fi, 스위치, 브리지, MAC 주소.\n3 . 네트워크층 Network Layer\n역할: 서로 다른 네트워크 간의 데이터 전달 및 경로 선택(Routing). 주요 기능: 논리 주소(IP), 라우팅, 패킷 분할/조립. 예시: IP, ICMP, 라우터, 서브넷.\n4 . 전송층 Transport Layer\n역할: 종단 간(end-to-end) 신뢰성 있는 데이터 전송 제공. 주요 기능: 세그먼트 분할, 오류 제어, 흐름 제어, 연결 관리. 예시: TCP(연결형, 신뢰성), UDP(비연결형, 빠름), 포트 번호.\n5 . 세션층 Session Layer\n역할: 통신 세션(연결)을 설정·관리·종료. 주요 기능: 세션 동기화, 체크포인트, 재연결 지원. 예시: NetBIOS, RPC(Remote Procedure Call).\n6 . 표현층 Presentation Layer\n역할: 데이터 표현 형식과 인코딩 관리. 주요 기능: 암호화/복호화, 압축/복원, 문자 코드 변환. 예시: JPEG, MPEG, SSL/TLS, ASCII ↔︎ Unicode 변환.\n7 . 응용층 Application Layer\n역할: 사용자와 직접 상호작용하며 네트워크 서비스를 제공. 주요 기능: 메일 전송, 파일 전송, 웹 서비스 등. 예시: HTTP, FTP, SMTP, DNS, Telnet, Web Browser, Email Client.\nTCP/IP 모델은 OSI 7계층을 단순화하여 4 ~ 5층 정도로 나눈 실무 중심의 네트워크 모델이다.\n1 . 네트워크 인터페이스층 Network Interface / Link Layer\n역할: 물리적인 네트워크 연결과 데이터 전송 담당. 주요 기능: 프레임 전송, MAC 주소 기반 통신, 에러 검출. 장치/프로토콜 예시: Ethernet, Wi-Fi, 스위치, NIC.\n2 . 인터넷층 Internet Layer\n역할: 서로 다른 네트워크 간 데이터 전달과 경로 선택(Routing). 주요 기능: 논리 주소(IP) 기반 패킷 전달, 라우팅, 주소 지정. 프로토콜 예시: IP, ICMP, ARP, IPv4/IPv6.\n3 . 전송층 Transport Layer\n역할: 종단 간(end-to-end) 신뢰성 있는 데이터 전송 제공. 주요 기능: 포트 번호, 오류 제어, 흐름 제어, 연결 관리. 프로토콜 예시: TCP(신뢰성, 연결형), UDP(비연결형, 빠름).\n4 . 응용층 Application Layer\n역할: 사용자와 직접 상호작용하며 네트워크 서비스를 제공. 주요 기능: 데이터 포맷 처리, 응용 서비스 제공. 프로토콜 예시: HTTP, FTP, SMTP, DNS, Telnet, SSH.\n07 개발 방법론\n1 . Waterfall 방식 단계별(분석 → 설계 → 개발 → 테스트 → 배포) 순차적으로 진행되는 전통적 방법론. 장점: 체계적이고 관리가 용이, 문서 기반으로 요구사항 추적 가능. 단점: 변경에 취약, 초기 요구사항이 명확하지 않으면 리스크 증가.\n2 . Agile 방식 짧은 개발 주기(Iteration, Sprint)마다 요구사항을 반영하고 점진적으로 개선. 장점: 유연성, 빠른 피드백, 고객 중심 개발 가능. 단점: 문서화 부족 시 유지보수 어려움, 팀 역량 의존도 큼.\n3 . 개발 조직 및 역할 개발 조직(회사): 실제 시스템을 설계·구현·테스트하는 주체. 사업자: 서비스나 제품을 최종적으로 운영·제공하는 주체. 요구사항 주체(ISP): 사업 목표에 따른 요구사항을 도출하고 정리. APO(기술 영업): 사업자 요구와 기술적 가능성을 조율, 고객과 개발팀 간 가교 역할. OPO(다수 배치 가능): 각 세부 기능/모듈 단위 책임, 애자일 Scrum 팀 내에서 구체적 백로그 관리.\n4 . Scrum 및 협업 방식 애자일 프레임워크인 Scrum에서는 짧은 주기(Sprint) 단위로 개발. 일반적으로 3 주 단위 Sprint를 운영하며, 반복되는 사이클을 통해 요구사항을 점진적으로 구현.\nSprint 계획 회의 (목표 설정) Daily Scrum (매일 15분 점검) Sprint Review (성과 검토) Sprint Retrospective (개선점 도출)"
  },
  {
    "objectID": "da/tm/tm_06_0.html",
    "href": "da/tm/tm_06_0.html",
    "title": "6장: 감성 분석",
    "section": "",
    "text": "감성분석에 대해 다루고자 한다.\n01 제목1 [ID 사용하기] 감성 사전, 말에는 감성이 있다. 단어에 관한 감성 분석, 태도, 성향, 의견 한 개인의 감정이라도 이것도 많아지면 사람들이 생각하는 한 방향이 됨\n극성, 과학에서 말하는 극성\n어휘 기반은 수동으로 구축, 도메인(산업군, 어떤 카테고리 또는 영역인가?) 같은 언어, 단어가 도메인에 따라 다른 감성을 가질 수 있다.\n사전 기반은 이미 누군가가 만들어 놓은 감성 사전을 사용하는 것\n한국어 사전, 한국어, 국문학을 전공한 사람들이 참여했을 것이다. 리커트 척도? 보통이라는 의미를 담는 3점을 고른 생각이 같지 않을 것이라는 것이다. 정량적인 분석이 가능해야 리커트 척도라고 볼 수 있다. 즉, 각 등간 간격이 모두 동일해야 한다는 것이다.\n디테일하게 보고 싶으면 에뮬렉스를 사용.\n1 . 제목2 감성 분석을 위해 군산대학교 감성사전 웹사이트를 참고한다.\nKNU 한국어 감성사전\ndilab.kunsan.ac.kr\n또는 깃허브로 바로 이동해도 된다.\nGitHub - park1200656/KnuSentiLex: KNU(케이앤유) 한국어 감성사전\nKNU(케이앤유) 한국어 감성사전. Contribute to park1200656/KnuSentiLex development by creating an account on GitHub.\ngithub.com\n이 사이트는 다음과 같은 이유로 감성 분석에 유용하다.\nKNUSL 감성 사전의 특징 국내에서 구축한 한글 감성 어휘 사전으로, 한국어 감성 분석에 특화되어 있다. 각 단어에 감성 점수(긍정/부정/중립 등급)를 부여할 수 있다. 다양한 도메인(예: 리뷰, 뉴스 등)에 맞춘 감성 어휘 제공한다. 데이터는 연구 및 학습 목적으로 자유롭게 다운로드 가능 (단, 출처 명시 필요)\nDownload ZIP 클릭.\n다운로드 된 파일 압축 풀기.\ndata 폴더로 들어간 다음 SentiWord_info 파일 경로 복사하기\n감성 사전 JSON 파일을 읽어서 단어만 추출해보기.\nimport json\nfile_path = r’C:-master-master_info.json’\nwith open(file_path, ‘r’, encoding=‘utf-8’) as f: json_data = json.load(f)\n\n단어 리스트 생성\nword_list = [] for item in json_data: # 리스트 반복 word_txt = item[‘word’] word_list.append(word_txt)\n\n\n결과 출력 (앞 10개만 보기)\nprint(word_list[:10])\nKOSAC 감성사전 다운로드\nGitHub - mrlee23/KoreanSentimentAnalyzer: 한국어 감성 분석기\n한국어 감성 분석기. Contribute to mrlee23/KoreanSentimentAnalyzer development by creating an account on GitHub.\ngithub.com\n\n\nvalues 긍정부정중립\n01 제목1 [ID 사용하기]\n1 . 제목2\nNRC Emotion Lexicon\nImpact Some notable ways in which the NRC Emotion Lexicon has made impact include: First of its kind: It was the first word-emotion association lexicon, with entries for eight basic emotions as well as positive and negative sentiment. It still remains the\nsaifmohammad.com"
  },
  {
    "objectID": "da/tm/tm_04_0.html",
    "href": "da/tm/tm_04_0.html",
    "title": "4장: 크롤링 데이터 전처리",
    "section": "",
    "text": "크롤링 데이터의 통합 및 전처리에 대해 다루고자 한다.\n너저문한 데이터를 정리\n개행문자는 스페이스바를 눌렸기에 생기는 것임. 이를 처리하는 것이 전처리 과정임.\n의미있는 인사이트를 얻기 위해서 정리\n원데이터, 쿠팡에서 들어오는 데이터는 트랜젝션데이터(거래 데이터) 어떤 고객이 가입, 비가입, 어떤 카드로 결제, 회원가입 정보(최소한의 정보), 배송을 위한 성명, 휴대폰 번호, 본인인증 정도, - 고객의 프로파일링, 또는 데모그래픽 데이터 품목명, 가격명, 시간대, 카드 정보 – 정형데이터 이러한 필드로 저장됨, 댓글 정보 – 비정형 데이터, 사람마다 쓰는 댓글 양이 다름 고객의 반응을 보기 위해 전처리를 시도함, 이는 IT팀, 마케팅팀, MD가 사용\n성별을 구별하는 방법\n룰세팅을 하여 전처리 필요 없이 데이터를 뽑아서 써야 됨 여기선 가공 변수를 만드는 것이 필요함(예: 특정한 시간대에서 발생되는 매출)\n품목별 페이지에 대한 로그분석, 리뷰 데이터, 댓글의 패턴, 쿠키로 데이터 가져오기 SKT 전화요금제,만 있을 경우, 전화거래량 패턴 분석 정도 밖에 할 수 없음. 이름으로 성별을 판별 –\n추정을 하는 것임\n02 정규표현식\nimport re text = ‘core core883core’ re.findall(r’, text)\n\n단어 중간에 있는디\nre.findall(r’’, text)\nre.findall(r’1’, text)\ntext = ‘12 month 365 days 2023?’ re.findall(r’, text)\ntext = ‘12 month 365 days 2023?’ re.findall(r’’, text) # 숫자에 해당되는 걸 다 가져와\nre.findall(r’+’, text) # 숫자를 제외한 모든 것\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text # 특수문자를 제외한 모든 문자출력\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text # 특수문자까지 포함하여 출력\n03 데이터 합치기\n\n라이브러리 불러오기\n\nimport pandas as pd import pickle import os import re\n\n데이터 병합하기\n\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n\n\npkl 파일 로드 함수\ndef pklopen(text): f = open(file_path + ‘{}.pkl’.format(text),“rb”) a = pickle.load(f) f.close() return a\n\n\n수집된 데이터\ndata1 = pklopen(‘노인부양blog’) data2 = pklopen(‘노인부양cafe’) data3 = pklopen(‘노인부양cafe2’)\n\n\n데이터 결합\n\n\n행(row) 방향으로 데이터를\n\n\n밑으로 합치는(concatenate) 방식\ndata = pd.concat([data1, data2, data3])\n\n\n수집된 데이터가 모두 같은 컬럼 구조를 갖는다면,\n\n\n위에서 아래로 이어붙이는 방식으로 결합된다.\n\n\n각 채널 사이즈 확인\ndata.groupby([‘ch’, ‘ch2’]).size()\n\n인덱스 재설정하기\n\n\n\n인덱스를 0, 1, 2, …로 초기화하고,\n\n\n기존 인덱스는 새로운 열로 남기지 않도록 하는 명령어.\n\n\n주로 데이터 정제 후 인덱스를 깔끔하게 맞출 때 사용된다.\ndata = data.reset_index(drop=True) data\n채널별 수집한 데이터의 병합 결과\n04 데이터 전처리\n\n한글화\n\n정제, 정규화, 토큰화의 3단계를 거친다. 비정형 데이터일 경우,\n문서 날리기\n100 정열\nf = open(file_path + ‘노인부양병합’, ‘wb’) pickle.dump(data, f) f.close()\nf = open(file_path + ‘노인부양병합’, ‘rb’) docs = pickle.load(f) f.close() docs\n\n\n병합된 데이터를 피클 파일로 저장 및 출력한 것으로\n\n\n아래와 같이 파일이 저장된 것을 확인할 수 있다.\n\n\n제목, 본문, 댓글의 한글화 및 특수문자 제거\nfor i in range(len(docs)): docs.loc[i, ‘title’] = re.sub( r”[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]“,”“, str(data.loc[i, ‘title’]))\ndocs.loc[i, 'doc'] = re.sub(\n    r\"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(data.loc[i, 'doc']))\n\ndocs.loc[i, 'comment_cnt'] = re.sub(\n    r\"[^0-9]\", \"\", str(docs.loc[i, 'comment_cnt']))\n\ndocs.loc[i, 'comment_list'] = re.sub(\n    r\"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(data.loc[i, 'comment_list']))\ndocs\n\n\n조건에 맞는 row만 남기기\ndocs = docs[ ~( (docs[‘doc’].str.len() &lt; 2) | (docs[‘doc’].str.isspace()) )].reset_index(drop=True) 안에 ^를 넣을 때 한글만 가져오기\n\n문자형으로는 카우트 할 수 없으므로\n\n\n\nlike, comment_cnt, img, div의 데이터 타입을 숫자로 변환\n\n\n변환 중 에러 발생 시 NaN으로 처리\ndocs[‘like’] = pd.to_numeric(docs[‘like’], errors=‘coerce’).astype(‘Int64’) docs[‘comment_cnt’] = pd.to_numeric(docs[‘comment_cnt’], errors=‘coerce’).astype(‘Int64’) docs[‘img’] = pd.to_numeric(docs[‘img’], errors=‘coerce’).astype(‘Int64’) docs[‘div’] = pd.to_numeric(docs[‘div’], errors=‘coerce’).astype(‘Int64’)\ndocs\n원데이터와 값이 일치하는 지 확인하기.\n\n숫자형 결측치로 단어, 또는 문장, 형태소 분석 어간, 어근 어조 어미, 단순 띄어쓰기만으로는 힘듦, 형태소 분석을 쓰는 것임\n\n자립 형태소, 의존형태소\nOkt, 메캅, 코모란, 한나눔, 꼬꼬마\nfrom tqdm import tqdm # 진행상황 시각화 from konlpy.tag import Komoran\n\n\nKomoran 형태소 분석기 초기화\nkomoran = Komoran() # 클래스의 인스턴스 지정 # 이는 형태소 분석기 하나를 준비해서 계속 쓰기 위함이다.\n\n형태소 분석을 하고 명사들을 리스트로 저장\n\n이름은 단순하게 지정해도 상관없지만 너무 이름이 단순하면 이를 구별하거나 변수를 이해하기 어려으므로 다른 사람도 알아볼 수 있도록 객관적으로 판단하여 룰 세팅을 하는 것이 좋음\ntitle_token_list = [] # 제목의 형태소를 담아낼 리스트 title_token_noun = [] # 제목의 명사를 담아낼 리스트\nfor i in tqdm(range(len(docs))): # for문 - :\n# komoran.pos() 메서드를 사용하여 형태소 분석 실시\npos = komoran.pos(str(docs['title'][i]))\n\n# komoran.nouns() 메서드를 사용하여 추출하고 리스트에 저장\nnoun = [term for term in komoran.nouns( # 명사만 추출하며,\n    str(docs['title'][i])) if len(term) &gt; 1] # 명사의 길이는 2 이상이어야 한다.\n\ntitle_token_list.append(pos)\ntitle_token_noun.append(noun)\n리스트 이름구조 형태내용\ntitle_token_list [[(‘단어’, ‘품사’), …], …] 모든 형태소와 품사 정보 title_token_noun [[‘명사’, ‘명사’], …] 2글자 이상 명사만\n\n본문 토큰화 # 본문 형태소 및 명사 리스트 doc_token_list = [] doc_token_noun = []\n\nfor i in tqdm(range(len(docs))):\npos = komoran.pos(u'{}'.format(docs['doc'][i]))\n\nnoun = [term for term in komoran.nouns(\n    u'{}'.format(docs['doc'][i])) if len(term) &gt; 1]\n\ndoc_token_list.append(pos)\ndoc_token_noun.append(noun)\n\n\n댓글 형태소 및 명사 리스트\ncomment_token_list = [] comment_token_noun = []\nfor i in tqdm(range(len(docs))):\npos = komoran.pos(u'{}'.format(docs['comment_list'][i]))\n\nnoun = [term for term in komoran.nouns(\n    u'{}'.format(docs['comment_list'][i])) if len(term) &gt; 1]\n\ncomment_token_list.append(pos)\ncomment_token_noun.append(noun)\n형태소 분석만으로는 전처리가 끝났다고 볼 수 없다 아래 쓸대없는 불용어 때문에 찾고자 하는 문맥을 못 볼 수 있다.\n예를 들면, 광고글이 있는데 이는 광고글을 쓴 자가 정성스럽게 알맞은 단어 (의미없는 개행 문자 등만을 나열하지 않는) 말 그대로의 정돈된 글이기 떄문에 이를 제거하려면 일일이 광고글을 제거해야 한다.\n그러므로, 불용어 처리까지 해야 한다.\n다만, 불용어 처리에도 의미가 있는 명사를 제거하지 않도록 주의해야 한다. 예를 들어, ‘la’ 라는 문자만 본다면 의미없는 불용어라고 착각할 수 있다. 그러나 이는 LA를 의미하며, 미국 현지에서는 la, La, lA, LA와 같이 다양하게 사용되는 것으로 나타났다.\n따라서 이러한 불용어 처리 전에는 혹은 처리 중에 이러한 용어들이 나온다면 즉시 문서나 원데이터를 들여다봐서 실제로 그 값이 어떤 문맥상에서 어떤 의미를 지니는지를 확인해야 한다.\n어휘에 대한 이해를 할 수 있어야 한다.\n\n불용어 사전 다운받기\n\nstopwords-ko/stopwords-ko.txt at master · stopwords-iso/stopwords-ko\nKorean stopwords collection. Contribute to stopwords-iso/stopwords-ko development by creating an account on GitHub.\ngithub.com # 불용어 사전 기반 불용어 리스트 정리 f = open( file_path + “stopwords-ko.txt”, “r”, encoding=“UTF-8”) # UTF-8 인코딩으로 불용어 파일 열기\nst = f.readlines() # 한 줄씩 읽어서 리스트에 저장 f.close()\n\n\n줄 끝 개행 문자 제거\nst = [word.strip() for word in st] st\n\n불용어 사전 깔끔하게 만들기 stw = [word.strip() for word in st if word.strip() != ’’] stw\n나만의 불용어 사전 만들기 # 사용자가 정의한 불용어 추가 # 목적: 순수한 노인부양과 관련된 이야기 수집 # 광고글을 제외하기 위한 사용자 지정 불용어 사전 # 사용자가 정의한 불용어 추가 user_stopwords = [ ‘노인’, ‘부양’, ‘무자’, ‘양의’, ‘기초’, ‘노인학’, ‘계급’, ‘보험’, ‘고령’, ‘경제’, ‘바탕’, ‘국가’, ‘어르신’,‘지역’, ‘생각’, ‘포함’, ‘사업’, ‘한부모’, ‘일상생활’, ‘국민’, ‘확인’, ‘우리나라’, ‘적용’, ‘위해’, ‘기본’, ‘수준’, ‘예방’, ‘방법’, ‘주택’, ‘가능’, ‘방안’, ‘진행’, ‘행위’, ‘등의’, ‘대한민국’, ‘내년’, ‘개념’, ‘모집’, ‘개선’, ‘자격증’, ‘대상자’, ‘자격’, ‘과제’, ‘토론’, ‘청주’, ‘감소’, ‘증가’, ‘대의’, ‘추천’, ‘자부’, ‘경우’, ‘게시판’, ‘자금’, ‘본인’, ‘사람’, ‘연령’, ‘등급’, ‘활동’, ‘정부’, ‘평균’, ‘일반’, ‘파일’, ‘자의’, ‘더보’, ‘주간’, ‘기대’, ‘결과’, ‘통해’, ‘인가’, ‘자료’, ‘두레’, ‘포트’, ‘사이트’, ‘회원’, ‘다운’, ‘추가’, ‘완성’, ‘포인트’, ‘다운로드’, ‘충전’, ‘신규’, ‘제휴’, ‘작성’, ‘이벤트’, ‘저도’, ‘바우’, ‘해주’, ‘아래’, ‘링크’, ‘자가’, ‘해주시’, ‘등록’, ‘특례’, ‘네이버’, ‘구부’, ‘다이’, ‘이얼’, ‘마나’, ‘한일’, ‘서로’, ‘이다’, ‘현재’, ‘해서’, ‘댓글’, ‘하기’, ‘니다’, ‘이하’, ‘안녕하세요’, ‘해도’, ‘오늘’, ‘하면’, ‘키메’, ‘고맙습니다’, ‘이고’, ‘제가’, ‘내세’, ‘가요’, ‘만세’, ‘이노’, ‘때문’, ‘블로그’, ‘블로거’, ‘카페’, ‘만원’, ‘보내기’, ‘질문’, ‘재가’, ‘한국’, ‘세계’, ‘사회’, ‘가족’, ‘기준’, ‘서비스’, ‘장기’]\n\n\n\n불용어 리스트 확장\nstw.extend(user_stopwords)\n\n\n불용어 리스트 CSV 파일로 저장\nimport csv\nwith open(‘불용어 리스트’, “w”) as file: writer = csv.writer(file) writer.writerow(stw)\n\n정리된 불용어를 각문서의 제목, 본문, 댓글에서 제거 for word in stw: for i in range(len(title_token_noun)): # 제목에서 불용어 제거 while word in title_token_noun[i]: title_token_noun[i].remove(word)\n# 본문에서 불용어 제거\nwhile word in doc_token_noun[i]:\n    doc_token_noun[i].remove(word)\n\n# 댓글에서 불용어 제거\nwhile word in comment_token_noun[i]:\n    comment_token_noun[i].remove(word)\n\n\n\n문서파일 docs에 적용\ndocs[‘title_token_noun’] = title_token_noun # 제목 명사 리스트 docs[‘title_token_list_pos’] = title_token_list # 형태소+품사 리스트\ndocs[‘doc_token_noun’] = doc_token_noun # 본문 명사 리스트 docs[‘doc_token_list_pos’] = doc_token_list # 형태소+품사 리스트\ndocs[‘comment_token_noun’] = comment_token_noun # 본문 명사 리스트 docs[‘comment_token_list_pos’] = comment_token_list # 형태소+품사 리스트\n\n불용어를 제거한 최종 파일 저장 및 불러오기 # pickle로 저장 (최초 1회만 실시) import pickle with open(file_path + “total_doc.pkl”, “wb”) as f: pickle.dump(docs, f)\n\n\n\npickle로 다시 불러오기\nwith open(file_path + “total_doc.pkl”, “rb”) as f: data = pickle.load(f)"
  },
  {
    "objectID": "da/tm/tm_03_0.html",
    "href": "da/tm/tm_03_0.html",
    "title": "3장: 네이버 블로그 크롤링",
    "section": "",
    "text": "동적 크롤링을 위한 준비 및 네이버 블로그 크롤링 실습에 대해 다루고자 한다.\n01 자바 설치 방법\n1 . 파이썬과 자바의 관계 일반적으로 파이썬은 자바 없이 독립적으로 실행할 수 있다.\n하지만, 특정 라이브러리(예: JPype, PySpark, Jython 등)는 자바(Java)를 필요로 한다.\n따라서 사용하려는 기능이 자바 기반이라면, 먼저 자바가 설치되어 있어야 한다.\n2 . 자바 설치 여부 확인 Anaconda 프롬프트 실행한 다음 명령어 입력 후 실행.\n\n자바가 설치되어 있다면 버전 정보가 출력됨.\n\n\n“java is not recognized…” 오류가 발생하면 자바가 설치되지 않은 것임.\njava -version\n3 . 내 컴퓨터에 자바 설치하기 Oracle 공식 홈페이지에서 JDK 다운로드 설치 후, 환경 변수를 설정해야 한다.\nDownload the Latest Java LTS Free\nSubscribe to Java SE and get the most comprehensive Java support available, with 24/7 global access to the experts.\nwww.oracle.com\n환경 변수 설정 (Windows 기준)\n제어판 → 시스템 및 보안 → 시스템 → 고급 시스템 설정 고급 탭 → 환경 변수 버튼 클릭 시스템 변수에서 “새로 만들기” 클릭 변수 이름: JAVA_HOME 변수 값: C:Files-XX.X.X (설치된 JDK 경로 입력) Path 변수 편집 → ;%JAVA_HOME%추가\nAnaconda 프롬프트 또는 명령 프롬프트에서 다시 입력하여 정상적으로 출력되는지 확인한다.\n02 Selenium을 사용한 동적 크롤링\n1 . Selenium 설치 웹 브라우저에서 동적 크롤링 시 가장 많이 사용하는 패키지.\n과거에는 웹드라이버 버전에 맞는 경로를 지정해줘야 했지만, 현재는 패키지의 새버전에 의해 자동적으로 맞춰진다.\npip install selenium\n2 . 웹드라이버 다운로드 사용하는 브라우저에 맞는 WebDriver를 다운로드해야 한다.\nChrome 다운로드 및 설치 - 컴퓨터 - Google Chrome 고객센터\n도움이 되었나요? 어떻게 하면 개선할 수 있을까요? 예아니요\nsupport.google.com 다운로드한 WebDriver를 실행 파일 경로에 두거나, Python 코드에서 직접 경로를 지정해야 한다.\n03 네이버 블로그 크롤링\n\n라이브러리 불러오기.\n\nfrom selenium import webdriver # 웹 브라우저 자동화 from bs4 import BeautifulSoup as BS # HTML 및 XML 파싱\nimport pandas as pd # 데이터 조작 및 분석 import requests # HTTP 요청을 보내기 위한 모듈 import datetime # 날짜 및 시간 연산 import pickle # 파이썬 객체 직렬화 import time # 코드 실행 간격 조절 import re # 정규 표현식을 사용하여 문자열 처리\n\n\nSelenium에서 다양한 방법으로 HTML 요소를 찾기\nfrom selenium.webdriver.common.by import By\n\nSelenium을 이용한 네이버 블로그 검색 자동화.\n\n\n\n크롬 드라이버 실행\ndriver = webdriver.Chrome()\n\n\n네이버 블로그 검색 페이지로 이동\n\n\n검색할 키워드 지정 및 데이터 수집기간 설정한 뒤\n\n\n복사한 URL을 붙여 넣으면 되며, 아래 코드는 가독성을 위해 일부러 줄바꿈을 시도함\ndriver.get(’’’ https://search.naver.com/search.naver? ssc=tab.blog.all&query=%EB%85%B8%EC%9D%B8%20%EB%B6%80%EC%96%91 &sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20240301to20240325’’’.replace(“”, ““))\nURL 가져오는 방법\n실행 화면\n\n웹 페이지 자동 스크롤 함수.\n\ndef doScrollDown(whileSeconds): start = datetime.datetime.now() # 스크롤 다운 시작 시간 설정 end = start + datetime.timedelta(seconds=whileSeconds) # 스크롤 다운 종료 시간\nwhile True:\n    # 페이지 맨 아래로 스크롤 다운\n    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n    time.sleep(1) # 1초 대기\n\n    # 종료 시간에 도달하면 반복 종료\n    if datetime.datetime.now() &gt; end:\n        break\n        \ndoScrollDown(2) # 스크롤 다운 시간 설정\n\n웹 페이지에서 제목과 URL 추출하기.\n\n\n\n제목과 URL을 저장할 리스트 초기화\ntitle_list = [] url_list = []\n\n\n현재 페이지에서 클래스명이 ’title_link’인 요소들을 찾음\ntitles = driver.find_elements(By.CLASS_NAME, ‘title_link’)\nfor i, title_element in enumerate(titles): try: # 요소에서 제목을 추출하여 title_list에 추가 title_list.append(title_element.text) # 요소에서 URL을 추출하여 url_list에 추가 url_list.append(title_element.get_attribute(‘href’)) except: print(“오류 발생”) # 예외 발생 시 출력 continue # 오류가 발생해도 다음 요소 처리 계속 진행\n# 10번째 항목마다 진행 상황 출력\nif (i + 1) % 10 == 0:\n    print(f\"{i + 1}개 수집 완료\")\n\n블로그 본문 및 메타데이터 크롤링 자동화.\n\n\n\n1] 크롤링 데이터 저장 리스트 초기화\nnew_doc, like_cnt, comment_cnt, comment_list, img_cnt, div_cnt = [], [], [], [], [], []\n\n\n2] 블로그 본문 크롤링\nfor i in range(len(url_list)): url_path = url_list[i] # URL 불러오기 driver.switch_to.window(driver.window_handles[0]) # 첫 번째 탭으로 이동 driver.execute_script(“window.open(‘{}’)”.format(url_path)) # 새 탭 열기(URL 실행) driver.switch_to.window(driver.window_handles[1]) # 두 번째 탭으로 이동\ntime.sleep(1)  # 1초 대기\ntry:\n    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n    d = ''     # 댓글 변수 초기화\n\n    # 댓글 영역의 HTML 코드 가져오기\n    if len(iframes) &gt; 0:            # iframes의 존재 확인\n        driver.switch_to.frame(0)       # 첫 번째 iframe으로 전환 및 내용 가져옴\n        html = driver.page_source       # HTML 코드 가져와 변수 저장\n        soup = BS(html, \"html.parser\")  # 저장된 코드 파싱 및 soup 생성\n\n        # 3] 블로그 본문 추출\n        try:\n            a = soup.find(\"div\", class_=\"se-main-container\").get_text()\n        except: # 블로그 본문을 찾지 못할 경우\n            a = soup.find(\"div\", id=\"postListBody\")     # 일반 블로그에 경우\n            a = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(a)) # 정규표현식 -&gt; 한글만 남김\n\n        # 4] 좋아요 수 추출\n        try:\n            b = soup.find(\"em\", class_=\"u_cnt_count\").get_text()\n        except:\n            b = \"null\"\n\n        # 5] 댓글 수 추출\n        try:\n            c = soup.find(\"em\", id=\"commentCount\").get_text()\n        except:\n            c = \"null\"\n\n        # 6] 댓글 추출\n        try: # 댓글을 모두 보기 위해 버튼 클릭\n            comment = driver.find_elements(By.CLASS_NAME, \"btn_arr\")\n            comment[-1].click()  # 마지막 댓글 버튼 클릭\n            time.sleep(1)\n            commentLen = len(driver.find_elements(By.CLASS_NAME, \"u_cbox_page\"))\n            d = \"\\n\".join([comment.text for comment in driver.find_elements(By.CLASS_NAME, \"u_cbox_text_wrap\")])\n        except:\n            d = \"null\"\n\n        # 7] 이미지 및 영상 수 추출\n        e = len(soup.find_all(\"img\", class_=\"se-image-resource egjs-visible\"))\n        f = len(soup.find_all(\"div\", class_=\"pzp-ui-dimmed pzp-dimmed pzp-pc__dimmed\"))\n\n        # 8] 데이터 리스트에 추가\n        new_doc.append(a)\n        like_cnt.append(b)\n        comment_cnt.append(c)\n        comment_list.append(d)\n        img_cnt.append(e)\n        div_cnt.append(f)\n\n        driver.switch_to.default_content()  # 기본 콘텐츠로 전환\n    else:\n        # 데이터가 없을 경우 빈 값 추가\n        new_doc.append(' ')\n        like_cnt.append(' ')\n        comment_cnt.append(' ')\n        comment_list.append(' ')\n        img_cnt.append(' ')\n        div_cnt.append(' ')\n\nexcept Exception as e:\n    # 예외 발생 시 에러 메시지와 함께 빈 값 추가\n    print(f\"Error at {url_path}: {e}\")\n    new_doc.append(' ')\n    like_cnt.append(' ')\n    comment_cnt.append(' ')\n    comment_list.append(' ')\n    img_cnt.append(' ')\n    div_cnt.append(' ')\n\ndriver.close()  # 현재 탭 닫기\ntime.sleep(0.3)  # 0.3초 대기\n\n# 매 10번마다 진행 상황 출력\nif (i+1) % 10 == 0:\n    print(f\"진행 상황: {i+1}/{len(url_list)}\")\n\n데이터프레임으로 변환.\n\n\n\n크롤링한 데이터를 데이터프레임으로 변환\nraw_data = pd.DataFrame({ “title”: title_list, “doc”: new_doc, “like”: like_cnt, “comment_cnt”: comment_cnt, “commnet_list”: comment_list, “img”: img_cnt, “div”: div_cnt, “ch”: “naver”, “ch2”: “blog” })\n\n\n데이터프레임을 pickle 파일로 저장\nfile_path = “C:/Users/jkl12/텍스트마이닝/” # 슬래시 사용 with open(file_path + “노인부양blog.pkl”, “wb”) as f: pickle.dump(raw_data, f)\n\n\n크롬 드라이버 종료\ndriver.quit()\n\n\n저장된 pickle 파일을 불러오기\nwith open(file_path + “노인부양blog.pkl”, “rb”) as f: temp_file = pickle.load(f)\n\n\n데이터프레임을 CSV 파일로 저장\ntemp_file.to_csv(file_path + “노인부양blog.csv”, index=False, encoding=“utf-8-sig”) # 파일 경로 지정 file_path = r”C:.csv”\n\n\nCSV 파일 불러오기\ndf = pd.read_csv(file_path, encoding=“utf-8-sig”) df"
  },
  {
    "objectID": "cybersec.html",
    "href": "cybersec.html",
    "title": "Cyber-Security",
    "section": "",
    "text": "제목\n\n\n\n1\n\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n제목\n\n\n\n1\n\n\n\n\n\n\n\n\n\nDec 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n전송계층\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n서브넷 마스크\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIP 프로토콜 및 QoS\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n라우팅(Routing)\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n네트워크계층의이해\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n무선통신시스템의이해\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n데이터링크계층의핵심구조\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n데이터 링크\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n유선 통신망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n무선 통신망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n네트워크 개론\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cs/bn_12.html",
    "href": "cs/bn_12.html",
    "title": "제목",
    "section": "",
    "text": "OSI 모델의 표현 계층(Presentation Layer)은 데이터의 변환, 암호화, 압축 등 상위 애플리케이션 간 데이터 호환성을 보장하는 역할을 수행한다. 이를 통해 하드웨어 및 운영체제 독립성을 유지하고, 다양한 표현 방식을 표준화할 수 있다.\n\n\n응용 계층은 최종 사용자와 직접 상호작용하며, 데이터의 구조화, 처리, 전송을 담당한다.\n\n\n\nHTML: 웹 콘텐츠 구조화, 확장성 및 호환성 제공\nHTML5: 멀티미디어 통합, Canvas API, 반응형 디자인, 로컬 스토리지, WebSocket 지원으로 현대 웹의 표준화 구현\n\n\n\n\n\n플랫폼 독립적이며 언어 제약이 없음\n요청 시마다 프로세스 생성으로 성능 저하 발생 가능\n유지보수 어려움과 보안 취약점 존재\n\n\n\n\n\n구조화된 데이터 저장 및 전송에 최적화\n사용자 정의 태그 지원, 가독성 우수, 플랫폼 독립성 확보"
  },
  {
    "objectID": "cs/bn_12.html#osi-표현-계층과-응용-계층",
    "href": "cs/bn_12.html#osi-표현-계층과-응용-계층",
    "title": "제목",
    "section": "",
    "text": "OSI 모델의 표현 계층(Presentation Layer)은 데이터의 변환, 암호화, 압축 등 상위 애플리케이션 간 데이터 호환성을 보장하는 역할을 수행한다. 이를 통해 하드웨어 및 운영체제 독립성을 유지하고, 다양한 표현 방식을 표준화할 수 있다.\n\n\n응용 계층은 최종 사용자와 직접 상호작용하며, 데이터의 구조화, 처리, 전송을 담당한다.\n\n\n\nHTML: 웹 콘텐츠 구조화, 확장성 및 호환성 제공\nHTML5: 멀티미디어 통합, Canvas API, 반응형 디자인, 로컬 스토리지, WebSocket 지원으로 현대 웹의 표준화 구현\n\n\n\n\n\n플랫폼 독립적이며 언어 제약이 없음\n요청 시마다 프로세스 생성으로 성능 저하 발생 가능\n유지보수 어려움과 보안 취약점 존재\n\n\n\n\n\n구조화된 데이터 저장 및 전송에 최적화\n사용자 정의 태그 지원, 가독성 우수, 플랫폼 독립성 확보"
  },
  {
    "objectID": "cs/bn_12.html#웹-시스템-구조와-아키텍처",
    "href": "cs/bn_12.html#웹-시스템-구조와-아키텍처",
    "title": "제목",
    "section": "웹 시스템 구조와 아키텍처",
    "text": "웹 시스템 구조와 아키텍처\n웹 시스템의 주요 목표는 데이터 처리 효율, 확장성 확보, 보안 및 안정성 유지이다.\n\n2. 핵심 구성 요소\n\n클라이언트(Client): 사용자 요청 생성 및 화면 표시\n웹 서버(Web Server): HTTP 요청 처리, 정적 콘텐츠 제공\n애플리케이션 서버(App Server): 비즈니스 로직 처리\n데이터베이스(DB): 데이터 저장 및 관리\n\n\n\n2.1 확장 및 최적화 구성 요소\n\n로드 밸런서(Load Balancer): 서버 부하 분산\n캐시 서버(Cache Server): 반복 데이터 빠른 접근 제공\nCDN(Content Delivery Network): 글로벌 데이터 전송 최적화\n보안 시스템(Security Layer): 접근 제어 및 위협 방어\n\n\n\n2.2 아키텍처 진화\n단일 계층 → 2계층 → 3계층 → 마이크로서비스 아키텍처로 발전, 모듈화 및 확장성 강화"
  },
  {
    "objectID": "cs/bn_12.html#웹-요청-및-응답-흐름",
    "href": "cs/bn_12.html#웹-요청-및-응답-흐름",
    "title": "제목",
    "section": "웹 요청 및 응답 흐름",
    "text": "웹 요청 및 응답 흐름\n\n3. DNS(Domain Name System)\n\n필요성: 기억하기 어려운 IP 주소를 도메인 이름으로 변환, 유연한 변경, 효율적 운영\n구조: 최상위 루트 → TLD → 세컨드 레벨 도메인 → 서브 도메인 → 호스트, 계층적 트리 구조\n쿼리 방식: Recursive, Iterative\n활용 사례: 웹 브라우징, 이메일, CDN, 부하 분산, 보안"
  },
  {
    "objectID": "cs/bn_12.html#이메일과-ftp-프로토콜",
    "href": "cs/bn_12.html#이메일과-ftp-프로토콜",
    "title": "제목",
    "section": "이메일과 FTP 프로토콜",
    "text": "이메일과 FTP 프로토콜\n\n4. 이메일 시스템\n\n흐름: 메일 작성 → 송신 → 라우팅 → 저장 → 수신\n프로토콜: SMTP(송신), POP3/IMAP(수신)\nMIME(Multipurpose Internet Mail Extensions): 다양한 언어, 이미지, 동영상 지원\n\n\n\n5. POP3 vs IMAP\n\nPOP3: 서버에서 메일 다운로드 후 로컬 저장, 단순 구조\nIMAP: 서버에서 메일 동기화 및 관리, 멀티 디바이스 지원\n\n\n\n6. FTP(File Transfer Protocol)\n\n동작 모드: 액티브 모드, 패시브 모드\n보안 강화 프로토콜: FTPS, SFTP, FTP over SSL/TLS 등 안전한 파일 전송 지원"
  },
  {
    "objectID": "cs/bn_10.html",
    "href": "cs/bn_10.html",
    "title": "서브넷 마스크",
    "section": "",
    "text": "IP 주소에서 네트워크와 호스트 영역을 명확히 구분하기 위한 핵심적인 비트 마스크이다. 이를 통해 네트워크 식별, 호스트 구분, 라우팅 효율화가 가능하며, IPv4 주소 체계에서 필수적인 구성 요소로 작동한다.\n\n\n\nIPv4 주소는 32비트로 구성\n‘1’: 네트워크 영역\n‘0’: 호스트 영역\n서브넷팅(Subnetting):\n기존 네트워크를 보다 작은 서브넷으로 분할하여 네트워크 관리 효율과 보안을 향상시키고, 라우팅 최적화를 달성한다.\n네트워크 비트 수 증가 → 서브넷 수 증가, 호스트 수 감소.\n슈퍼넷팅(Supernetting):\n다수의 작은 네트워크를 하나의 큰 네트워크로 결합하여\nCIDR(Classless Inter-Domain Routing) 구현 및 라우팅 테이블 축소 효과를 달성한다.\n\n\n\n\nHierarchical Routing 라우팅 테이블의 항목 수를 최소화하여 라우터 부하를 줄이고, 네트워크 관리 효율성을 높인다. 이를 통해 네트워크 범위 식별, 라우팅 영역 구분, 상위 네트워크 및 외부 네트워크와의 효율적인 데이터 전달이 가능하다.\n장점: 주소 활용 효율화, 보안 강화, 트래픽 최적화, 계층적 설계. 단점: 설계 복잡성 증가, 오류 가능성, 관리 부담 증가.\n\n\n\n게이트웨이, DNS, 라우팅 테이블과 밀접히 연관되어 네트워크 통합 관리와 안정적 운영을 지원한다."
  },
  {
    "objectID": "cs/bn_10.html#네트워크-분할",
    "href": "cs/bn_10.html#네트워크-분할",
    "title": "서브넷 마스크",
    "section": "",
    "text": "IPv4 주소는 32비트로 구성\n‘1’: 네트워크 영역\n‘0’: 호스트 영역\n서브넷팅(Subnetting):\n기존 네트워크를 보다 작은 서브넷으로 분할하여 네트워크 관리 효율과 보안을 향상시키고, 라우팅 최적화를 달성한다.\n네트워크 비트 수 증가 → 서브넷 수 증가, 호스트 수 감소.\n슈퍼넷팅(Supernetting):\n다수의 작은 네트워크를 하나의 큰 네트워크로 결합하여\nCIDR(Classless Inter-Domain Routing) 구현 및 라우팅 테이블 축소 효과를 달성한다."
  },
  {
    "objectID": "cs/bn_10.html#계층적-라우팅",
    "href": "cs/bn_10.html#계층적-라우팅",
    "title": "서브넷 마스크",
    "section": "",
    "text": "Hierarchical Routing 라우팅 테이블의 항목 수를 최소화하여 라우터 부하를 줄이고, 네트워크 관리 효율성을 높인다. 이를 통해 네트워크 범위 식별, 라우팅 영역 구분, 상위 네트워크 및 외부 네트워크와의 효율적인 데이터 전달이 가능하다.\n장점: 주소 활용 효율화, 보안 강화, 트래픽 최적화, 계층적 설계. 단점: 설계 복잡성 증가, 오류 가능성, 관리 부담 증가."
  },
  {
    "objectID": "cs/bn_10.html#연계-요소",
    "href": "cs/bn_10.html#연계-요소",
    "title": "서브넷 마스크",
    "section": "",
    "text": "게이트웨이, DNS, 라우팅 테이블과 밀접히 연관되어 네트워크 통합 관리와 안정적 운영을 지원한다."
  },
  {
    "objectID": "cs/bn_10.html#arp",
    "href": "cs/bn_10.html#arp",
    "title": "서브넷 마스크",
    "section": "1. ARP",
    "text": "1. ARP\nAddress Resolution Protocol\n\nIPv4 환경에서 IP 주소를 MAC 주소로 변환.\n송신자 확인 → 브로드캐스트 요청 → 유니캐스트 응답 → 캐시 저장.\n특징: 브로드캐스트-유니캐스트 혼합 통신, 일정 기간 IP-MAC 매핑 캐시, 편리하지만 보안 취약점 존재."
  },
  {
    "objectID": "cs/bn_10.html#rarp-bootp-dhcp",
    "href": "cs/bn_10.html#rarp-bootp-dhcp",
    "title": "서브넷 마스크",
    "section": "2. RARP, BOOTP, DHCP",
    "text": "2. RARP, BOOTP, DHCP\n\nRARP: MAC 주소를 기반으로 IP 주소 요청.\nBOOTP: IP 주소가 없는 장치에 초기 IP 할당.\nDHCP: BOOTP 기능을 확장하여 IP, 서브넷 마스크, 게이트웨이, DNS 등 자동 제공.\n동작 순서: Discover → Offer → Request → Acknowledge.\n일반 가정용 및 기업 환경에서 널리 사용."
  },
  {
    "objectID": "cs/bn_10.html#기타-네트워크-프로토콜",
    "href": "cs/bn_10.html#기타-네트워크-프로토콜",
    "title": "서브넷 마스크",
    "section": "03 기타 네트워크 프로토콜",
    "text": "03 기타 네트워크 프로토콜"
  },
  {
    "objectID": "cs/bn_10.html#icmp",
    "href": "cs/bn_10.html#icmp",
    "title": "서브넷 마스크",
    "section": "1. ICMP",
    "text": "1. ICMP\nInternet Control Message Protocol\n\nIP 네트워크에서 오류 보고와 제어 메시지 전달용 보조 프로토콜.\nIP 자체에는 오류 보고 기능이 없으므로, ICMP를 통해 안정적 네트워크 동작을 지원.\n주요 메시지: 오류 보고, 상태 진단, 경로 추적 등."
  },
  {
    "objectID": "cs/bn_10.html#igmp",
    "href": "cs/bn_10.html#igmp",
    "title": "서브넷 마스크",
    "section": "2. IGMP",
    "text": "2. IGMP\nInternet Group Management Protocol\n\nIPv4에서 멀티캐스트 그룹 관리. IPv6에서는 MLD 사용.\n필요성: 멀티캐스트 트래픽 효율적 관리, 실시간 서비스 지원.\n동작: 가입 요청 → 라우터 정보 유지 → 주기적 멤버 확인 → 탈퇴 알림.\n버전별 특징: v1 기본 가입 기능, v2 빠른 탈퇴 지원, v3 SSM(Source-Specific Multicast) 지원.\n적용 사례: 유튜브 라이브 스트리밍, 금융 거래 서비스."
  },
  {
    "objectID": "cs/bn_10.html#라우팅-프로토콜",
    "href": "cs/bn_10.html#라우팅-프로토콜",
    "title": "서브넷 마스크",
    "section": "3. 라우팅 프로토콜",
    "text": "3. 라우팅 프로토콜\n\n내부 라우팅 프로토콜: OSPF(Link State 기반), 빠른 수렴, 계층적 네트워크 구조에 적합.\n외부 라우팅 프로토콜: BGP(자율 시스템 간 경로 선택), 인터넷 백본 및 대규모 네트워크에서 표준으로 사용."
  },
  {
    "objectID": "cs/bn_08.html",
    "href": "cs/bn_08.html",
    "title": "라우팅(Routing)",
    "section": "",
    "text": "네트워크 계층(Network Layer)의 핵심 기능으로, 데이터 패킷이 출발지에서 목적지까지 도달하기 위한 최적의 경로를 결정하는 과정을 의미한다.\n이 과정은 네트워크의 토폴로지 변화, 트래픽 부하, 링크 상태 등에 따라 동적으로 변화하며, 효율성과 안정성 간의 균형이 중요하다.\n라우팅은 크게 정적(Static)과 동적(Dynamic) 방식으로 구분된다.\n\n정적 라우팅 (Static Routing):\n관리자가 수동으로 경로를 지정.\n소규모 네트워크에서 안정적이지만, 링크 장애나 토폴로지 변화에 대응 불가.\n동적 라우팅 (Dynamic Routing):\n라우터가 인접 노드들과 정보를 교환하며 최적 경로를 실시간 계산.\n자율성과 적응력이 높으나, 연산 부하와 제어 메시지 비용이 증가."
  },
  {
    "objectID": "cs/bn_08.html#애자일-개발-방법론-전략적-관점",
    "href": "cs/bn_08.html#애자일-개발-방법론-전략적-관점",
    "title": "라우팅(Routing)",
    "section": "애자일 개발 방법론: 전략적 관점",
    "text": "애자일 개발 방법론: 전략적 관점\n\n1. 개발 방법론의 분류 및 특성\n소프트웨어 개발 프로젝트는 전통적으로 폭포수(Waterfall) 방식과 애자일(Agile) 방식으로 구분된다. 폭포수 방식은 요구사항 정의 → 분석 → 설계 → 구현 → 테스트 → 배포로 이어지는 선형적 단계 모델을 따르며, 각 단계가 완료되어야 다음 단계로 진행된다. 이 접근법은 명확한 요구사항과 엄격한 일정 관리가 필요할 때 효과적이지만, 요구사항 변경이나 시장 변화에 유연하게 대응하기 어렵다는 한계가 있다.\n반면 애자일 방식은 반복적·점진적 개발(iterative and incremental development)을 기반으로 한다. 핵심 원칙은 고객 요구사항의 변화에 민첩하게 대응하며, 기능 단위의 점진적 납품을 통해 피드백을 신속하게 반영하는 것이다. 프로젝트 관리에서는 WBS 기반의 일정 중심 관리보다 작업(Task) 중심 관리가 강조되며, 각 개발 주기를 스프린트(Sprint)로 정의한다. 일반적으로 스프린트는 1~4주 단위로 반복되며, 각 사이클 종료 시 실제 동작 가능한 소프트웨어를 제공하여 고객 검증과 요구사항 반영을 동시에 달성한다.\n\n\n2. 애자일의 전략적 활용\n\n고객 중심성: 초기 버전부터 고객과 지속적으로 상호작용하여 요구사항을 점진적으로 명확화한다.\n적응적 계획(Adaptive Planning): 시장 변화나 기술적 요건에 맞춰 스프린트 계획을 유연하게 조정한다.\n위험 관리: 단기 목표 기반 반복 개발로 프로젝트 실패 가능성을 조기에 탐지하고 완화한다.\n팀 자율성: 크로스 기능적 팀(Cross-functional Team)이 스스로 우선순위를 판단하고 작업을 수행한다.\n\n\n\n3. 폭포수 방식 대비 애자일의 장점과 한계\n\n\n\n구분\n폭포수(Waterfall)\n애자일(Agile)\n\n\n\n\n개발 진행\n선형, 단계별\n반복적, 점진적\n\n\n요구사항 변경 대응\n어려움\n용이, 적응적\n\n\n일정 관리\nWBS 중심, 고정\n스프린트 중심, 유연\n\n\n고객 참여\n제한적\n지속적 피드백 포함\n\n\n위험 관리\n후기 발견\n조기 탐지 및 완화\n\n\n문서화\n상세 문서 중심\n최소 문서, 실행 중심\n\n\n\n애자일은 무제한적 유연성을 의미하지 않는다. 프로젝트 규모, 조직 구조, 규제 환경, 기술적 복잡성에 따라 적용 전략을 신중히 설계해야 하며, 과도한 변화 반영은 일정 지연과 품질 저하를 초래할 수 있다.\n\n\n4. 적용 사례 및 실무 전략\n\n스타트업 제품 개발: 빠른 시장 반응과 경쟁력 확보를 위해 애자일 방식이 적합하다.\n대기업 내부 시스템 개발: 일부 핵심 모듈은 폭포수 방식으로 안정성을 확보하고, 신규 기능 모듈은 애자일 방식으로 점진적 개발을 수행한다.\n혼합 접근(Hybrid Approach): 전통적 개발 프로세스에 애자일 스프린트를 통합하여 안정성과 민첩성 간 균형을 확보한다.\n\n\n\n5. 결론\n애자일은 단순한 개발 방법론이 아니라, 조직의 전략적 의사결정, 프로젝트 관리 체계, 고객 가치 창출을 통합적으로 고려한 개발 패러다임이다. 이를 통해 변화하는 요구사항과 시장 환경 속에서도 효과적인 소프트웨어 개발과 관리가 가능하다."
  },
  {
    "objectID": "cs/bn_06.html",
    "href": "cs/bn_06.html",
    "title": "무선통신시스템의이해",
    "section": "",
    "text": "IEEE 802.11 기술 유선 LAN 형태 이너넷 단점보완을 위해 기기가 AP와 연동해서 AC 또는 라우터를 통해서 인터넷 망을 주고 받는다. 연결 교모별 BSS, AP 없음 ESS, 다중 AP 연결 유형별 Ad Hoc, Infrastructure, 기술 발전의 세부규격들의 기준은 대부분 속도이다.\n6G로 도약이 어려운 이유 중 하나는 현재 한국은 4G와 5G가 혼용된 형태의 환경인데 6G는 대부분이 5G로 되어 있어야 설치 가능한 환경이 만들어진다. 그러려면 대용량 데이터 전송이 필요한 매체(AR) 등의 플랫폼이 있어야 하는데 아직까지 또 그러지도 않아서 기술의 발전이 더디는 상황이다.\nCSMA/CA 동작원리 4가지(IEEE 802.11 관련 기술) 채널감지, 대기, 프레임 전송, 수신확인\nIEEE 802.15와의 차이점\n블루투스 기호의 유래: 스칸다비아의 문자 두개를 합쳐 탄생 전파 간섭 문제가 자주 있으며 인원이 많아지면 보통 일어난다. pairing, BLE 2가지 기술용도로 씀, 그래서 유용하면서도 보완에는 상대적으로 취약에서 범용사용에는 국한된다. 블루투스 비콘의 등장배경: 2010 코인 배터리 등장 스마트폰 중에서도 애플의 도움으로 이것이 실현하게된 가장 큰 요인이 됨. 사실상 휴대폰의 확장 기능.\nZigbee 개요: 간단한 기능 사용. 짧은 보고 시간 + 낮은 에너지 사용 + 높은 배터리(수개월-수년) =&gt; 전기 및 도시 가스 계량기, 거리 조명 등 사용 이런 장치의 보완 문제를 해결하는 것이 주요 문제, 그 이유는 앞으로는 이런 종류의 장치가 많아질 것이라는 보고가 있기 때문 이동통신망 사용이 필요치 않는 곳에서 사용할 가능성이 있음. 동시에 사업자 입장에서 인원 단축 및 편리성을 위해 이는 우리나라보단 미국이나 동남아 처럼 국가의 크기 규모가 큰 나라에서 더 적용될 가능성이 높음 중국이 한국처럼 카드를 사용하지 않고 QR로 사용하듯 나라마다 필요한 기술이 다르다. 구성: Coordinator &gt;Router &gt; Device\nREID: 버스, 지하철, 시설 출입증 카드, 톨게이트 이것은 수동 소자이며, 전원을 공급하는 형태의 카드는 REIC이다. 구조는 태그,(데이터-에너지), 안테나, 리더, 호스트 무선 상태에서 얼마나 데이터 손실없이 잘 받아들이는 지가 주요 관점 현재의 대중화와 달리 초기엔 잘 작동하지 않았음. 효과미미\nNFC: 모바일 결정, 교통카드(FeliCa, MIFARE), 출입통제\nRFID와 NFC의 차이점 비교하기\n\n무선 LAN 에서 신뢰성 있는 통신을 위해 고려해야 할 부분이 있다면 어떤것이 있을까 ?\n저속통신 방식을 통해 향 후 실생활에서 적용이 가능한 범위와 사례는무엇이 있을까 ?"
  },
  {
    "objectID": "cs/bn_04.html",
    "href": "cs/bn_04.html",
    "title": "데이터 링크",
    "section": "",
    "text": "데이터 링크가 왜 필요한가? 1. 물리계층의 한계 신호의 시작과 끝, 신호의 왜곡, 유실\n\n노드 간 신뢰성 확보 노드 간의 충돌 최소화(CSMA/CD), 오류 제어, 백오프(충돌 시 지연) 문제를 예방해야 상위 쪽에서의 노력이 덜 들어가는 편의가 있음 이를 해결하는 방법은 매우 다양함\n\n그 중 기본적인 것을 배우고자 함 데이터 링크 계층의 5가지 핵심 역할 프레임화, 주소 지정, 오류 제어, 프름 제어 링크 제어\n프레임의 정의 문자 프레임(과거) 대 비트 프레임(현대) 예시: 이더넷 프레임 구조\n보통의 형태는 동기화 형태로 데이터를 보내며 여기서 preamble 가 사용되며 SFD가 프레임 시작을 알린다.\n과거의 통신 장비를 바로 최신 기종으로 바꾸지 않는 이유는 호환성 문제가 가장 크다. 바꿀 때에도 백워드 컴퓨터를 잘 고려해야 한다.\n이태원 지역은 하웨이 장비 계열의 통신 장비(BTS) 등을 사용하지 못한다. 미군이 살고 있어 해킹의 위험이 있기 때문.\n\n데이터 전송 오류 유형 단순 비트 오류(한 비트만 변경), 버스트 오류(연속적으로), 프레임 손실(수신측 미도착), 프레임 중복(동일 프레임 중복 수신) 오류 검출 방식 패리티 비트, 체크섬, CRC 단순 오류 검출, 데이터 단위 합산, 다항식 기반 검출 등 짝수 패리티(전체를 짝수로), 홀수 패리티(전체를 홀수로) 그러나 이 방법으로는 2개 이상의 비트 변경은 오류를 알기 어렵다. 체크섬은 더 발전된 형태, 각각을 16비트 진수 형태로 만들어 검증 그러나 내부가 완전히 깨진 형태는 검출하기 어렵다. CRC은 더 발전된 형태, 다항식 계산을 통해 거의 완전하게 검출함 그러나 이는 곧 다른 방법에 비해선 시간과 비용이 들어간다는 것 때문에 보안의 중요도에 따라서 다르게 사용할 수 있다.\n오류 정정 코드 해밍 코드, 패리티를 최소 몇 개를 붙일 것인지 공식으로 계산 FEC 실무적용사례: 이더넷 LAN, 와이파이, 5G 등 슬라이딩 윈도우 프로토콜, 윈도우 개념, 흐름제어, 부분 재전송 stop-and-Wait, timeout 시간 동안 기다리고 유실 등으로 수신층이 데이터를 보내지 않으면 다시 보냄. 다만 과거엔 개발자 위주 였으나 최근은 사용자 관점에서 편의성을 갖추어야 하므로 기다리는 방식이 오히려 불편할 수도 있음 go-back N 중간에 못 받은 게 있으면 이후 데이터는 받더라도 무시하고 다시 그 지점부터 보냄. 불필요한 로스들이 발생할 수 있다는 단점이 있음 Selective Repeat 위 단점을 해소하기 위한 해결 방안, 이후 데이터는 받으면 버퍼링을 돌리고 이전에 못 받은 걸 다시 받으면 지금까지 받을 걸 잘 정리 및 합쳐서 상위계층으로 올림 다만 상황에 따라서는 오히려 비효율적일 수도 있다. 그래도 대부분 데이터 유실은 개별 단위 보다는 전체가 유실되거나 파괴되는 경우가 더 많다.\n\n데이터 링크계층에서의 신뢰성 확보가 중요한 이유는 무엇일까 ?\n프레임 전송시 슬라이딩 원도우 방식이 나오게 된 배경과 좀 더 효율적으로 개선이 필요한부분이 있다면 어떠한 부분이 있을까 ?\n\n슬라이딩 윈도우 프로토콜 슬라이딩 윈도우 프로토콜은 송신자가 여러 프레임을 연속적으로 전송하고, 수신자가 ACK를 통해 수신 확인을 하는 방식입니다. 이 방식은 흐름 제어와 오류 제어를 동시에 수행할 수 있습니다. 이것이 언제 어디에서 처음 개발 되었는가? 이것은 현대까지 포함해서 어떻게 발전되었는지 웹에서 찾아서 자세하게 말해주세요"
  },
  {
    "objectID": "cs/bn_02.html",
    "href": "cs/bn_02.html",
    "title": "무선 통신망",
    "section": "",
    "text": "물리계층 1. 물리적 연결 보장, 2. 데이터 전송 담당\n비트에서 신호로 변환 0과1의 물리적 신호로 변환 전압, 빛, 전파의 유무\n아날로그, 정보 저장 공간 많음 복제 시 품질 이 저하, 수정 변경 어려움 시간이 지나면 품질에 영향, 선명하고 세밀한 표현\n디지털은 반대, 마그네틱이 있음\n전송매체의 종류, 유선: UTP 케이블, 동축 케이블, 광섬유 케이블, MDMI/USB, 전력선(철탑) 무선: 와이파이, 블루투스, LTE, 5G, (주파수대역이 높아질수록 기지국을 더 많이 세워야 함.)\n전송 방식의 분류 1. 직렬&병렬 2. 동기&비동기 3. 아날로그&디지털\n물리적 특정 정의 최대 케이블 길이 100m 전송 속도 1Gbps 신호전압 5V\n택배 지유로 이해하기 물리계층 = 로도, 택배상자 = 데이터\n\n신호, 파형: 주파수(시간), 파장(거리), 진폭, 위상 소리 관점으로 본 특징: 1. 고음/ 저음 Hz 2. 크고/ 작음 dB (0은 상대적 개념 0이라고 아예 안들리는 건 아님) 가청주파수 대역, 아날로그-디지털로의 변화 샘플링, 양자화/부호화, 압축 용어 정리, 감쇠, 간섭, 지터, 신호대 잡음비\n무선 통신, 주파수가 높을 떄와 낮을 때의 특징들(직진성, 투과성, 정보량, 안테나 크기, 울림) 기본성질, 굴절, 반사, 회절, 감쇠, 산란 변조, 신호를 다른 종류로 변환 1. 상대방에게 전달 용이하도록 2. 허가 받은 곳으로 전달하려고 진폭 변조 대 주파수 변조 사람의 목소리 - Carrier 교류 신호(발진기에서 발생) - AM, FM 변조 그 이외 ASK, PSK, FSK QAM의 등장 제한된 대역폭에서 전송 효율을 높이기 위함 위상과 진폭의 개수 기준에 따라 데이터를 구분\n다중 접속, 특정 주파수 대역에서 여러 명이 동시에 해당 대역을 사용하기 위함 FDMA(1세대), TDMA, CDMA - 복잡성/효율성, Generation, 주요기술, 수용가입자, Handover 으로 구분\n\n유선 통신 또는 무선 통신에서 개선이 필요한 사항\n향후 AI에 통신을 어떻게 적용할 건지"
  },
  {
    "objectID": "cs/bn_03.html",
    "href": "cs/bn_03.html",
    "title": "유선 통신망",
    "section": "",
    "text": "01 유선망 예: 이더넷·전용회선\n1 . 제목2 무선보다 지연·지터가 작고 예측 가능성이 높음 — 물리적 접속·스위치 경로가 고정적이기 때문입니다.\n광섬유는 단일 링크에서 Tbps급(실험·상용 사례: 수십~수백 Tbps 기록)으로 확장 가능 — DWDM 등 기술로 용량을 극대화합니다. (nict.go.jp)\n도청 위험은 상대적으로 낮다. 광탭은 구리보다 어렵고 탐지도 까다롭지만 완전히 불가능한 것은 아니므로 중요 트래픽은 종단암호화 등 추가 보호가 필요합니다. (VIAVI Perspectives)\n매체·용도 정리: 데스크/사무실 단거리 연결은 구리(Cat5e/6)·PoE가 일반적, 빌딩 간·IDC/DCI·백본은 주로 광섬유 사용 — 요구 대역폭·거리·보안·운영비용에 따라 선택합니다. (실무적 근거: 광섬유의 고용량·장거리 특성). (Corning)\n1 . 회선교환 통신 시작 시 전용 경로(회선)를 설정 → 대역폭 예약·순서 보장·지연 안정(예: 전통 PSTN). (위키백과)\n2 . 패킷교환 데이터를 패킷으로 분할해 라우터가 최적 경로로 전달 → 경로·지연이 패킷별로 달라질 수 있어 효율적이나 순서 뒤바뀜(재정렬)·지터가 발생할 수 있음. (Obkio)\n보완(현대적 관점): MPLS·가상회선(ATM 등)처럼 패킷망 위에서 회선형(예약/QoS) 성격을 흉내내는 기술이 널리 사용됩니다 — 따라서 “회선교환=항상 더 단순/우수”라는 이분법은 현실을 반영하지 않습니다. (Cisco)\n초기 경로 설정 필요 · 경로 고정(전용 자원 예약)\n회선교환은 통신 시작 시 종단 간 회선을 설정하고 그 회선의 자원을 전용으로 예약합니다 — 설정된 경로가 통신 기간 동안 유지됩니다. (TechTarget)\n순서적 수신(정렬 보장)\n물리적 전용 회로를 쓰므로 데이터(음성 샘플)는 순서가 보장되어 전송됩니다(정해진 지연·순서 장점). (Simon Fraser University) 경로 공유 불가(전용) — 기본적으로 맞음 회선이 설정되면 그 회선의 대역폭은 다른 통신에 재할당되지 않습니다(효율성 측면에서는 비효율). (TechTarget) 경로 상 장비 오류 시 — ’우회 불가’는 부분적 진술 단일 확립된 회로 내에서는 경로상 고장이 발생하면 그 회로 연결은 유지되지 못함(콜이 끊김). 다만 현대 전화망·통신사업자 인프라는 중복 경로·스위치 레벨의 재라우팅·트렁크 다양화로 장애를 회피하거나 콜을 다른 경로로 재설정하는 메커니즘을 갖추고 있으므로 “우회 경로 전혀 없음”은 과도한 단정입니다. (NIST CSRC) 과금 방식(시간 기반) — 역사적/일반적 사실 전통 PSTN/회선 기반 서비스는 통화 시간 단위 과금이 일반적이었으나(또는 전용회선은 월정액 등), 사업자·서비스 유형에 따라 과금 모델은 다를 수 있습니다. (위키백과) 패킷교환이 ’전부 반대’라는 표현의 문제점 패킷교환은 일반적으로 패킷별 라우팅(경로 가변), 통신 자원의 통계적 다중화(공유), 순서 뒤바뀜·지연 변동 가능성, 링크 장애 시 라우팅 프로토콜에 의한 우회 등이 특징입니다 — 그러나 패킷망에도 가상회선(VC: X.25, Frame Relay, ATM, MPLS 등) 같은 연결지향 모드가 있어 회선형 특성을 흉내낼 수 있습니다. 따라서 “전부 정반대”로 단순화하면 정확하지 않습니다. (위키백과) 실무적 함의(요점)\n음성·실시간 제어 등 지연·순서 보장이 중요한 트래픽은 전용 회선 또는 패킷망 위의 QoS/가상회선(MPLS, SR-TE 등)으로 보장한다. (위키백과) 신뢰성 설계 시에는 물리적 중복·트렁크 다양화·신호 레벨 재설정 정책을 고려해야 함(단일 링크 고장으로 서비스 전체 중단 방지). (NIST CSRC) 원하시면 위 근거(논문·교재·운영자 문서)를 근거별로 요약해 표로 정리해 드리겠습니다.\n\n통신망의 다양화와 발전에 영향을 준 부분과 그것의 산물은 어떠한 것이 있는가 ?\n향 후 미래 통신에서 더 중점적으로 고려할 부분은 어떠한 것이 있을까 ?"
  },
  {
    "objectID": "cs/bn_05.html",
    "href": "cs/bn_05.html",
    "title": "데이터링크계층의핵심구조",
    "section": "",
    "text": "LAN의 특징, (범위, 속도, 낮은 지연, 구조) IEEE 802가 LAN의 표준\nLLC, MAC 세부구조, OSI 레이어 2개로 이뤄짐. 802.3, 11, 15 은 주로 쓰는 것.\nLLC: 802.2 표준 MAC: 3, 5, 11\n오류제어 정리 MAC 오류검출만 끝남, CRC, 체크섬 하드웨어 프레임 단위, 오류 시 프레임 폐기\nLLC 복구까지, ARQ SW/논리링크 단위, 오류시 재전송 요청\n802.3 대표적 표준 기술 접근 방식(CSMA/CD), 속도 발전, 토폴로지(버스형, 스타형)\n이더넷: 현대LAN 의 표준 초기 공유 매체, 스위치 전화, 현대 기능\n\nToken-Ring 802.5 토큰 (획득, 반환, 순환) 보내는 쪽이 없앱, 받는 쪽이 다수일 수 있으므로 장점: 충돌 데이터 없음, Qos 보장 용이, 고정 대역폭 제공 단점: 장치 하나의 장애로 전체 마비, 설치 및 유지보수 복잡하여 높은 비용 그래서 현대에서는 거의 쓰지 않음 이더넷은 상대적으로 고속 전송, 저비용, 표준화, 유연한 확장성을 가졌기 때문. 프레임 분석 도구(wireshark, tcpdump, SPAN 포트 등)\nHDLC 구조 Flag, 주소, 컨트롤(I, S, U프레임), 정보, FCS, Flag 공통점: 데이터 링크 계층 사용되는 프로토콜 차이점은 WAN에서 주로 사용, 상대적으로 느림\n\n데이터 링크계층를 세부 LLC와 MAC sublayer로 구분 지은 이유는 무엇이고 각 sublayer에서의 역할에 대하여 설명해 보세요\nIEEE 802에서 MAC sublayer를 세분화하여 표준으로 선정한 이유와 대표적인 MAC 표준을 찾아보고 해당 내용에 대하여 설명해 보세요"
  },
  {
    "objectID": "cs/bn_07.html",
    "href": "cs/bn_07.html",
    "title": "네트워크계층의이해",
    "section": "",
    "text": "네트워크 계층의 핵심 기능 주소지정, 라우팅, 패킷 전달, 단편화와 재조립\n필수 설정 4가지 요소 IP 주소, Subnet Mask, Default Gateway, DNS Server\n\nIP 주소 논리적 주소의 고유 번호, 네트워크 계층(3계층 해당), 2가지 버전(IPv4, IPv6)\n\nIPv4 클레스 체계(약 42억개, E는 연구원 용도이므로 실제 사용 가능한 건 더 적음) 보통은 A~E 중 C를 사용함\nA 클레스의 경우 할당 받는 주소 만큼 실제 사용 안 함 망의 증가, 축소 시 IP주소의 재배치가 경직됨. 해결: Subnet, CIDR 도입, 결과적으로 클래스 타입 IP에서 클레스 네트워크로 발전\nIPv6), 매우 많은 수의 주소\n\nSubnet Mask IP주소의 네트워크(1), 호스트(0) 부분을 나눈다.\nDefault Gateway 내부 네트워크, 게이트 웨이(라우터도 포함하는 큰 개념), 외부 네트워크 대표 사례: 무선공유기 네트워크 입장: 다른 네트워크로 나가는 출구, 입구 데이터 입장: 통로, 호스트 기기 입장: 반드시 도달해야 하는 접속 지점\n\n역할: 프로토콜 반환, 네크워크 연결, 데이터 경로 설정, 보안 종류: 기본 게이트웨이, 인터넷 게이트 웨이, 단반향 게이트웨이\n\nDNS Server 도메인 입력, 질의, 응답, 서버 통신 구성 요소: root, TLD, Authoritative, 캐시 서버 레코드 타입: A, AAAA, MX 레코드 / CNAME\n\nVPN 가상사설망 1. 공중 퍼블릭 인터넷 / 2. 전용선 설치 원격 근무자가 회사 내부 네트워크에 접속하기 위함 가상회선, 패킷 교환망에서 논리적으로 전용 회선처럼 동작하는 경로를 미리 설정 경로대로 순서대로 전달.\n가상회선과 데이터그램의 차이., 표 작성\n\n게이트웨이, 방화벽 그리고 라우터의 용도와 차이는 무엇인가?\n가상 사설망이 사용되는 예와 장점은 무엇인가 ?"
  },
  {
    "objectID": "cs/bn_09.html",
    "href": "cs/bn_09.html",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "IP, Internet Protocol 네트워크 계층에서 논리적 주소 지정과 패킷 전달을 담당하는 핵심 프로토콜이다. IP는 비연결성(connectionless)과 비신뢰성(unreliable)을 특징으로 하며, 최선형 전달(best-effort delivery)을 제공한다.\n즉, 패킷을 가능한 한 손실 없이, 가능한 빠르게 전달하는 것이 주 목표이다.\n\n\n\n논리 주소 지정: 호스트 식별 및 라우팅 경로 선택에 사용되는 IP 주소 체계\nIP 버전:\n\n\n\n\n\n\n\n\n\n특징\nIPv4\nIPv6\n\n\n\n\n주소 길이\n32비트\n128비트\n\n\n기본 필드\n고정 20바이트 + 옵션\n고정 40바이트 + 확장 헤더\n\n\n옵션 처리\n헤더 내 포함\n확장 헤더로 분리, 간소화\n\n\n패킷 분할\n라우터 단편화 가능\n송신지 단편화만 가능, 라우터 단편화 불가\n\n\n기타\n주소 부족 문제, 옵션 포함\n128비트 주소, 확장 헤더로 유연성 확보\n\n\n\n\n\n\n\n혼합 제어(Mixed Control)\n패킷 단편화(Fragmentation) 및 재조립(Reassembly)\n터널링(Tunneling): 데이터 패킷 캡슐화 → 전송 → 목적지에서 디캡슐화 (VPN, MPLS)\nQoS 지원 및 트래픽 관리"
  },
  {
    "objectID": "cs/bn_09.html#논리-주소와-버전-관리",
    "href": "cs/bn_09.html#논리-주소와-버전-관리",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "논리 주소 지정: 호스트 식별 및 라우팅 경로 선택에 사용되는 IP 주소 체계\nIP 버전:\n\n\n\n\n\n\n\n\n\n특징\nIPv4\nIPv6\n\n\n\n\n주소 길이\n32비트\n128비트\n\n\n기본 필드\n고정 20바이트 + 옵션\n고정 40바이트 + 확장 헤더\n\n\n옵션 처리\n헤더 내 포함\n확장 헤더로 분리, 간소화\n\n\n패킷 분할\n라우터 단편화 가능\n송신지 단편화만 가능, 라우터 단편화 불가\n\n\n기타\n주소 부족 문제, 옵션 포함\n128비트 주소, 확장 헤더로 유연성 확보"
  },
  {
    "objectID": "cs/bn_09.html#ip-패킷-처리-주요-기능",
    "href": "cs/bn_09.html#ip-패킷-처리-주요-기능",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "혼합 제어(Mixed Control)\n패킷 단편화(Fragmentation) 및 재조립(Reassembly)\n터널링(Tunneling): 데이터 패킷 캡슐화 → 전송 → 목적지에서 디캡슐화 (VPN, MPLS)\nQoS 지원 및 트래픽 관리"
  },
  {
    "objectID": "cs/bn_09.html#배경-및-필요성",
    "href": "cs/bn_09.html#배경-및-필요성",
    "title": "IP 프로토콜 및 QoS",
    "section": "2.1 배경 및 필요성",
    "text": "2.1 배경 및 필요성\n\n네트워크 서비스 품질 저하 문제 심화\n인프라 확장 한계로 효율적 자원 관리 필요\n비용 절감 및 성능 보장 요구 증가\n특정 애플리케이션 기능 및 성능 보장 필요"
  },
  {
    "objectID": "cs/bn_09.html#qos-4대-요소",
    "href": "cs/bn_09.html#qos-4대-요소",
    "title": "IP 프로토콜 및 QoS",
    "section": "2.2 QoS 4대 요소",
    "text": "2.2 QoS 4대 요소\n\n대역폭(Bandwidth): 충분한 전송 용량 확보\n지연(Latency): 실시간 서비스의 낮은 지연 확보\n지터(Jitter): 패킷 간 전송 간격 일정 유지\n손실 제어(Packet Loss): 패킷 손실률 최소화"
  },
  {
    "objectID": "cs/bn_09.html#중간-지점-기반-qos-기술",
    "href": "cs/bn_09.html#중간-지점-기반-qos-기술",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.1 중간 지점 기반 QoS 기술",
    "text": "3.1 중간 지점 기반 QoS 기술\n\nQueue 관리: 패킷 순서 및 처리 우선순위 제어, 대표 5가지 기법\nTraffic Shaping: 전송률 제한 및 버스트 제어, 대표 3가지 기법\n사전 패킷 폐기: 혼잡 상황 시 우선순위 낮은 패킷 폐기, 대표 3가지 기법\nQoS 보장 기술: 자원 예약 및 클래스별 관리, 대표 2가지 기법"
  },
  {
    "objectID": "cs/bn_09.html#홉-제어-개념",
    "href": "cs/bn_09.html#홉-제어-개념",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.2 홉-제어 개념",
    "text": "3.2 홉-제어 개념\nHop-by-Hop Congestion Control * 각 라우터(홉)에서 혼잡을 감지하고 대응하는 기술 * 목적: 혼잡 완화 → 지연 감소, 패킷 손실 최소화, 공정한 대역폭 사용\n즉, 패킷이 목적지까지 가는 동안 각 홉에서 처리 방식을 결정"
  },
  {
    "objectID": "cs/bn_09.html#포함되는-요소와-대응-기법",
    "href": "cs/bn_09.html#포함되는-요소와-대응-기법",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.2 포함되는 요소와 대응 기법",
    "text": "3.2 포함되는 요소와 대응 기법\n\n\n\n\n\n\n\n\n요소\n대응 기법\n설명\n\n\n\n\n대역폭\nQueuing, Shaping, Policing\n각 홉에서 전송 우선순위, 송신률 제한, 초과 트래픽 차단\n\n\n패킷 손실\nQueuing, Policing, Early Drop\n혼잡 시 패킷 폐기, 우선순위 기반 손실 최소화\n\n\n지연 및 지터\nQueuing, Shaping\n패킷 순서·처리 지연 관리, 트래픽 평탄화로 지터 감소"
  },
  {
    "objectID": "cs/bn_09.html#임의-우선순위-예시",
    "href": "cs/bn_09.html#임의-우선순위-예시",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.3 임의 우선순위 예시",
    "text": "3.3 임의 우선순위 예시\n\n이메일 &lt; 일반 애플리케이션 &lt; 대용량 데이터/영상 &lt; 미션 크리티컬 &lt; 음성"
  },
  {
    "objectID": "cs/bn_11.html",
    "href": "cs/bn_11.html",
    "title": "전송계층",
    "section": "",
    "text": "01 Transport Layer\n전송계층은 신뢰성 있는 데이터 전송과 효율적인 네트워크 통신을 담당하는 계층으로, 다음 네 가지 핵심 역할을 수행한다:\n\n세그먼트화(Segmentation)\n전송 제어(Transmission Control)\n흐름 제어(Flow Control)\n다중화(Multiplexing)\n\n\n1. 세그먼트화(Segmentation)\n세그먼트화는 상위 계층에서 전송되는 데이터를 작은 단위로 나누고, 수신 측에서 이를 재조립하는 과정이다.\n\n송신 측: 큰 데이터 스트림을 효율적 전송을 위해 작은 세그먼트로 분할\n수신 측: 세그먼트를 원래 데이터 스트림으로 재조립하여 상위 계층에 전달\n\n목적: 전송 효율성 증가, 오류 복구 용이, 신뢰성 있는 데이터 전달 보장\n\n\n2. 전송 제어(Transmission Control)\n전송 제어는 송신자와 수신자 간의 신뢰성 있는 데이터 전송을 보장한다.\n\n오류 제어(Error Control): 전송 중 발생한 데이터 손상이나 손실을 검출하고, 재전송 요청을 통해 데이터 무결성을 확보\n혼잡 제어(Congestion Control): 네트워크 과부하 시 전송 속도를 조절하여 혼잡을 방지하고 성능 저하 최소화\n\n\n\n3. 흐름 제어(Flow Control)\n흐름 제어는 데이터 전송 속도를 조절하여 수신 측 버퍼 오버플로를 방지하고 네트워크 혼잡을 줄인다.\n\nStop-and-Wait 방식: 송신 측에서 한 패킷 전송 후 수신 측 확인 응답을 기다린 후 다음 패킷 전송\n슬라이딩 윈도우(Sliding Window) 방식: 여러 패킷을 연속 전송하고 수신 측의 확인 응답을 통해 전송을 관리\n\n\n\n4. 다중화(Multiplexing)\n다중화는 여러 애플리케이션 계층 데이터를 하나의 전송 채널로 통합하는 과정이다. 각 세그먼트에는 식별용 포트 번호를 포함하여 수신 측에서 올바른 애플리케이션으로 전달된다.\n\n포트(Port): IP 주소 내 논리적 통신 지점, 애플리케이션 식별 및 데이터 분배 역할\n소켓(Socket): 실제 데이터 송수신 통로 제공. 서버와 클라이언트 간 세션 연결 관리\n\n\n\n5. 전송 계층 프로토콜\n\n5.1 TCP(Transmission Control Protocol)\n\n연결 지향형 프로토콜\n신뢰성 있는 데이터 전달 및 순서 유지, 흐름 제어 및 혼잡 제어 지원\n3-way handshake를 통한 연결 설정\n사용 사례: 웹, 이메일, 파일 전송 등\n헤더 구조: 복잡하지만 신뢰성 확보에 필수적\n\n\n\n5.2 UDP(User Datagram Protocol)\n\n비연결형 프로토콜\n빠른 전송 속도, 최소 헤더 구조, 신뢰성 보장 없음\n사용 사례: 스트리밍, 실시간 게임\n헤더 구조: 송수신 포트, 길이, 체크섬 등 최소 정보 포함\n\n\n\n\n6. TCP 연결 설정 과정\nTCP 연결은 3-way handshake를 통해 이루어진다:\n\nSYN: 클라이언트 → 서버, 연결 요청\nSYN-ACK: 서버 → 클라이언트, 연결 수락 및 응답\nACK: 클라이언트 → 서버, 연결 확정\n\n각 단계에서 시퀀스 번호(Seq)와 확인 응답 번호(Ack)를 사용하여 신뢰성을 유지하며, 데이터 흐름과 재전송 관리를 지원한다."
  },
  {
    "objectID": "cs/bn_13.html",
    "href": "cs/bn_13.html",
    "title": "제목",
    "section": "",
    "text": "네트워크 보안의 핵심은 세 가지 요소로 정의된다: 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)\n\n\n\n정의: 민감 정보가 권한 없는 주체에게 노출되지 않도록 보호\n기술적 수단: 데이터 암호화, 사용자 인증, 접근 제어, 방화벽\n목표: 데이터 전송 및 저장 과정에서 정보 유출 방지\n\n\n\n\n\n정의: 데이터가 전송 또는 저장 중 변경되거나 훼손되지 않음을 보장\n기술적 수단: 접근 제어, 암호화, 로그 기록 및 감시, 해시 함수 활용\n목표: 위변조 방지 및 신뢰성 확보\n\n\n\n\n\n정의: 사용자가 필요할 때 언제든지 네트워크와 서비스를 이용할 수 있도록 보장\n기술적 수단: 백업 시스템, 네트워크 이중화, DDoS 방어, 실시간 시스템 모니터링, 보안 패치 및 취약점 관리\n목표: 서비스 연속성과 장애 회복력 강화"
  },
  {
    "objectID": "cs/bn_13.html#네트워크-보안network-security-심화-분석",
    "href": "cs/bn_13.html#네트워크-보안network-security-심화-분석",
    "title": "제목",
    "section": "",
    "text": "네트워크 보안의 핵심은 세 가지 요소로 정의된다: 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)\n\n\n\n정의: 민감 정보가 권한 없는 주체에게 노출되지 않도록 보호\n기술적 수단: 데이터 암호화, 사용자 인증, 접근 제어, 방화벽\n목표: 데이터 전송 및 저장 과정에서 정보 유출 방지\n\n\n\n\n\n정의: 데이터가 전송 또는 저장 중 변경되거나 훼손되지 않음을 보장\n기술적 수단: 접근 제어, 암호화, 로그 기록 및 감시, 해시 함수 활용\n목표: 위변조 방지 및 신뢰성 확보\n\n\n\n\n\n정의: 사용자가 필요할 때 언제든지 네트워크와 서비스를 이용할 수 있도록 보장\n기술적 수단: 백업 시스템, 네트워크 이중화, DDoS 방어, 실시간 시스템 모니터링, 보안 패치 및 취약점 관리\n목표: 서비스 연속성과 장애 회복력 강화"
  },
  {
    "objectID": "cs/bn_13.html#주요-사이버-위협",
    "href": "cs/bn_13.html#주요-사이버-위협",
    "title": "제목",
    "section": "주요 사이버 위협",
    "text": "주요 사이버 위협\n\n1. 피싱(Phishing)\n\n원인 분석: 국내 금융 및 카드 서비스 발달로 인해 공격 타겟 관계망이 촘촘함\n유형: 이메일 피싱, 스피어 피싱, 스미싱, 파밍\n대응 전략: URL 검증, 2단계 인증, 출처 확인, 보안 솔루션 활용\n제한 사항: 파밍 공격은 개인 차원에서 예방이 어려움\n\n\n\n2. DDoS 공격(Distributed Denial of Service)\n\n특징: 볼륨 기반, 프로토콜 기반, 애플리케이션 계층 공격을 통해 서비스 마비\n대응: 트래픽 분석, 방화벽 규칙, CDN 및 부하 분산 활용\n\n\n\n3. 악성 소프트웨어(Malware)\n\n종류: 바이러스, 웜, 트로이 목마, 랜섬웨어, 스파이웨어\n대응: 실시간 탐지, 정기적 업데이트, 침입 방지 시스템 적용"
  },
  {
    "objectID": "cs/bn_13.html#암호화-기술",
    "href": "cs/bn_13.html#암호화-기술",
    "title": "제목",
    "section": "암호화 기술",
    "text": "암호화 기술\n\n대칭키 암호화(Symmetric Encryption): 동일 키로 암호화·복호화\n비대칭키 암호화(Asymmetric Encryption): 공개키-개인키 구조 사용\n기타 암호화: 치환 암호, 코드북 암호 등\n응용: VPN, SSL/TLS, 데이터 전송 보호, 클라우드 암호화"
  },
  {
    "objectID": "cs/bn_13.html#네트워크-보안-기술",
    "href": "cs/bn_13.html#네트워크-보안-기술",
    "title": "제목",
    "section": "네트워크 보안 기술",
    "text": "네트워크 보안 기술\n\n기술적 보안: 방화벽, WAF(Web Application Firewall), 제로 트러스트(Zero Trust), AI 기반 위협 분석\n관리적 보안: 보안 교육, 정책 수립\n실시간 모니터링: 네트워크 트래픽 분석, 침입 탐지 및 대응"
  },
  {
    "objectID": "cs/bn_13.html#사이버-보안-관리-체계cso",
    "href": "cs/bn_13.html#사이버-보안-관리-체계cso",
    "title": "제목",
    "section": "사이버 보안 관리 체계(CSO)",
    "text": "사이버 보안 관리 체계(CSO)\n\n범위: 네트워크, 클라우드, IoT, 데이터, 애플리케이션, 원격 접속 엔드포인트\n운영 요소: 인력, 프로세스, 기술\n기술 적용 사례: 제로 트러스트 아키텍처, 동작 기반 이상 탐지, 침입 방지 시스템, 클라우드 데이터 암호화"
  },
  {
    "objectID": "da/tm/tm_02_0.html",
    "href": "da/tm/tm_02_0.html",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "",
    "text": "웹 크롤링 개념 및 정적 크롤링 실습에 대해 다루고자 한다."
  },
  {
    "objectID": "da/tm/tm_02_0.html#정형-데이터",
    "href": "da/tm/tm_02_0.html#정형-데이터",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "1 . 정형 데이터",
    "text": "1 . 정형 데이터\n(Structured Data)\n일정한 형식을 갖춘 데이터로, 데이터베이스의 테이블처럼 행과 열로 정리된다.\n예: 엑셀, SQL 데이터베이스, 고객 정보(이름, 나이, 주소 등).\n룰세팅(Rule Setting) 데이터를 저장할 때 고정된 형식(테이블, 행/열 구조, 스키마 등)을 미리 정의하는 것.\n2 . 비정형 데이터 (Unstructured Data)\n형식이 일정하지 않아 체계적으로 저장하기 어려운 데이터.\n예: 텍스트(SNS 게시글, 이메일), 이미지, 동영상, 음성 데이터.\n주로 자연어 처리(NLP)나 텍스트 마이닝 등의 기법을 활용해 분석.\n3 . 반정형 데이터 (Semi-Structured Data)\n일정한 구조를 가지지만 완전히 정형화되지 않은 데이터. 태그나 특정한 형식(XML, JSON 등)을 포함하여 구조화가 가능하다.\n예: HTML, JSON, XML 파일, 로그 데이터.\n02 크롤링\n1 . 정적 크롤링 웹 페이지의 HTML 소스 코드를 직접 가져와서 필요한 데이터를 추출하는 방식.\n기본적으로, requests 라이브러리로 웹 페이지 HTML을 가져와 BeautifulSoup으로 데이터 추출한다.\n페이지 로딩 속도가 빠르고, 서버 부하가 적으나 JavaScript로 생성되는 데이터는 가져올 수 없다.\nHTML만으로 필요한 정보를 얻을 수 있다면 → 정적 크롤링이 유리하다.\n2 . 동적 크롤링 웹 브라우저를 실제로 실행하여 JavaScript로 로드되는 데이터까지 가져오는 방식.\n기본적으로, Selenium이나 Playwright 같은 브라우저 자동화 도구 사용한다.\nJavaScript 렌더링된 데이터를 포함하여 크롤링 가능하나, 속도가 느리고, 서버 부하가 높다.\nJavaScript로 데이터가 동적으로 로딩된다면 → 동적 크롤링이 필요하다.\n03 라이브러리 Jupyter Notebook에서 실행하는 명령어는 기본적으로 일반적인 Python 실행 환경에서도 동일하게 사용할 수 있다.\n(예: 터미널, 명령 프롬프트, 다른 IDE)\n1 . requests 파이썬에서 HTTP 요청을 보내고 응답을 받을 수 있는 라이브러리로, 주로 웹에서 데이터를 가져오거나 서버에 데이터를 전송하는 데 사용된다.\n웹사이트와 데이터를 주고받는 과정에서 사용되는 HTTP 프로토콜을 쉽게 다룰 수 있도록 도와준다.\n① GET 요청 - requests.get() 웹 페이지의 정보를 가져올 때 사용된다. 이는 브라우저에서 주소를 입력하고 페이지를 여는 것과 같은 동작이다.\n② POST 요청 - requests.post() 서버에 데이터를 전송할 때 사용된다. 회원가입, 로그인, 데이터 저장 등의 작업에서 활용된다.\n③ JSON 응답 처리 - response.json() 서버에서 JSON 형식의 데이터를 받으면, .json() 메서드를 사용하여 딕셔너리로 변환할 수 있다.\n2 . BeautifulSoup4 HTML/XML 문서를 파싱하여 원하는 데이터를 추출하는 라이브러리로, 문서를 구성하는 요소를 개별적인 구조(태그, 속성, 텍스트 등)로 나눈다.\n먼저, HTML 문서를 파싱하여 태그 간의 계층을 이해할 수 있는 트리 구조로 변환한다.\n이를 통해 특정 태그나 클래스에 접근할 수 있으며, CSS 선택자를 활용하여 원하는 요소를 쉽게 선택할 수 있다.\n또한, get_text() 메서드를 사용하면 태그 내부의 텍스트만 추출할 수 있어 데이터 정제 작업이 용이하다.\n3 . selenium 웹 브라우저를 자동으로 제어하는 라이브러리로, 클릭, 입력, 스크롤 등의 동작을 수행할 수 있다.\nJavaScript로 동적으로 변경되는 웹 페이지의 데이터도 가져올 수 있어 정적인 크롤링 방식보다 더 유연하다.\n이를 사용하려면 Chrome, Firefox 등 웹 브라우저에 맞는 드라이버가 필요하며, 이를 통해 실제 브라우저를 실행하고 조작할 수 있다.\n과거에는 웹 브라우저와 드라이버의 버전이 맞아야 했지만, 현재는 자동 업데이트 기능 덕분에 큰 문제가 없다.\n4 . pandas 데이터 분석 및 처리를 위한 필수 라이브러리로, CSV, Excel, JSON 등의 다양한 형식의 데이터를 데이터프레임으로 불러와 조작할 수 있다.\n또한, 결측값을 처리하거나 특정 조건에 따라 데이터를 필터링하고 정렬하는 등 정리 작업이 가능하다.\n뿐만 아니라, 데이터를 그룹화하여 분석할 수 있는 groupby() 기능, 기초 통계를 확인할 수 있는 describe() 메서드,\n특정 연산을 적용할 수 있는 apply() 메서드 등을 제공하여, 보다 효과적인 데이터 분석을 지원한다.\n04 정적 크롤링 다음은 네이버 뉴스 기사에 대해 정적 크롤링을 수행하는 코드이다.\n\n설치된 라이브러리를 불러오는 과정."
  },
  {
    "objectID": "da/tm/tm_03_1.html",
    "href": "da/tm/tm_03_1.html",
    "title": "3장: 네이버 카페 크롤링",
    "section": "",
    "text": "네이버 카페 크롤랑에 대해 다루고자 한다.\n합칠려면 모든 변수가 동일하게 들어가야 한다.\nfrom selenium import webdriver # 브라우저 자동화 from bs4 import BeautifulSoup as BS # html 내용 파싱 from selenium.webdriver.common.by import By # 다양한 방법으로 엘리먼트를 찾기 from selenium.webdriver.common.keys import Keys # Keys 클래스 가져오기(키보드 입력 제어)\nimport pandas as pd # 데이터 조작 및 분석 import datetime # 날짜와 시간 연산 import requests # Http 요청을 보내기 import pickle # 파이썬 객체 직렬화 import time # 코드 실행 속도 조절 import re # 정규 표현식 사용\n\n\n\ndriver = webdriver.Chrome() driver.get(‘https://search.naver.com/search.naver?ssc=tab.cafe.all&sm=tab_jum&query=%EB%85%B8%EC%9D%B8+%EB%B6%80%EC%96%91&nso=so%3Ar%2Cp%3Afrom20240301to20240325’)\n\n\n\ntitle_list = [] url_list = []\n\n검색 결과에서 모든 제목 링크 요소 가져오기 (스크롤 다운 포함)\nfor _ in range(2): # 5번 스크롤 내리기 (필요에 따라 조절 가능) driver.execute_script(“window.scrollTo(0, document.body.scrollHeight);”) # 스크롤 맨 아래로 이동 time.sleep(1) # 데이터 로딩을 기다리기 위해 1초 대기\ntitles = driver.find_elements(By.XPATH, “//*[@id='main_pack']/section/div[1]/ul/li/div/div[2]/div[2]/a”)\nfor i, title_element in enumerate(titles, start=1): # 1부터 카운트 시작 try: title_list.append(title_element.text) # 제목 추가 url_list.append(title_element.get_attribute(“href”)) # URL 추가\nexcept Exception as e:\n    print(f\"오류 발생: {e}\")  # 오류 메시지 출력\n\nif i % 10 == 0:  # 진행 상황 출력 (10개 단위)\n    print(f\"진행 중: {i}개 완료\")\nprint(“데이터 수집 완료!”) # 최종 완료 메시지 출력\n\n\n\n\n\n본문, 좋아요 수, 댓글 수, 댓글, 이미지 수, 영상 수를 저장할 리스트 초기화\nnew_doc = []\nlike_cnt = []\ncomment_cnt = []\ncomment_list = []\nimg_cnt = []\ndiv_cnt = []\n\n\n카페 글 크롤링\nfor i in range(len(url_list)): url_path = url_list[i] # URL 불러오기 driver.switch_to.window(driver.window_handles[0]) # 첫 번째 탭으로 이동 driver.execute_script(f”window.open(‘{url_path}’)“) # 새 탭에서 URL 실행 driver.switch_to.window(driver.window_handles[1]) # 두 번째 탭으로 이동\ntime.sleep(2)  # 2초 대기\n\ntry:\n    iframes = driver.find_elements(By.TAG_NAME, 'iframe')  # 카페 iframe 찾기\n    \n    if len(iframes) &gt; 0:\n        # iframe 전환\n        driver.switch_to.frame('cafe_main') # ifame의 첫부분\n        html = driver.page_source           # html 가져오고\n        soup = BS(html, 'html.parser')      # html 파싱하라\n\n        # 본문 추출\n        try:\n            a = soup.find('div', class_='article_viewer').get_text() # 값을 가져와라\n        except:\n            # 본문을 찾지 못할 경우\n            a = 'null'\n\n        # 좋아요 수 추출\n        try:\n            b = soup.find('em', class_='u_cnt _count').get_text()\n        except:\n            b = 'null'\n\n        # 댓글 수 추출\n        try:\n            c = soup.find('strong', class_='num').get_text()\n        except:\n            c = 'null'\n\n        # 댓글 추출\n        try:\n            d = \"\\n\".join([t.get_text() for t in soup.find_all('span', class_='text_comment')])\n        except:\n            d = 'null'\n\n        # 이미지 수 추출\n        e = len(soup.find_all('img', class_='se-image-resource'))\n\n        # 영상 수 추출\n        f = len(soup.find_all('div', class_='pzp-ui-dimmed pzp-dimmed pzp-pc_dimmed'))\n\n        # iframe에서 기본 컨텐츠로 전환\n        driver.switch_to.default_content()\n    else:\n        a, b, c, d, e, f = 'null', 'null', 'null', 'null', 0, 0  # iframe이 없을 경우 기본값\n\n    # 데이터 저장\n    new_doc.append(a)\n    like_cnt.append(b)\n    comment_cnt.append(c)\n    comment_list.append(d)\n    img_cnt.append(e)\n    div_cnt.append(f)\n\nexcept Exception as e:\n    # 오류 발생 시 기본값 저장\n    new_doc.append('null')\n    like_cnt.append('null')\n    comment_cnt.append('null')\n    comment_list.append('null')\n    img_cnt.append(0)\n    div_cnt.append(0)\n    print(f\"Error occurred at index {i}\")\n\nfinally:\n    # 현재 열린 탭 닫기\n    driver.close()\n    time.sleep(0.3)  # 0.3초 대기\n    driver.switch_to.window(driver.window_handles[0])  # 첫 번째 탭으로 복귀\n\n# 매 10번째 URL마다 진행 상황 출력\nif (i + 1) % 10 == 0:\n    print(f\"진행 상황: {i + 1}/{len(url_list)}\")\n\n\n\n\n\n크롤링 데이터를 데이터프레임으로 변환\nraw_data = pd.DataFrame() # 초기화 raw_data[‘title’] = title_list # 제목 리스트 raw_data[‘doc’] = new_doc # 본문 리스트 raw_data[‘like’] = like_cnt # 좋아요 수 리스트 raw_data[‘comment_cnt’] = comment_cnt # 댓글 수 리스트 raw_data[‘comment_list’] = comment_list # 댓글 리스트 raw_data[‘img’] = img_cnt # 이미지 수 리스트 raw_data[‘div’] = div_cnt # 영상 수 리스트 raw_data[‘ch’] = ‘naver’ # 채널 정보 raw_data[‘ch2’] = ‘cafe’ # 채널 정보 (세부)\n\n\n데이터프레임을 pickle 파일로 저장\nfile_path = “C:/Users/jkl12/텍스트마이닝/” # 슬래시 사용 with open(file_path + “노인부양cafe.pkl”, “wb”) as f: pickle.dump(raw_data, f)\n\n\n크롬 드라이버 종료\ndriver.quit()\n\n\n저장된 pickle 파일을 불러오기\nwith open(file_path + “노인부양cafe.pkl”, “rb”) as f: temp_file = pickle.load(f)\n\n\n데이터프레임을 CSV 파일로 저장\ntemp_file.to_csv(file_path + “노인부양cafe.csv”, index=False, encoding=“utf-8-sig”)"
  },
  {
    "objectID": "da/tm/tm_05_0.html",
    "href": "da/tm/tm_05_0.html",
    "title": "5장: 텍스트 데이터 마이닝",
    "section": "",
    "text": "텍스트 데이터 마이닝에 대해 다루고자 한다.\n\n01 텍스트 데이터 마이닝\n광도들이 보석을 캐는 과정.\n노인 부양에 관한 가설 세우기.\n텍스트 데이터 전처리\n텍스트 마이닝의 핵심적인 시작 단계로, 데이터의 품질을 높이기 위한 여러 과정으로 구성된다.\n먼저, 데이터 수집 후에는 한글화, 결측치 처리, 단어 및 형태소 분석 등의 전처리를 진행합니다.\n한글화는 텍스트에서 한글 이외의 문자를 제거하거나 블랭크 처리하여 분석에 적합한 형태로 만드는 과정입니다.\n이때 특수기호는 유지하며 한글만 남기는 방식으로 필터링한다.\n이렇게 정제된 데이터는 피클(pickle) 파일 형태로 저장하며, 작업 시에는 파일 경로와 파일명을 명확히 지정해야 한다.\n예를 들어, 보험연수원에서 제공한 연금 관련 텍스트 데이터를 수년간 6개 채널에서 크롤링해 5개의 피클 파일로 저장한 사례가 있다.\n이 파일들은 병합한 후 인덱스를 지정해 다시 저장하며, 저장 경로는 작업 환경에 맞춰 지정해야 한다.\nimport pickle import pandas as pd import itertools import os import re\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n이후 분석을 위해 저장된 피클 파일을 다시 로드하여 활용한다.\nf = open(file_path + ‘total_doc.pkl’, “rb”) # 데이터 불러오기 data = pickle.load(f) f.close()\ndata # 문서 전체의 명사 리스트 확보\n\n단어들의 빈도 데이터 정제 과정에서는 불필요한 기호나 단어를 제거하고, 결측값은 일괄 삭제하며 인덱스를 재정비합니다. 예컨대 ‘샵’, ’펀드’와 같은 특정 요소는 정제 대상이 되며, 본문 일부 삭제 시 데이터의 일관성을 유지하기 위해 인덱스를 재조정합니다. 정제는 원본을 복사한 후 진행하는 것이 안전합니다.\n\n형태소 분석은 한글 데이터 분석에 필수적인 과정이며, 이는 텍스트를 의미 단위로 나누어주는 작업입니다. 형태소 분석을 위해서는 Java 설치와 버전 확인, 인터넷 환경 설정이 필요하며, 대표적으로 사용하는 라이브러리는 코모란(Komoran)입니다. 코모란은 GitHub에서 설치 가능하며, 설치 후 환경 변수 설정 및 보안 설정 등을 완료한 후 사용합니다.\n형태소 분석을 통해 본문에서 추출된 단어들은 토큰화 과정을 거쳐 리스트 형태로 정리됩니다. 이때 불용어(의미 없는 단어)를 제거하기 위해 스탑워드 리스트를 활용하며, 불용어와 일치하는 형태소는 제외합니다. 최종적으로 정제된 단어 리스트와 형태소 리스트는 데이터프레임 형태로 저장하고, 이를 다시 파일로 변환하여 보관합니다.\nimport itertools\n\n\n제목 리스트 언패킹\ntitle_noun = list(itertools.chain(*data[‘title_token_noun’])) print(title_noun[:15]) # 앞에서 5개 요소 출력\n\n\n본문 리스트 언패킹\ndoc_noun = list(itertools.chain(*data[‘doc_token_noun’])) print(doc_noun[:15])\n\n\n댓글 리스트 언패킹\ncomment_noun = list(itertools.chain(*data[‘comment_token_noun’])) print(comment_noun[:15])\n\n제목, 본문, 댓글 데이터 빈도\n\n\n\n빈도를 카운트하는 라이브러리\nfrom collections import Counter\ntitle_count = Counter(title_noun) # 리스트 원소의 개수가 계산됨 title_top = dict(title_count.most_common(100)) # 상위 100개 출력하기 title_top\n#—\ndoc_count = Counter(doc_noun) # 리스트 원소의 개수가 계산됨 doc_top = dict(doc_count.most_common(100)) # 상위 100개 출력하기 doc_top\n\n\n—\ncomment_count = Counter(comment_noun) # 리스트 원소의 개수가 계산됨 comment_top = dict(comment_count.most_common(100)) # 상위 100개 출력하기 comment_top\n\nimport csv\n\n\n\n제목별 빈도수 저장\nwith open(file_path + ‘\\title_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in title_top.items(): w.writerow([k, v]) # k, v -&gt; 딕셔너리의 key, value # 즉, 단어와 빈도\n\n\n본문별 빈도수 저장\nwith open(file_path + ‘\\doc_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in doc_top.items(): w.writerow([k, v])\n\n\n댓글별 빈도수 저장\nwith open(file_path + ‘\\comment_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in comment_top.items(): w.writerow([k, v])\n4 . 워드 클라우드 정제된 단어들을 기반으로 워드 클라우드를 그릴 수 있다.\n워드 클라우드는 단어의 빈도를 시각적으로 표현하는 기법으로, 가장 자주 등장한 단어를 강조하는 방식으로 표현된다.\n이때 Counter의 most_common 함수를 이용해 빈도를 계산하고, 원하는 형태의 마스크 이미지(예: 사람 모양, 네모형 등)를 적용해 시각화할 수 있다.\n백그라운드 설정, 폰트 다운로드, 컬러 맵 지정 등 세부 설정도 가능하며, 정보 전달력이 높은 네모 형태를 권장한다.\nimport matplotlib.pyplot as plt from wordcloud import WordCloud\nfont_path = r”C:.otf”\n\n\n워드클라우드 생성\nwordcloud = WordCloud( font_path=font_path, background_color=‘white’, colormap=“Accent”, width=600, height=400 ).generate_from_frequencies(doc_top)\nplt.figure(figsize=(8, 10)) plt.imshow(wordcloud) plt.axis(‘off’) plt.show()\nTF - IDF 결과 텀은 단어, 단어들의 빈도, 워드클라우드도 사용 이것이 결합된 형태가 TF - IDF임\n단어 뿐만아니라 문서 전체도 고려 한다.\n\n워드 클라우드의 문제점, 본문에서 자주 등장하는 것이 높은 가중치를 문맥에 따라서는 다른 의미(완전히 다른 단어)를 가질 수 있음 또한, 그 단어는 낮은 가중치일 수도 있음\n\n즉, 데이터가 진짜 말하고자 하는 것을 찾는 인사이트에서는 부적합할 수 있음.\n단순한 빈도만으로는 판별하기 애매하다.\n\n문서의 길이의 따라 사용 어휘의 중요도가 바뀔 수 있다.\n\n특정 단어가 포함된 문서가 몇 개가 되느냐? d=문서, t=단어, 특정 단어가 나타나는 문서수의 역수(역수의 로그를 취해준 개념)\n각 문서에 포함된 단어 카운트 - DTM 행렬\n이는 특정 문서에서만 많이 나오지만 전체 문서에서는 적은 단어와 전체적으로 많이 나오게 분포하지만 개별 문서에서는 적게 나오는 단어 2가지가 존재하고 그 중 후자가 더 중요한 가중치를 가진다.\n로그를 취해서 소수점으로 나오고 TF-IDF를 곱한다. 이떄 여기저기 많이 나오면 상대적인 가중치가 비슷하고 낮게 나옴\n먼저, 단어들을 문자열로 만들어 주어야 한다. 명사들의 문자열 리스트 만들고, sklearn 가져오기.\n\n\n명사들의 문자열 구성\ndoc_noun = [] for i in range(0, len(data[“doc_token_noun”])): doc_noun.append(’ ’.join(data[‘doc_token_noun’][i])) # 각 문서의 명사들을 str로 연결\n너무 희박한 것들은 제외할 수도 있다.(최소치, 최대치)\n\n\n텍스트 문서 모음을 단어 tf-idf 행렬로 변환\nfrom sklearn.feature_extraction.text import TfidfVectorizer vec_y = TfidfVectorizer(min_df=0.01, max_df=0.95)\n\n\n문서의 1% ~ 95%로 나타나는 단어들을 고려\nY = vec_y.fit_transform(doc_noun) print(Y)\n10번째 문서 21번째 단어이다. +1\nk개의 평균을 갖는다는 것. 비지도, 타겟X 타겟이 있어, 예측을 시도하는 지도학습과는 달리 어떤 패턴을 가진 그룹이 있는지를 보려는 것.\n구조화, 군집 분석을 시도하는 것.\n임의의 k개의 중심점을 지정, 각각의 개별 데이터를 가장 가까운 곳으로 할당시킴 이 거리를 유클리드의 거리를 한다.\n그 그룹이 생성되면 그 그룹 안에서 새로운 중심점을 찾음 그 중심점을 가지고 위의 일련의 과정을 더 이상 중심점이 움직이지 않을 때까지 반복한다.\n합리적인 k를 찾는 방법 - 대표적으로 엘보우 기법 팔굽치 처럼 꺾이는 지점을 k값으로 정하는 것.\n2개에서 6개 정도가 타당하다 너무 적거나 많으면 의미가 없음.\n거리에 대한 SSE 손실함수 구하는 과정 10번 반복\nimport os os.environ[“OMP_NUM_THREADS”] = “2” # 선택 사항\nimport matplotlib.pyplot as plt from sklearn.cluster import KMeans\ndef elbow(X): sse = []\nfor i in range(1, 10):\n    km = KMeans(n_clusters=i, n_init=10, \n                algorithm='lloyd', random_state=0)\n    km.fit(X)\n    sse.append(km.inertia_)\n    print(i)\n\nplt.plot(range(1, 10), sse, marker='o')\nplt.xlabel('K')\nplt.ylabel('SSE')\nplt.xticks(range(1, 10))\nplt.show()\nelbow(X)\nconda install -c conda-forge pyldavis\nmodel_y = KMeans(n_clusters=2, algorithm=‘lloyd’, random_state=0) # 모델 정의 model_y.fit(Y) # 모델 학습\nprint(“Doc Top terms for each cluster”) order_centroids = model_y.cluster_centers_.argsort()[:, ::-1] # 클러스터 중심 정렬 terms_y = vec_y.get_feature_names_out() # 단어 목록\nfor i in range(2): # 두 개의 클러스터에 대해 반복 print(“Cluster %d:” % i) for ind in order_centroids[i, :50]: # 각 클러스터의 상위 50개 단어 출력 print(‘%s’ % terms_y[ind]) print(‘’)\n데이터 프레임의 형식\nimport pandas as pd\n\n\n클러스터 중심에서 가장 중요한 단어 인덱스 정렬\norder_centroids = model_y.cluster_centers_.argsort()[:, ::-1] terms_y = vec_y.get_feature_names_out()\n\n\n각 클러스터의 상위 50개 단어 수집\ntop_terms = {}\nfor i in range(2): # 클러스터 수만큼 반복 top_terms[f’Cluster {i}’] = [terms_y[ind] for ind in order_centroids[i, :50]]\n\n\nDataFrame으로 변환\ndf_top_terms = pd.DataFrame(top_terms) df_top_terms\n1 . 제목2 더 나아가 워드 클러스터링과 토픽 모델링을 통해 텍스트의 의미 구조를 분석할 수 있다.\n워드 클러스터링은 문서 내 단어 빈도를 기반으로 단어들을 군집화하는 방법으로, TF (Term Frequency) 및 IDF (Inverse Document Frequency) 값을 활용해 중요 단어를 판단합니다.\n이후 유클리디안 거리 기반의 K-means와 같은 알고리즘으로 최적의 군집을 형성합니다.\n토픽 모델링은 문서 집합에서 주제를 추출하는 기법으로, LDA(Latent Dirichlet Allocation) 같은 확률 기반 모델을 활용합니다.\n혼잡도 그래프와 일관성 지표 등을 통해 토픽 수를 결정하고, 각 토픽의 특징을 평가합니다. 이를 통해 시스템화된 분석 체계를 구축할 수 있습니다."
  },
  {
    "objectID": "da/tm/tm_06_1.html",
    "href": "da/tm/tm_06_1.html",
    "title": "6장: 감성 분석",
    "section": "",
    "text": "감성분석에 대해 다루고자 한다.\n01 감성 사전 다운\n1 . KNU\nGitHub - park1200656/KnuSentiLex: KNU(케이앤유) 한국어 감성사전\nKNU(케이앤유) 한국어 감성사전. Contribute to park1200656/KnuSentiLex development by creating an account on GitHub.\ngithub.com import json\nfile_path = r’C:-master-master_info.json’\nwith open(file_path, ‘r’, encoding=‘utf-8’) as f: json_data = json.load(f)\n\n단어 리스트 생성\nword_list = [] for item in json_data: # 리스트 반복 word_txt = item[‘word’] word_list.append(word_txt)\n\n\n결과 출력 (앞 10개만 보기)\nprint(word_list[:10])\n2 . KoreanSentimentAnalyzer\nGitHub - mrlee23/KoreanSentimentAnalyzer: 한국어 감성 분석기\n한국어 감성 분석기. Contribute to mrlee23/KoreanSentimentAnalyzer development by creating an account on GitHub.\ngithub.com import pandas as pd\n\n\n파일 경로\nfile_path = r’C:-master-master.csv’\n\n\nCSV 파일 읽기\nsenti_df = pd.read_csv(file_path, encoding=‘utf-8’) # 또는 encoding=‘cp949’ senti_df.head()\nimport pickle\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n\n\ndoc_topic과 comment_topic이 포함된 파일\nf = open(file_path + ‘topic_doc.pkl’, “rb”) # 데이터 불러오기 data = pickle.load(f) f.close()\ndata # 문서 전체의 명사 리스트 확보\nKNU 감성사전을 이용해서 텍스트 데이터에 감정 점수를 부여하는 Python 스크립트입니다. 각 텍스트가 긍정적인지, 부정적인지, 중립적인지를 파악하기 위해 사용됩니다.\nimport json import pandas as pd from tqdm import tqdm\n\n\n감정분석 JSON 데이터 (KNU 감성사전) 불러오기\nfile_path = r’C:-master-master’\nwith open(file_path + r’_info.json’, encoding=‘UTF-8’) as json_file: sentiword = json.load(json_file)\n\n\n감성 단어 리스트 및 점수 초기화\ns_word = [] values = [] score = []\n\n\n평균 계산 함수\ndef average(lst): return sum(lst) / len(lst)\n텍스트에서 감성 단어 찾고 점수 계산\n\n\n감성 점수 계산\nfor word in tqdm(data[‘doc’]): temp_s_word = [] # 본문에서 가져옴 temp_value = []\nfor s in sentiword:\n    if s['word'] in word:\n        temp_s_word.append(s['word'])\n        temp_value.append(int(s['polarity']))\n\ns_word.append(temp_s_word)\nvalues.append(temp_value)\n\nif len(temp_value) &gt; 0:\n    score.append(average(temp_value))\nelse:\n    score.append(0)\n\n\n결과 삽입\ndata = data.assign(sentiword=s_word, values=values, score=score) data\n가상 공간 안에서만 있는 것, 이를 저장 함.\nimport pickle import pandas as pd\n\n\n저장\nfile_path = r’C:\\’ with open(file_path + “total_docs_KNU.pkl”, “wb”) as f: pickle.dump(data, f)\n\n\n불러오기\nwith open(file_path + “total_docs_KNU.pkl”, “rb”) as f: ff = pickle.load(f)\n\n\n데이터프레임 복원\ntotal_docs = pd.DataFrame() total_docs[‘doc’] = ff[‘doc’] total_docs[‘doc_token_noun’] = ff[‘doc_token_noun’] total_docs[‘doc_topic’] = ff[‘doc_topic’] total_docs[‘comment_topic’] = ff[‘comment_topic’] total_docs[‘sentiword’] = ff[‘sentiword’] total_docs[‘values’] = ff[‘values’] total_docs[‘score’] = ff[‘score’]\ntotal_docs\ndoc_token_noun의 모든 단어가 감성 단어가 아니다. 그들 중 감성 단어를 sentiword로 불러온 것.\nfrom wordcloud import WordCloud import matplotlib.pyplot as plt\n\n\n폰트 경로 (Windows용 예시 - 나눔고딕)\nfont_path = r”C:.otf”\n\n\n토픽 개수만큼 반복\nnum_topics = total_docs[‘doc_topic’].nunique()\nfor topic_num in range(num_topics): # 해당 토픽의 문서 필터링 topic_docs = total_docs[total_docs[‘doc_topic’] == topic_num]\n# 토큰 리스트를 하나로 합치기 (flatten)\nall_tokens = sum(topic_docs['doc_token_noun'], [])\n\n# 문자열로 변환 (공백으로 연결)\ntext = ' '.join(all_tokens)\n\n# 워드클라우드 생성\nwordcloud = WordCloud(font_path=font_path, background_color='white', width=800, height=400).generate(text)\n\n# 시각화\nplt.figure(figsize=(10, 8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(f\"Topic {topic_num} Word Cloud\", fontsize=16)\n이 코드는 감성 점수(score)를 기준으로 각 토픽(doc_topic)에 대해 감정 분포를 분류하고 있습니다.\nif score &gt; 0.3: # 긍정 elif -0.3 &lt;= score &lt;= 0.3: # 중립 else: # 부정 여기서 “0.3”과 “-0.3”이라는 기준은 사용자가 임의로 정한 값 (threshold)입니다. → 즉, 이 기준이 정해진 절대값이 아니라, → 분석 목적에 따라 조정해야 하는 값이에요.\n🧠 사용자가 결정해야 할 것들 score의 값 범위가 어떻게 구성되어 있는가? 감성 점수가 -2 ~ 2인지, -1 ~ 1인지, -5 ~ 5인지 먼저 확인해야 합니다. 0.3이 의미 있는 경계값인가? 감성 점수의 분포가 대부분 -0.1 ~ 0.1이라면, 0.3은 너무 높은 기준일 수 있습니다. 반대로 감성 점수 범위가 크다면, 0.3은 너무 낮은 기준일 수 있죠. 목표에 따라 기준이 달라질 수 있음 예를 들어: 마케팅 분석이라면 조금만 긍정적이어도 긍정으로 간주 감정 민감도 분석이라면 더 엄격한 기준 적용 필요\n코드에서 senti_0 = [0, 0, 0, 0, 0, 0] 의미 이 리스트는 특정 토픽(topic 0, topic 1 등)에 대한 감성 점수 분포를 저장하려고 만든 것입니다. 즉, 각 인덱스가 감정의 강도나 구간을 나타냅니다.\n0 매우 긍정 1 긍정 2 중립 3 부정 4 매우 부정 5 기타 / 미정 (혹은 사용자 정의) 그런데 본문 코드에서는 실제로는 0, 2, 4만 사용되고 있죠? 이건 중간 단계일 가능성도 있고, 나중에 더 구체적인 등급으로 확장하기 위해 미리 6칸 확보해둔 것일 수도 있어요.\n🧠 핵심 포인트 senti_0, senti_1은 토픽별 감정 분포를 담기 위한 그릇입니다. 토픽이 더 늘어나면? → senti_2, senti_3, … 식으로 계속 만들어야겠죠. 또는 다음처럼 딕셔너리 구조로 더 깔끔하게 관리할 수도 있어요: senti_dict = {i: [0, 0, 0, 0, 0, 0] for i in total_docs[‘doc_topic’].unique()} ✅ 요약 항목 설명\nsenti_0, senti_1 토픽별 감정 분포 저장용 리스트 길이 6 감정 강도 6단계로 분류하려는 구조 (0~5) 사용자 정의 가능 목적에 따라 개수, 의미를 직접 설정 필요하시면 이 구조를 자동화해서 n개의 토픽에 대해 감정 분포를 계산하는 코드도 만들어드릴 수 있어요. 원하시나요?\n긍정 / 중립 / 부정의 빈도와 비율을 토픽별로 확인하려는 거군요.\n각 토픽(topic)마다 긍정: score &gt; 0.3 중립: -0.3 ≤ score ≤ 0.3 부정: score &lt; -0.3 해당 범주의 빈도수와 비율(%)을 구하기\n감정 빈도 doc_topic negative neutral positive\n0 12 56 32 1 8 30 62\n감정 비율 (%) doc_topic negative neutral positive\n0 12.0 56.0 32.0 1 8.0 30.0 62.0\n\n\n토픽별 감성 점수 분류 리스트 초기화 (긍정, 중립, 부정)\nsenti_0 = [0, 0, 0, 0, 0, 0] senti_1 = [0, 0, 0, 0, 0, 0] senti_2 = [0, 0, 0, 0, 0, 0] senti_3 = [0, 0, 0, 0, 0, 0]\nfor i in range(len(total_docs)): topic = total_docs[‘doc_topic’].iloc[i] score = total_docs[‘score’].iloc[i]\nif topic == 0:\n    if score &gt; 0.3:\n        senti_0[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_0[2] += 1\n    else:\n        senti_0[4] += 1\n\nelif topic == 1:\n    if score &gt; 0.3:\n        senti_1[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_1[2] += 1\n    else:\n        senti_1[4] += 1\nfor i in range(len(total_docs)): topic = total_docs[‘comment_topic’].iloc[i] score = total_docs[‘score’].iloc[i]\nif topic == 0:\n    if score &gt; 0.3:\n        senti_2[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_2[2] += 1\n    else:\n        senti_2[4] += 1\n\nelif topic == 1:\n    if score &gt; 0.3:\n        senti_3[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_3[2] += 1\n    else:\n        senti_3[4] += 1\n지금 작성하신 코드는 토픽별 감정 분포 리스트에서 비율(%)을 1칸씩 띄워서 저장하는 방식입니다.\n📦 구조 요약 senti_0 = [긍정_빈도, 긍정_비율, 중립_빈도, 중립_비율, 부정_빈도, 부정_비율]\n인덱스 0, 2, 4: 빈도수 (count) 인덱스 1, 3, 5: 비율 (ratio, 혹은 percentage) 🔁 반복문 설명 for i in range(1, 7, 2): # i는 1, 3, 5 i-1 → 현재 비율을 계산할 빈도 인덱스 i → 비율을 저장할 인덱스 분모는 전체 감정의 합: 긍정 + 중립 + 부정 즉, 예를 들어:\nsenti_0[1] = senti_0[0] / (senti_0[0] + senti_0[2] + senti_0[4]) 이건 긍정 비율, 그다음 senti_0[3]은 중립 비율, senti_0[5]는 부정 비율이 되는 식입니다.\n\n\n감성 클래스별 비율 계산 (분모가 0일 경우 예외 처리 추가)\nfor i in range(1, 7, 2): if (senti_0[0] + senti_0[2] + senti_0[4]) != 0: senti_0[i] = senti_0[i-1] / (senti_0[0] + senti_0[2] + senti_0[4]) else: senti_0[i] = 0 # 분모가 0이면 비율을 0으로 설정\nif (senti_1[0] + senti_1[2] + senti_1[4]) != 0:\n    senti_1[i] = senti_1[i-1] / (senti_1[0] + senti_1[2] + senti_1[4])\nelse:\n    senti_1[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\nif (senti_2[0] + senti_2[2] + senti_2[4]) != 0:\n    senti_2[i] = senti_2[i-1] / (senti_2[0] + senti_2[2] + senti_2[4])\nelse:\n    senti_2[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\nif (senti_3[0] + senti_3[2] + senti_3[4]) != 0:\n    senti_3[i] = senti_3[i-1] / (senti_3[0] + senti_3[2] + senti_3[4])\nelse:\n    senti_3[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\n\n토픽별 감성 비율 데이터프레임 생성\ngraph = pd.DataFrame( [senti_0, senti_1, senti_2, senti_3], index=[‘topic1’, ‘topic2’, ‘topic3’, ‘topic4’], columns=[[‘긍정’, ‘긍정’, ‘중립’, ‘중립’, ‘부정’, ‘부정’], [‘빈도’, ‘비율’, ‘빈도’, ‘비율’, ‘빈도’, ‘비율’]] )\ngraph\n🔍 왜 워드클라우드와 감정 점수(혹은 분류) 결과가 다를 수 있는가? 1. 워드클라우드는 감성 단어 필터 없이 모든 단어 사용 일반적으로 워드클라우드는 특정 토픽에서 자주 등장한 단어의 빈도만을 시각화합니다. 이 과정에서 감성 사전에 없는 중립 단어, 불용어(의미 없는 단어)들도 포함될 수 있습니다. 따라서 시각적으로 중요한 단어처럼 보여도 감성 점수 계산에서는 무시될 수 있습니다. 2. KNU 감성사전 기반 감정 점수는 ’등록된 감성 단어’만 사용 예: 좋다, 싫다, 기쁘다, 화나다 등만 감성 점수로 환산됨. 감성 사전에 없는 단어는 아무리 많이 나와도 score에 기여하지 않음. 3. 토픽의 특성과 감성 단어 간 연관성 결여 예를 들어, 주제는 부정적인 사건이라도 직접적으로 부정 단어(예: “나쁘다”, “불편하다”)가 없을 수 있음. 이 경우 토픽 자체는 부정적으로 보이지만, 감성 점수는 중립 혹은 긍정이 나올 수 있습니다. 4. 토픽 내 감성 단어 비율이 낮은 경우 감성 점수를 계산할 때 사용하는 감성 단어 수가 전체 단어에 비해 매우 적다면, score의 분포도 좁거나 왜곡될 수 있습니다. 이로 인해 score는 0 근처로 몰리거나, 예외적으로 높은 감성 단어 하나에 과도하게 영향받을 수 있습니다. ✅ 요약 요소특징감성 점수에 반영됨? 워드클라우드 주요 단어 빈도가 높은 모든 단어 ❌ 감성 단어만 반영됨 감정 점수(score) 감성사전에 있는 단어 기반 ✅ 해당 단어만 반영됨 감정 판단 정확도 단어 수, 감성 단어 존재 여부에 민감 상황에 따라 다름\n💡 개선 팁 워드클라우드 만들 때 감성 단어만 필터링해서 시각화할 수도 있습니다. python 복사편집 sentiment_words = [s[‘word’] for s in sentiword] topic_words = [word for word in topic_docs if word in sentiment_words] 감성 점수 외에도 TF-IDF 기반 상위 감성 단어 추출도 좋은 방법입니다. 감성 점수 분포와 함께 워드클라우드 결과를 비교 분석하면 더 풍부한 인사이트를 얻을 수 있습니다.\n01 감성 사전 다운\n1 . NRC\nNRC Emotion Lexicon\nImpact Some notable ways in which the NRC Emotion Lexicon has made impact include: First of its kind: It was the first word-emotion association lexicon, with entries for eight basic emotions as well as positive and negative sentiment. It still remains the\nsaifmohammad.com\n이 코드는 NRC 감성사전 (Korean NRC Emotion Lexicon)을 기반으로 각 문서의 감정값을 계산하는 과정입니다. 간단히 말하면, 문서에 등장하는 감성 단어를 찾아서 해당 감정 점수를 누적하는 구조입니다.\n아래에 코드의 의미를 단계별로 설명드리겠습니다:\n🔢 코드 설명 for i in range(1, len(nrc)): # NRC 감성사전의 각 단어에 대해 반복 nrc: NRC 감성사전을 담은 DataFrame입니다. nrc[‘Korean Word’]: 감성사전에 있는 한국어 단어. range(1, len(nrc)): 아마 첫 번째 행(헤더 또는 불필요한 데이터)을 생략하고자 1부터 시작한 것 같습니다. if nrc[‘Korean Word’][i] in word: 현재 문서(word)에 감성사전의 단어가 포함되어 있는지 확인. if len(nrc[‘Korean Word’][i]) &gt; 1: 글자 수가 1자인 경우(ex. “다”, “게”)는 보통 의미가 불분명하거나 너무 일반적이라 제외. 따라서 두 글자 이상인 감성 단어만 사용. temp_s_word.append(nrc[‘Korean Word’][i]) 해당 감성 단어를 temp_s_word 리스트에 저장 (이 문서에서 발견된 감성 단어 목록). b = list(map(int, nrc.iloc[i, 1:11].tolist())) nrc.iloc[i, 1:11]: 해당 단어에 대한 감정 점수들 (예: 긍정, 부정, 분노, 기쁨 등 10가지 감정). map(int, …): 감정 점수들이 문자열로 되어 있다면 정수로 변환. 결과적으로 b는 해당 단어의 10개 감정 점수 리스트. temp_value = [x + y for x, y in zip(temp_value, b)] temp_value: 현재 문서에서 감정 점수를 누적하는 리스트. b를 더해가며 문서 전체의 감정 점수를 계산. 🔍 이 코드의 목적 NRC 감성사전 기반 다중 감정 분석입니다. 단순히 긍·부정 점수만 계산하는 것이 아니라, 여러 감정 카테고리(기쁨, 슬픔, 분노 등)의 누적 점수를 구해서 문서의 감정 프로파일을 생성합니다."
  }
]