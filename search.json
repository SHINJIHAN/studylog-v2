[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StudyLog",
    "section": "",
    "text": "Home\n\n\nProfile\n\n\n\n\n\n\nStudyLog는 학습 과정을 규칙적으로 기록하고 체계화한다.\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\nAI·ML·DL\n\n\n\n\n\n\n\n\n실습·과제\n\n\n\n\n\n\n\n\nCG·HW\n\n\n\n\n\n\n\n\n보안·네트워크\n\n\n\n\n\n\n\n\n랩실 활동\n\n\n\n\n\n\n\n\n기타\n\n\n\n\n\n\n\n\n\n\n프로필 탭 내용\n\n\n\n\n\n이 웹사이트는 Quarto를 이용해 제작되었습니다."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "제목",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "da/ida_02.html",
    "href": "da/ida_02.html",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "",
    "text": "자료를 효과적으로 요약하고 이해하기 위해 표나 그림을 사용하는 방법에 대해 다루고자 한다. 자료 요약은 분석 대상인 자료의 형태와 특성에 따라 다양한 방법으로 이루어진다."
  },
  {
    "objectID": "da/ida_02.html#자-료-의-입-력",
    "href": "da/ida_02.html#자-료-의-입-력",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "da/ida_02.html#도수분포표",
    "href": "da/ida_02.html#도수분포표",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table\n특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1"
  },
  {
    "objectID": "da/ida_02.html#막대-그래프",
    "href": "da/ida_02.html#막대-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart\n데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18284\\2514237286.py:19: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`"
  },
  {
    "objectID": "da/ida_02.html#원형-그래프",
    "href": "da/ida_02.html#원형-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart\n전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "da/ida_02.html#파레토그림",
    "href": "da/ida_02.html#파레토그림",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart\n막대그래프와 누적선그래프를 결합한 형태의 그래프.\n파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "da/ida_02.html#도수다각형",
    "href": "da/ida_02.html#도수다각형",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency Polygon\n도수분포표의 도수를 선으로 연결한 그래프.\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data-Analysis",
    "section": "",
    "text": "두 모집단에 대한 비교\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n빅데이터 개요\n\n\n\n개론\n\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\n2026 이기적 빅데이터분석기사 필기 기본서\n\n\n\n\n\n\n\n\n\n\n\n\n주성분 분석(PCA)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 2, 2025\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 18, 2025\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n6장: 감성 분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n6장: 감성 분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n5장: 텍스트 데이터 마이닝\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n3장: 네이버 카페 크롤링\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n4장: 크롤링 데이터 전처리\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n3장: 네이버 블로그 크롤링\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n2장: 네이버 뉴스 기사 제목 크롤링\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\n파이썬기반 SNS텍스트 데이터마이닝 개정판\n\n\n\n\n\n\n\n\n\n\n\n\n상관분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n카이제곱 통계\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n연관성의 측도\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n오즈비\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n제 9장 회귀분석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n다중 회귀\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n분산분석: 일원분류\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n분산분석: 일원분류 실습\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n분산분석: 이원분류 실습\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n제 10장 공분산\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n제 11장 비모수\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n파이썬을 활용한 데이터 분석과 응용\n\n\n\n\n\n\n\n\n\n\n\n\n12장: 두 모집단의 비교\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n11장: 정규모집단에서의 추론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n10장: 통계적 추론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n9장: 표집분포\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n8장: 정규분포\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n7장: 이항분포와 그에 관련된 분포들\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n6장: 확률분포\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n5장: 확률\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n4장: 두 변수 자료의 요약\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n3장: 연속형 자료\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n2장: 자료의 요약\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 9, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n13장:\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n14장:\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\n\n\n\n\n\n\n\n15장:\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "da/ida_12.html",
    "href": "da/ida_12.html",
    "title": "12장: 두 모집단의 비교",
    "section": "",
    "text": "두 모집단의 비교를 위한 추론과정은 자료를 어떻게 수집하느냐에 따라 추론 방법이 달라진다. 대표적인 두 종류의 자료수집과정에 따른 추론방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida_12.html#통계용어",
    "href": "da/ida_12.html#통계용어",
    "title": "12장: 두 모집단의 비교",
    "section": "2. 통계용어",
    "text": "2. 통계용어\n비교 연구 시 자주 사용되는 통계용어.\n실험단위 ( Experimental Unit ) : 실험의 대상. 반응값 ( Response ) : 실험 후 얻어지는 수치. 처리 ( Treatment ) : 비교하고자 하는 특성.\n예제12를 위 통계용어로 다음과 같이 설명할 수 있다:\n숲 지역 ⇨ 처리 1 , 도시 지역 ⇨ 처리 2 각각의 스캐너 측정값 ⇨ 실험단위\n그 측정값의 수치 ⇨ 반응값"
  },
  {
    "objectID": "da/ida_12.html#두-개-의-독-립-표-본",
    "href": "da/ida_12.html#두-개-의-독-립-표-본",
    "title": "12장: 두 모집단의 비교",
    "section": "3. 두 개 의 독 립 표 본",
    "text": "3. 두 개 의 독 립 표 본\n독립인 두 개의 표본으로부터 두 모집단, 혹은 두 가지의 처리효과를 비교하는 통계추론의 방법. 다음은 두 모집단으로부터 추출된 표본과 그로부터 계산되는 통계량을 정리한 것이다: 두 모집단으로부터 추출된 표본.\n위 표본으로부터 계산되는 통계량. 여기서 우리의 관심사는 두 모집단의 평균 반응값의 차이다.\n\n3-1. 모평균의 차에 대한 추론\n두 모평균의 차 ( μ 1 – μ 2 ) 에 대한 추론을 위해서는 두 표본평균의 차 ( x̄ – ȳ )를 이용한다. 두 표본의 크기 n 1, n 2 가 모두 큰 경우 ( 30 이상 )중심극한정리에 의해 두 표본평균은 근사적으로 정규분포를 따른다:\n평균이 μ​ 1 ± μ 2​ 이고, 분산이 σ 12 ​+ σ 22 ​인 정규분포를 따른다:\n이때, X 와 Y 는 서로 독립이다. 복호동순 : 식에서 부호를 2 개 이상 사용할 때, 부호를 앞에서부터 같은 순서로 적용하는 것. 따라서, 두 독립적인 정규분포 변수의 차를 표준화하면 표준정규분포를 따르게 된다: 위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다: 추정량 ± (z값) × (추정된 표준오차)\n두 표본의 크기 n 1, n 2 가 모두 30 이상일 때, 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n델타( Δ , δ: 그리스 알파벳의 네번째 글자)\n검정통계량은 H 0 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-2. 모평균의 차에 대한 추론\n표본의 크기가 작을 경우, 두 모집단에 대하여 정규분포와 표준편차에 대한 가정이 필요하다.\n두 모집단이 모두 정규분포를 따른다. 두 모집단의 표준편차가 일치한다.\n값이 ½보다 작거나 2보다 큰 경우, 위 가정이 적절하지 못한다고 판단한다. 대부분의 경우, σ 를 모르므로 이를 추정하여야 한다.\nσ 에 대한 정보는 편차제곱합에 모두 들어 있다. 따라서 이 두 제곱합을 더하여 각각의 자유도의 합으로 나누어 σ 2 추정량으로 사용하게 된다.\n이를 공통분산 σ 2 의 합동추정량 ( Pooled Variance ) 이라 한다:\n위 식을 이용하여, ( μ 1 – μ 2 ) 에 대한 표준화된 확률변수는 다음과 같다:\n자유도가 ( n ₁ + n ₂ – 2 )인 t 분포를 따른다.\n위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다:\n( μ ₁ – μ ₂ ) 에 대한 신뢰구간은 추정량 ± ( t 값 ) × ( 추정된 표준오차 )의 형식에 의해 정리된다.\n두 모집단이 모두 정규분포를 따르고 두 모표준편차가 같은 때 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-3. 모평균의 차에 대한 추론\n표본의 크기가 작고, 두 모집단의 표준편차가 일치하지 않을 경우, 근사적으로 t 분포를 따르며, 자유도는 ( n 1 – 1 )과 ( n 2 – 1 ) 중 작은 값이다.\n이 분포를 이용한 ( μ 1 – μ 2 ) 에 대한 추론방법은 다음과 같다:\n모평균 차에 대한 100 ( 1 − α ) % 신뢰구간은 근사적으로 위 식과 같다.\n신뢰구간의 경우, 그 구간이 넓어지는 경향이 있다. 따라서 실제 신뢰도는 100 ( 1 − α ) % 이상이 된다.\n가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n검정의 경우, 기각역이 좁아지는 경향이 있다. 따라서 실제 유의수준이 α 이하가 되므로 귀무가설을 기각하지 못할 가능성이 높아진다."
  },
  {
    "objectID": "da/ida_12.html#짝비교",
    "href": "da/ida_12.html#짝비교",
    "title": "12장: 두 모집단의 비교",
    "section": "4. 짝비교",
    "text": "4. 짝비교\nMatched Pair Comparisons 실험 단위들이 비슷해야 한다는 점과 다양한 실험 단위들을 비교해야 한다는 점을 절충하는 접근법.\n짝지워서 각각의 쌍으로 만드는 방법:\n같은 쌍의 실험단위들은 서로 비슷해야 한다. 각 쌍 내에서 두 조건이 다르게 설정되어야 한다.\n짝 비교를 시행할 때의 자료의 형태:\n차의 표본평균과 분산은 다음과 같다:\nX i 와 Y i 는 서로 독립이 아니다. 서로 높은 상관관계를 가질 경우, 짝비교의 효과는 크다. 즉, 전체적인 변화의 폭 ( 변동성 ) 을 줄여 처리효과를 알아내기 수월한 짝비교를 할 수 있다.\n모평균 δ 에 대한 100 ( 1 − α ) % 신뢰구간은 다음과 같다:\n귀무가설 H 0 : δ = δ 0 에 대한 검정통계량은 다음과 같다:\nH ₀ 가 맞을 때 자유도가 ( n – 1 )인 t 분포를 따른다.\n자료가 짝지워져 있는 경우,두 개의 처리를 어떻게 배정할 것인가가 전혀 문제되지 않는다. 자료가 짝지워져 있지 않은 경우, 여러 조건들이 확률적으로 같은 정도로 영향이 미치도록 해야 한다. (어느 한쪽의 처리에만 영향을 주지 않아야 한다.)이와 같이 무작위로 배정하는 것을 랜덤화 ( Randomization ) 라고 한다."
  },
  {
    "objectID": "da/ida_12.html#두-모비율의-차에-대한-추론",
    "href": "da/ida_12.html#두-모비율의-차에-대한-추론",
    "title": "12장: 두 모집단의 비교",
    "section": "5. 두 모비율의 차에 대한 추론",
    "text": "5. 두 모비율의 차에 대한 추론\n두 모집단의 비율을 비교하는 추론하는 방법. 관심의 대상이 되는 어떤 특성의 모집단 1 의 비율을 p 1, 모집단 2 의 비율을 p 2 라고 할 때 두 모집단으로부터 크기가 n 1, n 2 인 표본을 추출하여 각각 특성이 A 인 것과 A 가 아닌 것으로 분류하였다고 가정한다.\n이때 A 를 ’ 성공 ‘, A 가 아닌 것을’ 실패 ’ 라고 하고 두 표본의 성공의 개수를 각각 X, Y 표현한다.\n두 모집단의 특성 A 의 비율을 각각 p 1, p 2 라고 하면 그 추정량은 각 표본으로부터 표본의 비율을 사용하게 된다:\n두 모비율의 차 ( p ₁ – p ₂ ) 의 추정량은 ( p̂ ₁ – p̂ ₂ ) 이 된다. ( p 1 – p 2 ) 에 대한 추론을 하기 위해서는 ( p̂ 1 – p̂ 2 ) 의 분포를 알아야 한다.\n표본의 크기 n 1, n 2 가 큰 경우, 아래 식이 근사적으로 성립한다:\n아래 식도 표본이 서로 독립이므로 정규분포로 근사된다:\n따라서 이를 표준화하면:\n두 확률변수 간의 차이를 표준화하는 것이다.\n위 식을 이용하여 ( p 1 – p 2 ) 에 대한 ( 1 − α ) % 신뢰구간은 다음과 같다:\n( 추정량 ) ± ( z 값 ) × ( 표준오차 )의 형식에 의해 정리된다.\n신뢰구간을 계산할 때, ( 제곱근 속의 ) 실제 모평균 차이 p 1, p 2 는 미지수이므로, 이를 표본 비율의 차이 p̂ 1, p̂ 2 로 대체하여 계산한다.\n\n5-1. 두 모비율의 검정\n표본의 크기가 클 때 두 모비율이 같은지를 검정하는 방법은 두 비율의 차이에 대한 검정을 통해 이루어진다.\n귀무가설 H 0 : p = p 0 을 검정하기 위해 (​ p̂ 1 – p̂ 2 ) 을 이용하게 된다. 이 가설이 맞을 경우의 통계량 분포는 다음과 같다:\np 는 모비율이 같은 두 모집단의 공통비율이다.\n통합된 두 표본으로부터 이를 추정하면:\n위 과정을 바탕으로 검정통계량을 정리할 수 있다:\np̂ 는 귀무가설하에서의 공통비율 p 의 추정량이고, 검정통계량은 H 0 가 맞을 때 근사적으로 N ( 0, 1 ) 을 따른다.\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 12의 ( 1 ) 추정량의 표준오차 ( se ) 를 계산하기 위해 두 변수의 요약통계량 ( s 12, s 22 ) 을 구하는 파이썬 함수를 사용할 수 있다.\n\nvar1 = np.var(x, ddof=1);print(var1) # x의 표본 분산 (자유도=1)\nvar2 = np.var(y, ddof=1);print(var2) # y의 표본 분산 (자유도=1)\n\nn1 = len(x);print(n1) # x의 데이터 수\nn2 = len(y);print(n2) # y의 데이터 수\n\nse = math.sqrt(var1 / n1 + var2 / n2);print(se) # 표준오차\n\n48.06374040272343\n24.789102564102567\n118\n40\n1.0134334699544658\n\n\n표준오차 계산 ( 1.0134 )\n예제 12의 ( 2 ) 모평균의 차 μ 1 – μ 2 에 대한 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 z–값\nz_alpha = stats.norm.ppf(1 - 0.05 / 2);print(z_alpha)\ninterval_z = z_alpha * se; print(interval_z) # 신뢰구간 범위\n\n# x와 y의 평균을 계산\nxbar1 = np.mean(x); print(xbar1) \nxbar2 = np.mean(y); print(xbar2) \n\n# 두 평균의 차이를 계산\ndiff = xbar1 - xbar2; print(diff) \n\n# 신뢰구간\nCI_1 = [diff - interval_z, diff + interval_z]; print(CI_1)\n\n\n## 해석: 95%의 신뢰구간이 0을 포함하지 않으므로 (양측검정에서) \n## 유의수준 5%에서 두 수치가 같다는 귀무가설 (H₀ : μ1 = μ2) 을 기각할 수 있다.\n\n1.959963984540054\n1.986293101838208\n92.9322033898305\n82.075\n10.857203389830502\n[np.float64(8.870910287992295), np.float64(12.84349649166871)]\n\n\n스캐너 자료 μ ₁ – μ ₂ 의 신뢰구간은 10.857 ± 1.986 이다.\n예제 12의 ( 3 ) 두 모집단이 모두 정규분포를 따르고, 분산이 같다는 가정 하에서 합동분산추정량 및 추정량의 표준오차를 구하라.\n\n# 합쳐진 분산 계산\nspooled = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2); print(spooled) \n\n# 합쳐진 표준 오차 계산\nse_spooled = math.sqrt(spooled) * math.sqrt(1 / n1 + 1 / n2); print(se_spooled)\n\n42.24508094306821\n1.1891745810061622\n\n\n합동추정량 ( 42.245 ) / 표준오차 ( 1.189 )\n예제 12의 ( 4 ) 위 결과를 바탕으로 t – 분포의 백분위수 함수를 이용하여 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 t–값\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n1 + n2 - 2); print(t_alpha) \n\n# 신뢰 구간의 범위\ninterval_t = t_alpha * se_spooled; print(interval_t) \n\n# 신뢰구간\nCI_2 = [diff - interval_t, diff + interval_t]; print(CI_2)\n\n## 해석: (2)번의 정규분포를 이용한 신뢰구간보다 \n## 오차범위 값인 interval_t가 더 크므로 신뢰구간이 더 넓어졌음을 알 수 있다.\n## 이는 t–분포의 백분위수 값이 (정규분포의 백분위수 값보다) 더 크기 때문이다.\n\n1.9752875076954723\n2.3489616943304696\n[np.float64(8.508241695500033), np.float64(13.206165084160972)]\n\n\n신뢰구간은 10.857 ± 2.348 이다."
  },
  {
    "objectID": "da/ida_10.html",
    "href": "da/ida_10.html",
    "title": "10장: 통계적 추론",
    "section": "",
    "text": "추출된 표본으로부터 모집단의 일반적인 특성을 추론해내는 것을 통계적 추론이라고 하며, 이는 표본의 크기가 클 때 더 정확하게 성립한다."
  },
  {
    "objectID": "da/ida_10.html#자료의-입력",
    "href": "da/ida_10.html#자료의-입력",
    "title": "10장: 통계적 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 13: 예제 1 에 주어진 자료에 근거해서 \n# 중학교 1학년 남학생의 평균키에 대한 95% 신뢰구간을 구하라. (p.311)\n\nimport numpy as np \n\nheight = np.array([163, 161, 168 , 161, 157, 162, 153, 159, 164, 170, \n                   152, 160, 157, 168, 150, 165, 156, 151, 162, 150, \n                   156, 152, 161, 165, 168, 167, 165, 168, 159, 156])\n                   \n\n#_____________________________________________________________________________\n\n# 예제 14: 예제 8의 검정을 시행하고 그 결과를 비교하여라. (p.313)\n\n# 예제 8: 앞의 예제 1에 주어진 중학생의 키 자료로부터 \n# 그 도시의 중학교 1학년 남학생의 평균키(m)가 다른 도시의 중학교 1학년 \n# 남학생의 평균키인 159 cm와 차이가 있다고 할 수 있는지 판단하라. (p.297)\n\n\n#_____________________________________________________________________________\n\n# 예제 15: 3 장의 예제 11 에 주어진 자료로부터 \n# 평균교통소음정도(μ)에 대한 98% 신뢰구간을 구하고, \n\n# 평균교통소음정도가 60 을 초과한다고 주장할 수 있는지 \n# 유의수준 5% 에서 가설검정을 실시하라. (p.314)\n\nnoise = np.array([55.9, 63.8, 57.2, 59.8, 65.7, 62.7, 60.8, 51.3, 61.8, 56.0, \n                  66.9, 56.8, 66.2, 64.6, 59.5, 63.1, 60.6, 62.0, 59.4, 67.2,\n                  63.6, 60.5, 66.8, 61.8, 64.8, 55.8, 55.7, 77.1, 62.1, 61.0, \n                  58.9, 60.0, 66.9, 61.7, 60.3, 51.5, 67.0, 60.2, 56.2, 59.4, \n                  67.9, 64.9, 55.7, 61.4, 62.6, 56.4, 56.4, 69.4, 57.6, 63.8])"
  },
  {
    "objectID": "da/ida_10.html#통계적-추론",
    "href": "da/ida_10.html#통계적-추론",
    "title": "10장: 통계적 추론",
    "section": "2. 통계적 추론",
    "text": "2. 통계적 추론\nStatistical Inference\n표본이 갖고 있는 정보를 분석하여 모수에 관한 결론을 유도하고, 모수에 대한 가설의 옳고 그름을 판단하는 것을 말한다.\n모집단의 일부인 표본으로부터 전체 모집단의 성질을 추론해내는 것이므로 100% 확실하다고 할 수는 없다. 따라서 통계적인 추론을 할 때에는 그 결론의 부정확한 정도를 반드시 언급하여야 하는데 이러한 정도를 수치로 표시할 수 있게 하는 도구로 앞에서 공부한 확률론과 표준분포 등이 이용된다.\n통계적 추론에는 두 가지 주요 방법이 있다:\n모수의 추정 모수에 대한 가설 검증"
  },
  {
    "objectID": "da/ida_10.html#모평균의-추정",
    "href": "da/ida_10.html#모평균의-추정",
    "title": "10장: 통계적 추론",
    "section": "3. 모평균의 추정",
    "text": "3. 모평균의 추정\nEstimation of Parameters\n모수 중 하나로 포함되는, 모집단의 평균에 대한 점추정과 구간추정을 다루고자 한다.\n\n3-1. 점추정\nPoint Estimation\n추정하고자 하는 하나의 모수에 대해, 여러 개의 확률변수를 사용하여 하나의 통계량을 만들고, 주어진 표본으로부터 그 값을 계산하여 하나의 수치를 제시하는 과정.\n모수 ( Parameter ) : 모집단의 실제 값으로, 우리가 알고 싶어하는 대상. 추정량 ( Estimator ) : 모수를 측정하기 위해 만들어진 통계량. 추정치 ( Estimate ) : 주어진 관측값으로부터 계산된 추정량의 실제 값.\n추정량은 하나의 확률변수이므로, 추출된 표본의 따라 그 값이 달라진다. 수치들의 변화의 정도는 추정량의 정확도와 관계가 있다. 이 정확도를 측정하는 도구 중 하나가 표준오차 ( 추정량의 표준편차 ) 이다.\n\n\n3-2. 표준오차\nStandard Error, SE\n추정량의 정확도를 평가하는 데 중요한 지표이며, 값이 작을수록 추정량이 모집단 모수를 더 정확하게 반영한다고 할 수 있다. 표본평균의 기댓값과 표준오차는, 모집단의 평균과 표준편차가 μ, σ 일 때 위와 같이 구할 수 있다.\n표본평균을 가지고 μ 를 추정할 경우, n 이 클수록 표준오차가 작아져 좀 더 정확한 추정이 가능하다. 그러나 표준오차 계산 시 σ 가 주어지지 않는 경우가 있다. 이 경우, σ 를 표본표준편차로 추정하여 사용할 수 있다.\n\n\n3-3. 구간추정\nInterval Estimation\n추정량의 분포를 이용하여 표본으로부터 모수 값을 포함하리라고 예상되는 구간을 제시하는 것. 이때 제시되는 구간을 신뢰구간이라고 한다.\n\n\n3-4. 신뢰구간\nConfidence Interval\n모집단의 어떤 모수를 추정하기 위해 계산된 범위. 신뢰구간은 ( L , U )의 형태로 이루어지며, 여기서 L 과 U 는 표본으로부터 계산된 통계량이다.\nL ( Lower bound ) : 신뢰구간의 하한값 U ( Upper bound ) : 신뢰구간의 상한값 따라서 표본마다 계산되는 신뢰구간은 서로 다를 수 있다.\n가장 확실한 신뢰구간은 항상 모수를 포함하는 구간이다. 그러나 이는 이론적으로는 가능하지만 실질적으로는 불가능하다. 그렇게 되기 위해서는 신뢰구간이 상당히 길어질 수 밖에 없다.\n이 경우, 신뢰구간 CI 는 매우 넓어질 수밖에 없다.\n항상 모수를 포함하는 신뢰구간은 실질적으로 너무 넓어서 유용하지 않다. 모수에 대한 정확한 정보를 얻으려면 신뢰구간을 가능한 한 줄일 필요가 있다.\n신뢰구간이 좁을수록 추정의 정확성이 높아진다.\n실용적인 측면에서 신뢰구간을 적절히 좁히기 위해, ” 모든 표본에서 항상 모수를 포함해야 한다 ” 는 엄격한 조건을 완화하고, 대부분의 경우에서 모수를 포함하도록 설정하는 것이 필요하다.\n신뢰구간이 모수를 포함할 확률을 1 보다는 작은 일정한 수준에 유지하여 구간의 길이를 줄이는 것이 바람직하다.\n이때 모수를 포함할 확률을 신뢰수준 ( Level of Confidence ) 또는 신뢰도라고 한다.\n\n\n3-5. 모평균 μ에 대한 신뢰구간\n여기에서는 신뢰구간을 계산하는 두 가지 경우를 설명한다:\n\n모집단의 표준편차 σ 를 알고 있는 경우:\n\n정규분포 개념을 이용하여 신뢰구간을 계산한다:\n신뢰수준 95 % 에 해당하는 정규분포의 임계값 Z α / 2 ​를 사용하여 신뢰구간의 범위를 설정한다. 예시 : α = 0.05 일 때, Z ₀.₀₅ / ₂ = Z ₀.₀₂₅ = 1.96\n정규분포는 평균 μ 와 표준편차 σ 를 알 때, 그 분포의 형태가 완전히 결정된다. 평균 μ 를 중심으로 좌우 대칭이며, σ 가 클수록 분포가 넓어진다. 이 특성 덕분에, 정규분포는 모수에 대해 많은 정보를 제공할 수 있다. 모집단이 정규분포를 따른다면, 표본 평균의 분포 역시 정규분포를 따른다.\n위의 식에서 괄호 안에 부등식을 풀어 쓰면:\n위 식을 μ 에 대해 정리하면:\n따라서:\n\n모집단의 표준편차 σ를 모르는 경우:\n일반적인 경우에 관심있는 모집단은 그 분포나 표준편차가 알려져 있지 않다. 이런 경우, 9장에서 다룬 중심극한정리를 이용한다.\n\n위 식은 약간의 수정이 필요한데, 이것은 11장을 참고하기를 바란다.\n이 경우에도, 확률 1 − α 는 근사적으로 얻어진다. n 이 클 때는 σ 대신 s 를 사용해도 확률값에 크게 영향을 주지 않는다.\n일반적으로 추정량의 기댓값이 추정하고자하는 모수값을 갖고 그 분포가 정규분포일 때 100 ( 1 − α ) % 신뢰구간은 다음과 같은 형태를 따른다:\n사용 상황에 따라 표준오차의 정의가 달라질 수 있다.\n\n\n3-6. 신뢰구간의 의미\n이 그래프는 주어진 모수에 대해 95% 신뢰구간을 표시하였다. 주어진 모수 (평균 100, 표준편차 10), 표본 크기 15를 사용하여 총 25개의 표본을 추출하였다.\n위 그래프와 같은 방식으로 표본을 계속 추출하고 신뢰구간을 계산하면, 그 신뢰구간들이 모평균을 포함하는 비율이 95% 에 가까워지는 것을 확인할 수 있다.\n\n\n3-7. 표본 크기의 결정\n많은 수의 표본을 추출하여 신뢰구간을 생성하는 것은 시간과 비용이 많이 소모된다. 따라서 우리가 원하는 정확도를 얻을 수 있는 범위 내에서 표본의 크기를 줄이는 것이 바람직하다.\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α ) % 가 되려면:\n표준화된 확률변수의 분포가 표준정규분포를 따르므로:\n위 사실로부터 아래 식이 성립해야 하며:\n이를 n 에 대하여 풀면 아래 식을 만족해야 한다:\n적정한 표본의 크기는 위의 부등식을 만족하는 ’ 최소의 정수 ’ 가 된다. 만약 모집단이 정규분포라는 가정이 없다면 표본의 크기는 중심극한정리를 이용할 수 있도록 30 이상이 되어야 한다."
  },
  {
    "objectID": "da/ida_10.html#모평균에-대한-검정",
    "href": "da/ida_10.html#모평균에-대한-검정",
    "title": "10장: 통계적 추론",
    "section": "4. 모평균에 대한 검정",
    "text": "4. 모평균에 대한 검정\n통계적 추론 중 하나인 모수에 대한 가설 검증 ( Testing Statistical Hypotheses ) 에 대해 다루고자 한다.\n\n4-1. 가설\nHypotheses 가설검증에는 2 개의 가설이 있다.\n대립가설 ( Alternative Hypothesis ; H ₁ ) : 입증하여 주장하고자 하는 가설.\n귀무가설 ( Null Hypothesis ; H ₀ ) : 대립가설을 입증할 수 없을 때, 대립가설을 무효화하면서 받아들이는 가설.\n\n\n4-2. 오류의 종류\n가설검증에서 내리는 판단이란 다음 2 가지 형태 중 하나로 나타난다. 상황에 따라 다르지만, 일반적으로 제1종 오류에 더 주의를 기울이게 된다.\n제1종 오류 ( Type I Error ; α ) : 귀무가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 없는데, 효과가 있다고 결론 내리는 경우.\n제2종 오류 ( Type II Error ; β ) : 대립가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 있는데, 효과가 없다고 결론 내리는 경우.\n\n\n4-3. 검정통계량\nTest Statistic 표본 데이터를 요약하여 귀무가설을 검정하는 데 사용하는 값. 귀무가설이 참이라는 가정 하에 표본에서 계산된 값으로, 이 값이 귀무가설 하에서 얼마나 극단적인지를 평가한다.\n\n\n4-4. 기각역\nCritical Region 귀무가설을 기각할 수 있는 값들의 집합.\n검정 통계량이 기각역에 속할 경우 귀무가설을 기각한다. 아닐 경우 귀무가설을 기각하지 않는다.\nc 이하이면 H₀ 을 기각한다.\n가장 바람직한 기각역이란 아래 두 확률을 최소화하는 것이 될 것이다.\nα : 제 1 종 오류를 범하게 될 확률. β : 제 2 종 오류를 범하게 될 확률.\n위 두 확률은 다음과 같은 특징이 있다:\n( 1 ) α 와 β 는 서로 반비례 관계에 있다.\n( 2 ) α 는 너무 크게 설정하지 않는 것이 좋다. 너무 큰 α 는 제1종 오류의 확률을 높여 잘못된 결론을 내릴 가능성을 높인다.\n이를 방지하기 위해, 유의수준이라는 상한선을 둘 수 있다.\n\n\n4-5. 유의수준\nSignificance Level 일반적으로 사용되는 유의수준은 0.05 ( 5% ) 또는 0.01 ( 1% ) 이다. 이는 연구자가 제1종 오류의 확률을 낮추어 신뢰할 수 있는 결과를 얻기 위함이다.\n0.05 ( 5% ) : 가장 흔히 사용되는 유의수준. 제1종 오류를 5% 로 제한. 0.01 ( 1% ) : 더 엄격한 기준으로, 제1종 오류를 1% 로 제한. 0.10 ( 10% ) : 덜 엄격한 기준으로, 제1종 오류를 10% 로 제한.\n양측 검정 ( Two — tailed Test ) : 유의 수준 α 는 두 방향 ( 양쪽 끝 ) 에 분배되어 α / 2 씩 기각역을 형성한다.\n단측 검정 ( One — tailed Test ) : 유의 수준 α 는 한 방향에 집중되어 기각역을 형성한다.\n\n\n4-6. 모평균 μ에 대한 검 정\n표본의 크기가 클 때 모평균 μ 에 대한 가설 H ₀ : μ = μ₀ 을 검정하기 위한 검정통계량은 다음과 같다: 단 모집단의 표준편차 σ 가 주어져 있을 때 s를 σ 로 대체한다.\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n4-7. 유의확률\nSignificance Probability 주어진 검정통계량의 관측치로부터 H ₀ 을 기각하게 하는 최소의 유의수준.\nP – 값 ( P – Value )\n일반적으로 유의확률은 주어진 관측값을 경계점으로 하는 기각역의 유의수준으로 얻어진다. Z = z 일 때 각 기각역의 형태에 따라 P – 값을 구하는 식을 정리하면 다음과 같다:"
  },
  {
    "objectID": "da/ida_10.html#모비율에-대한-추론",
    "href": "da/ida_10.html#모비율에-대한-추론",
    "title": "10장: 통계적 추론",
    "section": "5. 모비율에 대한 추론",
    "text": "5. 모비율에 대한 추론\n모집단에서 특정 속성을 가진 개체의 비율을 추정하거나 검정하는 것을 의미한다.\n\n5-1. 점추정\n모비율에 대한 추정량으로 표본비율을 사용할 수 있다: 점추정량이 결정되면 그 추정량의 정확도를 알기 위해 표준오차를 계산할 필요가 있다.\n모집단의 크기가 매우 커서 그에 비해 표본의 크기가 작은 경우, X 의 분포는 반복 횟수 n , 성공의 확률 p 인 이항분포가 된다. 따라서, X 의 기댓값과 분산는 아래와 같다:\nE ( X ) = np Var ( X ) = npq\n표본비율의 표준오차는 다음과 같이 계산된다:\n이 식은 이항분포의 분산을 이용한 결과이다. ​​ 표본비율​의 분산 은 아래 식과 같다:\n표본비율의 표준오차는 이 분산의 제곱근으로 표현된다.\n\n\n5-2. 구간추정\n모비율 P 에 대한 구간추정을 하려면 P 의 추정량인 p̂ 의 분포를 알아야 한다.\nX 의 분포는 이항분포를 따르므로, 중심극한정리를 이용하여 표본 비율 p̂​ 의 분포를 근사적으로 정규분포로 취급할 수 있다:\n분모, 분자를 n으로 나누었을 때, 위와 같은 식이 나온다.\n위 구간이 p를 포함할 확률이 ( 1 − α ) 가 됨을 알 수 있다.\n\n\n5-3. 신뢰구간\np 를 추정량인 p̂ 으로 대체하면, 원하는 신뢰구간을 구할 수 있다.\n\n\n5-4. 표본크기의 결정\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α )% 가 되려면:\n표본 크기가 클 경우, 정규분포를 이용하여 위와 같은 식을 구할 수 있다. 따라서, 표본 크기는 아래 식을 만족해야 한다:\n\n\n5-5. 모비율 p에 대한 검정\n표본 크기가 클 때 모비율 p 에 대한 가설 H ₀ : p = p ₀ 을 검정하기 위한 검정통계량은 다음과 같다:\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 13의 ( 1 ) height 에 대한 요약 통계량 계산하기.\n\nxbar_h = np.mean(height);print(xbar_h) # 평균\n\nvar_h = np.var(height, ddof=1);print(var_h) # 분산 (자유도 1 사용)\n\nsd_h = np.std(height, ddof=1);print(sd_h) # 표준편차 (자유도 1 사용)\n\nmedian_h = np.median(height);print(median_h) # 중앙값\n\nmin_h = np.min(height);print(min_h) # 최솟값\n\nmax_h = np.max(height);print(max_h) # 최댓값\n\nsum_h = np.sum(height);print(sum_h) # 합계\n\nn = height.size;print(n) # 데이터 개수\n\n160.2\n35.88965517241378\n5.990797540596225\n161.0\n150\n170\n4806\n30\n\n\n예제 13의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수. 정규분포의 백분위수 함수.\n\nfrom scipy import stats \n\nse_h = stats.sem(height);print(se_h) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.05 / 2); print(z_alpha) # 95% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_h;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_h - interval, xbar_h + interval]; print(CI) # 신뢰구간\n\n1.0937649834770078\n1.959963984540054\n2.1437399751659827\n[np.float64(158.05626002483402), np.float64(162.34373997516596)]\n\n\n이로부터 95% 신뢰구간은 160.2 ± 2.1437 임을 알 수 있다.\n예제 14 검정하고자 하는 가설은 H ₀ : μ = 159 대 H ₀ : μ ≠ 159 이며, 표본의 크기는 30 이상이다.\n\nzval = (xbar_h-159)/se_h;print(zval) # 가설 검정을 위한 z값\n\npval = 2 * (1 - stats.norm.cdf(zval));print(pval) # p값\n\n## 해석: p값이 크므로 귀무가설을 기각할 수 없음\n## → 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\n\n1.097127827392377\n0.27258551722126834\n\n\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고, 이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\n\nxbar_n = np.mean(noise);print(xbar_n) # 평균\n\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\n\nn = noise.size;print(n) # 데이터 개수\n\n61.373999999999995\n4.780137902544961\n50\n\n\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\n\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\n\n0.6760135851792765\n2.3263478740408408\n1.5726427667045366\n[np.float64(59.801357233295455), np.float64(62.946642766704535)]\n\n\n\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\n\npval = stats.norm.sf(np.abs(zval));print(pval) # p값\n\n## 해석: P—값이 0.021로 0.05보다 작게 나왔으므로 \n## 유의수준 5% 에서 귀무가설을 기각할 수 있다.\n\n## 그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\n\n2.032503532655509\n0.021051353256926374"
  },
  {
    "objectID": "da/ida_08.html",
    "href": "da/ida_08.html",
    "title": "8장: 정규분포",
    "section": "",
    "text": "6장에서 언급되었던, 연속적인 값을 가지는 연속확률분포들 중에서 대부분의 통계학 이론의 기본이 되는 정규분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_08.html#자료의-입력",
    "href": "da/ida_08.html#자료의-입력",
    "title": "8장: 정규분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n!pip install numpy\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7: 어느 대학교의 일반수학 중간고사 성적은\n# 분포가 평균이  63 이고, 분산이  100 인 정규분포를 따른다고 가정한다. (p.232)\n\n# (1) 50 점 이하의 학생은 몇 퍼센트나 되겠는가?\n\n# (2) 상위 10 %의 학생에게  A 를 준다고 하면 \n# 몇 점 이상이 되어야  A 를 받을 수 있겠는가?\n\n# ___________________________________________________________\n\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\n\nimport numpy as np \n\ndata1 = np.array([4001, 3927, 3048, 4298, 4000, 3445, \n                 4949, 3530, 3075, 4012, 3797, 3550, \n                 4027, 3571, 3738, 5157, 3598, 4749, \n                 4263, 3894, 4262, 4232, 3852, 4256, \n                 3271, 4315, 3078, 3607, 3889, 3147, \n                 3421, 3531, 3987, 4120, 4349, 4071, \n                 3683, 3332, 3285, 3739, 3544, 4103, \n                 3401, 3601, 3717, 4846, 5005, 3991, \n                 2866, 3561, 4003, 4387, 3510, 2884, \n                 3819, 3173, 3470, 3340, 3214, 3670, \n                 3694])\n                 \n# ___________________________________________________________\n\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\n\ndata2 = np.array([39.3, 14.8, 6.3, 0.9, 6.5, \n                 3.5, 8.3, 10.0, 1.3, 7.1, \n                 6.0, 17.1, 16.8, 0.7, 7,9, \n                 2.7, 26.2, 24.3, 17.7, 3.2, \n                 7.4, 6.6, 5.2, 8.3, 5.9, \n                 3.5, 8.3, 44.8, 8.3, 13.4, \n                 19.4, 19.0, 14.1, 1.9, 12.0, \n                 19.7, 10.3, 3,4, 16.7, 4.3, \n                 1.0, 7.6, 28.33, 26.2, 31.7, \n                 8.7, 18.9, 3.4, 10.0])\n\nCollecting numpy\n  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\nInstalling collected packages: numpy\nSuccessfully installed numpy-2.2.6\n\n\nWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\nYou should consider upgrading via the 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command."
  },
  {
    "objectID": "da/ida_08.html#연속확률분포",
    "href": "da/ida_08.html#연속확률분포",
    "title": "8장: 정규분포",
    "section": "2. 연속확률분포",
    "text": "2. 연속확률분포\nContinuous Probability Distribution\n연속확률변수 X 가 가질 수 있는 값들의 분포를 나타낸다. 연속확률변수: 특정 범위 내에서 모든 실수 값을 가질 수 있는 변수."
  },
  {
    "objectID": "da/ida_08.html#확률밀도함수",
    "href": "da/ida_08.html#확률밀도함수",
    "title": "8장: 정규분포",
    "section": "2-1. 확률밀도함수",
    "text": "2-1. 확률밀도함수\nProbability Density Function, PDF\n연속확률변수 \\(X\\) 의 확률분포는 확률의 밀도를 나타내는 \\(X\\) 의 확률밀도함수에 의해 결정된다. 아래의 조건을 만족하는 함수 \\(f(X)\\) 를 \\(X\\) 의 확률밀도함수라고 한다.\n\n연속확률변수에서 특정 값에 대한 확률은 0 이다. 예를 들어, [ 0, 1 ] 구간에 있는 실수 값은 무한히 많다. 이러한 경우, 특정한 하나의 값을 가질 확률은 무한히 작은 값이 되며, 이는 수학적으로 0 으로 표현된다.\n\n범위 : 확률밀도함수 f ( x ) 의 값은 항상 0 이상이어야 한다. 이는, 어떤 사건의 확률이 음수일 수 없다는 것을 의미한다.\n확률밀도함수는 이산확률분포의 확률함수와는 달리 확률을 나타내지 않는다. 그러므로, f ( x ) 가 1 보다 작아야 된다는 조건 은 필요가 없다.\n\nPDF 를 사용하여 특정 구간 [ a, b ] 에 속할 확률을 계산할 수 있다.\nPDF 를 전체 범위에 대해 적분 하면 1 이 되어야 한다.\n\n위 조건으로부터 다음과 같은 결론을 내릴 수 있다:\n어떤 구간의 확률을 구할 때에는 그 구간의 경계점이 포함되는가 포함되지 않는가에 영향을 받지 않는다."
  },
  {
    "objectID": "da/ida_08.html#정규분포",
    "href": "da/ida_08.html#정규분포",
    "title": "8장: 정규분포",
    "section": "3. 정규분포",
    "text": "3. 정규분포\nNormal Distribution\n평균 \\(μ\\) 와 분산 \\(σ^2\\) 의해서 그 분포가 확정된다. 그 확률밀도함수 ( PDF ) 의 대략적인 특성은 다음과 같이 표현할 수 있다:\n위 정규분포는 N ( μ, σ² )으로 표시할 수 있다. X 가 평균 μ 로부터 ± σ, ± 2σ, ± 3σ, ± 4σ 의 사이의 있을 확률은 다음과 같다.\n이를 통해 ± 4σ 이상 떨어진 데이터는 매우 드문 현상임을 알 수 있다. 그러므로, 정규분포의 ± 4σ 이상의 영역은 실질적인 분석에서 종종 무시할 수 있다."
  },
  {
    "objectID": "da/ida_08.html#정규분포의-특성",
    "href": "da/ida_08.html#정규분포의-특성",
    "title": "8장: 정규분포",
    "section": "3-1. 정규분포의 특성",
    "text": "3-1. 정규분포의 특성\nμ ± 3σ 안에 거의 모든 확률이 집중된다.\n3 - 2 . 표 준 정 규 분 포 ( Standard Normal Distribution ) 평균 ( μ ) 이 0 이고 표준편차 ( σ )가 1 인 특수한 정규분포.\n확률변수 Z 가 N ( 0, 1 ) 이라고 할 때, Z 는 0 을 중심으로 대칭인 분포를 갖게 되며,\n이를 이용해 다음과 같이 나타낼 수 있다:\n누적분포함수 ( CDF ) 는 특정 값 이하의 확률을 나타낸다.\nP ( Z ≤ b ) 는 Z 가 b 이하일 확률이고, P ( Z ≤ a ) 는 Z 가 a 이하일 확률이다. 따라서 a 에서 b 까지의 확률은 다음과 같다:\n\n3-3. 표준정규확률변수\nStandard Normal Random Variable\n표준정규분포 Z 에 관한 확률계산 방법을 일반 정규분포 X 의 확률계산에 적용할 수 있다. 일반 정규분포 X 를 표준정규분포 Z 로의 식변환을 통해 쉽게 계산할 수 있으며, 이를 표준화라고 한다.\n확률변수 X 가 N ( μ, σ² ) 일 때 표준화된 확률변수 Z 는 정규분포 N ( 0, 1 ) 을 따른다. X 를 표준화하여 Z 로 표현한 후 표준정규분포표를 이용하면 된다.\n예제 7의 ( 1 ) 중간고사 성적을 확률변수 X 라고 하면 주어진 정보를 바탕으로 다음과 같이 표현할 수 있다:\n평균 (μ) = 63, 분산 (σ²) = 100 = 10², 표준편차 (σ) = 10 정규분포표의 −1.3행, 0.00열의 값 = 0.0968\n\nfrom scipy.stats import norm\nprint(norm.cdf(x=50, loc=63, scale=10))\n\n## 해석: 50 점 이하의 학생의 비율은  0.0968 = 9.68% 이다.\n\n0.09680048458561036\n\n\n예제 7의 ( 2 ) x 점 이상의 학생들에게 A 를 준다고 하면 x 는 다음을 만족해야 한다: 상위 10 % = 0.10\n먼저, 표준정규분포표로부터 P [Z ≥ z] = 0.10 이 되는 z 값을 찾는다: 이 경우, 0.10 에 가까운 값을 주는 z = 1.28 을 고르면 된다:\n\nfrom scipy.stats import norm\nprint(norm.ppf(q=0.9, loc=63, scale=10))\n\n## 해석: 상위 10 %의 학생에게  A 를 준다고 하면 \n## 75.8 점 이상의 점수를 받은 학생에게 주면 된다.\n\n75.815515655446"
  },
  {
    "objectID": "da/ida_08.html#이항분포의-정규분포근사",
    "href": "da/ida_08.html#이항분포의-정규분포근사",
    "title": "8장: 정규분포",
    "section": "4. 이항분포의 정규분포근사",
    "text": "4. 이항분포의 정규분포근사\n대규모 시행에서 이항 분포를 정규 분포로 근사하는 방법. 7장의 초기하 분포나, 포아송 분포 마찬가지로, 정규 분포로도 근사하여 계산할 수 있다.\n이항 분포는 n 이 커짐에 따라 근사적으로 정규분포를 따르게 된다. 이때 정규분포의 평균과 분산은 이항분포의에서와 일치하여야 한다:"
  },
  {
    "objectID": "da/ida_08.html#연속성수정",
    "href": "da/ida_08.html#연속성수정",
    "title": "8장: 정규분포",
    "section": "4-1. 연속성수정",
    "text": "4-1. 연속성수정\nContinuity Correction\n이항 분포의 이산적인 값을 연속적인 정규 분포의 구간에 맞추는 작업. 보통 ± ½ ​를 가감하여 근삿값을 계산한다.\n\\[\nX ~ Bin (n, p) ≈ Y \\~ N (μ, σ²)\n\\]\n확률변수 X 가 이항분포, 즉 X ~ Bin ( n, p ) 이고, np, nq ( =1 − p ) 가 모두 클 경우 ( 보통 10 이상 ) X 는 근사적으로 평균이 np, 표준편차가 √npq 인 정규분포를 따른다."
  },
  {
    "objectID": "da/ida_08.html#정규분포가정의-조사",
    "href": "da/ida_08.html#정규분포가정의-조사",
    "title": "8장: 정규분포",
    "section": "5. 정규분포가정의 조사",
    "text": "5. 정규분포가정의 조사\n모집단의 분포가 정규 분포를 따른다는 가정을 조사하기 위해 사용할 수 있는 효과적인 그림. 정규점수그림 ( Normal Scores Plot ) 또는 정규확률그림 ( Normal Probability Plot ) 으로 불린다.\n\n5-1. 정규점수\nNormal Scores\n표본 데이터와 표준 정규 분포 ( 평균 0, 표준편차 1 ) 를 비교하여 데이터의 정규성을 평가하는 데 사용된다. 정규성( Normality ) : 데이터가 정규 분포를 따르는지를 의미한다.\n즉, 표준정규분포의 확률밀도함수 ( PDF ) 를 등확률 구간으로 나누어 주는 경곗값 ( z 값 ) 을 의미한다. 그림으로부터 분포의 형태를 알기 위해선 자료의 크기가 적어도 15 이상은 되어야 한다.\n\n\n5-2. 정규확률그림 그리는 순서\n자료를 작은 것부터 크기순으로 나열한다. 각 자료에 해당하는 점수를 계산한다. i 번째 순서의 자료와 i 번째 순서의 정규점수를 하나의 쌍으로 2 차원 공간상에 나타낸다.\n\n\n5-3. 정규그림의 해석\n정규분포를 따른다면, 양쪽의 값은 서로 가까울 것이라고 예상할 수 있다.\n\n\n5-3. 정규확률그림을 이용한 정규성 판정\n직선식일 경우, 정규분포의 가정이 타당하다고 볼 수 있다. (곡선 등) 직선식을 벗어날 경우, 가정이 의문시된다고 할 수 있다.\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsm.qqplot(data1, line = \"s\")\n\n# \"s\"는 \"standardized\"를 의미한다. \n\n# 이 옵션은 플롯에 표준화된 선\n# (평균 0, 표준편차 1의 정규 분포)에 맞추어 선을 그린다.\n\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에 가까우므로 \n## 데이터가 정규분포를 따를 가능성이 높다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n\n5-4. 자료의 변환\n만약 추출된 표본이 정규확률그림 등에서 정규분포와 상당히 벗어난 것으로 판명되면, 자료의 변환을 통해 정규분포의 형태를 갖도록 시도해 볼 수 있다.\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\nimport matplotlib.pyplot as plt\n\nplt.hist(data2, bins=5, range=(0,50)) \nplt.xlabel('data2') \nplt.ylabel('Density') \nplt.title('Historam of data')\n\n## 해석: 아래 히스토그램을 보면 자료의 분포가 왼쪽으로 편중되어 있으므로\n## 정규분포가 아니라는 의심을 할 수 있다.\n\nText(0.5, 1.0, 'Historam of data')\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nsm.qqplot(data2, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에서 크게 벗어나\n## 데이터가 정규 분포를 따르지 않을 가능성이 있음을 확인할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n이 경우, 큰 자룟값을 더 작게 만드는 과정을 수행한다:\n\n# 원 자료를 제곱근(체적^(0.5))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata3 = np.sqrt(data2)\nsm.qqplot(data3, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n# 원 자료를 네제곱근(체적^(0.25))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata4 = np.power(data2, 0.25)\nsm.qqplot(data4, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida_06.html",
    "href": "da/ida_06.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "5장에서 다룬 표본공간의 근원사건들은 특성을 표현하는 형태로 다뤘다. 이제는 확률변수를 중심으로 실험의 수치적 결과에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_06.html#확률변수",
    "href": "da/ida_06.html#확률변수",
    "title": "6장: 확률분포",
    "section": "1. 확률변수",
    "text": "1. 확률변수\nRandom Variable\n각각의 근원사건들에 실수값을 “대응시키는 함수”이며 X, Y, … 등으로 표시한다.\n확률변수가 가질 수 있는 “값의 개수” 가 유한하거나 무한이라도 “셀 수 있는 경우” 에 이를 “이산확률변수” 라고 한다.\n또한, 연속적인 구간에 속하는 모든 값을 다 가질 수 있는 “연속확률변수” 도 있다."
  },
  {
    "objectID": "da/ida_06.html#이산확률분포",
    "href": "da/ida_06.html#이산확률분포",
    "title": "6장: 확률분포",
    "section": "2. 이산확률분포",
    "text": "2. 이산확률분포\nDiscrete Probability Distribution\n확률변수가 갖는 값들과 그에 “대응하는 확률값” 을 나타내는 것으로, 나열된 표나 수식으로 표현되며, 보통은 “확률변수 X 의 분포” 라고 한다.\n\n2-1. 확률질량함수\nProbability Mass Function, PMF\n확률변수 X가 k개의 값 x₁, x₂, …, xk를 가질 때, 그에 대응하는 확률을 f(x₁), f(x₂), …, f(xk)라고 하면 X의 확률분포는 다음과 같다:\nf(x)는 확률변수 X 가 값 x 를 갖게 되는 확률 P ( X = x ) 을 나타낸다:\n이산확률변수 X 의 확률변수는 다음 조건을 만족해야 한다:\n모든 확률은 0 이상 1 이하의 값을 가진다. 확률변수가 가질 수 있는 모든 값에 대한 확률의 합은 1 이다."
  },
  {
    "objectID": "da/ida_06.html#이산확률변수의-평균과-표준편차",
    "href": "da/ida_06.html#이산확률변수의-평균과-표준편차",
    "title": "6장: 확률분포",
    "section": "3. 이산확률변수의 평균과 표준편차",
    "text": "3. 이산확률변수의 평균과 표준편차\n\n3-1. 기댓값\nExpected Value\nE ( X ) 는 확률변수 X 의 “기댓값(평균)” 또는 X 가 갖는 확률분포의 “모평균” 이라고 한다.\n뮤( M , μ: 그리스 알파벳의 열두째 글자)\n3-2. 모분산 Population Variance\n편차 (즉, 각 값이 기대값에서 얼마나 떨어져 있는지)를 제곱하고, 그 제곱된 값을 각 값이 발생할 확률로 가중평균하는 것이다.\nV a r ( X ) = ∑ ( 편차 ) ² × 확률:\n시그마(Σ, σ: 그리스어 알파벳의 열여덟째 글자)\n모분산의 간편식:\n기댓값을 알고 있다면, 직접적인 정의를 사용하지 않고 게산할 수 있다.\n\n\n3-3. 모표준편차\nPopulation Standard Deviation 모분산의 양의 제곱근으로 계산된다:\n모표준편차 ( σ ) 의 단위는 확률변수 X와 “동일” 하다. 반면 모분산 ( σ² ) 의 단위는 X 의 단위를 “제곱” 한 것이므로, 퍼진정도를 측정하는데 적절하지 않다.\n예를 들어, X 의 단위가 센티미터( cm )라면, 모분산 σ² 의 단위는 제곱센티미터( cm² )가 된다."
  },
  {
    "objectID": "da/ida_06.html#두-확률분포의-결합분포",
    "href": "da/ida_06.html#두-확률분포의-결합분포",
    "title": "6장: 확률분포",
    "section": "4. 두 확률분포의 결합분포",
    "text": "4. 두 확률분포의 결합분포\n하나의 실험에서도 2 개 이상의 측면에 대한 관측이 가능하다. 이 경우 그 2 가지 특성 간의 관계 여부 및 그 관계 정도에 대해 분석할 수 있다.\n\n4-1. 결합확률분포\nJoint Probability Distribution\n2개 이상의 확률변수가 동시에 특정한 값을 가질 확률을 나타내는 분포이다.\n2 개의 확률변수가 이산일 경우,\nX 가 취하는 값을 x₁, …, xm Y 가 취하는 값을 y₁, …, yn 이라고 할 때\nX 와 Y 의 결합확률분포는 모든 1 ≤ i ≤ m, 1 ≤ j ≤ n 에 대하여\n위 식을 구하므로써 결정되며, 다음과 같이 표현할 수 있다:\n\n\n4-2. 주변확률분포\nMarginal Probability Distribution\n결합확률분포에서 한 확률변수를 “고정” 하고 다른 변수의 분포를 고려하는 분포이다. 이는 2 개 이상의 확률변수 중 하나에 대한 “단일 확률분포” 를 얻기 위해 사용한다:\n각각의 주변확률을 이용해서 하나의 변수 때와 마찬가지로 구하면 된다:"
  },
  {
    "objectID": "da/ida_06.html#공분산과-상관계수",
    "href": "da/ida_06.html#공분산과-상관계수",
    "title": "6장: 확률분포",
    "section": "5. 공분산과 상관계수",
    "text": "5. 공분산과 상관계수"
  },
  {
    "objectID": "da/ida_06.html#공분산",
    "href": "da/ida_06.html#공분산",
    "title": "6장: 확률분포",
    "section": "5-1. 공분산",
    "text": "5-1. 공분산\nCovariance\n두 확률변수 X 와 Y 가 함께 변하는 정도를 측정한다.\nX, Y가 같은 방향으로 변화할 경우, (즉, 둘 다 증가하거나 둘 다 감소하는 경우) ( X − μX ​), ( Y − μY ​) 의 부호가 일치할 확률이 상대적으로 커진다. 따라서, 이에 대한 기댓값은 양수가 된다.\nX, Y가 다른 방향으로 변화할 경우, (즉, 한 변수가 증가할 때 다른 변수는 감소하는 경우) ( X − μX ​), ( Y − μY ​) 가 서로 다른 부호를 갖게 될 확률이 상대적으로 커진다. 따라서 이에 대한 기댓값은 음수가 될 것이다.\nX, Y 의 공분산은 아래와 같이 정의된다:"
  },
  {
    "objectID": "da/ida_06.html#상관계수",
    "href": "da/ida_06.html#상관계수",
    "title": "6장: 확률분포",
    "section": "5-2. 상관계수",
    "text": "5-2. 상관계수\nCorrelation Coefficient\n두 확률변수 간의 선형 관계의 강도와 방향을 측정한다. 이것은 공분산을 표준화한 형태로, −1 과 1 사이의 값을 가진다.\n상관계수의 성질:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n정확한 선형관계 Y = aX + b 가 성립할 때, 상관계수의 값은 −1 또는 1 이다.\nX, Y 의 상관계수는 각 확률변수에 상수가 더해지거나 감해지는 것에 영향을 받지 않는다. 상수가 곱해진 경우, 그 부호에만 영향을 받는다.\n상수 c 와 d 의 부호가 다르면 상관계수의 부호가 반대가 된다."
  },
  {
    "objectID": "da/ida_06.html#두-확률변수의-독립성",
    "href": "da/ida_06.html#두-확률변수의-독립성",
    "title": "6장: 확률분포",
    "section": "6. 두 확률변수의 독립성",
    "text": "6. 두 확률변수의 독립성\n2 개의 확률변수 X, Y 가 독립이 되기 위해서는 X, Y 가 취하는 모든 쌍의 값 ( xi, yi ) 에 대해 아래 식을 만족해야 한다.\n두 확률변수 X, Y 가 서로 독립일 때, 아래의 식이 성립한다:\n단, 공분산과 상관계수가 0 이라는 사실이 항상 두 변수가 독립이라는 것을 보장하지 않는다. 이는 두 변수 간에 선형 관계가 없음을 의미하지만, 비선형 관계가 존재할 수 있다.\n두 확률변수가 독립일 경우, 공분산이 0 이 되므로 합과 차의 분산을 쉽게 계산할 수 있다.\n분산과 공분산의 정의를 이용하면: 공분산 항을 제외하여 다음과 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida_04.html",
    "href": "da/ida_04.html",
    "title": "4장: 두 변수 자료의 요약",
    "section": "",
    "text": "조사 대상의 각 개체로부터 둘 또는 그 이상의 변수들을 동시에 관측하는 경우가 더 많다. 두 변수에 관한 관측값을 도표로 요약하고 해석하는 방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida_04.html#자료의-입력",
    "href": "da/ida_04.html#자료의-입력",
    "title": "4장: 두 변수 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제5: 통계학과 신입생 51명의 키와 몸무게를 기록한 것이다.\n# 키와 몸무게의 표본상관계수를 구하고, 산점도를 그려라. (p.108)\n\nimport numpy as np\n\n# 키와 몸무게 자료의 입력\nheight = np.array([181,161,170,160,158,168,162,179,183,178,171,177,163,\n                   158,160,160,158,173,160,163,167,165,163,173,178,170,\n                   167,177,175,169,152,158,160,160,159,180,169,162,178,\n                   173,173,171,171,170,160,167,168,166,164,173,180]) \n\nweight = np.array([78,49,52,53,50,57,53,54,71,73,55,73,51,53,65,48,59,\n                   64,48,53,78,45,56,70,68,59,55,64,59,55,38,45,50,46,\n                   50,63,71,52,74,52,61,65,68,57,47,48,58,59,55,74,74])"
  },
  {
    "objectID": "da/ida_04.html#표본상관계수",
    "href": "da/ida_04.html#표본상관계수",
    "title": "4장: 두 변수 자료의 요약",
    "section": "2. 표본상관계수",
    "text": "2. 표본상관계수\nSample Correlation Coefficient\n산점도에서 점들이 얼마나 “직선에 가까운가” 의 정도를 나타내는 데 쓰이는 측도.\n두 변수 ( x, y ) 에 대하여 관측값 n개의 짝 ( x₁, y₁ ), ( x₂, y₂ ), …, (xn, yn) 이 주어진 때, 상관계수는 다음과 같이 계산된다:\n\n2-1. 공분산\nCovariance\n분산(Variance)은 한 변수의 데이터가 평균 주위에서 얼마나 흩어져 있는지를 나타낸다. 공분산은 “두 변수” 가 함께 변하는 정도를 나타낸다.\n공분산이 “양수” 이면, 두 변수는 “같은 방향” 으로 변한다. 공분산이 “음수” 이면, 두 변수는 “반대 방향” 으로 변한다. 공분산이 “0” 에 가까우면, 두 변수 간에 “선형 관계가 거의 없음” 을 의미한다.\n위 값들은 다음과 같은 수식을 통해 계산된다:\nx̄, ȳ 는 변수 x, y 의 표본 평균을 의미한다:\n표본상관계수는 두 변수의 직선관계의 정도(강도, 방향)를 나타내며, 다음과 같은 특징이 있다:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n계수가 “0” 에 가까울수록 두 변수 간의 직선의 관계가 “매우 약함” 을 의미한다. 표본상관계수의 단위는 없다.\n각 변수의 편차들의 곱을 사용해 계산할 때, 이 곱은 원래 변수들의 단위를 곱한 단위이지만, 분모에 각 변수의 분산을 곱한 값의 제곱근이 들어가기 때문에 단위가 사라지게 된다.\n위 연산은 단위를 제곱근 단위로 바꾸는 역할을 하며, 이로 인해 단위가 없는 숫자가 된다. 변수들의 단위에 영향을 받지 않아, 서로 다른 단위를 가진 변수들 간에도 관계를 비교할 수 있다.\n\n# 키와 몸무게의 표본상관계수\nnp.corrcoef(height, weight)[0][1]\n\n## 해석: 표본상관계수 𝑟이 약 0.74로 나왔다는 것은\n## 두 변수 사이에 강한 '양의 선형 관계'가 있음을 의미한다.\n\nnp.float64(0.7362765055636866)\n\n\n아래에서 설명할 산점도를 포함한 대부분의 그림 요약 방법은 “주관적” 일 수 있다. 반면, 표본상관계수는 “객관적” 인 수치 자료로, 이러한 문제를 “보완” 해준다.\n단, 직선이 아닌 다른 관계(곡선 등)가 있을 수 있으며, 이를 표본상관계수가 제대로 나타내지 못할 수 있다는 점을 유념해야 한다."
  },
  {
    "objectID": "da/ida_04.html#산점도",
    "href": "da/ida_04.html#산점도",
    "title": "4장: 두 변수 자료의 요약",
    "section": "3. 산점도",
    "text": "3. 산점도\nScatter Plot\n변수 x 를 “수평축” 에 놓고, 변수 y 를 “수직축” 에 놓은 후에 각 관측값의 찍을 좌표 위에 표시함 으로써 얻게 되는 그림.\n이를 통해, “두 변수 간의 관계” 를 시각적으로 대략 파악할 수 있다.\n\nimport matplotlib.pyplot as plt\n\n# 산점도 작성\nplt.figure(figsize=(8, 6))  # 그래프의 크기 (가로 8, 세로 6)\nplt.scatter(height, weight, color='slateblue')  # 산점도 색상 설정\n\nplt.xlabel('height (cm)')  # x축 레이블\nplt.ylabel('weight (kg)')  # y축 레이블\nplt.title('height(cm) and weight(kg)')  # 그래프의 제목\n\nplt.grid(True)  # 그리드 추가 (경계선)\nplt.show()"
  },
  {
    "objectID": "da/ida_03.html",
    "href": "da/ida_03.html",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "",
    "text": "연속형 자료가 어떤 값을 중심으로 분포되어 있는가를 나타내는 중심위치의 측도, 각 자료가 중심위치의 값으로부터 흩어진 정도를 나타내는 퍼진 정도의 측도 등을 다루고자 한다."
  },
  {
    "objectID": "da/ida_03.html#자료의-입력",
    "href": "da/ida_03.html#자료의-입력",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제13: 정량 100인 음료수 80병을 임의로 추출하여 그 내용물의 실제 측정된 양을 잰 자료이다.(p.42)\n\nimport numpy as np\n\n# 변수 drink에 NumPy 배열을 할당\ndrink = np.array([98, 99, 100, 99, 99.4, 101.7, 98.8, 101.8, 101.5, \n                 101.8, 102.6, 101, 98.8, 101.4, 99.7, 99.7, 99.7, \n                 100.9, 98.6, 101.4, 102.1, 102.9, 100.8, 101.8, \n                 100, 101.2, 100.5, 101.2, 100.1, 101.6, 101.3, 99.9, \n                 99.4, 99.3, 99.4,101.6, 96.1, 100, 99.7, 99.1, 100.7, \n                 100.8, 100.8, 95.5,100.1, 100.5, 98.9, 99.9, 96.8, \n                 102.4, 100, 103.7, 101.4,99.7, 97.4, 99.5, 97.5, \n                 99.9, 100.3, 100.2, 101.5, 99.4, 99.7, 98.2, 100.3, \n                 100.2, 100.5, 100.4, 101.5, 98.4, 101.4, 98.8, 100.9, \n                 101.1, 100.9, 98.1, 98.7, 99.2, 98.1, 97.2])\n\n중심위치의 측도"
  },
  {
    "objectID": "da/ida_03.html#평균-mean",
    "href": "da/ida_03.html#평균-mean",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "2. 평균 (Mean)",
    "text": "2. 평균 (Mean)\n총 자료의 개수 \\(n\\) 을 모든 관측값 \\(x_1, x_2, \\dots , x_n\\) 의 합으로 나눈 값. 이를 “산술 평균” 이라고도 하며, 공식으로 표현하면 다음과 같다:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n표본 평균은 관측값의 산술평균이며, “극단값” 에 영향을 받는다.\n\n표본 평균: 전체 데이터인 “모집단” 에서 추출한 일부 데이터의 평균을 의미한다. (예시1) 예제13 에서 추출된 80 병의 음료수는 표본에 해당된다.\n극단값(Outlier): 데이터 집합에서 다른 값들과 현저히 다른 값들을 의미한다. (예시2) 데이터 집합 { 11, 12, 13, 300 } 에서 극단값은 300 이다.\n\n\n# 2장의 예제 13에서 주어진 음료수 한 병의 부피 데이터를 기반으로 \n# 평균, 중앙값, 분산, 표준편차, 범위, 사분위수범위를 파이썬을 이용하여 계산하라.(p.83)\n\n## numpy 모듈을 이용하여 계산할 수 있음. ##\n\n# 평균\nprint(np.mean(drink))\n\n100.04125"
  },
  {
    "objectID": "da/ida_03.html#중앙값median",
    "href": "da/ida_03.html#중앙값median",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "3. 중앙값(Median)",
    "text": "3. 중앙값(Median)\n전체 관측값을 크기 순서로 배열했을 때, 가운데에 위치한 값.\n데이터의 개수가 홀수 일 경우, 중앙에 위치한 값이, 짝수 일 경우, 중앙에 위치한 두 값의 평균 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, 극단값의 영향을 받지 않는다. 따라서, 평균과 값이 다를 수 있다.\n\n# 중앙값 계산\nprint(np.median(drink))\n\n100.05\n\n\n퍼진 정도의 측도"
  },
  {
    "objectID": "da/ida_03.html#분-산-variance",
    "href": "da/ida_03.html#분-산-variance",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "4. 분 산 (Variance)",
    "text": "4. 분 산 (Variance)\n확률 분포나 데이터 집합의 산포도(분포도)를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 지표로 사용된다.\n관측값이 \\(x_1, x_2, \\dots , x_n\\) 이고, 표본평균이 \\(\\bar{x}\\) 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평균제곱 오차의 평균: 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n\n4-1. 편차(Deviation):\n각 관측값과 평균의 차이\n편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로,  따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n\n\n4-2. 자유도\nDegrees of Freedom\n위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "da/ida_03.html#표준편차",
    "href": "da/ida_03.html#표준편차",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "5. 표준편차",
    "text": "5. 표준편차\nStandard Deviation\n분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다.\n\n# 표준편차 계산 (표본의 표준편차, 자유도를 1로 설정)\nprint(np.std(drink, ddof=1))\n\n# 표준편차 계산 (모집단의 표준편차)\nprint(np.std(drink))\n\n1.521882058505192\n1.5123403841397614\n\n\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다."
  },
  {
    "objectID": "da/ida_03.html#범위",
    "href": "da/ida_03.html#범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "6. 범위",
    "text": "6. 범위\nRange\n관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다.\n\n# 범위 계산 (최대값 - 최소값)\nprint(np.max(drink) - np.min(drink))\n\n8.200000000000003"
  },
  {
    "objectID": "da/ida_03.html#사분위수범위",
    "href": "da/ida_03.html#사분위수범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "7. 사분위수범위",
    "text": "7. 사분위수범위\nQuartile\n전체 관측값을 작은 순서로 배열 하였을 때, 전체를 사등분 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n\n7-1. 사분위수범위\nInterquartile Range, IQR\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 std 는 자동적으로 n – 1 로 나누어져서 계산된다."
  },
  {
    "objectID": "da/ida_05.html",
    "href": "da/ida_05.html",
    "title": "5장: 확률",
    "section": "",
    "text": "통계적인 추론을 통해서도 모집단에 대한 다양한 정보를 얻을 수 있다. 그 통계적 추론의 기초가 되는 확률이론에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_05.html#사건의-확률",
    "href": "da/ida_05.html#사건의-확률",
    "title": "5장: 확률",
    "section": "1. 사건의 확률",
    "text": "1. 사건의 확률\n동일 조건하에서 한 가지 실험을 반복할 때, 전체 실험 횟수에서 그 사건이 일어나리라고 예상되는 횟수의 비율을 말한다. \n사건을 \\(A\\) 라고 하면, 사건 \\(A\\) 의 확률은 \\(P(A)\\) 로 표시한다.\n\n1-1. 표본공간\nSample Space: Ω\n한 실험에서 나올 수 있는 모든 결과들의 모임. 유한표본공간 ( Finite Sample Space )\n주사위 던지기의 표본공간\n연속표본공간 ( Continuous Sample Space )\n0과 1 사이의 모든 실수"
  },
  {
    "objectID": "da/ida_05.html#근원사건",
    "href": "da/ida_05.html#근원사건",
    "title": "5장: 확률",
    "section": "1-2. 근원사건",
    "text": "1-2. 근원사건\nElementary Outcomes: ω ₁ , ω ₂ … \n표본공간을 구성하는 개개의 결과.\n주사위 던지기의 근원사건\n\n1-3. 사건\nEvent: A, B, …\n표본공간의 부분집합으로, 어떤 특성을 갖는 결과들의 모임 (=근원사건들의 집합)\n주사위 던지기의 사건 A, B"
  },
  {
    "objectID": "da/ida_05.html#확률의-법칙",
    "href": "da/ida_05.html#확률의-법칙",
    "title": "5장: 확률",
    "section": "2. 확률의 법칙",
    "text": "2. 확률의 법칙\n위 정의로부터 확률의 특성을 유추할 수 있다.\n사건 A 가 일어날 확률은, 사건 A 에 속하는 근원사건이 일어날 확률의 “합” 과 같다. Ω 를 하나의 사건이라고 하면, 이 사건은 “반드시” 일어나므로 확률은 “1” 이 되어야 한다."
  },
  {
    "objectID": "da/ida_05.html#확률의-계산",
    "href": "da/ida_05.html#확률의-계산",
    "title": "5장: 확률",
    "section": "3. 확률의 계산",
    "text": "3. 확률의 계산\n\n3-1. 균일 확률\n주사위 던지기에서, 각 숫자가 나올 확률 Ω 가 k 개의 원소로 이루어져 있고, 각 근원사건이 일어날 가능성이 “동일” 하다고 가정할 때, 근원사건 중 하나가 일어날 확률은 1 / k 로 주어진다.\n또 사건 A 가 m 개의 근원사건으로 이루어져 있다면, 사건 A 가 일어날 확률은 위와 같다.\n\n\n3-2. 상대도수 수렴치로서의 확률\n주사위를 60번 던져 10번 2가 나왔을 때, 2가 나올 확률 동일한 실험 N 회를 반복할 때, 사건 A 의 상대도수는 위와 같이 표현된다.\nN 이 증가함에 따라 상대도수가 “일정한 값으로 수렴” 한다면, 그 값으로 사건 A 가 일어날 확률 P ( A ) 를 추정한다."
  },
  {
    "objectID": "da/ida_05.html#확률-법칙",
    "href": "da/ida_05.html#확률-법칙",
    "title": "5장: 확률",
    "section": "4. 확률 법칙",
    "text": "4. 확률 법칙\n여 사 건 ( Complementary Event ): 사건 A 가 일어나지 않는 사건 합 사 건 ( Sum Event ): 사건 A 또는 B 가 일어나는 사건 곱 사 건 ( Product Event ): 사건 A 또는 B 가 동시에 일어나는 사건 배 반 사 건 ( Exclusive Event ): 두 사건이 동시에 일어날 수 없는 경우"
  },
  {
    "objectID": "da/ida_05.html#조건부-확률",
    "href": "da/ida_05.html#조건부-확률",
    "title": "5장: 확률",
    "section": "5. 조건부 확률",
    "text": "5. 조건부 확률\nConditional Probability\n한 사건의 결과가 다른 사건의 발생에 영향을 미치는 확률.\n사건 B가 발생했을 때, 사건 A가 발생할 확률:\n곱 사 건 의 확 률 법 칙 ( Multiplication Rule for Probability ) 두 사건이 동시에 발생할 확률을 계산하는 방법. 이 법칙은 두 사건이 독립적인지 여부에 따라 다르게 적용된다:\n\n5-1. 표본공간의 분할\nPartition\n사건 A₁, A₂, …, An 이 서로 배반사건이고, Ω = A₁ ∪ … ∪ An 일 때, 사건 A₁, A₂, …, An 을 Ω 의 분할이라고 한다:\n이때, 각각의 부분집합은 서로 겹치지 않는다:\n예시: 주사위를 던지는 실험에서, Ω 를 두 개의 부분집합으로 나눌 수 있다.\nA₁ ​: 홀수가 나오는 경우 { 1, 3, 5 } A₂ ​: 짝수가 나오는 경우 { 2, 4, 6 }\n이 경우, Ω 는 다음과 같이 표현된다:\n\n\n5-2. 총확률의 법칙\nLaw of Total Probability\n표본공간의 분할 개념을 사용하여 전체 확률을 계산하는 방법.\n사건 A₁, A₂, …, An 이 표본공간의 분할일 때, 임의의 사건 B 의 확률 “P(B)” 는 다음과 같이 계산할 수 있다:\n이를 조건부 확률 ( 종속사건 ) 을 사용하여 다시 쓰면:\n예시: 사건 B 를 “주사위 눈이 4 이하인 사건” 으로 정의하면,\nP(B) = P({ 1, 2, 3, 4 }) = ⅔ P(A₁)​ = P({ 1, 3, 5 }) = ½ P(A₂)​ = P({ 2, 4, 6 }) = ½\n이 경우, P ( B ) 는 다음과 같이 표현된다:\n각 확률 값을 대입해보면:\n\n\n5-3. 베이즈 정리\nBayes’ rule\n사건 A₁, A₂, …, An 이 Ω 의 분할일 때, 임의의 사건 B 에 대하여 다음 식이 성립한다:\n예시: 사건 B 가 발생한 후, 사건 A₁ ​ 이 발생할 확률:"
  },
  {
    "objectID": "da/ida_07.html",
    "href": "da/ida_07.html",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "",
    "text": "모집단의 구성원들이 두 그룹으로 나누어져 있는 경우의 표본추출에서 광범위하게 쓰이는 확률모형과 그의 특징 및 관련된 다른 확률모형들을 다루고자 한다."
  },
  {
    "objectID": "da/ida_07.html#자료의-입력",
    "href": "da/ida_07.html#자료의-입력",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7 :  어떤 초등학교에서  10 년간 조사결과\n# 평균적으로  4 % 의 학생이 색맹인 것으로 나타났다고 한다. (p.213)\n\n# 올해에도 색맹인 학생의 비율이 예년과 같다고 할 때,\n# 임의로 추출된  200 명의 학생 중 색맹인 학생이  10 명 이하일 확률은 얼마인가?"
  },
  {
    "objectID": "da/ida_07.html#베르누이-시행",
    "href": "da/ida_07.html#베르누이-시행",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "2. 베르누이 시행",
    "text": "2. 베르누이 시행\nBernoulli Distribution\n모집단의 각 구성원이 두 그룹 중 하나에 속하는 경우, 각각의 구성원이 특정 그룹에 속할 확률 p 와 속하지 않을 확률 1 − p 를 따르는 이산 확률 분포.\n시 행 ( Trial ) : 매번 반복되는 추출( 실험 )\n2 개의 가능한 결과 중, 하나는 성공 ( Success, S ), 다른 하나는 실패 ( Failure, F ) 로 이름을 붙인다.\n이는 시행의 결과가 2 개 뿐임을 강조하며, 보통 우리가 관심이 있는 결과에 성공이란 이름을 붙인다.\n각 시행은 독립으로, 각 시행의 결과가 다른 시행의 결과에 영향을 미치지 않는다.\n일반적으로 복원 추출 ( Sampling With Replacement )로 간주된다. 각 시행 후 다시 원래의 상태로 복귀하여, 다음 시행에 영향을 주지 않는다.\n예제 7에 경우, 추출되는 학생은 색맹 ( S ) 또는 정상 ( F )으로 나눌 수 있다."
  },
  {
    "objectID": "da/ida_07.html#이항분포",
    "href": "da/ida_07.html#이항분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "3. 이항분포",
    "text": "3. 이항분포\nBinomial Distribution\n각 시도가 성공 또는 실패 두 가지 결과 중 하나를 가지는 독립적인 시행이 n 번 반복될 때, 성공 횟수를 나타내는 확률분포.\n성공할 확률이 p 인 베르누이 시행을 n 번 반복할 때 일어나는 성공의 횟수가 X 라면, 이 확률변수 X 는 모수가 ( n, p )인 이항분포를 따른다.\n모수(Parameter): 우리가 관심을 가지는 수치\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, x = 0, 1, …, n에 대하여 확률질량함수( PMF ) 는 다음과 같다:\n이 항 계 수 ( Binomial Coefficient )\n주어진 수의 집합에서 특정한 수의 원소를 선택하는 방법의 수. 조 합 ( Combination ) 으로도 알려져 있다.\n예제 7에서 주어진 정보를 바탕으로, 아래와 같이 이항 분포의 모수 ( 파라미터 ) 를 설정할 수 있다:\n성공 확률: p = 0.04 ( 학생이 색맹일 확률 ) 시도 횟수: n = 200 ( 추출된 학생 수 ) 성공 횟수: k ≤ 10 ( 색맹인 학생 수 )\n즉, 확률변수 X 를 200 명 중 색맹인 학생의 수 라고 하면, X는 모수가 ( n, p ) = ( 200, 0.04 ) 인 이항분포를 따르게 된다.\n이때, k ≤ 10 인 경우의 확률을 구해야 하므로, 누적 분포 함수 ( CDF ) 를 사용해야 한다:\n원하는 확률을 계산하려면, 다음 식을 계산해야 한다:\n다만, 위 식을 직접 계산하기에는 번거로운 면이 있다. 부록의 이항분포표에서도 n = 25 까지가 최대이기 때문이다.\n이럴 경우, 아래에서 설명할 포아송분포로 근사하여 계산할 수 있다.\n\nfrom scipy import stats\nstats.binom.cdf(10, 200, 0.04)\n\n## 출력된 값 &gt; 0.8199789826230907\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n\nnp.float64(0.8199789826230907)\n\n\n\n3-1. 이항분포의 기댓값과 표준편차\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:"
  },
  {
    "objectID": "da/ida_07.html#초기하분포",
    "href": "da/ida_07.html#초기하분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "4. 초기하분포",
    "text": "4. 초기하분포\nHypergeometric Distribution\n유한한 모집단에서 비복원 추출 ( Sampling Without Replacement )을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인(FPC)이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다. 결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다."
  },
  {
    "objectID": "da/ida_07.html#포아송분포",
    "href": "da/ida_07.html#포아송분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "5. 포아송분포",
    "text": "5. 포아송분포\nPoisson Distribution\n특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포. 주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n\n119 구조대에 시간당 걸려오는 전화횟수\n국내 발생하는 진도 4 이상 지진의 횟수\n\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\n\nfrom scipy import stats\nstats.poisson.cdf(10, 8)\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\nnp.float64(0.8158857925585467)\n\n\n\n참고용: Finite Population Correction Factor FPC"
  },
  {
    "objectID": "da/ida_09.html",
    "href": "da/ida_09.html",
    "title": "9장: 표집분포",
    "section": "",
    "text": "주어진 표본을 통해 모집단의 성격을 알아내는 과정을 추론 ( Inference )이라 한다. 그 통계적 추론에서 모집단의 특성을 추정하거나 가설 검정을 수행할 때, 사용되는 표집분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_09.html#자료의-입력",
    "href": "da/ida_09.html#자료의-입력",
    "title": "9장: 표집분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 4:  0 ~ 9 까지  10 개의 정수값을 균등하게 갖는 모집단이 있다.\n# (위 모집단의 분포를 이산균등분포라고 한다.) \n\n# 예시: 전화번호 끝자리 번호의 분포\n\n# (1) 여기에서 크기가  5 인 표본을  100 번 뽑아서 \n# (2) 매번 추출된 표본에서 표본평균을 구하고, \n# (3) 그 평균들을 가지고 히스토그램을 그려라. (p.268)"
  },
  {
    "objectID": "da/ida_09.html#통계량",
    "href": "da/ida_09.html#통계량",
    "title": "9장: 표집분포",
    "section": "2. 통계량",
    "text": "2. 통계량\nStatistic\n표본의 관측값들에 의하여 결정되는 양. 표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데, 이때 유념하여야 할 3가지 조건이 있다.\n\n표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n통계량의 값은 추출된 표본의 영향을 받는다:\n다른 표본을 추출할 때마다 통계량의 값은 변한다:"
  },
  {
    "objectID": "da/ida_09.html#표집분포",
    "href": "da/ida_09.html#표집분포",
    "title": "9장: 표집분포",
    "section": "3. 표집분포",
    "text": "3. 표집분포\nSampling Distribution\n통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n\n3-1. 불편추정량\nUnbiased Estimator 분포의 평균값이 추정하려는 모수와 일치하는 추정량. 불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n\n\n3-2. 편의추정량\nBiased Estimator\n분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다. 이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다. 따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n\n\n3-3. 임의표본\nRandom Sample 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn. 위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n\n\n3-4. 표본평균\nSample Mean\n모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:"
  },
  {
    "objectID": "da/ida_09.html#중심극한정리",
    "href": "da/ida_09.html#중심극한정리",
    "title": "9장: 표집분포",
    "section": "4. 중심극한정리",
    "text": "4. 중심극한정리\nCentral Limit Theorem, CLT 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이 표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\n\nimport numpy as np \n\na = np.random.randint(0, 100, size=5) \nb = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nc = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nd = np.random.randint(0, 100, size=5) \n\nprint(\"a :\", a) \nprint(\"b :\", b) \nprint(\"c :\", c)\nprint(\"d :\", d)\n\n## 해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n\na : [57  4 57 42 14]\nb : [79 71  3 65 21]\nc : [37 12 72  9 75]\nd : [37 12 72  9 75]\n\n\n( 2 ) 표본평균을 출력한다.\n\nimport numpy as np \n\nm = [] \n\nnp.random.seed(1234) \nfor i in range(100): \n    sample = np.random.randint(0, 10, size = 5) \n    m.append(np.mean(sample))\n    \nm = np.array(m)\nprint(m)\n\n[5.2 6.4 4.4 3.  5.  1.8 3.  3.4 5.4 7.8 3.4 4.8 4.8 4.8 6.4 4.8 4.4 6.\n 2.8 4.8 4.2 4.4 6.8 1.8 6.  4.6 3.2 2.4 2.8 6.2 3.2 6.8 5.8 5.8 4.4 5.\n 4.4 6.  3.6 4.8 4.8 4.  4.4 5.6 5.2 6.2 1.8 3.8 1.4 6.4 3.8 5.2 4.4 4.4\n 4.6 0.4 3.  5.8 2.2 4.  4.4 3.6 5.2 1.6 3.2 5.8 6.8 3.8 6.2 2.6 2.  4.8\n 2.6 6.  7.6 5.6 6.  3.6 4.2 3.8 5.2 3.4 8.2 4.6 6.8 3.8 4.2 3.8 4.6 2.4\n 4.  4.4 6.  4.4 2.6 3.4 6.  4.6 4.6 2.8]\n\n\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\n\nimport matplotlib.pyplot as plt \n\nplt.hist(m, bins=7)\nplt.xlabel('m') \nplt.ylabel('Frquency') \nplt.title('Historam of m')\n\n## 해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해\n## 정규분포에 가까우리라 예상할 수 있다.\n\nText(0.5, 1.0, 'Historam of m')\n\n\n\n\n\n\n\n\n\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8장에서 배운 정규확률그림을 그려본다.\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nsm.qqplot(m, line='s')\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 점들이 거의 직선상의 있으므로\n## 어느 정도 정규분포를 따른다고 할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida_11.html",
    "href": "da/ida_11.html",
    "title": "11장: 정규모집단에서의 추론",
    "section": "",
    "text": "표본의 크기가 작을 경우에는 일반적인 통계적 추론 방법을 적용하기 어려울 수 있다. 이 경우, 정규분포 대신 t – 분포를 이용한 통계적 추론 방법에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida_11.html#자료의-입력",
    "href": "da/ida_11.html#자료의-입력",
    "title": "11장: 정규모집단에서의 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 9: 예제 4에 주어져 있는 자료를 가지고 파이썬을 이용하여 \n# μ 에 대한 90% 신뢰구간을 구하고 예제 4에서 실시한 검정도 \n# 파이썬을 이용하여 다시 시행한 후 그 결과를 비교하라. (p.346)\n\n# ------------------------------------------------------------------------------\n\n# 예제 4: 어느 도시의 보건복지과에서는 \n# 그 도시의 상수원인 어느 호수의 수질에 관심이 있다고 한다. \n\n# 수질을 나타내는 하나의 수치로 단위부피당 평균 세균수가 있는데, \n# 그 수가 200 이상이면 상수원으로 적합 하지 않다고 한다. \n\n# 호수의 열 군데에서 물을 떠서 조사한 결과 단위부피당 세균수가 다음과 같이 나타났다. \n# 이 자료로부터 호수의 단위부피당 평균 세균수(μ)가 200보다 적다고 주장할 수 있겠는가? (p.330)\n\nimport numpy as np \nbacteria = np.array([175, 190, 215, 198, 184, 207, 210, 193, 196, 180])\n\n#_______________________________________________________________________________\n\n# 예제 10: 다음에 주어진 자료를 파이썬을 이용하여 분석하고자 한다. (p.348)\n\nx = np.array([31, 35, 37, 38, 38, 38, 39, 40, 40, 41, 42, 43, 44, 44, 46, 48])"
  },
  {
    "objectID": "da/ida_11.html#t-분포",
    "href": "da/ida_11.html#t-분포",
    "title": "11장: 정규모집단에서의 추론",
    "section": "2. t 분포",
    "text": "2. t 분포\nStudent’s t Distribution 통계학에서 모집단의 표본 평균이 정규분포를 따르지 않는 경우에도 사용가능한 분포. 모집단의 분포가 N ( μ, σ2 ) 일 때 크기가 n 인 표본의 평균 x̄ 의 분포는 정확하게 N ( μ, σ2 / n ) 이다.\n이를 표준화한 것이 아래와 같다:\n일반적으로 σ 는 미지수이므로 이를 표본의 표준편차 s 로 추정하여 사용한다. 표본의 크기가 큰 경우, s 로 대체하여도 그 분포가 큰 영향을 받지 않는다.\n그러나 표본의 크기가 작은 경우, 대체하게 되면 표준화된 확률변수의 분포는 표준정규분포와 달라지게 되며, 이를 t 분포 라고 한다.\n정규모집단 N ( μ, σ2 ) 으로부터 임의추출된 표본을 X 1, …, X n 이라고 할 때, 표본 평균과 표본 분산을 아래와 같이 정의한다:\n위 정의가 성립할 때 아래 식이 성립한다:\n자유도가 (n – 1)인 t 분포를 따르고, 이를 기호로써 t (n – 1)로 표현한다.\n표준정규분포와의\n공통점 : 0 을 중심으로 대칭 & 종모양 분포 차이점 : 양 꼬리부분에 상대적으로 많은 확률이 존재 → 더 두꺼운 꼬리를 갖는다. 이때 자유도가 증가하면, t 분포의 꼬리는 표준정규분포의 꼬리에 가까워진다:\ndf = 1 : 자유도가 1인 경우, df = 5 : 자유도가 5인 경우"
  },
  {
    "objectID": "da/ida_11.html#모평균에-대한-추론",
    "href": "da/ida_11.html#모평균에-대한-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3. 모평균에 대한 추론",
    "text": "3. 모평균에 대한 추론\n표본의 크기가 작거나 모집단이 정규분포를 따르지 않는 경우, t – 분포를 사용한다. 이때는 Z α / 2​ 대신 t α / 2 (n − 1) ​을 사용하여 신뢰구간을 계산해야 한다:\n위 식을 μ 에 대해 정리하면:\n10장: 통계적 추론 → 3 - 5 → (2)번의 식은 위와 같이 수정되어야 한다.\n표본 크기가 작을수록\nt – 분포의 임계값이 커지므로, 신뢰구간이 넓어져 표본표준편차의 불확실성을 반영한다. t – 분포를 사용하여 신뢰구간을 더 넓게 잡으므로, 정확한 추정에 도움이 된다. 표본 크기가 충분히 크다면, t – 분포와 정규분포가 거의 같아지므로 이 경우에만, Z – 분포를 사용할 수 있다.\n따라서:\n또는 아래와 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida_11.html#가설-검정",
    "href": "da/ida_11.html#가설-검정",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3-1. 가설 검정",
    "text": "3-1. 가설 검정\n검정통계량은 H 0 가 맞을 때 자유도가 ( n − 1 ) 인 t – 분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n검정통계량이 t 분포를 따르는 경우의 검정을 t 검정이라고 한다."
  },
  {
    "objectID": "da/ida_11.html#신뢰구간과-양측검정의-관계",
    "href": "da/ida_11.html#신뢰구간과-양측검정의-관계",
    "title": "11장: 정규모집단에서의 추론",
    "section": "4. 신뢰구간과 양측검정의 관계",
    "text": "4. 신뢰구간과 양측검정의 관계\nμ 에 대한 100 ( 1 − α ) % 신뢰구간은 아래와 같다:\nH 0 : μ = μ 0 에 대한 양측검정에서의 기각역은 유의수준이 α 일 때 아래와 같다:\n위 기각역의 여집합인 H 0 를 기각하지 못하고 받아들이는 영역을 ’ 채택영역 ’ 이라고 할 때\n이 채택영역은 다음과 같다:\n이를 μ 0 를 중심으로 풀어쓰면 다음과 같다:\n위 과정을 바탕으로 다음과 같은 결론을 내릴 수 있다:\n모수 θ 에 대한 100 ( 1 − α ) % 신뢰구간이 ( L, U ) 로 구해졌을 때, 가설 H 0 : θ = θ 0 대 H 1 : θ ≠ θ 0 에 대하여 유의수준 α 로 검정을 시행할 때의 결론을 의미한다."
  },
  {
    "objectID": "da/ida_11.html#모표준편차의-추론",
    "href": "da/ida_11.html#모표준편차의-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "5. 모표준편차의 추론",
    "text": "5. 모표준편차의 추론\n모표준편차의 추정과 검정에서는 정규성 가정이 중요한 역할을 한다. 이 가정이 충족되지 않으면, 신뢰구간 계산이나 가설 검정의 결과를 신뢰할 수 없다.\n모표준편차 σ 를 추정하는 과정은 모분산 σ 2 에 대한 추정에서 출발하며, 이 모분산 σ 2 에 대한 추정에서 사용되는 것이 표본분산이다:\n\n5-1. 점추정\nPoint Estimation 모집단의 모수를 단일 값으로 추정하는 방법이다. 모표준편차 σ 의 경우, 점추정은 표본표준편차를 사용하는 방식이다:\n\n\n5-2. 구간추정\nInterval Estimation 모집단의 모수를 특정 신뢰수준에서 포함할 것으로 예상되는 구간을 제공하는 방법이다. 모표준편차 σ 에 대한 구간추정은 모분산에 대한 신뢰구간을 기반으로 하여 계산되며, 이때 모분산 s 2 의 신뢰구간을 구하기 위해 카이제곱 분포를 사용한다.\n\n\n5-3. 카이제곱 분포\nChi-Square Distribution 표본 분산을 모집단 분산과 비교하거나 범주형 변수 간의 독립성을 검정할 때 유용한 분포.\n자유도에 따라 그 형태가 달라지며, 자유도가 커질수록 정규 분포와 유사해진다. 아래 수식은 표본 분산을 모집단 분산으로 표준화한 것이다:\n자유도가 (n – 1)인 x2 분포를 따르고, 이를 기호로써 x2 (n – 1)로 표현한다. 모집단 σ 2 의 신뢰구간을 계산하기 위해 카이제곱 통계량을 사용한다: 위 분포로부터 구한 신뢰구간은 아래 식과 같다:\n위 식에서 괄호 안에 있는 부등식을 σ 2 을 중심으로 풀어 쓰면 다음과 같다:\n따라서 이 식으로부터 σ 2 의 100 ( 1 − α ) % 신뢰구간을 구하면 다음과 같다:\n표준편차 σ 는 σ 2 의 양의 제곱근이므로,그의 신뢰구간은 σ 2 의 신뢰구간의 경곗값의 제곱근을 취하여 얻을 수 있다: 결론적으로, σ 에 대한 100 ( 1 − α ) % 신뢰구간은 위와 같다.\n검정통계량은 H 0 이 맞을 때 자유도 df 인 카이제곱분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 9의 ( 1 ) bacteria 에 대한 요약 통계량 계산하기.\n\nxbar_b = np.mean(bacteria);print(xbar_b) # 평균\n\nvar_b = np.var(bacteria, ddof=1);print(var_b) # 분산 (자유도 1 사용)\n\nsd_b = np.std(bacteria, ddof=1);print(sd_b) # 표준편차 (자유도 1 사용)\n\nmedian_b = np.median(bacteria);print(median_b) # 중앙값\n\n194.8\n172.62222222222226\n13.138577633146681\n194.5\n\n\n\nmin_b = np.min(bacteria);print(min_b) # 최솟값\n\nmax_b = np.max(bacteria);print(max_b) # 최댓값\n\nsum_b = np.sum(bacteria);print(sum_b) # 합계\n\nn = bacteria.size;print(n) # 데이터 개수\n\n175\n215\n1948\n10\n\n\n예제 9의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수 t – 분포의 백분위수 함수\n\nfrom scipy import stats \n\nse_b = stats.sem(bacteria); print(se_b) # 표본표준오차\n\n# 유의수준 0.1에 해당하는 t-분포의 임계값 \nt_alpha = stats.t.ppf(1 - 0.1 / 2, n - 1); print(t_alpha)\n\ninterval = t_alpha * se_b;print(interval) # 신뢰구간의 범위를 계산\n\nCI = [xbar_b - interval, xbar_b + interval]; print(CI) # 신뢰구간\n\n4.154783053568769\n1.8331129326536335\n7.6161865478670645\n[np.float64(187.18381345213294), np.float64(202.41618654786708)]\n\n\n90 % 신뢰구간은 194.8 ± 7.616 임을 알 수 있다.\n예제 9의 ( 3 ) 검정하고자 하는 가설은 H 0 : μ = 200 대 H 1 : μ &lt; 200 이며, 표본의 크기는 10 이다.\n\ntval = (xbar_b - 200) / se_b;print(tval) \n\n# 단측검정: 귀무가설 μ=200, 대립가설 μ&lt;200\npval = stats.t.cdf(tval, n - 1);print(pval)\n\n## 해석: P–값이 0.1211 로 유의수준 5% 에서 귀무가설을 기각할 수 없으므로\n## 주어진 10 개의 자료로부터 호수의 단위 부피당 평균세균수가 200 보다 \n## 적다고 안심할 수 없다.\n\n-1.2515695604210733\n0.12113884687382763\n\n\n예제 10의 ( 1 ) x 에 대한 요약 통계량 계산하기.\n\nxbar_x = np.mean(x);print(xbar_x) # 평균\n\nvar_x = np.var(x, ddof=1);print(var_x) # 분산 (자유도 1 사용)\n\nsd_x = np.std(x, ddof=1);print(sd_x) # 표준편차 (자유도 1 사용)\n\nmedian_x = np.median(x);print(median_x) # 중앙값\n\nmin_x = np.min(x);print(min_x) # 최솟값\n\nmax_x = np.max(x);print(max_x) # 최댓값\n\nsum_x = np.sum(x);print(sum_x) # 합계\n\nn = x.size;print(n) # 데이터 개수\n\n40.25\n18.2\n4.266145801540309\n40.0\n31\n48\n644\n16\n\n\n예제 10의 ( 2 ) x 에 대한 정규확률그림 그리기.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 정규확률그림 그리기\nsm.qqplot(x, line='s')\nplt.title(\"Normal Q-Q Plot\")\n\nText(0.5, 1.0, 'Normal Q-Q Plot')\n\n\n\n\n\n\n\n\n\n예제 10의 ( 3 ) 모평균 μ 에 대한 95% 신뢰구간을 구한다.\n\nfrom scipy import stats \n\nse = stats.sem(x);print(se) # 표준편차\n\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n - 1);print(t_alpha) # 95% 신뢰구간을 위한 t값 \n\ninterval = t_alpha * se;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_x - interval, xbar_x + interval];print(CI) # 신뢰구간\n\n1.0665364503850772\n2.131449545559323\n2.2732686324957263\n[np.float64(37.97673136750427), np.float64(42.52326863249573)]\n\n\n95 % 신뢰구간은 40.25 ± 2.273 임을 알 수 있다.\n예제 10의 ( 4 ) 검정하고자 하는 가설은 H 0 : μ = 38 대 H 1 : μ &gt; 38 이며, 표본의 크기는 16 이다.\n\ntval = (xbar_x - 38) / se;print(tval) # 표본표준오차\n\npval = 1 - stats.t.cdf(tval, n - 1);print(pval) # p값\n\n## 해석: P–값이 0.026 이므로 유의수준 5% 에서 귀무가설을 기각하게 된다.\n## 따라서, 평균이 38 보다 크다고 할 수 있다.\n\n2.109632539223229\n0.026050840503660355"
  },
  {
    "objectID": "da/ida/ida_12.html",
    "href": "da/ida/ida_12.html",
    "title": "12장: 두 모집단의 비교",
    "section": "",
    "text": "두 모집단의 비교를 위한 추론과정은 자료를 어떻게 수집하느냐에 따라 추론 방법이 달라진다. 대표적인 두 종류의 자료수집과정에 따른 추론방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_12.html#통계용어",
    "href": "da/ida/ida_12.html#통계용어",
    "title": "12장: 두 모집단의 비교",
    "section": "2. 통계용어",
    "text": "2. 통계용어\n비교 연구 시 자주 사용되는 통계용어.\n실험단위 ( Experimental Unit ) : 실험의 대상. 반응값 ( Response ) : 실험 후 얻어지는 수치. 처리 ( Treatment ) : 비교하고자 하는 특성.\n예제12를 위 통계용어로 다음과 같이 설명할 수 있다:\n숲 지역 ⇨ 처리 1 , 도시 지역 ⇨ 처리 2 각각의 스캐너 측정값 ⇨ 실험단위\n그 측정값의 수치 ⇨ 반응값"
  },
  {
    "objectID": "da/ida/ida_12.html#두-개-의-독-립-표-본",
    "href": "da/ida/ida_12.html#두-개-의-독-립-표-본",
    "title": "12장: 두 모집단의 비교",
    "section": "3. 두 개 의 독 립 표 본",
    "text": "3. 두 개 의 독 립 표 본\n독립인 두 개의 표본으로부터 두 모집단, 혹은 두 가지의 처리효과를 비교하는 통계추론의 방법. 다음은 두 모집단으로부터 추출된 표본과 그로부터 계산되는 통계량을 정리한 것이다: 두 모집단으로부터 추출된 표본.\n위 표본으로부터 계산되는 통계량. 여기서 우리의 관심사는 두 모집단의 평균 반응값의 차이다.\n\n3-1. 모평균의 차에 대한 추론\n두 모평균의 차 ( μ 1 – μ 2 ) 에 대한 추론을 위해서는 두 표본평균의 차 ( x̄ – ȳ )를 이용한다. 두 표본의 크기 n 1, n 2 가 모두 큰 경우 ( 30 이상 )중심극한정리에 의해 두 표본평균은 근사적으로 정규분포를 따른다:\n평균이 μ​ 1 ± μ 2​ 이고, 분산이 σ 12 ​+ σ 22 ​인 정규분포를 따른다:\n이때, X 와 Y 는 서로 독립이다. 복호동순 : 식에서 부호를 2 개 이상 사용할 때, 부호를 앞에서부터 같은 순서로 적용하는 것. 따라서, 두 독립적인 정규분포 변수의 차를 표준화하면 표준정규분포를 따르게 된다: 위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다: 추정량 ± (z값) × (추정된 표준오차)\n두 표본의 크기 n 1, n 2 가 모두 30 이상일 때, 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n델타( Δ , δ: 그리스 알파벳의 네번째 글자)\n검정통계량은 H 0 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-2. 모평균의 차에 대한 추론\n표본의 크기가 작을 경우, 두 모집단에 대하여 정규분포와 표준편차에 대한 가정이 필요하다.\n두 모집단이 모두 정규분포를 따른다. 두 모집단의 표준편차가 일치한다.\n값이 ½보다 작거나 2보다 큰 경우, 위 가정이 적절하지 못한다고 판단한다. 대부분의 경우, σ 를 모르므로 이를 추정하여야 한다.\nσ 에 대한 정보는 편차제곱합에 모두 들어 있다. 따라서 이 두 제곱합을 더하여 각각의 자유도의 합으로 나누어 σ 2 추정량으로 사용하게 된다.\n이를 공통분산 σ 2 의 합동추정량 ( Pooled Variance ) 이라 한다:\n위 식을 이용하여, ( μ 1 – μ 2 ) 에 대한 표준화된 확률변수는 다음과 같다:\n자유도가 ( n ₁ + n ₂ – 2 )인 t 분포를 따른다.\n위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다:\n( μ ₁ – μ ₂ ) 에 대한 신뢰구간은 추정량 ± ( t 값 ) × ( 추정된 표준오차 )의 형식에 의해 정리된다.\n두 모집단이 모두 정규분포를 따르고 두 모표준편차가 같은 때 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-3. 모평균의 차에 대한 추론\n표본의 크기가 작고, 두 모집단의 표준편차가 일치하지 않을 경우, 근사적으로 t 분포를 따르며, 자유도는 ( n 1 – 1 )과 ( n 2 – 1 ) 중 작은 값이다.\n이 분포를 이용한 ( μ 1 – μ 2 ) 에 대한 추론방법은 다음과 같다:\n모평균 차에 대한 100 ( 1 − α ) % 신뢰구간은 근사적으로 위 식과 같다.\n신뢰구간의 경우, 그 구간이 넓어지는 경향이 있다. 따라서 실제 신뢰도는 100 ( 1 − α ) % 이상이 된다.\n가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n검정의 경우, 기각역이 좁아지는 경향이 있다. 따라서 실제 유의수준이 α 이하가 되므로 귀무가설을 기각하지 못할 가능성이 높아진다."
  },
  {
    "objectID": "da/ida/ida_12.html#짝비교",
    "href": "da/ida/ida_12.html#짝비교",
    "title": "12장: 두 모집단의 비교",
    "section": "4. 짝비교",
    "text": "4. 짝비교\nMatched Pair Comparisons 실험 단위들이 비슷해야 한다는 점과 다양한 실험 단위들을 비교해야 한다는 점을 절충하는 접근법.\n짝지워서 각각의 쌍으로 만드는 방법:\n같은 쌍의 실험단위들은 서로 비슷해야 한다. 각 쌍 내에서 두 조건이 다르게 설정되어야 한다.\n짝 비교를 시행할 때의 자료의 형태:\n차의 표본평균과 분산은 다음과 같다:\nX i 와 Y i 는 서로 독립이 아니다. 서로 높은 상관관계를 가질 경우, 짝비교의 효과는 크다. 즉, 전체적인 변화의 폭 ( 변동성 ) 을 줄여 처리효과를 알아내기 수월한 짝비교를 할 수 있다.\n모평균 δ 에 대한 100 ( 1 − α ) % 신뢰구간은 다음과 같다:\n귀무가설 H 0 : δ = δ 0 에 대한 검정통계량은 다음과 같다:\nH ₀ 가 맞을 때 자유도가 ( n – 1 )인 t 분포를 따른다.\n자료가 짝지워져 있는 경우,두 개의 처리를 어떻게 배정할 것인가가 전혀 문제되지 않는다. 자료가 짝지워져 있지 않은 경우, 여러 조건들이 확률적으로 같은 정도로 영향이 미치도록 해야 한다. (어느 한쪽의 처리에만 영향을 주지 않아야 한다.)이와 같이 무작위로 배정하는 것을 랜덤화 ( Randomization ) 라고 한다."
  },
  {
    "objectID": "da/ida/ida_12.html#두-모비율의-차에-대한-추론",
    "href": "da/ida/ida_12.html#두-모비율의-차에-대한-추론",
    "title": "12장: 두 모집단의 비교",
    "section": "5. 두 모비율의 차에 대한 추론",
    "text": "5. 두 모비율의 차에 대한 추론\n두 모집단의 비율을 비교하는 추론하는 방법. 관심의 대상이 되는 어떤 특성의 모집단 1 의 비율을 p 1, 모집단 2 의 비율을 p 2 라고 할 때 두 모집단으로부터 크기가 n 1, n 2 인 표본을 추출하여 각각 특성이 A 인 것과 A 가 아닌 것으로 분류하였다고 가정한다.\n이때 A 를 ’ 성공 ‘, A 가 아닌 것을’ 실패 ’ 라고 하고 두 표본의 성공의 개수를 각각 X, Y 표현한다.\n두 모집단의 특성 A 의 비율을 각각 p 1, p 2 라고 하면 그 추정량은 각 표본으로부터 표본의 비율을 사용하게 된다:\n두 모비율의 차 ( p ₁ – p ₂ ) 의 추정량은 ( p̂ ₁ – p̂ ₂ ) 이 된다. ( p 1 – p 2 ) 에 대한 추론을 하기 위해서는 ( p̂ 1 – p̂ 2 ) 의 분포를 알아야 한다.\n표본의 크기 n 1, n 2 가 큰 경우, 아래 식이 근사적으로 성립한다:\n아래 식도 표본이 서로 독립이므로 정규분포로 근사된다:\n따라서 이를 표준화하면:\n두 확률변수 간의 차이를 표준화하는 것이다.\n위 식을 이용하여 ( p 1 – p 2 ) 에 대한 ( 1 − α ) % 신뢰구간은 다음과 같다:\n( 추정량 ) ± ( z 값 ) × ( 표준오차 )의 형식에 의해 정리된다.\n신뢰구간을 계산할 때, ( 제곱근 속의 ) 실제 모평균 차이 p 1, p 2 는 미지수이므로, 이를 표본 비율의 차이 p̂ 1, p̂ 2 로 대체하여 계산한다.\n\n5-1. 두 모비율의 검정\n표본의 크기가 클 때 두 모비율이 같은지를 검정하는 방법은 두 비율의 차이에 대한 검정을 통해 이루어진다.\n귀무가설 H 0 : p = p 0 을 검정하기 위해 (​ p̂ 1 – p̂ 2 ) 을 이용하게 된다. 이 가설이 맞을 경우의 통계량 분포는 다음과 같다:\np 는 모비율이 같은 두 모집단의 공통비율이다.\n통합된 두 표본으로부터 이를 추정하면:\n위 과정을 바탕으로 검정통계량을 정리할 수 있다:\np̂ 는 귀무가설하에서의 공통비율 p 의 추정량이고, 검정통계량은 H 0 가 맞을 때 근사적으로 N ( 0, 1 ) 을 따른다.\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 12의 ( 1 ) 추정량의 표준오차 ( se ) 를 계산하기 위해 두 변수의 요약통계량 ( s 12, s 22 ) 을 구하는 파이썬 함수를 사용할 수 있다.\n\nvar1 = np.var(x, ddof=1);print(var1) # x의 표본 분산 (자유도=1)\nvar2 = np.var(y, ddof=1);print(var2) # y의 표본 분산 (자유도=1)\n\nn1 = len(x);print(n1) # x의 데이터 수\nn2 = len(y);print(n2) # y의 데이터 수\n\nse = math.sqrt(var1 / n1 + var2 / n2);print(se) # 표준오차\n\n48.06374040272343\n24.789102564102567\n118\n40\n1.0134334699544658\n\n\n표준오차 계산 ( 1.0134 )\n예제 12의 ( 2 ) 모평균의 차 μ 1 – μ 2 에 대한 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 z–값\nz_alpha = stats.norm.ppf(1 - 0.05 / 2);print(z_alpha)\ninterval_z = z_alpha * se; print(interval_z) # 신뢰구간 범위\n\n# x와 y의 평균을 계산\nxbar1 = np.mean(x); print(xbar1) \nxbar2 = np.mean(y); print(xbar2) \n\n# 두 평균의 차이를 계산\ndiff = xbar1 - xbar2; print(diff) \n\n# 신뢰구간\nCI_1 = [diff - interval_z, diff + interval_z]; print(CI_1)\n\n\n## 해석: 95%의 신뢰구간이 0을 포함하지 않으므로 (양측검정에서) \n## 유의수준 5%에서 두 수치가 같다는 귀무가설 (H₀ : μ1 = μ2) 을 기각할 수 있다.\n\n1.959963984540054\n1.986293101838208\n92.9322033898305\n82.075\n10.857203389830502\n[np.float64(8.870910287992295), np.float64(12.84349649166871)]\n\n\n스캐너 자료 μ ₁ – μ ₂ 의 신뢰구간은 10.857 ± 1.986 이다.\n예제 12의 ( 3 ) 두 모집단이 모두 정규분포를 따르고, 분산이 같다는 가정 하에서 합동분산추정량 및 추정량의 표준오차를 구하라.\n\n# 합쳐진 분산 계산\nspooled = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2); print(spooled) \n\n# 합쳐진 표준 오차 계산\nse_spooled = math.sqrt(spooled) * math.sqrt(1 / n1 + 1 / n2); print(se_spooled)\n\n42.24508094306821\n1.1891745810061622\n\n\n합동추정량 ( 42.245 ) / 표준오차 ( 1.189 )\n예제 12의 ( 4 ) 위 결과를 바탕으로 t – 분포의 백분위수 함수를 이용하여 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 t–값\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n1 + n2 - 2); print(t_alpha) \n\n# 신뢰 구간의 범위\ninterval_t = t_alpha * se_spooled; print(interval_t) \n\n# 신뢰구간\nCI_2 = [diff - interval_t, diff + interval_t]; print(CI_2)\n\n## 해석: (2)번의 정규분포를 이용한 신뢰구간보다 \n## 오차범위 값인 interval_t가 더 크므로 신뢰구간이 더 넓어졌음을 알 수 있다.\n## 이는 t–분포의 백분위수 값이 (정규분포의 백분위수 값보다) 더 크기 때문이다.\n\n1.9752875076954723\n2.3489616943304696\n[np.float64(8.508241695500033), np.float64(13.206165084160972)]\n\n\n신뢰구간은 10.857 ± 2.348 이다."
  },
  {
    "objectID": "da/ida/ida_10.html",
    "href": "da/ida/ida_10.html",
    "title": "10장: 통계적 추론",
    "section": "",
    "text": "추출된 표본으로부터 모집단의 일반적인 특성을 추론해내는 것을 통계적 추론이라고 하며, 이는 표본의 크기가 클 때 더 정확하게 성립한다."
  },
  {
    "objectID": "da/ida/ida_10.html#자료의-입력",
    "href": "da/ida/ida_10.html#자료의-입력",
    "title": "10장: 통계적 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 13: 예제 1 에 주어진 자료에 근거해서 \n# 중학교 1학년 남학생의 평균키에 대한 95% 신뢰구간을 구하라. (p.311)\n\nimport numpy as np \n\nheight = np.array([163, 161, 168 , 161, 157, 162, 153, 159, 164, 170, \n                   152, 160, 157, 168, 150, 165, 156, 151, 162, 150, \n                   156, 152, 161, 165, 168, 167, 165, 168, 159, 156])\n                   \n\n#_____________________________________________________________________________\n\n# 예제 14: 예제 8의 검정을 시행하고 그 결과를 비교하여라. (p.313)\n\n# 예제 8: 앞의 예제 1에 주어진 중학생의 키 자료로부터 \n# 그 도시의 중학교 1학년 남학생의 평균키(m)가 다른 도시의 중학교 1학년 \n# 남학생의 평균키인 159 cm와 차이가 있다고 할 수 있는지 판단하라. (p.297)\n\n\n#_____________________________________________________________________________\n\n# 예제 15: 3 장의 예제 11 에 주어진 자료로부터 \n# 평균교통소음정도(μ)에 대한 98% 신뢰구간을 구하고, \n\n# 평균교통소음정도가 60 을 초과한다고 주장할 수 있는지 \n# 유의수준 5% 에서 가설검정을 실시하라. (p.314)\n\nnoise = np.array([55.9, 63.8, 57.2, 59.8, 65.7, 62.7, 60.8, 51.3, 61.8, 56.0, \n                  66.9, 56.8, 66.2, 64.6, 59.5, 63.1, 60.6, 62.0, 59.4, 67.2,\n                  63.6, 60.5, 66.8, 61.8, 64.8, 55.8, 55.7, 77.1, 62.1, 61.0, \n                  58.9, 60.0, 66.9, 61.7, 60.3, 51.5, 67.0, 60.2, 56.2, 59.4, \n                  67.9, 64.9, 55.7, 61.4, 62.6, 56.4, 56.4, 69.4, 57.6, 63.8])"
  },
  {
    "objectID": "da/ida/ida_10.html#통계적-추론",
    "href": "da/ida/ida_10.html#통계적-추론",
    "title": "10장: 통계적 추론",
    "section": "2. 통계적 추론",
    "text": "2. 통계적 추론\nStatistical Inference\n표본이 갖고 있는 정보를 분석하여 모수에 관한 결론을 유도하고, 모수에 대한 가설의 옳고 그름을 판단하는 것을 말한다.\n모집단의 일부인 표본으로부터 전체 모집단의 성질을 추론해내는 것이므로 100% 확실하다고 할 수는 없다. 따라서 통계적인 추론을 할 때에는 그 결론의 부정확한 정도를 반드시 언급하여야 하는데 이러한 정도를 수치로 표시할 수 있게 하는 도구로 앞에서 공부한 확률론과 표준분포 등이 이용된다.\n통계적 추론에는 두 가지 주요 방법이 있다:\n모수의 추정 모수에 대한 가설 검증"
  },
  {
    "objectID": "da/ida/ida_10.html#모평균의-추정",
    "href": "da/ida/ida_10.html#모평균의-추정",
    "title": "10장: 통계적 추론",
    "section": "3. 모평균의 추정",
    "text": "3. 모평균의 추정\nEstimation of Parameters\n모수 중 하나로 포함되는, 모집단의 평균에 대한 점추정과 구간추정을 다루고자 한다.\n\n3-1. 점추정\nPoint Estimation\n추정하고자 하는 하나의 모수에 대해, 여러 개의 확률변수를 사용하여 하나의 통계량을 만들고, 주어진 표본으로부터 그 값을 계산하여 하나의 수치를 제시하는 과정.\n모수 ( Parameter ) : 모집단의 실제 값으로, 우리가 알고 싶어하는 대상. 추정량 ( Estimator ) : 모수를 측정하기 위해 만들어진 통계량. 추정치 ( Estimate ) : 주어진 관측값으로부터 계산된 추정량의 실제 값.\n추정량은 하나의 확률변수이므로, 추출된 표본의 따라 그 값이 달라진다. 수치들의 변화의 정도는 추정량의 정확도와 관계가 있다. 이 정확도를 측정하는 도구 중 하나가 표준오차 ( 추정량의 표준편차 ) 이다.\n\n\n3-2. 표준오차\nStandard Error, SE\n추정량의 정확도를 평가하는 데 중요한 지표이며, 값이 작을수록 추정량이 모집단 모수를 더 정확하게 반영한다고 할 수 있다. 표본평균의 기댓값과 표준오차는, 모집단의 평균과 표준편차가 μ, σ 일 때 위와 같이 구할 수 있다.\n표본평균을 가지고 μ 를 추정할 경우, n 이 클수록 표준오차가 작아져 좀 더 정확한 추정이 가능하다. 그러나 표준오차 계산 시 σ 가 주어지지 않는 경우가 있다. 이 경우, σ 를 표본표준편차로 추정하여 사용할 수 있다.\n\n\n3-3. 구간추정\nInterval Estimation\n추정량의 분포를 이용하여 표본으로부터 모수 값을 포함하리라고 예상되는 구간을 제시하는 것. 이때 제시되는 구간을 신뢰구간이라고 한다.\n\n\n3-4. 신뢰구간\nConfidence Interval\n모집단의 어떤 모수를 추정하기 위해 계산된 범위. 신뢰구간은 ( L , U )의 형태로 이루어지며, 여기서 L 과 U 는 표본으로부터 계산된 통계량이다.\nL ( Lower bound ) : 신뢰구간의 하한값 U ( Upper bound ) : 신뢰구간의 상한값 따라서 표본마다 계산되는 신뢰구간은 서로 다를 수 있다.\n가장 확실한 신뢰구간은 항상 모수를 포함하는 구간이다. 그러나 이는 이론적으로는 가능하지만 실질적으로는 불가능하다. 그렇게 되기 위해서는 신뢰구간이 상당히 길어질 수 밖에 없다.\n이 경우, 신뢰구간 CI 는 매우 넓어질 수밖에 없다.\n항상 모수를 포함하는 신뢰구간은 실질적으로 너무 넓어서 유용하지 않다. 모수에 대한 정확한 정보를 얻으려면 신뢰구간을 가능한 한 줄일 필요가 있다.\n신뢰구간이 좁을수록 추정의 정확성이 높아진다.\n실용적인 측면에서 신뢰구간을 적절히 좁히기 위해, ” 모든 표본에서 항상 모수를 포함해야 한다 ” 는 엄격한 조건을 완화하고, 대부분의 경우에서 모수를 포함하도록 설정하는 것이 필요하다.\n신뢰구간이 모수를 포함할 확률을 1 보다는 작은 일정한 수준에 유지하여 구간의 길이를 줄이는 것이 바람직하다.\n이때 모수를 포함할 확률을 신뢰수준 ( Level of Confidence ) 또는 신뢰도라고 한다.\n\n\n3-5. 모평균 μ에 대한 신뢰구간\n여기에서는 신뢰구간을 계산하는 두 가지 경우를 설명한다:\n\n모집단의 표준편차 σ 를 알고 있는 경우:\n\n정규분포 개념을 이용하여 신뢰구간을 계산한다:\n신뢰수준 95 % 에 해당하는 정규분포의 임계값 Z α / 2 ​를 사용하여 신뢰구간의 범위를 설정한다. 예시 : α = 0.05 일 때, Z ₀.₀₅ / ₂ = Z ₀.₀₂₅ = 1.96\n정규분포는 평균 μ 와 표준편차 σ 를 알 때, 그 분포의 형태가 완전히 결정된다. 평균 μ 를 중심으로 좌우 대칭이며, σ 가 클수록 분포가 넓어진다. 이 특성 덕분에, 정규분포는 모수에 대해 많은 정보를 제공할 수 있다. 모집단이 정규분포를 따른다면, 표본 평균의 분포 역시 정규분포를 따른다.\n위의 식에서 괄호 안에 부등식을 풀어 쓰면:\n위 식을 μ 에 대해 정리하면:\n따라서:\n\n모집단의 표준편차 σ를 모르는 경우:\n일반적인 경우에 관심있는 모집단은 그 분포나 표준편차가 알려져 있지 않다. 이런 경우, 9장에서 다룬 중심극한정리를 이용한다.\n\n위 식은 약간의 수정이 필요한데, 이것은 11장을 참고하기를 바란다.\n이 경우에도, 확률 1 − α 는 근사적으로 얻어진다. n 이 클 때는 σ 대신 s 를 사용해도 확률값에 크게 영향을 주지 않는다.\n일반적으로 추정량의 기댓값이 추정하고자하는 모수값을 갖고 그 분포가 정규분포일 때 100 ( 1 − α ) % 신뢰구간은 다음과 같은 형태를 따른다:\n사용 상황에 따라 표준오차의 정의가 달라질 수 있다.\n\n\n3-6. 신뢰구간의 의미\n이 그래프는 주어진 모수에 대해 95% 신뢰구간을 표시하였다. 주어진 모수 (평균 100, 표준편차 10), 표본 크기 15를 사용하여 총 25개의 표본을 추출하였다.\n위 그래프와 같은 방식으로 표본을 계속 추출하고 신뢰구간을 계산하면, 그 신뢰구간들이 모평균을 포함하는 비율이 95% 에 가까워지는 것을 확인할 수 있다.\n\n\n3-7. 표본 크기의 결정\n많은 수의 표본을 추출하여 신뢰구간을 생성하는 것은 시간과 비용이 많이 소모된다. 따라서 우리가 원하는 정확도를 얻을 수 있는 범위 내에서 표본의 크기를 줄이는 것이 바람직하다.\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α ) % 가 되려면:\n표준화된 확률변수의 분포가 표준정규분포를 따르므로:\n위 사실로부터 아래 식이 성립해야 하며:\n이를 n 에 대하여 풀면 아래 식을 만족해야 한다:\n적정한 표본의 크기는 위의 부등식을 만족하는 ’ 최소의 정수 ’ 가 된다. 만약 모집단이 정규분포라는 가정이 없다면 표본의 크기는 중심극한정리를 이용할 수 있도록 30 이상이 되어야 한다."
  },
  {
    "objectID": "da/ida/ida_10.html#모평균에-대한-검정",
    "href": "da/ida/ida_10.html#모평균에-대한-검정",
    "title": "10장: 통계적 추론",
    "section": "4. 모평균에 대한 검정",
    "text": "4. 모평균에 대한 검정\n통계적 추론 중 하나인 모수에 대한 가설 검증 ( Testing Statistical Hypotheses ) 에 대해 다루고자 한다.\n\n4-1. 가설\nHypotheses 가설검증에는 2 개의 가설이 있다.\n대립가설 ( Alternative Hypothesis ; H ₁ ) : 입증하여 주장하고자 하는 가설.\n귀무가설 ( Null Hypothesis ; H ₀ ) : 대립가설을 입증할 수 없을 때, 대립가설을 무효화하면서 받아들이는 가설.\n\n\n4-2. 오류의 종류\n가설검증에서 내리는 판단이란 다음 2 가지 형태 중 하나로 나타난다. 상황에 따라 다르지만, 일반적으로 제1종 오류에 더 주의를 기울이게 된다.\n제1종 오류 ( Type I Error ; α ) : 귀무가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 없는데, 효과가 있다고 결론 내리는 경우.\n제2종 오류 ( Type II Error ; β ) : 대립가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 있는데, 효과가 없다고 결론 내리는 경우.\n\n\n4-3. 검정통계량\nTest Statistic 표본 데이터를 요약하여 귀무가설을 검정하는 데 사용하는 값. 귀무가설이 참이라는 가정 하에 표본에서 계산된 값으로, 이 값이 귀무가설 하에서 얼마나 극단적인지를 평가한다.\n\n\n4-4. 기각역\nCritical Region 귀무가설을 기각할 수 있는 값들의 집합.\n검정 통계량이 기각역에 속할 경우 귀무가설을 기각한다. 아닐 경우 귀무가설을 기각하지 않는다.\nc 이하이면 H₀ 을 기각한다.\n가장 바람직한 기각역이란 아래 두 확률을 최소화하는 것이 될 것이다.\nα : 제 1 종 오류를 범하게 될 확률. β : 제 2 종 오류를 범하게 될 확률.\n위 두 확률은 다음과 같은 특징이 있다:\n( 1 ) α 와 β 는 서로 반비례 관계에 있다.\n( 2 ) α 는 너무 크게 설정하지 않는 것이 좋다. 너무 큰 α 는 제1종 오류의 확률을 높여 잘못된 결론을 내릴 가능성을 높인다.\n이를 방지하기 위해, 유의수준이라는 상한선을 둘 수 있다.\n\n\n4-5. 유의수준\nSignificance Level 일반적으로 사용되는 유의수준은 0.05 ( 5% ) 또는 0.01 ( 1% ) 이다. 이는 연구자가 제1종 오류의 확률을 낮추어 신뢰할 수 있는 결과를 얻기 위함이다.\n0.05 ( 5% ) : 가장 흔히 사용되는 유의수준. 제1종 오류를 5% 로 제한. 0.01 ( 1% ) : 더 엄격한 기준으로, 제1종 오류를 1% 로 제한. 0.10 ( 10% ) : 덜 엄격한 기준으로, 제1종 오류를 10% 로 제한.\n양측 검정 ( Two — tailed Test ) : 유의 수준 α 는 두 방향 ( 양쪽 끝 ) 에 분배되어 α / 2 씩 기각역을 형성한다.\n단측 검정 ( One — tailed Test ) : 유의 수준 α 는 한 방향에 집중되어 기각역을 형성한다.\n\n\n4-6. 모평균 μ에 대한 검 정\n표본의 크기가 클 때 모평균 μ 에 대한 가설 H ₀ : μ = μ₀ 을 검정하기 위한 검정통계량은 다음과 같다: 단 모집단의 표준편차 σ 가 주어져 있을 때 s를 σ 로 대체한다.\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n4-7. 유의확률\nSignificance Probability 주어진 검정통계량의 관측치로부터 H ₀ 을 기각하게 하는 최소의 유의수준.\nP – 값 ( P – Value )\n일반적으로 유의확률은 주어진 관측값을 경계점으로 하는 기각역의 유의수준으로 얻어진다. Z = z 일 때 각 기각역의 형태에 따라 P – 값을 구하는 식을 정리하면 다음과 같다:"
  },
  {
    "objectID": "da/ida/ida_10.html#모비율에-대한-추론",
    "href": "da/ida/ida_10.html#모비율에-대한-추론",
    "title": "10장: 통계적 추론",
    "section": "5. 모비율에 대한 추론",
    "text": "5. 모비율에 대한 추론\n모집단에서 특정 속성을 가진 개체의 비율을 추정하거나 검정하는 것을 의미한다.\n\n5-1. 점추정\n모비율에 대한 추정량으로 표본비율을 사용할 수 있다: 점추정량이 결정되면 그 추정량의 정확도를 알기 위해 표준오차를 계산할 필요가 있다.\n모집단의 크기가 매우 커서 그에 비해 표본의 크기가 작은 경우, X 의 분포는 반복 횟수 n , 성공의 확률 p 인 이항분포가 된다. 따라서, X 의 기댓값과 분산는 아래와 같다:\nE ( X ) = np Var ( X ) = npq\n표본비율의 표준오차는 다음과 같이 계산된다:\n이 식은 이항분포의 분산을 이용한 결과이다. ​​ 표본비율​의 분산 은 아래 식과 같다:\n표본비율의 표준오차는 이 분산의 제곱근으로 표현된다.\n\n\n5-2. 구간추정\n모비율 P 에 대한 구간추정을 하려면 P 의 추정량인 p̂ 의 분포를 알아야 한다.\nX 의 분포는 이항분포를 따르므로, 중심극한정리를 이용하여 표본 비율 p̂​ 의 분포를 근사적으로 정규분포로 취급할 수 있다:\n분모, 분자를 n으로 나누었을 때, 위와 같은 식이 나온다.\n위 구간이 p를 포함할 확률이 ( 1 − α ) 가 됨을 알 수 있다.\n\n\n5-3. 신뢰구간\np 를 추정량인 p̂ 으로 대체하면, 원하는 신뢰구간을 구할 수 있다.\n\n\n5-4. 표본크기의 결정\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α )% 가 되려면:\n표본 크기가 클 경우, 정규분포를 이용하여 위와 같은 식을 구할 수 있다. 따라서, 표본 크기는 아래 식을 만족해야 한다:\n\n\n5-5. 모비율 p에 대한 검정\n표본 크기가 클 때 모비율 p 에 대한 가설 H ₀ : p = p ₀ 을 검정하기 위한 검정통계량은 다음과 같다:\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 13의 ( 1 ) height 에 대한 요약 통계량 계산하기.\n\nxbar_h = np.mean(height);print(xbar_h) # 평균\n\nvar_h = np.var(height, ddof=1);print(var_h) # 분산 (자유도 1 사용)\n\nsd_h = np.std(height, ddof=1);print(sd_h) # 표준편차 (자유도 1 사용)\n\nmedian_h = np.median(height);print(median_h) # 중앙값\n\nmin_h = np.min(height);print(min_h) # 최솟값\n\nmax_h = np.max(height);print(max_h) # 최댓값\n\nsum_h = np.sum(height);print(sum_h) # 합계\n\nn = height.size;print(n) # 데이터 개수\n\n160.2\n35.88965517241378\n5.990797540596225\n161.0\n150\n170\n4806\n30\n\n\n예제 13의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수. 정규분포의 백분위수 함수.\n\nfrom scipy import stats \n\nse_h = stats.sem(height);print(se_h) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.05 / 2); print(z_alpha) # 95% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_h;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_h - interval, xbar_h + interval]; print(CI) # 신뢰구간\n\n1.0937649834770078\n1.959963984540054\n2.1437399751659827\n[np.float64(158.05626002483402), np.float64(162.34373997516596)]\n\n\n이로부터 95% 신뢰구간은 160.2 ± 2.1437 임을 알 수 있다.\n예제 14 검정하고자 하는 가설은 H ₀ : μ = 159 대 H ₀ : μ ≠ 159 이며, 표본의 크기는 30 이상이다.\n\nzval = (xbar_h-159)/se_h;print(zval) # 가설 검정을 위한 z값\n\npval = 2 * (1 - stats.norm.cdf(zval));print(pval) # p값\n\n## 해석: p값이 크므로 귀무가설을 기각할 수 없음\n## → 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\n\n1.097127827392377\n0.27258551722126834\n\n\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고, 이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\n\nxbar_n = np.mean(noise);print(xbar_n) # 평균\n\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\n\nn = noise.size;print(n) # 데이터 개수\n\n61.373999999999995\n4.780137902544961\n50\n\n\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\n\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\n\n0.6760135851792765\n2.3263478740408408\n1.5726427667045366\n[np.float64(59.801357233295455), np.float64(62.946642766704535)]\n\n\n\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\n\npval = stats.norm.sf(np.abs(zval));print(pval) # p값\n\n## 해석: P—값이 0.021로 0.05보다 작게 나왔으므로 \n## 유의수준 5% 에서 귀무가설을 기각할 수 있다.\n\n## 그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\n\n2.032503532655509\n0.021051353256926374"
  },
  {
    "objectID": "da/ida/ida_08.html",
    "href": "da/ida/ida_08.html",
    "title": "8장: 정규분포",
    "section": "",
    "text": "6장에서 언급되었던, 연속적인 값을 가지는 연속확률분포들 중에서 대부분의 통계학 이론의 기본이 되는 정규분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_08.html#자료의-입력",
    "href": "da/ida/ida_08.html#자료의-입력",
    "title": "8장: 정규분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n!pip install numpy\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7: 어느 대학교의 일반수학 중간고사 성적은\n# 분포가 평균이  63 이고, 분산이  100 인 정규분포를 따른다고 가정한다. (p.232)\n\n# (1) 50 점 이하의 학생은 몇 퍼센트나 되겠는가?\n\n# (2) 상위 10 %의 학생에게  A 를 준다고 하면 \n# 몇 점 이상이 되어야  A 를 받을 수 있겠는가?\n\n# ___________________________________________________________\n\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\n\nimport numpy as np \n\ndata1 = np.array([4001, 3927, 3048, 4298, 4000, 3445, \n                 4949, 3530, 3075, 4012, 3797, 3550, \n                 4027, 3571, 3738, 5157, 3598, 4749, \n                 4263, 3894, 4262, 4232, 3852, 4256, \n                 3271, 4315, 3078, 3607, 3889, 3147, \n                 3421, 3531, 3987, 4120, 4349, 4071, \n                 3683, 3332, 3285, 3739, 3544, 4103, \n                 3401, 3601, 3717, 4846, 5005, 3991, \n                 2866, 3561, 4003, 4387, 3510, 2884, \n                 3819, 3173, 3470, 3340, 3214, 3670, \n                 3694])\n                 \n# ___________________________________________________________\n\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\n\ndata2 = np.array([39.3, 14.8, 6.3, 0.9, 6.5, \n                 3.5, 8.3, 10.0, 1.3, 7.1, \n                 6.0, 17.1, 16.8, 0.7, 7,9, \n                 2.7, 26.2, 24.3, 17.7, 3.2, \n                 7.4, 6.6, 5.2, 8.3, 5.9, \n                 3.5, 8.3, 44.8, 8.3, 13.4, \n                 19.4, 19.0, 14.1, 1.9, 12.0, \n                 19.7, 10.3, 3,4, 16.7, 4.3, \n                 1.0, 7.6, 28.33, 26.2, 31.7, \n                 8.7, 18.9, 3.4, 10.0])\n\nRequirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.6)\n\n\nWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\nYou should consider upgrading via the 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command."
  },
  {
    "objectID": "da/ida/ida_08.html#연속확률분포",
    "href": "da/ida/ida_08.html#연속확률분포",
    "title": "8장: 정규분포",
    "section": "2. 연속확률분포",
    "text": "2. 연속확률분포\nContinuous Probability Distribution\n연속확률변수 X 가 가질 수 있는 값들의 분포를 나타낸다. 연속확률변수: 특정 범위 내에서 모든 실수 값을 가질 수 있는 변수."
  },
  {
    "objectID": "da/ida/ida_08.html#확률밀도함수",
    "href": "da/ida/ida_08.html#확률밀도함수",
    "title": "8장: 정규분포",
    "section": "2-1. 확률밀도함수",
    "text": "2-1. 확률밀도함수\nProbability Density Function, PDF\n연속확률변수 \\(X\\) 의 확률분포는 확률의 밀도를 나타내는 \\(X\\) 의 확률밀도함수에 의해 결정된다. 아래의 조건을 만족하는 함수 \\(f(X)\\) 를 \\(X\\) 의 확률밀도함수라고 한다.\n\n연속확률변수에서 특정 값에 대한 확률은 0 이다. 예를 들어, [ 0, 1 ] 구간에 있는 실수 값은 무한히 많다. 이러한 경우, 특정한 하나의 값을 가질 확률은 무한히 작은 값이 되며, 이는 수학적으로 0 으로 표현된다.\n\n범위 : 확률밀도함수 f ( x ) 의 값은 항상 0 이상이어야 한다. 이는, 어떤 사건의 확률이 음수일 수 없다는 것을 의미한다.\n확률밀도함수는 이산확률분포의 확률함수와는 달리 확률을 나타내지 않는다. 그러므로, f ( x ) 가 1 보다 작아야 된다는 조건 은 필요가 없다.\n\nPDF 를 사용하여 특정 구간 [ a, b ] 에 속할 확률을 계산할 수 있다.\nPDF 를 전체 범위에 대해 적분 하면 1 이 되어야 한다.\n\n위 조건으로부터 다음과 같은 결론을 내릴 수 있다:\n어떤 구간의 확률을 구할 때에는 그 구간의 경계점이 포함되는가 포함되지 않는가에 영향을 받지 않는다."
  },
  {
    "objectID": "da/ida/ida_08.html#정규분포",
    "href": "da/ida/ida_08.html#정규분포",
    "title": "8장: 정규분포",
    "section": "3. 정규분포",
    "text": "3. 정규분포\nNormal Distribution\n평균 \\(μ\\) 와 분산 \\(σ^2\\) 의해서 그 분포가 확정된다. 그 확률밀도함수 ( PDF ) 의 대략적인 특성은 다음과 같이 표현할 수 있다:\n위 정규분포는 N ( μ, σ² )으로 표시할 수 있다. X 가 평균 μ 로부터 ± σ, ± 2σ, ± 3σ, ± 4σ 의 사이의 있을 확률은 다음과 같다.\n이를 통해 ± 4σ 이상 떨어진 데이터는 매우 드문 현상임을 알 수 있다. 그러므로, 정규분포의 ± 4σ 이상의 영역은 실질적인 분석에서 종종 무시할 수 있다."
  },
  {
    "objectID": "da/ida/ida_08.html#정규분포의-특성",
    "href": "da/ida/ida_08.html#정규분포의-특성",
    "title": "8장: 정규분포",
    "section": "3-1. 정규분포의 특성",
    "text": "3-1. 정규분포의 특성\nμ ± 3σ 안에 거의 모든 확률이 집중된다.\n3 - 2 . 표 준 정 규 분 포 ( Standard Normal Distribution ) 평균 ( μ ) 이 0 이고 표준편차 ( σ )가 1 인 특수한 정규분포.\n확률변수 Z 가 N ( 0, 1 ) 이라고 할 때, Z 는 0 을 중심으로 대칭인 분포를 갖게 되며,\n이를 이용해 다음과 같이 나타낼 수 있다:\n누적분포함수 ( CDF ) 는 특정 값 이하의 확률을 나타낸다.\nP ( Z ≤ b ) 는 Z 가 b 이하일 확률이고, P ( Z ≤ a ) 는 Z 가 a 이하일 확률이다. 따라서 a 에서 b 까지의 확률은 다음과 같다:\n\n3-3. 표준정규확률변수\nStandard Normal Random Variable\n표준정규분포 Z 에 관한 확률계산 방법을 일반 정규분포 X 의 확률계산에 적용할 수 있다. 일반 정규분포 X 를 표준정규분포 Z 로의 식변환을 통해 쉽게 계산할 수 있으며, 이를 표준화라고 한다.\n확률변수 X 가 N ( μ, σ² ) 일 때 표준화된 확률변수 Z 는 정규분포 N ( 0, 1 ) 을 따른다. X 를 표준화하여 Z 로 표현한 후 표준정규분포표를 이용하면 된다.\n예제 7의 ( 1 ) 중간고사 성적을 확률변수 X 라고 하면 주어진 정보를 바탕으로 다음과 같이 표현할 수 있다:\n평균 (μ) = 63, 분산 (σ²) = 100 = 10², 표준편차 (σ) = 10 정규분포표의 −1.3행, 0.00열의 값 = 0.0968\n\nfrom scipy.stats import norm\nprint(norm.cdf(x=50, loc=63, scale=10))\n\n## 해석: 50 점 이하의 학생의 비율은  0.0968 = 9.68% 이다.\n\n0.09680048458561036\n\n\n예제 7의 ( 2 ) x 점 이상의 학생들에게 A 를 준다고 하면 x 는 다음을 만족해야 한다: 상위 10 % = 0.10\n먼저, 표준정규분포표로부터 P [Z ≥ z] = 0.10 이 되는 z 값을 찾는다: 이 경우, 0.10 에 가까운 값을 주는 z = 1.28 을 고르면 된다:\n\nfrom scipy.stats import norm\nprint(norm.ppf(q=0.9, loc=63, scale=10))\n\n## 해석: 상위 10 %의 학생에게  A 를 준다고 하면 \n## 75.8 점 이상의 점수를 받은 학생에게 주면 된다.\n\n75.815515655446"
  },
  {
    "objectID": "da/ida/ida_08.html#이항분포의-정규분포근사",
    "href": "da/ida/ida_08.html#이항분포의-정규분포근사",
    "title": "8장: 정규분포",
    "section": "4. 이항분포의 정규분포근사",
    "text": "4. 이항분포의 정규분포근사\n대규모 시행에서 이항 분포를 정규 분포로 근사하는 방법. 7장의 초기하 분포나, 포아송 분포 마찬가지로, 정규 분포로도 근사하여 계산할 수 있다.\n이항 분포는 n 이 커짐에 따라 근사적으로 정규분포를 따르게 된다. 이때 정규분포의 평균과 분산은 이항분포의에서와 일치하여야 한다:"
  },
  {
    "objectID": "da/ida/ida_08.html#연속성수정",
    "href": "da/ida/ida_08.html#연속성수정",
    "title": "8장: 정규분포",
    "section": "4-1. 연속성수정",
    "text": "4-1. 연속성수정\nContinuity Correction\n이항 분포의 이산적인 값을 연속적인 정규 분포의 구간에 맞추는 작업. 보통 ± ½ ​를 가감하여 근삿값을 계산한다.\n\\[\nX ~ Bin (n, p) ≈ Y \\~ N (μ, σ²)\n\\]\n확률변수 X 가 이항분포, 즉 X ~ Bin ( n, p ) 이고, np, nq ( =1 − p ) 가 모두 클 경우 ( 보통 10 이상 ) X 는 근사적으로 평균이 np, 표준편차가 √npq 인 정규분포를 따른다."
  },
  {
    "objectID": "da/ida/ida_08.html#정규분포가정의-조사",
    "href": "da/ida/ida_08.html#정규분포가정의-조사",
    "title": "8장: 정규분포",
    "section": "5. 정규분포가정의 조사",
    "text": "5. 정규분포가정의 조사\n모집단의 분포가 정규 분포를 따른다는 가정을 조사하기 위해 사용할 수 있는 효과적인 그림. 정규점수그림 ( Normal Scores Plot ) 또는 정규확률그림 ( Normal Probability Plot ) 으로 불린다.\n\n5-1. 정규점수\nNormal Scores\n표본 데이터와 표준 정규 분포 ( 평균 0, 표준편차 1 ) 를 비교하여 데이터의 정규성을 평가하는 데 사용된다. 정규성( Normality ) : 데이터가 정규 분포를 따르는지를 의미한다.\n즉, 표준정규분포의 확률밀도함수 ( PDF ) 를 등확률 구간으로 나누어 주는 경곗값 ( z 값 ) 을 의미한다. 그림으로부터 분포의 형태를 알기 위해선 자료의 크기가 적어도 15 이상은 되어야 한다.\n\n\n5-2. 정규확률그림 그리는 순서\n자료를 작은 것부터 크기순으로 나열한다. 각 자료에 해당하는 점수를 계산한다. i 번째 순서의 자료와 i 번째 순서의 정규점수를 하나의 쌍으로 2 차원 공간상에 나타낸다.\n\n\n5-3. 정규그림의 해석\n정규분포를 따른다면, 양쪽의 값은 서로 가까울 것이라고 예상할 수 있다.\n\n\n5-3. 정규확률그림을 이용한 정규성 판정\n직선식일 경우, 정규분포의 가정이 타당하다고 볼 수 있다. (곡선 등) 직선식을 벗어날 경우, 가정이 의문시된다고 할 수 있다.\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsm.qqplot(data1, line = \"s\")\n\n# \"s\"는 \"standardized\"를 의미한다. \n\n# 이 옵션은 플롯에 표준화된 선\n# (평균 0, 표준편차 1의 정규 분포)에 맞추어 선을 그린다.\n\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에 가까우므로 \n## 데이터가 정규분포를 따를 가능성이 높다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n\n5-4. 자료의 변환\n만약 추출된 표본이 정규확률그림 등에서 정규분포와 상당히 벗어난 것으로 판명되면, 자료의 변환을 통해 정규분포의 형태를 갖도록 시도해 볼 수 있다.\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\nimport matplotlib.pyplot as plt\n\nplt.hist(data2, bins=5, range=(0,50)) \nplt.xlabel('data2') \nplt.ylabel('Density') \nplt.title('Historam of data')\n\n## 해석: 아래 히스토그램을 보면 자료의 분포가 왼쪽으로 편중되어 있으므로\n## 정규분포가 아니라는 의심을 할 수 있다.\n\nText(0.5, 1.0, 'Historam of data')\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nsm.qqplot(data2, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에서 크게 벗어나\n## 데이터가 정규 분포를 따르지 않을 가능성이 있음을 확인할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n이 경우, 큰 자룟값을 더 작게 만드는 과정을 수행한다:\n\n# 원 자료를 제곱근(체적^(0.5))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata3 = np.sqrt(data2)\nsm.qqplot(data3, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n# 원 자료를 네제곱근(체적^(0.25))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata4 = np.power(data2, 0.25)\nsm.qqplot(data4, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida/ida_06.html",
    "href": "da/ida/ida_06.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "5장에서 다룬 표본공간의 근원사건들은 특성을 표현하는 형태로 다뤘다. 이제는 확률변수를 중심으로 실험의 수치적 결과에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_06.html#확률변수",
    "href": "da/ida/ida_06.html#확률변수",
    "title": "6장: 확률분포",
    "section": "1. 확률변수",
    "text": "1. 확률변수\nRandom Variable\n각각의 근원사건들에 실수값을 “대응시키는 함수”이며 X, Y, … 등으로 표시한다.\n확률변수가 가질 수 있는 “값의 개수” 가 유한하거나 무한이라도 “셀 수 있는 경우” 에 이를 “이산확률변수” 라고 한다.\n또한, 연속적인 구간에 속하는 모든 값을 다 가질 수 있는 “연속확률변수” 도 있다."
  },
  {
    "objectID": "da/ida/ida_06.html#이산확률분포",
    "href": "da/ida/ida_06.html#이산확률분포",
    "title": "6장: 확률분포",
    "section": "2. 이산확률분포",
    "text": "2. 이산확률분포\nDiscrete Probability Distribution\n확률변수가 갖는 값들과 그에 “대응하는 확률값” 을 나타내는 것으로, 나열된 표나 수식으로 표현되며, 보통은 “확률변수 X 의 분포” 라고 한다.\n\n2-1. 확률질량함수\nProbability Mass Function, PMF\n확률변수 X가 k개의 값 x₁, x₂, …, xk를 가질 때, 그에 대응하는 확률을 f(x₁), f(x₂), …, f(xk)라고 하면 X의 확률분포는 다음과 같다:\nf(x)는 확률변수 X 가 값 x 를 갖게 되는 확률 P ( X = x ) 을 나타낸다:\n이산확률변수 X 의 확률변수는 다음 조건을 만족해야 한다:\n모든 확률은 0 이상 1 이하의 값을 가진다. 확률변수가 가질 수 있는 모든 값에 대한 확률의 합은 1 이다."
  },
  {
    "objectID": "da/ida/ida_06.html#이산확률변수의-평균과-표준편차",
    "href": "da/ida/ida_06.html#이산확률변수의-평균과-표준편차",
    "title": "6장: 확률분포",
    "section": "3. 이산확률변수의 평균과 표준편차",
    "text": "3. 이산확률변수의 평균과 표준편차\n\n3-1. 기댓값\nExpected Value\nE ( X ) 는 확률변수 X 의 “기댓값(평균)” 또는 X 가 갖는 확률분포의 “모평균” 이라고 한다.\n뮤( M , μ: 그리스 알파벳의 열두째 글자)\n3-2. 모분산 Population Variance\n편차 (즉, 각 값이 기대값에서 얼마나 떨어져 있는지)를 제곱하고, 그 제곱된 값을 각 값이 발생할 확률로 가중평균하는 것이다.\nV a r ( X ) = ∑ ( 편차 ) ² × 확률:\n시그마(Σ, σ: 그리스어 알파벳의 열여덟째 글자)\n모분산의 간편식:\n기댓값을 알고 있다면, 직접적인 정의를 사용하지 않고 게산할 수 있다.\n\n\n3-3. 모표준편차\nPopulation Standard Deviation 모분산의 양의 제곱근으로 계산된다:\n모표준편차 ( σ ) 의 단위는 확률변수 X와 “동일” 하다. 반면 모분산 ( σ² ) 의 단위는 X 의 단위를 “제곱” 한 것이므로, 퍼진정도를 측정하는데 적절하지 않다.\n예를 들어, X 의 단위가 센티미터( cm )라면, 모분산 σ² 의 단위는 제곱센티미터( cm² )가 된다."
  },
  {
    "objectID": "da/ida/ida_06.html#두-확률분포의-결합분포",
    "href": "da/ida/ida_06.html#두-확률분포의-결합분포",
    "title": "6장: 확률분포",
    "section": "4. 두 확률분포의 결합분포",
    "text": "4. 두 확률분포의 결합분포\n하나의 실험에서도 2 개 이상의 측면에 대한 관측이 가능하다. 이 경우 그 2 가지 특성 간의 관계 여부 및 그 관계 정도에 대해 분석할 수 있다.\n\n4-1. 결합확률분포\nJoint Probability Distribution\n2개 이상의 확률변수가 동시에 특정한 값을 가질 확률을 나타내는 분포이다.\n2 개의 확률변수가 이산일 경우,\nX 가 취하는 값을 x₁, …, xm Y 가 취하는 값을 y₁, …, yn 이라고 할 때\nX 와 Y 의 결합확률분포는 모든 1 ≤ i ≤ m, 1 ≤ j ≤ n 에 대하여\n위 식을 구하므로써 결정되며, 다음과 같이 표현할 수 있다:\n\n\n4-2. 주변확률분포\nMarginal Probability Distribution\n결합확률분포에서 한 확률변수를 “고정” 하고 다른 변수의 분포를 고려하는 분포이다. 이는 2 개 이상의 확률변수 중 하나에 대한 “단일 확률분포” 를 얻기 위해 사용한다:\n각각의 주변확률을 이용해서 하나의 변수 때와 마찬가지로 구하면 된다:"
  },
  {
    "objectID": "da/ida/ida_06.html#공분산과-상관계수",
    "href": "da/ida/ida_06.html#공분산과-상관계수",
    "title": "6장: 확률분포",
    "section": "5. 공분산과 상관계수",
    "text": "5. 공분산과 상관계수"
  },
  {
    "objectID": "da/ida/ida_06.html#공분산",
    "href": "da/ida/ida_06.html#공분산",
    "title": "6장: 확률분포",
    "section": "5-1. 공분산",
    "text": "5-1. 공분산\nCovariance\n두 확률변수 X 와 Y 가 함께 변하는 정도를 측정한다.\nX, Y가 같은 방향으로 변화할 경우, (즉, 둘 다 증가하거나 둘 다 감소하는 경우) ( X − μX ​), ( Y − μY ​) 의 부호가 일치할 확률이 상대적으로 커진다. 따라서, 이에 대한 기댓값은 양수가 된다.\nX, Y가 다른 방향으로 변화할 경우, (즉, 한 변수가 증가할 때 다른 변수는 감소하는 경우) ( X − μX ​), ( Y − μY ​) 가 서로 다른 부호를 갖게 될 확률이 상대적으로 커진다. 따라서 이에 대한 기댓값은 음수가 될 것이다.\nX, Y 의 공분산은 아래와 같이 정의된다:"
  },
  {
    "objectID": "da/ida/ida_06.html#상관계수",
    "href": "da/ida/ida_06.html#상관계수",
    "title": "6장: 확률분포",
    "section": "5-2. 상관계수",
    "text": "5-2. 상관계수\nCorrelation Coefficient\n두 확률변수 간의 선형 관계의 강도와 방향을 측정한다. 이것은 공분산을 표준화한 형태로, −1 과 1 사이의 값을 가진다.\n상관계수의 성질:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n정확한 선형관계 Y = aX + b 가 성립할 때, 상관계수의 값은 −1 또는 1 이다.\nX, Y 의 상관계수는 각 확률변수에 상수가 더해지거나 감해지는 것에 영향을 받지 않는다. 상수가 곱해진 경우, 그 부호에만 영향을 받는다.\n상수 c 와 d 의 부호가 다르면 상관계수의 부호가 반대가 된다."
  },
  {
    "objectID": "da/ida/ida_06.html#두-확률변수의-독립성",
    "href": "da/ida/ida_06.html#두-확률변수의-독립성",
    "title": "6장: 확률분포",
    "section": "6. 두 확률변수의 독립성",
    "text": "6. 두 확률변수의 독립성\n2 개의 확률변수 X, Y 가 독립이 되기 위해서는 X, Y 가 취하는 모든 쌍의 값 ( xi, yi ) 에 대해 아래 식을 만족해야 한다.\n두 확률변수 X, Y 가 서로 독립일 때, 아래의 식이 성립한다:\n단, 공분산과 상관계수가 0 이라는 사실이 항상 두 변수가 독립이라는 것을 보장하지 않는다. 이는 두 변수 간에 선형 관계가 없음을 의미하지만, 비선형 관계가 존재할 수 있다.\n두 확률변수가 독립일 경우, 공분산이 0 이 되므로 합과 차의 분산을 쉽게 계산할 수 있다.\n분산과 공분산의 정의를 이용하면: 공분산 항을 제외하여 다음과 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida/ida_04.html",
    "href": "da/ida/ida_04.html",
    "title": "4장: 두 변수 자료의 요약",
    "section": "",
    "text": "조사 대상의 각 개체로부터 둘 또는 그 이상의 변수들을 동시에 관측하는 경우가 더 많다. 두 변수에 관한 관측값을 도표로 요약하고 해석하는 방법을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_04.html#자료의-입력",
    "href": "da/ida/ida_04.html#자료의-입력",
    "title": "4장: 두 변수 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제5: 통계학과 신입생 51명의 키와 몸무게를 기록한 것이다.\n# 키와 몸무게의 표본상관계수를 구하고, 산점도를 그려라. (p.108)\n\nimport numpy as np\n\n# 키와 몸무게 자료의 입력\nheight = np.array([181,161,170,160,158,168,162,179,183,178,171,177,163,\n                   158,160,160,158,173,160,163,167,165,163,173,178,170,\n                   167,177,175,169,152,158,160,160,159,180,169,162,178,\n                   173,173,171,171,170,160,167,168,166,164,173,180]) \n\nweight = np.array([78,49,52,53,50,57,53,54,71,73,55,73,51,53,65,48,59,\n                   64,48,53,78,45,56,70,68,59,55,64,59,55,38,45,50,46,\n                   50,63,71,52,74,52,61,65,68,57,47,48,58,59,55,74,74])"
  },
  {
    "objectID": "da/ida/ida_04.html#표본상관계수",
    "href": "da/ida/ida_04.html#표본상관계수",
    "title": "4장: 두 변수 자료의 요약",
    "section": "2. 표본상관계수",
    "text": "2. 표본상관계수\nSample Correlation Coefficient\n산점도에서 점들이 얼마나 “직선에 가까운가” 의 정도를 나타내는 데 쓰이는 측도.\n두 변수 ( x, y ) 에 대하여 관측값 n개의 짝 ( x₁, y₁ ), ( x₂, y₂ ), …, (xn, yn) 이 주어진 때, 상관계수는 다음과 같이 계산된다:\n\n2-1. 공분산\nCovariance\n분산(Variance)은 한 변수의 데이터가 평균 주위에서 얼마나 흩어져 있는지를 나타낸다. 공분산은 “두 변수” 가 함께 변하는 정도를 나타낸다.\n공분산이 “양수” 이면, 두 변수는 “같은 방향” 으로 변한다. 공분산이 “음수” 이면, 두 변수는 “반대 방향” 으로 변한다. 공분산이 “0” 에 가까우면, 두 변수 간에 “선형 관계가 거의 없음” 을 의미한다.\n위 값들은 다음과 같은 수식을 통해 계산된다:\nx̄, ȳ 는 변수 x, y 의 표본 평균을 의미한다:\n표본상관계수는 두 변수의 직선관계의 정도(강도, 방향)를 나타내며, 다음과 같은 특징이 있다:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n계수가 “0” 에 가까울수록 두 변수 간의 직선의 관계가 “매우 약함” 을 의미한다. 표본상관계수의 단위는 없다.\n각 변수의 편차들의 곱을 사용해 계산할 때, 이 곱은 원래 변수들의 단위를 곱한 단위이지만, 분모에 각 변수의 분산을 곱한 값의 제곱근이 들어가기 때문에 단위가 사라지게 된다.\n위 연산은 단위를 제곱근 단위로 바꾸는 역할을 하며, 이로 인해 단위가 없는 숫자가 된다. 변수들의 단위에 영향을 받지 않아, 서로 다른 단위를 가진 변수들 간에도 관계를 비교할 수 있다.\n\n# 키와 몸무게의 표본상관계수\nnp.corrcoef(height, weight)[0][1]\n\n## 해석: 표본상관계수 𝑟이 약 0.74로 나왔다는 것은\n## 두 변수 사이에 강한 '양의 선형 관계'가 있음을 의미한다.\n\nnp.float64(0.7362765055636866)\n\n\n아래에서 설명할 산점도를 포함한 대부분의 그림 요약 방법은 “주관적” 일 수 있다. 반면, 표본상관계수는 “객관적” 인 수치 자료로, 이러한 문제를 “보완” 해준다.\n단, 직선이 아닌 다른 관계(곡선 등)가 있을 수 있으며, 이를 표본상관계수가 제대로 나타내지 못할 수 있다는 점을 유념해야 한다."
  },
  {
    "objectID": "da/ida/ida_04.html#산점도",
    "href": "da/ida/ida_04.html#산점도",
    "title": "4장: 두 변수 자료의 요약",
    "section": "3. 산점도",
    "text": "3. 산점도\nScatter Plot\n변수 x 를 “수평축” 에 놓고, 변수 y 를 “수직축” 에 놓은 후에 각 관측값의 찍을 좌표 위에 표시함 으로써 얻게 되는 그림.\n이를 통해, “두 변수 간의 관계” 를 시각적으로 대략 파악할 수 있다.\n\nimport matplotlib.pyplot as plt\n\n# 산점도 작성\nplt.figure(figsize=(8, 6))  # 그래프의 크기 (가로 8, 세로 6)\nplt.scatter(height, weight, color='slateblue')  # 산점도 색상 설정\n\nplt.xlabel('height (cm)')  # x축 레이블\nplt.ylabel('weight (kg)')  # y축 레이블\nplt.title('height(cm) and weight(kg)')  # 그래프의 제목\n\nplt.grid(True)  # 그리드 추가 (경계선)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html",
    "href": "da/ida/ida_02.html",
    "title": "2장: 자료의 요약",
    "section": "",
    "text": "자료를 효과적으로 이해하려면 자료의 형태와 특성에 맞춰 표와 그림을 활용해 요약해야 한다. \n자료 요약 방법은 자료의 종류와 구조에 따라 달라진다.\n\n정량 자료: 수치 중심, 평균·분산·빈도표, 히스토그램, 상자 그림(Box plot) 활용\n정성 자료: 텍스트·범주 중심, 분류표, 워드클라우드, 네트워크 도식 활용\n\n표와 그림은 단순히 보여주는 것이 아니라 핵심 정보 전달과 패턴·관계 파악을 돕는다. 자료를 요약할 때는 중요한 변수와 비교 지점을 우선 고려하여 시각화/표 형태로 정리하는 것이 효율적이다. \n\n01 예제 1\n사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\n\npython\n\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3,\n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3,\n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3,\n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3,\n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2,\n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4,\n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2,\n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2,\n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2,\n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])\n\n\n\n\n\n02 도수분포표\nFrequency Distribution Table 특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\n\npython\n\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1\n\n\n\n\n\n\n03 막대 그래프\nBar Chart 데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\n\npython\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_24892\\2514237286.py:19: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04 원형 그래프\nPie Chart 전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n\npython\n\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n05 파레토그림\nPareto Chart 막대그래프와 누적선그래프를 결합한 형태의 그래프. 파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\n\npython\n\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n06 도수다각형\nFrequency  Polygon 도수분포표의 도수를 선으로 연결한 그래프.\n\n\npython\n\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html#자-료-의-입-력",
    "href": "da/ida/ida_02.html#자-료-의-입-력",
    "title": "2장: 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "da/ida/ida_02.html#도수분포표",
    "href": "da/ida/ida_02.html#도수분포표",
    "title": "2장: 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table 특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1"
  },
  {
    "objectID": "da/ida/ida_02.html#막대-그래프",
    "href": "da/ida/ida_02.html#막대-그래프",
    "title": "2장: 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart 데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19716\\2514237286.py:19: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`"
  },
  {
    "objectID": "da/ida/ida_02.html#원형-그래프",
    "href": "da/ida/ida_02.html#원형-그래프",
    "title": "2장: 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart 전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html#파레토그림",
    "href": "da/ida/ida_02.html#파레토그림",
    "title": "2장: 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart 막대그래프와 누적선그래프를 결합한 형태의 그래프. 파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "da/ida/ida_02.html#도수다각형",
    "href": "da/ida/ida_02.html#도수다각형",
    "title": "2장: 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency  Polygon 도수분포표의 도수를 선으로 연결한 그래프.\n\n\npython\n\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()"
  },
  {
    "objectID": "da/ada/ada_06_0.html",
    "href": "da/ada/ada_06_0.html",
    "title": "두 모집단에 대한 비교",
    "section": "",
    "text": "두 모집단의 모평균, 모비율, 모분산의 차이에 대한 가설검증 문제를 다루고자 한다. (12장: 두 모집단의 비교와 이어지는 내용이다.)\n표본 평균을 추정하려면, 표본의 크기와 모분산을 고려해야 한다.\n[1] 두 모분산 σ12, σ 22 이 모두 알려져 있는 경우,\n두 모평균 차에 대한 “추정량” ⇨ “두 표본평균의 차” 통계적 추론을 위한 “준비물” ⇨ “추정량의 분포”\n이 분포는 다음과 같은 평균과 분산을 가진 정규분포를 따른다:\n표준화된 확률변수 Z는 표준정규분포 N(0, 1)를 따른다.\n[2] 두 모분산 σ12, σ 22 을 모두 모르는 경우, 표본의 크기를 고려하게 된다. 표본의 크기가 충분히 큰 경우 ( 25 이상 )\n중심극한정리에 의해 모집단의 분포에 관계없이 x̄ 와 ȳ 가 근사적으로 정규분포를 따른다. 두 모분산의 추정치인 표본분산 s₁², s₂² 를 고려한 통계량을 사용하여 검정을 수행한다.\n[3] 두 모집단이 알려져 있지는 않지만, 모분산이 동일한 것으로 가정할 수 있는 경우, 다음과 같은 평균과 분산을 가지는 정규분포를 따르며, [1]과 동일하다.\n공통분산 σ ² 의 합동추정량 (Pooled Variance) 자유도 n ₁ + n ₂ – 2인 t-분포를 따른다.\n[4] 두 모분산이 서로 다른 경우, [3]번 식은 t-분포를 따르지 않는다. 단, 아래와 같이 자유도를 수정할 경우, 근사적으로 t-분포를 따르게 된다. 근사적으로 t-분포를 따르게 된다. 수정된 자유도(df)."
  },
  {
    "objectID": "da/ada/ada_06_0.html#독립표본-t검정",
    "href": "da/ada/ada_06_0.html#독립표본-t검정",
    "title": "두 모집단에 대한 비교",
    "section": "독립표본 t–검정",
    "text": "독립표본 t–검정\n독립표본에 의한 두 모평균의 비교 두 개의 서로 독립적인 집단의 평균을 비교하여 그 차이가 통계적으로 유의한지 판단하는 방법이다.\n사례: 새로운 강의방식이 초등학생 독해력 향상에 도움이 되는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Reading.csv\"\nReading = pd.read_csv(url)\nReading.head()\n\n\n\n\n\n\n\n\nID\nGroup\nScore\n\n\n\n\n0\n1\nNew\n75\n\n\n1\n2\nNew\n80\n\n\n2\n3\nNew\n72\n\n\n3\n4\nNew\n77\n\n\n4\n5\nNew\n69\n\n\n\n\n\n\n\n가설검증을 결정하기 전에 데이터를 시각화한다.\n\nimport seaborn as sns  # 박스 플롯\nsns.boxplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n중위수와 같은 요인을 비교한 결과, 차이가 나타나므로 이를 근거로 검증을 진행할 수 있다.\n\n# 바이올린 플롯\nsns.violinplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n\n# 기술통계량\nReading.groupby('Group').Score.describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nGroup\n\n\n\n\n\n\n\n\n\n\n\n\nNew\n8.0\n75.375\n4.373214\n69.0\n71.75\n76.0\n78.50\n81.0\n\n\nOld\n8.0\n69.125\n4.086126\n63.0\n67.25\n69.0\n71.25\n76.0\n\n\n\n\n\n\n\n양측검정 적용.\n\n# 그룹 나누기\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n# 양측검증:\n# 두 강의 방식에 차이가 있다. vs 차이가 없다.\n\nfrom scipy.stats import ttest_ind  # 독립 t-검정\nttest_ind(New.Score, Old.Score, equal_var = True)\n\n\n# T통계량: 그룹 간 평균 차이가 실제로 존재하는지를 나타내는 통계량.\n# 통계량이 클수록 차이가 있을 가능성이 높다.\n\n# [3]번 통계량: statistic=2.9536127902039953\n\n\n# 두 꼬리 검정에서의 p-값: pvalue=0.010470744188033123\n\n# 통상적으로 p-값이 0.05보다 작으면 귀무가설을 기각할 수 있다. \n# 즉, 두 강의 방식에 차이가 있다고 결론 내릴 수 있다.\n\nTtestResult(statistic=np.float64(2.9536127902039953), pvalue=np.float64(0.010470744188033123), df=np.float64(14.0))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\n# 단측검정:\n# 새로운 학습법이 더 효과적이다. vs 효과적이지 않다.\n\nstat, pval = ttest_ind(New.Score, Old.Score, equal_var = True)\nprint(\"P\", pval/2)\n\n# p-값이 0.0052로 유의수준 0.05보다 작으므로, 대립가설을 채택할 수 있다.\n\nP 0.005235372094016561\n\n\n단측검정과 등분산 가정 적용.\n\n# 단측검정\nfrom statsmodels.stats.weightstats import ttest_ind\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'pooled') # 등분산 가정 적용:\n          # 두 그룹 간의 분산이 동일하다고 가정\n\n(np.float64(2.9536127902039953),\n np.float64(0.005235372094016561),\n np.float64(14.0))\n\n\n단측검정과 이분산 가정 적용.\n\n# 단측검정\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'unequal') # 이분산 가정 적용:\n          # 두 그룹의 분산이 서로 다르다는 가정\n\n# [4]번 통계량: usevar= 'pooled' ⇨ 'unequal'\n# 14 ⇨ 13.935945095796395 (자유도가 실수로 바뀜)\n\n(np.float64(2.9536127902039953),\n np.float64(0.005256688626975243),\n np.float64(13.935945095796395))\n\n\n결론적으로, 새로운 강의방식이 초등학생 독해력 향상에 도움이 된다고 할 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#대응표본-t검정",
    "href": "da/ada/ada_06_0.html#대응표본-t검정",
    "title": "두 모집단에 대한 비교",
    "section": "대응표본 t–검정",
    "text": "대응표본 t–검정\n대응표본에 의한 두 모평균의 비교 어떤 신발의 마모율을 비교할 때, 독립 표본 검정에 경우, 한 그룹의 사람이 왼쪽 신발을 신고, 다른 그룹의 사람이 오른쪽 신발을 신더라도 상관이 없다. 하지만 대응 표본 검정은 동일한 사람이 왼쪽 신발과 오른쪽 신발을 모두 신어야 만 한다. 각 쌍이 서로 연관되어 있으므로 두 신발을 신는 사람이 동일해야 하며, 표본의 수도 일치해야 한다.\n이는 마모율에 영향을 줄 수 있는 교락 요인(confounding factor), 즉 신발을 신는 사람의 특성 등을 배제하기 때문이다.\n그러므로, 대응 표본 검정은 같은 대상에 대한 실험 전후의 결과를 비교할 때 주로 사용된다.\n\n사례: 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있는가?\n\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Paired.csv\"\nPaired = pd.read_csv(url)\nPaired.head()\n\n\n\n\n\n\n\n\nID\nPretest\nPosttest\n\n\n\n\n0\n1\n80\n82\n\n\n1\n2\n73\n71\n\n\n2\n3\n70\n95\n\n\n3\n4\n60\n69\n\n\n4\n5\n88\n100\n\n\n\n\n\n\n\n박스플롯 시각화 및 기술 통계량 출력.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Pretest와 Posttest에 대한 박스플롯 시각화\nsns.boxplot(data = Paired.iloc[:, [1, 2]], \n            orient = 'h') # 수평 방향\n\n# Pretest와 Posttest의 차이 계산 및 새로운 열(Diff) 추가\nPaired[\"Diff\"] = Paired.Pretest - Paired.Posttest \n                # = 교육 전 성적 - 교육 후 성적\n                # 교육이 효과가 있다면 교육 후 성적이 더 높을 것이므로\n                # 결과적으로는 변수 Diff의 값이 음수로 나와야 한다.\n\n\n\n\n\n\n\n\n두 변수에 대한 상자 그림\n\nPaired.iloc[:,1:4].describe()\n\n# 변수 Diff 평균(mean)이 -7.93이며\n# 실제로 그래프 상에서도 대부분의 개체에서 \n# 변수 Diff의 값이 0보다 작음을 볼 수 있다.\n\n\n# 표준편차(std)는 데이터의 산포도(변동성)를 측정하는 지표로, \n# 데이터가 평균으로부터 얼마나 떨어져 있는지를 나타낸다. \n\n# 표준편차는 항상 0 이상의 값을 가지며, 음수가 될 수 없다. \n# 이는 표준편차가 데이터 값의 차이를 제곱하여 계산하기 때문이다.\n\n\n\n\n\n\n\n\nPretest\nPosttest\nDiff\n\n\n\n\ncount\n15.000000\n15.000000\n15.000000\n\n\nmean\n70.266667\n78.200000\n-7.933333\n\n\nstd\n18.041487\n14.313829\n9.931671\n\n\nmin\n37.000000\n60.000000\n-25.000000\n\n\n25%\n59.500000\n67.000000\n-12.500000\n\n\n50%\n73.000000\n75.000000\n-7.000000\n\n\n75%\n82.000000\n90.500000\n-2.500000\n\n\nmax\n98.000000\n100.000000\n13.000000\n\n\n\n\n\n\n\n히스토그램 및 커널 밀도 추정(KDE) 시각화\n\nsns.distplot(Paired.Diff)\n\n# Seaborn의 최신 버전에서는 더 이상 지원되지 않으므로,\n# sns.histplot 또는 sns.kdeplot을 사용하는 것이 권장된다.\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18240\\4004193321.py:1: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 히스토그램 그리기\nsns.histplot(Paired.Diff, \n             stat = 'density')  # y축을 밀도로 변경\n\n# KDE만 수정하기 위해 따로 그리기\nsns.kdeplot(Paired.Diff, \n            fill = True) # 음영 처리\n\nplt.xlim(-40, 30)     # x축 범위 설정\n\n\n\n\n\n\n\n\n양측검정 적용\n\n# ttest_rel에서 rel은 paired 또는 related를 의미한다.\n\n# 이 함수는 대응표본 t-검정을 수행하는 것으로, \n# 두 관련된 표본에 대한 평균의 차이를 비교하는 데 사용된다.\n\n\nfrom scipy.stats import ttest_rel\nttest_rel(Paired.Pretest, Paired.Posttest)\n\n# p-값이 0.0079(0.79%)로 0.05(5%)보다 작기 때문에 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다. \n# 이는 두 표본 간에 유의미한 차이가 있음을 의미한다.\n\nTtestResult(statistic=np.float64(-3.093705670004429), pvalue=np.float64(0.007930923229026533), df=np.int64(14))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\nstat, pval = ttest_rel(Paired.Pretest, Paired.Posttest)\nprint(\"one-sided p-value =\", pval/2)\n\n# 이 경우에도, p-값이 0.05보다 작으므로 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다.\n\none-sided p-value = 0.003965461614513267\n\n\n결론적으로 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있으며, 사후 테스트의 결과가 더 좋다고 할 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#피셔의-정확검정",
    "href": "da/ada/ada_06_0.html#피셔의-정확검정",
    "title": "두 모집단에 대한 비교",
    "section": "피셔의 정확검정",
    "text": "피셔의 정확검정\nFisher's Exact Test\n독립표본에 의한 두 모비율의 비교 두 모비율에 대한 검정을 수행하기 위해 사용할 수 있는 대표적인 검정법은 두 독립된 이항분포의 비율에 대한 z-검정이다.\n사례: 현 정부에 대한 지지율이 성인 남녀별로 차이가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Support.csv\"\nSupport = pd.read_csv(url)\nSupport.head()\n\n\n\n\n\n\n\n\nID\nGender\nYesNo\n\n\n\n\n0\n1\nMale\nNo\n\n\n1\n2\nFemale\nYes\n\n\n2\n3\nFemale\nNo\n\n\n3\n4\nFemale\nNo\n\n\n4\n5\nFemale\nNo\n\n\n\n\n\n\n\n이 데이터에 대한 2차원 분할표(빈도표) 작성하기.\n\nimport pandas as pd\nSupportTable = pd.crosstab(index = Support[\"Gender\"],\n                           columns = Support[\"YesNo\"])\n\nSupportTable\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n96\n104\n\n\nMale\n140\n110\n\n\n\n\n\n\n\n행 백분율 계산하기.\n\npd.crosstab(index=Support[\"Gender\"], columns=Support[\"YesNo\"],\n           normalize = \"index\") # 각 행의 합을 기준으로 비율을 계산\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n0.48\n0.52\n\n\nMale\n0.56\n0.44\n\n\n\n\n\n\n\n다음과 같은 교차 테이블(Cross Table)을 만들 수 있다.\n양측검증 적용.\n\nfrom scipy.stats import fisher_exact\nfisher_exact(SupportTable, \n             alternative = 'two-sided')\n             \n# 이 결과는 검정 통계량이 0.725이고 \n# p-값이 0.106(10.6%)이다.\n\n# 이는 일반적으로 사용되는 유의 수준 0.05(5%)에서 \n# 통계적으로 유의하지 않다는 것을 의미한다. \n\n# 결론적으로, 두 그룹(또는 변수) 간에 유의한 차이 또는 \n# 연관성을 찾지 못했다는 것을 나타낸다.\n\nSignificanceResult(statistic=np.float64(0.7252747252747253), pvalue=np.float64(0.10634531219761142))\n\n\n정규 근사 검정\n이항분포의 표본 크기 n이 충분히 크면, 이항분포는 정규분포로 근사할 수 있으며, 이를 정규 근사라고 한다. 일반적으로 n×p와 n×(1 − p)가 모두 5 이상이면, 정규분포로 근사할 수 있다고 간주한다.\n\n\\(p\\): 성공 확률\n\n이러한 정규화된 변수를 제곱하면, 자유도가 1인 카이제곱 분포를 따른다. 카이제곱검정(Chi-Square Test) 적용.\n\nfrom scipy.stats import chi2_contingency\nchi2_contingency(SupportTable)\n\n# 카이제곱 통계량: 2.54\n# 유의 수준이 일반적으로 0.05(5%)인 경우, \n# p-값이 0.111(11.1%)이므로 귀무가설을 기각할 수 없다.\n\n# 따라서 이 결과는 두 변수 간에 통계적으로 \n# 유의한 연관성이 없다고 결론지을 수 있다. \n\n# 즉, 이 교차표에 따르면 두 변수는 독립적이다.\n\nChi2ContingencyResult(statistic=np.float64(2.5395141968952935), pvalue=np.float64(0.1110289428837834), dof=1, expected_freq=array([[104.88888889,  95.11111111],\n       [131.11111111, 118.88888889]]))\n\n\n결론적으로, 현 정부에 대한 지지율이 성인 남녀별로 차이가 없다고 할 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#맥니머-검정",
    "href": "da/ada/ada_06_0.html#맥니머-검정",
    "title": "두 모집단에 대한 비교",
    "section": "맥니머 검정",
    "text": "맥니머 검정\n대응표본에 의한 두 모비율의 비교\n맥니머 검정은 피셔의 정확검정이나 카이제곱 검정과 달리 대응 표본에 적용할 수 있는 검정이다. 이 검정은 대응 표본 t-검정과 유사하게 교락 효과를 제거하는 것이 중요하다.\n독립 표본의 경우, 한 사람이 A, B 제품 모두를 사용하지 않아도 무방하다. 그러나 대응 표본에서는 한 사람이 반드시 두 제품 모두를 사용해야 한다.\n사례: 정부에서 정책 발표 후 지지율에 변화가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Prepost.csv\"\nPrepost = pd.read_csv(url)\nPrepost.head()\n\n\n\n\n\n\n\n\nID\nPre\nPost\n\n\n\n\n0\n1\nYes\nYes\n\n\n1\n2\nNo\nNo\n\n\n2\n3\nYes\nNo\n\n\n3\n4\nNo\nNo\n\n\n4\n5\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\n\nPrepostTable = pd.crosstab(index = Prepost[\"Pre\"], \n                           columns = Prepost[\"Post\"], \n                           margins = True, # 각 행과 열의 합계 추가\n                           margins_name = \"합계\")\nPrepostTable\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n18\n27\n45\n\n\nYes\n8\n67\n75\n\n\n합계\n26\n94\n120\n\n\n\n\n\n\n\n\npd.crosstab(index=Prepost[\"Pre\"], columns=Prepost[\"Post\"], \n            margins=True, margins_name=\"합계\", \n            normalize=\"all\") # 전체 데이터에 대한 비율 변환\n            \n# 정책 발표 이전 지지율(pre): 62.5%\n# 정책 발표 이후 지지율(post): 78.3%\n# 결과적으로 15.8%p가 상승하였음을 볼 수 있다.\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n0.150000\n0.225000\n0.375\n\n\nYes\n0.066667\n0.558333\n0.625\n\n\n합계\n0.216667\n0.783333\n1.000\n\n\n\n\n\n\n\n\n# pip install statsmodels\n\nfrom statsmodels.stats.contingency_tables import mcnemar\nprint(mcnemar(PrepostTable, \n              exact = True)) # 이항분포 기반의 정확 검정 방법\n              \n# 0.001(0.1%) &lt; 0.05(5%)\n\nprint(mcnemar(PrepostTable, \n              exact=False)) # 카이제곱분포를 사용한 근사 검정 방법\n              \n# 0.002(0.2%) &lt; 0.05(5%)\n\npvalue      0.0018782254774123432\nstatistic   8.0\npvalue      0.0023457869795667934\nstatistic   9.257142857142858\n\n\n결론적으로, 정부에서 정책 발표 전후 지지율에 변화가 있으며, 정책 발표 후에 지지율이 상승한 것으로 볼 수 있다."
  },
  {
    "objectID": "da/ada/ada_06_0.html#f검정",
    "href": "da/ada/ada_06_0.html#f검정",
    "title": "두 모집단에 대한 비교",
    "section": "F–검정",
    "text": "F–검정\nF–test\n모분산의 동일성에 대한 검정 가장 일반적인 검정 방법으로, 두 집단의 모분산이 동일한지 평가한다. 두 집단의 분산 비율을 계산하고, 이를 기반으로 F–분포를 사용하여 p–값을 구한다.\nReading 데이터의 모분산이 다른가?\n이전에 다루었던 Reading 데이터에 대해 분산의 동일성 검정을 위한 사용자 정의 함수를 작성하고, 가설검정을 수행하였다.\n\nimport pandas as pd\n\n# file_path = os.path.join('data', 'Reading.csv')\n# Reading = pd.read_csv(file_path)\n\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n\nimport numpy as np\nfrom scipy import stats\n\ndef F_test(x, y):\n    f = np.var(x, ddof = 1)/np.var(y, ddof = 1)\n    df1 = x.size -1 \n    df2 = y.size -1 \n    p = 2*(1-stats.f.cdf(f, df1, df2))\n    return f, p\n\nF_test(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\n(1.1454545454545453, np.float64(0.8624138071371459))\n\n\n\nBartlett’s Test\n\nfrom scipy import stats\nstats.bartlett(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nBartlettResult(statistic=np.float64(1.2110354068328009), pvalue=np.float64(0.27112715913152846))\n\n\n\n\nLevene’s Test\n\nstats.levene(New.Score, Old.Score)\n\n# 0.6(60%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nLeveneResult(statistic=np.float64(0.1978798586572438), pvalue=np.float64(0.6632376240724351))\n\n\n결론적으로, 두 집단의 모분산이 다르다고 말할 수 없다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "da/ida/ida_03.html",
    "href": "da/ida/ida_03.html",
    "title": "3장: 연속형 자료",
    "section": "",
    "text": "연속형 자료의 분포 특성을 이해하기 위해 중심위치와 퍼짐 정도를 나타내는 측도를 다룬다."
  },
  {
    "objectID": "da/ida/ida_03.html#자료의-입력",
    "href": "da/ida/ida_03.html#자료의-입력",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제13: 정량 100인 음료수 80병을 임의로 추출하여 그 내용물의 실제 측정된 양을 잰 자료이다.(p.42)\n\nimport numpy as np\n\n# 변수 drink에 NumPy 배열을 할당\ndrink = np.array([98, 99, 100, 99, 99.4, 101.7, 98.8, 101.8, 101.5, \n                 101.8, 102.6, 101, 98.8, 101.4, 99.7, 99.7, 99.7, \n                 100.9, 98.6, 101.4, 102.1, 102.9, 100.8, 101.8, \n                 100, 101.2, 100.5, 101.2, 100.1, 101.6, 101.3, 99.9, \n                 99.4, 99.3, 99.4,101.6, 96.1, 100, 99.7, 99.1, 100.7, \n                 100.8, 100.8, 95.5,100.1, 100.5, 98.9, 99.9, 96.8, \n                 102.4, 100, 103.7, 101.4,99.7, 97.4, 99.5, 97.5, \n                 99.9, 100.3, 100.2, 101.5, 99.4, 99.7, 98.2, 100.3, \n                 100.2, 100.5, 100.4, 101.5, 98.4, 101.4, 98.8, 100.9, \n                 101.1, 100.9, 98.1, 98.7, 99.2, 98.1, 97.2])\n\n중심위치의 측도"
  },
  {
    "objectID": "da/ida/ida_03.html#평균-mean",
    "href": "da/ida/ida_03.html#평균-mean",
    "title": "3장: 연속형 자료",
    "section": "2. 평균 (Mean)",
    "text": "2. 평균 (Mean)\n총 자료의 개수 \\(n\\) 을 모든 관측값 \\(x_1, x_2, \\dots , x_n\\) 의 합으로 나눈 값. 이를 “산술 평균” 이라고도 하며, 공식으로 표현하면 다음과 같다:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n표본 평균은 관측값의 산술평균이며, “극단값” 에 영향을 받는다.\n\n표본 평균: 전체 데이터인 “모집단” 에서 추출한 일부 데이터의 평균을 의미한다. (예시1) 예제13 에서 추출된 80 병의 음료수는 표본에 해당된다.\n극단값(Outlier): 데이터 집합에서 다른 값들과 현저히 다른 값들을 의미한다. (예시2) 데이터 집합 { 11, 12, 13, 300 } 에서 극단값은 300 이다.\n\n\n# 2장의 예제 13에서 주어진 음료수 한 병의 부피 데이터를 기반으로 \n# 평균, 중앙값, 분산, 표준편차, 범위, 사분위수범위를 파이썬을 이용하여 계산하라.(p.83)\n\n## numpy 모듈을 이용하여 계산할 수 있음. ##\n\n# 평균\nprint(np.mean(drink))\n\n100.04125"
  },
  {
    "objectID": "da/ida/ida_03.html#중앙값median",
    "href": "da/ida/ida_03.html#중앙값median",
    "title": "3장: 연속형 자료",
    "section": "3. 중앙값(Median)",
    "text": "3. 중앙값(Median)\n전체 관측값을 크기 순서로 배열했을 때, 가운데에 위치한 값.\n데이터의 개수가 홀수 일 경우, 중앙에 위치한 값이, 짝수 일 경우, 중앙에 위치한 두 값의 평균 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, 극단값의 영향을 받지 않는다. 따라서, 평균과 값이 다를 수 있다.\n\n# 중앙값 계산\nprint(np.median(drink))\n\n100.05\n\n\n퍼진 정도의 측도"
  },
  {
    "objectID": "da/ida/ida_03.html#분-산-variance",
    "href": "da/ida/ida_03.html#분-산-variance",
    "title": "3장: 연속형 자료",
    "section": "4. 분 산 (Variance)",
    "text": "4. 분 산 (Variance)\n확률 분포나 데이터 집합의 산포도(분포도)를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 지표로 사용된다.\n관측값이 \\(x_1, x_2, \\dots , x_n\\) 이고, 표본평균이 \\(\\bar{x}\\) 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평균제곱 오차의 평균: 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n\n4-1. 편차(Deviation):\n각 관측값과 평균의 차이\n편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로,  따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n\n\n4-2. 자유도\nDegrees of Freedom\n위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "da/ida/ida_03.html#표준편차",
    "href": "da/ida/ida_03.html#표준편차",
    "title": "3장: 연속형 자료",
    "section": "5. 표준편차",
    "text": "5. 표준편차\nStandard Deviation\n분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다.\n\n# 표준편차 계산 (표본의 표준편차, 자유도를 1로 설정)\nprint(np.std(drink, ddof=1))\n\n# 표준편차 계산 (모집단의 표준편차)\nprint(np.std(drink))\n\n1.521882058505192\n1.5123403841397614\n\n\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다."
  },
  {
    "objectID": "da/ida/ida_03.html#범위",
    "href": "da/ida/ida_03.html#범위",
    "title": "3장: 연속형 자료",
    "section": "6. 범위",
    "text": "6. 범위\nRange\n관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다.\n\n# 범위 계산 (최대값 - 최소값)\nprint(np.max(drink) - np.min(drink))\n\n8.200000000000003"
  },
  {
    "objectID": "da/ida/ida_03.html#사분위수범위",
    "href": "da/ida/ida_03.html#사분위수범위",
    "title": "3장: 연속형 자료",
    "section": "7. 사분위수범위",
    "text": "7. 사분위수범위\nQuartile\n전체 관측값을 작은 순서로 배열 하였을 때, 전체를 사등분 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n\n7-1. 사분위수범위\nInterquartile Range, IQR\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 std 는 자동적으로 n – 1 로 나누어져서 계산된다."
  },
  {
    "objectID": "da/ida/ida_05.html",
    "href": "da/ida/ida_05.html",
    "title": "5장: 확률",
    "section": "",
    "text": "통계적인 추론을 통해서도 모집단에 대한 다양한 정보를 얻을 수 있다. 그 통계적 추론의 기초가 되는 확률이론에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_05.html#사건의-확률",
    "href": "da/ida/ida_05.html#사건의-확률",
    "title": "5장: 확률",
    "section": "1. 사건의 확률",
    "text": "1. 사건의 확률\n동일 조건하에서 한 가지 실험을 반복할 때, 전체 실험 횟수에서 그 사건이 일어나리라고 예상되는 횟수의 비율을 말한다. \n사건을 \\(A\\) 라고 하면, 사건 \\(A\\) 의 확률은 \\(P(A)\\) 로 표시한다.\n\n1-1. 표본공간\nSample Space: Ω\n한 실험에서 나올 수 있는 모든 결과들의 모임. 유한표본공간 ( Finite Sample Space )\n주사위 던지기의 표본공간\n연속표본공간 ( Continuous Sample Space )\n0과 1 사이의 모든 실수"
  },
  {
    "objectID": "da/ida/ida_05.html#근원사건",
    "href": "da/ida/ida_05.html#근원사건",
    "title": "5장: 확률",
    "section": "1-2. 근원사건",
    "text": "1-2. 근원사건\nElementary Outcomes: ω ₁ , ω ₂ … \n표본공간을 구성하는 개개의 결과.\n주사위 던지기의 근원사건\n\n1-3. 사건\nEvent: A, B, …\n표본공간의 부분집합으로, 어떤 특성을 갖는 결과들의 모임 (=근원사건들의 집합)\n주사위 던지기의 사건 A, B"
  },
  {
    "objectID": "da/ida/ida_05.html#확률의-법칙",
    "href": "da/ida/ida_05.html#확률의-법칙",
    "title": "5장: 확률",
    "section": "2. 확률의 법칙",
    "text": "2. 확률의 법칙\n위 정의로부터 확률의 특성을 유추할 수 있다.\n사건 A 가 일어날 확률은, 사건 A 에 속하는 근원사건이 일어날 확률의 “합” 과 같다. Ω 를 하나의 사건이라고 하면, 이 사건은 “반드시” 일어나므로 확률은 “1” 이 되어야 한다."
  },
  {
    "objectID": "da/ida/ida_05.html#확률의-계산",
    "href": "da/ida/ida_05.html#확률의-계산",
    "title": "5장: 확률",
    "section": "3. 확률의 계산",
    "text": "3. 확률의 계산\n\n3-1. 균일 확률\n주사위 던지기에서, 각 숫자가 나올 확률 Ω 가 k 개의 원소로 이루어져 있고, 각 근원사건이 일어날 가능성이 “동일” 하다고 가정할 때, 근원사건 중 하나가 일어날 확률은 1 / k 로 주어진다.\n또 사건 A 가 m 개의 근원사건으로 이루어져 있다면, 사건 A 가 일어날 확률은 위와 같다.\n\n\n3-2. 상대도수 수렴치로서의 확률\n주사위를 60번 던져 10번 2가 나왔을 때, 2가 나올 확률 동일한 실험 N 회를 반복할 때, 사건 A 의 상대도수는 위와 같이 표현된다.\nN 이 증가함에 따라 상대도수가 “일정한 값으로 수렴” 한다면, 그 값으로 사건 A 가 일어날 확률 P ( A ) 를 추정한다."
  },
  {
    "objectID": "da/ida/ida_05.html#확률-법칙",
    "href": "da/ida/ida_05.html#확률-법칙",
    "title": "5장: 확률",
    "section": "4. 확률 법칙",
    "text": "4. 확률 법칙\n여 사 건 ( Complementary Event ): 사건 A 가 일어나지 않는 사건 합 사 건 ( Sum Event ): 사건 A 또는 B 가 일어나는 사건 곱 사 건 ( Product Event ): 사건 A 또는 B 가 동시에 일어나는 사건 배 반 사 건 ( Exclusive Event ): 두 사건이 동시에 일어날 수 없는 경우"
  },
  {
    "objectID": "da/ida/ida_05.html#조건부-확률",
    "href": "da/ida/ida_05.html#조건부-확률",
    "title": "5장: 확률",
    "section": "5. 조건부 확률",
    "text": "5. 조건부 확률\nConditional Probability\n한 사건의 결과가 다른 사건의 발생에 영향을 미치는 확률.\n사건 B가 발생했을 때, 사건 A가 발생할 확률:\n곱 사 건 의 확 률 법 칙 ( Multiplication Rule for Probability ) 두 사건이 동시에 발생할 확률을 계산하는 방법. 이 법칙은 두 사건이 독립적인지 여부에 따라 다르게 적용된다:\n\n5-1. 표본공간의 분할\nPartition\n사건 A₁, A₂, …, An 이 서로 배반사건이고, Ω = A₁ ∪ … ∪ An 일 때, 사건 A₁, A₂, …, An 을 Ω 의 분할이라고 한다:\n이때, 각각의 부분집합은 서로 겹치지 않는다:\n예시: 주사위를 던지는 실험에서, Ω 를 두 개의 부분집합으로 나눌 수 있다.\nA₁ ​: 홀수가 나오는 경우 { 1, 3, 5 } A₂ ​: 짝수가 나오는 경우 { 2, 4, 6 }\n이 경우, Ω 는 다음과 같이 표현된다:\n\n\n5-2. 총확률의 법칙\nLaw of Total Probability\n표본공간의 분할 개념을 사용하여 전체 확률을 계산하는 방법.\n사건 A₁, A₂, …, An 이 표본공간의 분할일 때, 임의의 사건 B 의 확률 “P(B)” 는 다음과 같이 계산할 수 있다:\n이를 조건부 확률 ( 종속사건 ) 을 사용하여 다시 쓰면:\n예시: 사건 B 를 “주사위 눈이 4 이하인 사건” 으로 정의하면,\nP(B) = P({ 1, 2, 3, 4 }) = ⅔ P(A₁)​ = P({ 1, 3, 5 }) = ½ P(A₂)​ = P({ 2, 4, 6 }) = ½\n이 경우, P ( B ) 는 다음과 같이 표현된다:\n각 확률 값을 대입해보면:\n\n\n5-3. 베이즈 정리\nBayes’ rule\n사건 A₁, A₂, …, An 이 Ω 의 분할일 때, 임의의 사건 B 에 대하여 다음 식이 성립한다:\n예시: 사건 B 가 발생한 후, 사건 A₁ ​ 이 발생할 확률:"
  },
  {
    "objectID": "da/ida/ida_07.html",
    "href": "da/ida/ida_07.html",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "",
    "text": "모집단의 구성원들이 두 그룹으로 나누어져 있는 경우의 표본추출에서 광범위하게 쓰이는 확률모형과 그의 특징 및 관련된 다른 확률모형들을 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_07.html#자료의-입력",
    "href": "da/ida/ida_07.html#자료의-입력",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7 :  어떤 초등학교에서  10 년간 조사결과\n# 평균적으로  4 % 의 학생이 색맹인 것으로 나타났다고 한다. (p.213)\n\n# 올해에도 색맹인 학생의 비율이 예년과 같다고 할 때,\n# 임의로 추출된  200 명의 학생 중 색맹인 학생이  10 명 이하일 확률은 얼마인가?"
  },
  {
    "objectID": "da/ida/ida_07.html#베르누이-시행",
    "href": "da/ida/ida_07.html#베르누이-시행",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "2. 베르누이 시행",
    "text": "2. 베르누이 시행\nBernoulli Distribution\n모집단의 각 구성원이 두 그룹 중 하나에 속하는 경우, 각각의 구성원이 특정 그룹에 속할 확률 p 와 속하지 않을 확률 1 − p 를 따르는 이산 확률 분포.\n시 행 ( Trial ) : 매번 반복되는 추출( 실험 )\n2 개의 가능한 결과 중, 하나는 성공 ( Success, S ), 다른 하나는 실패 ( Failure, F ) 로 이름을 붙인다.\n이는 시행의 결과가 2 개 뿐임을 강조하며, 보통 우리가 관심이 있는 결과에 성공이란 이름을 붙인다.\n각 시행은 독립으로, 각 시행의 결과가 다른 시행의 결과에 영향을 미치지 않는다.\n일반적으로 복원 추출 ( Sampling With Replacement )로 간주된다. 각 시행 후 다시 원래의 상태로 복귀하여, 다음 시행에 영향을 주지 않는다.\n예제 7에 경우, 추출되는 학생은 색맹 ( S ) 또는 정상 ( F )으로 나눌 수 있다."
  },
  {
    "objectID": "da/ida/ida_07.html#이항분포",
    "href": "da/ida/ida_07.html#이항분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "3. 이항분포",
    "text": "3. 이항분포\nBinomial Distribution\n각 시도가 성공 또는 실패 두 가지 결과 중 하나를 가지는 독립적인 시행이 n 번 반복될 때, 성공 횟수를 나타내는 확률분포.\n성공할 확률이 p 인 베르누이 시행을 n 번 반복할 때 일어나는 성공의 횟수가 X 라면, 이 확률변수 X 는 모수가 ( n, p )인 이항분포를 따른다.\n모수(Parameter): 우리가 관심을 가지는 수치\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, x = 0, 1, …, n에 대하여 확률질량함수( PMF ) 는 다음과 같다:\n이 항 계 수 ( Binomial Coefficient )\n주어진 수의 집합에서 특정한 수의 원소를 선택하는 방법의 수. 조 합 ( Combination ) 으로도 알려져 있다.\n예제 7에서 주어진 정보를 바탕으로, 아래와 같이 이항 분포의 모수 ( 파라미터 ) 를 설정할 수 있다:\n성공 확률: p = 0.04 ( 학생이 색맹일 확률 ) 시도 횟수: n = 200 ( 추출된 학생 수 ) 성공 횟수: k ≤ 10 ( 색맹인 학생 수 )\n즉, 확률변수 X 를 200 명 중 색맹인 학생의 수 라고 하면, X는 모수가 ( n, p ) = ( 200, 0.04 ) 인 이항분포를 따르게 된다.\n이때, k ≤ 10 인 경우의 확률을 구해야 하므로, 누적 분포 함수 ( CDF ) 를 사용해야 한다:\n원하는 확률을 계산하려면, 다음 식을 계산해야 한다:\n다만, 위 식을 직접 계산하기에는 번거로운 면이 있다. 부록의 이항분포표에서도 n = 25 까지가 최대이기 때문이다.\n이럴 경우, 아래에서 설명할 포아송분포로 근사하여 계산할 수 있다.\n\nfrom scipy import stats\nstats.binom.cdf(10, 200, 0.04)\n\n## 출력된 값 &gt; 0.8199789826230907\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n\nnp.float64(0.8199789826230907)\n\n\n\n3-1. 이항분포의 기댓값과 표준편차\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:"
  },
  {
    "objectID": "da/ida/ida_07.html#초기하분포",
    "href": "da/ida/ida_07.html#초기하분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "4. 초기하분포",
    "text": "4. 초기하분포\nHypergeometric Distribution\n유한한 모집단에서 비복원 추출 ( Sampling Without Replacement )을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인(FPC)이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다. 결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다."
  },
  {
    "objectID": "da/ida/ida_07.html#포아송분포",
    "href": "da/ida/ida_07.html#포아송분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "5. 포아송분포",
    "text": "5. 포아송분포\nPoisson Distribution\n특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포. 주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n\n119 구조대에 시간당 걸려오는 전화횟수\n국내 발생하는 진도 4 이상 지진의 횟수\n\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\n\nfrom scipy import stats\nstats.poisson.cdf(10, 8)\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\nnp.float64(0.8158857925585467)\n\n\n\n참고용: Finite Population Correction Factor FPC"
  },
  {
    "objectID": "da/ida/ida_09.html",
    "href": "da/ida/ida_09.html",
    "title": "9장: 표집분포",
    "section": "",
    "text": "주어진 표본을 통해 모집단의 성격을 알아내는 과정을 추론 ( Inference )이라 한다. 그 통계적 추론에서 모집단의 특성을 추정하거나 가설 검정을 수행할 때, 사용되는 표집분포에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_09.html#자료의-입력",
    "href": "da/ida/ida_09.html#자료의-입력",
    "title": "9장: 표집분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 4:  0 ~ 9 까지  10 개의 정수값을 균등하게 갖는 모집단이 있다.\n# (위 모집단의 분포를 이산균등분포라고 한다.) \n\n# 예시: 전화번호 끝자리 번호의 분포\n\n# (1) 여기에서 크기가  5 인 표본을  100 번 뽑아서 \n# (2) 매번 추출된 표본에서 표본평균을 구하고, \n# (3) 그 평균들을 가지고 히스토그램을 그려라. (p.268)"
  },
  {
    "objectID": "da/ida/ida_09.html#통계량",
    "href": "da/ida/ida_09.html#통계량",
    "title": "9장: 표집분포",
    "section": "2. 통계량",
    "text": "2. 통계량\nStatistic\n표본의 관측값들에 의하여 결정되는 양. 표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데, 이때 유념하여야 할 3가지 조건이 있다.\n\n표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n통계량의 값은 추출된 표본의 영향을 받는다:\n다른 표본을 추출할 때마다 통계량의 값은 변한다:"
  },
  {
    "objectID": "da/ida/ida_09.html#표집분포",
    "href": "da/ida/ida_09.html#표집분포",
    "title": "9장: 표집분포",
    "section": "3. 표집분포",
    "text": "3. 표집분포\nSampling Distribution\n통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n\n3-1. 불편추정량\nUnbiased Estimator 분포의 평균값이 추정하려는 모수와 일치하는 추정량. 불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n\n\n3-2. 편의추정량\nBiased Estimator\n분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다. 이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다. 따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n\n\n3-3. 임의표본\nRandom Sample 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn. 위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n\n\n3-4. 표본평균\nSample Mean\n모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:"
  },
  {
    "objectID": "da/ida/ida_09.html#중심극한정리",
    "href": "da/ida/ida_09.html#중심극한정리",
    "title": "9장: 표집분포",
    "section": "4. 중심극한정리",
    "text": "4. 중심극한정리\nCentral Limit Theorem, CLT 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이 표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\n\nimport numpy as np \n\na = np.random.randint(0, 100, size=5) \nb = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nc = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nd = np.random.randint(0, 100, size=5) \n\nprint(\"a :\", a) \nprint(\"b :\", b) \nprint(\"c :\", c)\nprint(\"d :\", d)\n\n## 해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n\na : [ 2 17  4 90 42]\nb : [99 73 67 33 12]\nc : [37 12 72  9 75]\nd : [37 12 72  9 75]\n\n\n( 2 ) 표본평균을 출력한다.\n\nimport numpy as np \n\nm = [] \n\nnp.random.seed(1234) \nfor i in range(100): \n    sample = np.random.randint(0, 10, size = 5) \n    m.append(np.mean(sample))\n    \nm = np.array(m)\nprint(m)\n\n[5.2 6.4 4.4 3.  5.  1.8 3.  3.4 5.4 7.8 3.4 4.8 4.8 4.8 6.4 4.8 4.4 6.\n 2.8 4.8 4.2 4.4 6.8 1.8 6.  4.6 3.2 2.4 2.8 6.2 3.2 6.8 5.8 5.8 4.4 5.\n 4.4 6.  3.6 4.8 4.8 4.  4.4 5.6 5.2 6.2 1.8 3.8 1.4 6.4 3.8 5.2 4.4 4.4\n 4.6 0.4 3.  5.8 2.2 4.  4.4 3.6 5.2 1.6 3.2 5.8 6.8 3.8 6.2 2.6 2.  4.8\n 2.6 6.  7.6 5.6 6.  3.6 4.2 3.8 5.2 3.4 8.2 4.6 6.8 3.8 4.2 3.8 4.6 2.4\n 4.  4.4 6.  4.4 2.6 3.4 6.  4.6 4.6 2.8]\n\n\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\n\nimport matplotlib.pyplot as plt \n\nplt.hist(m, bins=7)\nplt.xlabel('m') \nplt.ylabel('Frquency') \nplt.title('Historam of m')\n\n## 해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해\n## 정규분포에 가까우리라 예상할 수 있다.\n\nText(0.5, 1.0, 'Historam of m')\n\n\n\n\n\n\n\n\n\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8장에서 배운 정규확률그림을 그려본다.\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nsm.qqplot(m, line='s')\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 점들이 거의 직선상의 있으므로\n## 어느 정도 정규분포를 따른다고 할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')"
  },
  {
    "objectID": "da/ida/ida_11.html",
    "href": "da/ida/ida_11.html",
    "title": "11장: 정규모집단에서의 추론",
    "section": "",
    "text": "표본의 크기가 작을 경우에는 일반적인 통계적 추론 방법을 적용하기 어려울 수 있다. 이 경우, 정규분포 대신 t – 분포를 이용한 통계적 추론 방법에 대해 다루고자 한다."
  },
  {
    "objectID": "da/ida/ida_11.html#자료의-입력",
    "href": "da/ida/ida_11.html#자료의-입력",
    "title": "11장: 정규모집단에서의 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 9: 예제 4에 주어져 있는 자료를 가지고 파이썬을 이용하여 \n# μ 에 대한 90% 신뢰구간을 구하고 예제 4에서 실시한 검정도 \n# 파이썬을 이용하여 다시 시행한 후 그 결과를 비교하라. (p.346)\n\n# ------------------------------------------------------------------------------\n\n# 예제 4: 어느 도시의 보건복지과에서는 \n# 그 도시의 상수원인 어느 호수의 수질에 관심이 있다고 한다. \n\n# 수질을 나타내는 하나의 수치로 단위부피당 평균 세균수가 있는데, \n# 그 수가 200 이상이면 상수원으로 적합 하지 않다고 한다. \n\n# 호수의 열 군데에서 물을 떠서 조사한 결과 단위부피당 세균수가 다음과 같이 나타났다. \n# 이 자료로부터 호수의 단위부피당 평균 세균수(μ)가 200보다 적다고 주장할 수 있겠는가? (p.330)\n\nimport numpy as np \nbacteria = np.array([175, 190, 215, 198, 184, 207, 210, 193, 196, 180])\n\n#_______________________________________________________________________________\n\n# 예제 10: 다음에 주어진 자료를 파이썬을 이용하여 분석하고자 한다. (p.348)\n\nx = np.array([31, 35, 37, 38, 38, 38, 39, 40, 40, 41, 42, 43, 44, 44, 46, 48])"
  },
  {
    "objectID": "da/ida/ida_11.html#t-분포",
    "href": "da/ida/ida_11.html#t-분포",
    "title": "11장: 정규모집단에서의 추론",
    "section": "2. t 분포",
    "text": "2. t 분포\nStudent’s t Distribution 통계학에서 모집단의 표본 평균이 정규분포를 따르지 않는 경우에도 사용가능한 분포. 모집단의 분포가 N ( μ, σ2 ) 일 때 크기가 n 인 표본의 평균 x̄ 의 분포는 정확하게 N ( μ, σ2 / n ) 이다.\n이를 표준화한 것이 아래와 같다:\n일반적으로 σ 는 미지수이므로 이를 표본의 표준편차 s 로 추정하여 사용한다. 표본의 크기가 큰 경우, s 로 대체하여도 그 분포가 큰 영향을 받지 않는다.\n그러나 표본의 크기가 작은 경우, 대체하게 되면 표준화된 확률변수의 분포는 표준정규분포와 달라지게 되며, 이를 t 분포 라고 한다.\n정규모집단 N ( μ, σ2 ) 으로부터 임의추출된 표본을 X 1, …, X n 이라고 할 때, 표본 평균과 표본 분산을 아래와 같이 정의한다:\n위 정의가 성립할 때 아래 식이 성립한다:\n자유도가 (n – 1)인 t 분포를 따르고, 이를 기호로써 t (n – 1)로 표현한다.\n표준정규분포와의\n공통점 : 0 을 중심으로 대칭 & 종모양 분포 차이점 : 양 꼬리부분에 상대적으로 많은 확률이 존재 → 더 두꺼운 꼬리를 갖는다. 이때 자유도가 증가하면, t 분포의 꼬리는 표준정규분포의 꼬리에 가까워진다:\ndf = 1 : 자유도가 1인 경우, df = 5 : 자유도가 5인 경우"
  },
  {
    "objectID": "da/ida/ida_11.html#모평균에-대한-추론",
    "href": "da/ida/ida_11.html#모평균에-대한-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3. 모평균에 대한 추론",
    "text": "3. 모평균에 대한 추론\n표본의 크기가 작거나 모집단이 정규분포를 따르지 않는 경우, t – 분포를 사용한다. 이때는 Z α / 2​ 대신 t α / 2 (n − 1) ​을 사용하여 신뢰구간을 계산해야 한다:\n위 식을 μ 에 대해 정리하면:\n10장: 통계적 추론 → 3 - 5 → (2)번의 식은 위와 같이 수정되어야 한다.\n표본 크기가 작을수록\nt – 분포의 임계값이 커지므로, 신뢰구간이 넓어져 표본표준편차의 불확실성을 반영한다. t – 분포를 사용하여 신뢰구간을 더 넓게 잡으므로, 정확한 추정에 도움이 된다. 표본 크기가 충분히 크다면, t – 분포와 정규분포가 거의 같아지므로 이 경우에만, Z – 분포를 사용할 수 있다.\n따라서:\n또는 아래와 같이 나타낼 수 있다:"
  },
  {
    "objectID": "da/ida/ida_11.html#가설-검정",
    "href": "da/ida/ida_11.html#가설-검정",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3-1. 가설 검정",
    "text": "3-1. 가설 검정\n검정통계량은 H 0 가 맞을 때 자유도가 ( n − 1 ) 인 t – 분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n검정통계량이 t 분포를 따르는 경우의 검정을 t 검정이라고 한다."
  },
  {
    "objectID": "da/ida/ida_11.html#신뢰구간과-양측검정의-관계",
    "href": "da/ida/ida_11.html#신뢰구간과-양측검정의-관계",
    "title": "11장: 정규모집단에서의 추론",
    "section": "4. 신뢰구간과 양측검정의 관계",
    "text": "4. 신뢰구간과 양측검정의 관계\nμ 에 대한 100 ( 1 − α ) % 신뢰구간은 아래와 같다:\nH 0 : μ = μ 0 에 대한 양측검정에서의 기각역은 유의수준이 α 일 때 아래와 같다:\n위 기각역의 여집합인 H 0 를 기각하지 못하고 받아들이는 영역을 ’ 채택영역 ’ 이라고 할 때\n이 채택영역은 다음과 같다:\n이를 μ 0 를 중심으로 풀어쓰면 다음과 같다:\n위 과정을 바탕으로 다음과 같은 결론을 내릴 수 있다:\n모수 θ 에 대한 100 ( 1 − α ) % 신뢰구간이 ( L, U ) 로 구해졌을 때, 가설 H 0 : θ = θ 0 대 H 1 : θ ≠ θ 0 에 대하여 유의수준 α 로 검정을 시행할 때의 결론을 의미한다."
  },
  {
    "objectID": "da/ida/ida_11.html#모표준편차의-추론",
    "href": "da/ida/ida_11.html#모표준편차의-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "5. 모표준편차의 추론",
    "text": "5. 모표준편차의 추론\n모표준편차의 추정과 검정에서는 정규성 가정이 중요한 역할을 한다. 이 가정이 충족되지 않으면, 신뢰구간 계산이나 가설 검정의 결과를 신뢰할 수 없다.\n모표준편차 σ 를 추정하는 과정은 모분산 σ 2 에 대한 추정에서 출발하며, 이 모분산 σ 2 에 대한 추정에서 사용되는 것이 표본분산이다:\n\n5-1. 점추정\nPoint Estimation 모집단의 모수를 단일 값으로 추정하는 방법이다. 모표준편차 σ 의 경우, 점추정은 표본표준편차를 사용하는 방식이다:\n\n\n5-2. 구간추정\nInterval Estimation 모집단의 모수를 특정 신뢰수준에서 포함할 것으로 예상되는 구간을 제공하는 방법이다. 모표준편차 σ 에 대한 구간추정은 모분산에 대한 신뢰구간을 기반으로 하여 계산되며, 이때 모분산 s 2 의 신뢰구간을 구하기 위해 카이제곱 분포를 사용한다.\n\n\n5-3. 카이제곱 분포\nChi-Square Distribution 표본 분산을 모집단 분산과 비교하거나 범주형 변수 간의 독립성을 검정할 때 유용한 분포.\n자유도에 따라 그 형태가 달라지며, 자유도가 커질수록 정규 분포와 유사해진다. 아래 수식은 표본 분산을 모집단 분산으로 표준화한 것이다:\n자유도가 (n – 1)인 x2 분포를 따르고, 이를 기호로써 x2 (n – 1)로 표현한다. 모집단 σ 2 의 신뢰구간을 계산하기 위해 카이제곱 통계량을 사용한다: 위 분포로부터 구한 신뢰구간은 아래 식과 같다:\n위 식에서 괄호 안에 있는 부등식을 σ 2 을 중심으로 풀어 쓰면 다음과 같다:\n따라서 이 식으로부터 σ 2 의 100 ( 1 − α ) % 신뢰구간을 구하면 다음과 같다:\n표준편차 σ 는 σ 2 의 양의 제곱근이므로,그의 신뢰구간은 σ 2 의 신뢰구간의 경곗값의 제곱근을 취하여 얻을 수 있다: 결론적으로, σ 에 대한 100 ( 1 − α ) % 신뢰구간은 위와 같다.\n검정통계량은 H 0 이 맞을 때 자유도 df 인 카이제곱분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 9의 ( 1 ) bacteria 에 대한 요약 통계량 계산하기.\n\nxbar_b = np.mean(bacteria);print(xbar_b) # 평균\n\nvar_b = np.var(bacteria, ddof=1);print(var_b) # 분산 (자유도 1 사용)\n\nsd_b = np.std(bacteria, ddof=1);print(sd_b) # 표준편차 (자유도 1 사용)\n\nmedian_b = np.median(bacteria);print(median_b) # 중앙값\n\n194.8\n172.62222222222226\n13.138577633146681\n194.5\n\n\n\nmin_b = np.min(bacteria);print(min_b) # 최솟값\n\nmax_b = np.max(bacteria);print(max_b) # 최댓값\n\nsum_b = np.sum(bacteria);print(sum_b) # 합계\n\nn = bacteria.size;print(n) # 데이터 개수\n\n175\n215\n1948\n10\n\n\n예제 9의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수 t – 분포의 백분위수 함수\n\nfrom scipy import stats \n\nse_b = stats.sem(bacteria); print(se_b) # 표본표준오차\n\n# 유의수준 0.1에 해당하는 t-분포의 임계값 \nt_alpha = stats.t.ppf(1 - 0.1 / 2, n - 1); print(t_alpha)\n\ninterval = t_alpha * se_b;print(interval) # 신뢰구간의 범위를 계산\n\nCI = [xbar_b - interval, xbar_b + interval]; print(CI) # 신뢰구간\n\n4.154783053568769\n1.8331129326536335\n7.6161865478670645\n[np.float64(187.18381345213294), np.float64(202.41618654786708)]\n\n\n90 % 신뢰구간은 194.8 ± 7.616 임을 알 수 있다.\n예제 9의 ( 3 ) 검정하고자 하는 가설은 H 0 : μ = 200 대 H 1 : μ &lt; 200 이며, 표본의 크기는 10 이다.\n\ntval = (xbar_b - 200) / se_b;print(tval) \n\n# 단측검정: 귀무가설 μ=200, 대립가설 μ&lt;200\npval = stats.t.cdf(tval, n - 1);print(pval)\n\n## 해석: P–값이 0.1211 로 유의수준 5% 에서 귀무가설을 기각할 수 없으므로\n## 주어진 10 개의 자료로부터 호수의 단위 부피당 평균세균수가 200 보다 \n## 적다고 안심할 수 없다.\n\n-1.2515695604210733\n0.12113884687382763\n\n\n예제 10의 ( 1 ) x 에 대한 요약 통계량 계산하기.\n\nxbar_x = np.mean(x);print(xbar_x) # 평균\n\nvar_x = np.var(x, ddof=1);print(var_x) # 분산 (자유도 1 사용)\n\nsd_x = np.std(x, ddof=1);print(sd_x) # 표준편차 (자유도 1 사용)\n\nmedian_x = np.median(x);print(median_x) # 중앙값\n\nmin_x = np.min(x);print(min_x) # 최솟값\n\nmax_x = np.max(x);print(max_x) # 최댓값\n\nsum_x = np.sum(x);print(sum_x) # 합계\n\nn = x.size;print(n) # 데이터 개수\n\n40.25\n18.2\n4.266145801540309\n40.0\n31\n48\n644\n16\n\n\n예제 10의 ( 2 ) x 에 대한 정규확률그림 그리기.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 정규확률그림 그리기\nsm.qqplot(x, line='s')\nplt.title(\"Normal Q-Q Plot\")\n\nText(0.5, 1.0, 'Normal Q-Q Plot')\n\n\n\n\n\n\n\n\n\n예제 10의 ( 3 ) 모평균 μ 에 대한 95% 신뢰구간을 구한다.\n\nfrom scipy import stats \n\nse = stats.sem(x);print(se) # 표준편차\n\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n - 1);print(t_alpha) # 95% 신뢰구간을 위한 t값 \n\ninterval = t_alpha * se;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_x - interval, xbar_x + interval];print(CI) # 신뢰구간\n\n1.0665364503850772\n2.131449545559323\n2.2732686324957263\n[np.float64(37.97673136750427), np.float64(42.52326863249573)]\n\n\n95 % 신뢰구간은 40.25 ± 2.273 임을 알 수 있다.\n예제 10의 ( 4 ) 검정하고자 하는 가설은 H 0 : μ = 38 대 H 1 : μ &gt; 38 이며, 표본의 크기는 16 이다.\n\ntval = (xbar_x - 38) / se;print(tval) # 표본표준오차\n\npval = 1 - stats.t.cdf(tval, n - 1);print(pval) # p값\n\n## 해석: P–값이 0.026 이므로 유의수준 5% 에서 귀무가설을 기각하게 된다.\n## 따라서, 평균이 38 보다 크다고 할 수 있다.\n\n2.109632539223229\n0.026050840503660355"
  },
  {
    "objectID": "da/dap/dap_02.html",
    "href": "da/dap/dap_02.html",
    "title": "Clustering",
    "section": "",
    "text": "비지도 학습(Unsupervised Learning) 기법의 한 유형이다.\n사전에 정의된 타겟 변수(종속 변수)가 존재하지 않는 데이터로부터 데이터 간 유사성 또는 거리(distance)를 기반으로 군집(cluster)을 형성하는 방법론이다.\n이는 데이터가 어떠한 구조를 내재하고 있을 것으로 가정하되, 그 구조의 형태—군집의 개수, 모양, 분포—가 사전에 알려져 있지 않은 상태에서 적용된다.\n군집 분석의 핵심 목적은 다음 두 가지로 요약된다. 1. 군집 형성(Clustering): 개체들 간 거리 계산을 통해 자연스러운 그룹을 형성 2. 군집 해석(Cluster Interpretation): 형성된 군집의 특성과 군집 간 관계 구조를 분석하여 의미를 도출\n\n\n군집 분석에서 가장 기초적이며 중요한 요소는 데이터 간 거리(distance) 또는 유사성(similarity) 계산 방식이다. 거리 측정 방식에 따라 군집 결과는 크게 달라지므로, 데이터의 특성(연속형/희소벡터/텍스트 등)에 따라 적절한 측도를 선택해야 한다.\n측도의 형태가 다르더라도, 군집 분석에서는 “거리 기반으로 개체 간 유사성을 정의한다는 점”이 공통적이다.\n\n\nEuclidean Distance 연속형 변수에서 가장 일반적으로 사용되는 거리 척도로, L2 노름(Norm)에 해당한다. 두 관측치 \\(x_i, x_j \\in \\mathbb{R}^p\\) 사이의 유클리드 거리는 다음과 같이 정의된다.\n\\[\nd_{\\mathrm{euclid}}(x_i, x_j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}\n\\]\n예: 2차원 데이터 \\(p_1=(x_1, ~y_1), ~~~p_2=(x_2, ~y_2)\\) 의 경우, 다음과 같이 계산할 수 있다.\n\\[\nd(p_1, ~~~p_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\] \n\n\n각 변수의 단위가 상이할 경우, 거리 계산이 왜곡될 수 있으므로 표준화가 필요하다.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\n\\(\\mu\\): 평균\n\\(\\sigma\\): 표준편차\n\n\n\n\n\n모든 개체를 단일 군집(singleton cluster)으로 초기화한다. \\[\nC_1 = {x_1}, C_2 = {x_2}, \\dots, C_n = {x_n}\n\\]\n현재 존재하는 군집들 간 거리 행렬 \\(D_0\\) 를 계산한다.\n\n행렬은 대칭이며, 각 원소 \\(d(C_i,C_j)\\) 는 군집 \\(C_i, C_j\\) 간 거리이다.\n\n거리 행렬에서 가장 가까운 군집 쌍 \\((C_p,C_q)\\) 을 선택하여 병합한다.\n\n행렬 크기는 1줄씩 감소하며, 새로운 군집 \\(C_{new}=C_p \\cup C_q\\) 가 생성된다.\n\n새로운 군집과 나머지 군집 간 거리를 연결법(Linkage Method)에 따라 재계산한다.\n이 과정을 반복하여 최종적으로 모든 개체가 하나의 군집으로 통합될 때까지 진행한다.\n\n\n\n\n\nManhattan Distance L1 노름 기반 거리로, 고차원 데이터에서 유리할 수 있다.\n\\[\nd_{\\text{manhattan}}(x_i, x_j) = \\sum_{k=1}^{p}\\left|x_{ik} - x_{jk}\\right|\n\\]\n\n\n\nCosine Similarity 텍스트 마이닝 분야에서 주로 사용되며, 벡터 방향의 유사성을 측정한다.\n\\[\n\\text{cos}(x_i, x_j) = \\frac{x_i \\cdot x_j}{|x_i||x_j|}\n\\]\n코사인 거리(Cosine Distance)는 다음과 같이 정의된다.\n\\[\nd_{\\text{cosine}} = 1 - \\text{cos}(x_i, x_j)\n\\]\n\n\n\n\n\n\n\nk-means 기반 군집화 모델은 군집을 수학적으로 구형(spherical) 구조로 가정한다. k-means의 목적함수는 다음을 최소화한다.\n\\[\n\\min_{C_1, ..., C_K} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} |x_i - \\mu_k|^2\n\\]\n이는 각 군집 중심(centroid)으로부터의 제곱거리 최소화를 가정하므로, 군집이 타원형이 아닌 구형에 가까울 때 성능이 가장 안정적이다.\n\n\n\n아래의 경우는 k-means 모델에서 성능이 저하되는 대표 사례이다.\n\n군집의 형태가 길고 가는 모양(elongated cluster)인 경우\n\n구형 중심 거리 기준으로는 정확히 분리되지 않는다.\n\n개체 A, B가 서로 다른 군집 사이에서 ‘다리’ 역할을 하는 중간 위치에 존재하는 경우\n\n두 군집이 실제로 분리되어 있어도 k=2 가정에서 중심이 왜곡된다.\n\n\n이와 같은 경우에는 DBSCAN, 계층적 군집 등 모양 제약이 없는 알고리즘이 더 유리하다.\n\n\n\n\n군집 해석과 군집 수 결정에서 다양한 지표가 사용된다.\n\n\nSilhouette Coefficient\n개체 \\(i\\) 에 대해 * \\(a(i)\\): 같은 군집 내 평균 거리 * \\(b(i)\\): 가장 가까운 다른 군집과의 평균 거리\n실루엣 값은 다음과 같다:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max {a(i), b(i)}}\n\\]\n실루엣 계수는 군집의 응집도(cohesion)와 분리도(separation)를 동시에 평가하는 지표이다.\n\n\n\nElbow Method SSE(Sum of Squared Errors)의 감소율을 관찰하여 변곡점(elbow)을 최적 군집 개수로 간주한다. 수식은 k-means 목적함수와 동일하다.\n\n\n\n\n\n실무에서는 단순히 거리를 계산하여 k-means를 적용하는 것이 아니라, 다음과 같은 요소가 필수적으로 고려된다.\n\n정규화/표준화(Scaling):\n\n변수 간 단위 차이로 인한 거리 왜곡 방지\n\n차원 축소(PCA, t-SNE 등):\n\n고차원에서의 거리 희석 현상 해결\n\n거리 측정 방식 선택:\n\n텍스트 → 코사인\n연속형 수치 → 유클리드\n이상치 존재 → 맨해튼\n\n알고리즘 선택\n\n구형 군집 → k-means\n임의 형태의 군집 → DBSCAN\n계층적 구조 중요 → Hierarchical Clustering\n\n\n\n\n\n\n군집 분석은 다양한 산업 분야에서 핵심 기법으로 활용된다.\n\n금융 Finance\n\n신용카드 소비 패턴 분석\n리스크 기반 고객 세그멘테이션\n사기 탐지(비정상 패턴 발견)\n\n마케팅 Marketing\n\n고객 세분화(Customer Segmentation)\n구매 행동 기반 타겟 마케팅\n추천 시스템의 사용자 군집화\n\n헬스케어 Healthcare\n\n환자 유형 분류\n질병 패턴 분석\n개인 맞춤형 치료 전략 개발\n\n제조업 Manufacturing\n\n불량 패턴 탐지\n공정 조건 기반 군집화\n유지보수(Preventive Maintenance) 최적화\n\n\n군집 분석은 특히 세그멘테이션(Segmentation) 분야에서 실무적 가치가 매우 높다."
  },
  {
    "objectID": "da/dap/dap_02.html#거리유사성-측정-방법론",
    "href": "da/dap/dap_02.html#거리유사성-측정-방법론",
    "title": "Clustering",
    "section": "",
    "text": "군집 분석에서 가장 기초적이며 중요한 요소는 데이터 간 거리(distance) 또는 유사성(similarity) 계산 방식이다. 거리 측정 방식에 따라 군집 결과는 크게 달라지므로, 데이터의 특성(연속형/희소벡터/텍스트 등)에 따라 적절한 측도를 선택해야 한다.\n측도의 형태가 다르더라도, 군집 분석에서는 “거리 기반으로 개체 간 유사성을 정의한다는 점”이 공통적이다.\n\n\nEuclidean Distance 연속형 변수에서 가장 일반적으로 사용되는 거리 척도로, L2 노름(Norm)에 해당한다. 두 관측치 \\(x_i, x_j \\in \\mathbb{R}^p\\) 사이의 유클리드 거리는 다음과 같이 정의된다.\n\\[\nd_{\\mathrm{euclid}}(x_i, x_j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}\n\\]\n예: 2차원 데이터 \\(p_1=(x_1, ~y_1), ~~~p_2=(x_2, ~y_2)\\) 의 경우, 다음과 같이 계산할 수 있다.\n\\[\nd(p_1, ~~~p_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\] \n\n\n각 변수의 단위가 상이할 경우, 거리 계산이 왜곡될 수 있으므로 표준화가 필요하다.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\n\\(\\mu\\): 평균\n\\(\\sigma\\): 표준편차\n\n\n\n\n\n모든 개체를 단일 군집(singleton cluster)으로 초기화한다. \\[\nC_1 = {x_1}, C_2 = {x_2}, \\dots, C_n = {x_n}\n\\]\n현재 존재하는 군집들 간 거리 행렬 \\(D_0\\) 를 계산한다.\n\n행렬은 대칭이며, 각 원소 \\(d(C_i,C_j)\\) 는 군집 \\(C_i, C_j\\) 간 거리이다.\n\n거리 행렬에서 가장 가까운 군집 쌍 \\((C_p,C_q)\\) 을 선택하여 병합한다.\n\n행렬 크기는 1줄씩 감소하며, 새로운 군집 \\(C_{new}=C_p \\cup C_q\\) 가 생성된다.\n\n새로운 군집과 나머지 군집 간 거리를 연결법(Linkage Method)에 따라 재계산한다.\n이 과정을 반복하여 최종적으로 모든 개체가 하나의 군집으로 통합될 때까지 진행한다.\n\n\n\n\n\nManhattan Distance L1 노름 기반 거리로, 고차원 데이터에서 유리할 수 있다.\n\\[\nd_{\\text{manhattan}}(x_i, x_j) = \\sum_{k=1}^{p}\\left|x_{ik} - x_{jk}\\right|\n\\]\n\n\n\nCosine Similarity 텍스트 마이닝 분야에서 주로 사용되며, 벡터 방향의 유사성을 측정한다.\n\\[\n\\text{cos}(x_i, x_j) = \\frac{x_i \\cdot x_j}{|x_i||x_j|}\n\\]\n코사인 거리(Cosine Distance)는 다음과 같이 정의된다.\n\\[\nd_{\\text{cosine}} = 1 - \\text{cos}(x_i, x_j)\n\\]"
  },
  {
    "objectID": "da/dap/dap_02.html#군집-형성의-구조적-특징",
    "href": "da/dap/dap_02.html#군집-형성의-구조적-특징",
    "title": "Clustering",
    "section": "",
    "text": "k-means 기반 군집화 모델은 군집을 수학적으로 구형(spherical) 구조로 가정한다. k-means의 목적함수는 다음을 최소화한다.\n\\[\n\\min_{C_1, ..., C_K} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} |x_i - \\mu_k|^2\n\\]\n이는 각 군집 중심(centroid)으로부터의 제곱거리 최소화를 가정하므로, 군집이 타원형이 아닌 구형에 가까울 때 성능이 가장 안정적이다.\n\n\n\n아래의 경우는 k-means 모델에서 성능이 저하되는 대표 사례이다.\n\n군집의 형태가 길고 가는 모양(elongated cluster)인 경우\n\n구형 중심 거리 기준으로는 정확히 분리되지 않는다.\n\n개체 A, B가 서로 다른 군집 사이에서 ‘다리’ 역할을 하는 중간 위치에 존재하는 경우\n\n두 군집이 실제로 분리되어 있어도 k=2 가정에서 중심이 왜곡된다.\n\n\n이와 같은 경우에는 DBSCAN, 계층적 군집 등 모양 제약이 없는 알고리즘이 더 유리하다."
  },
  {
    "objectID": "da/dap/dap_02.html#군집의-품질-평가-지표",
    "href": "da/dap/dap_02.html#군집의-품질-평가-지표",
    "title": "Clustering",
    "section": "",
    "text": "군집 해석과 군집 수 결정에서 다양한 지표가 사용된다.\n\n\nSilhouette Coefficient\n개체 \\(i\\) 에 대해 * \\(a(i)\\): 같은 군집 내 평균 거리 * \\(b(i)\\): 가장 가까운 다른 군집과의 평균 거리\n실루엣 값은 다음과 같다:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max {a(i), b(i)}}\n\\]\n실루엣 계수는 군집의 응집도(cohesion)와 분리도(separation)를 동시에 평가하는 지표이다.\n\n\n\nElbow Method SSE(Sum of Squared Errors)의 감소율을 관찰하여 변곡점(elbow)을 최적 군집 개수로 간주한다. 수식은 k-means 목적함수와 동일하다."
  },
  {
    "objectID": "da/dap/dap_02.html#알고리즘-선택과-실무적-전처리-요건",
    "href": "da/dap/dap_02.html#알고리즘-선택과-실무적-전처리-요건",
    "title": "Clustering",
    "section": "",
    "text": "실무에서는 단순히 거리를 계산하여 k-means를 적용하는 것이 아니라, 다음과 같은 요소가 필수적으로 고려된다.\n\n정규화/표준화(Scaling):\n\n변수 간 단위 차이로 인한 거리 왜곡 방지\n\n차원 축소(PCA, t-SNE 등):\n\n고차원에서의 거리 희석 현상 해결\n\n거리 측정 방식 선택:\n\n텍스트 → 코사인\n연속형 수치 → 유클리드\n이상치 존재 → 맨해튼\n\n알고리즘 선택\n\n구형 군집 → k-means\n임의 형태의 군집 → DBSCAN\n계층적 구조 중요 → Hierarchical Clustering"
  },
  {
    "objectID": "da/dap/dap_02.html#실무-적용-분야",
    "href": "da/dap/dap_02.html#실무-적용-분야",
    "title": "Clustering",
    "section": "",
    "text": "군집 분석은 다양한 산업 분야에서 핵심 기법으로 활용된다.\n\n금융 Finance\n\n신용카드 소비 패턴 분석\n리스크 기반 고객 세그멘테이션\n사기 탐지(비정상 패턴 발견)\n\n마케팅 Marketing\n\n고객 세분화(Customer Segmentation)\n구매 행동 기반 타겟 마케팅\n추천 시스템의 사용자 군집화\n\n헬스케어 Healthcare\n\n환자 유형 분류\n질병 패턴 분석\n개인 맞춤형 치료 전략 개발\n\n제조업 Manufacturing\n\n불량 패턴 탐지\n공정 조건 기반 군집화\n유지보수(Preventive Maintenance) 최적화\n\n\n군집 분석은 특히 세그멘테이션(Segmentation) 분야에서 실무적 가치가 매우 높다."
  },
  {
    "objectID": "da/dap/dap_02.html#병합적-계층-군집-분석의-절차",
    "href": "da/dap/dap_02.html#병합적-계층-군집-분석의-절차",
    "title": "Clustering",
    "section": "1. 병합적 계층 군집 분석의 절차",
    "text": "1. 병합적 계층 군집 분석의 절차\n\n1.1 초기 단계\n분석 대상 개체가 \\(n\\) 개라고 할 때, 초기에는 모든 개체가 단독 군집으로 간주된다.\n\\[\nC_1 = {x_1}, C_2 = {x_2}, \\ldots, C_n = {x_n}\n\\]\n이후 각 군집 간 거리(유사성)가 거리 행렬(distance matrix)로 표현되며, 이 행렬은 군집 병합 과정에서 매 단계 재계산된다.\n\n\n1.2 단계별 병합(algo) 과정\n각 단계에서는 다음 두 규칙이 반복적으로 적용된다.\n\n현재 존재하는 모든 군집 쌍 중 가장 가까운 군집을 찾는다. \\[\n(C_p, C_q)=\\arg\\min_{C_i, C_j} d(C_i,C_j)\n\\]\n해당 두 군집을 하나의 군집으로 병합한다. \\[\nC_{new}=C_p \\cup C_q\n\\]\n병합 후, 새로운 군집과 다른 군집 간의 거리를 ’연결법(Linkage Method)’에 따라 재계산한다. 이 과정이 반복되어 최종적으로 하나의 군집으로 통합된다. \\[\nn \\rightarrow n-1 \\rightarrow n-2 \\rightarrow \\cdots \\rightarrow 1\n\\]\n\n이러한 병합 과정을 시각적으로 나타낸 것이 덴드로그램(Dendrogram)이며, 수평선의 높이(height)는 해당 병합 단계에서의 군집 간 거리 혹은 이질성(Heterogeneity)을 나타낸다."
  },
  {
    "objectID": "da/dap/dap_02.html#연결법",
    "href": "da/dap/dap_02.html#연결법",
    "title": "Clustering",
    "section": "2. 연결법",
    "text": "2. 연결법\nLinkage Methods 군집 간 거리 계산 방식은 계층적 군집 분석의 결과에 직접적으로 영향을 미치는 핵심 요소이다. 아래는 대표적 연결법들의 정의, 수학적 공식, 특징, 구조적 영향을 상세히 정리한 것이다.\n\n2.1 최단 연결법\nSingle Linkage 두 군집 간 최소 거리(minimum pairwise distance)를 사용한다. \\[\nd_{\\text{single}}(C_i,C_j)=\\min_{x \\in C_i,, y \\in C_j} d(x,y)\n\\]\n특징: * Chain Effect(사슬 현상) 발생 가능성이 높음 (길게 늘어지는 패턴이 나타나며, 여러 개체가 얇은 줄처럼 연결되어 있는 구조) * 개별 데이터들이 사슬처럼 연결되어 길게 늘어난 형태의 군집이 형성될 수 있음 * 군집 모양 취약: 좁고 길게 늘어진(Elongated) 군집에서는 적합하지 않음 * 잡음과 이상치 민감: 외곽 점(outlier)에 의해 군집 구조가 쉽게 왜곡됨\n실무적 주의: * 단순 거리만 고려하므로, 실제 데이터의 밀도나 분포를 충분히 반영하지 못할 수 있음 * 군집 결과가 직관적이지 않거나 왜곡될 수 있으므로, 데이터 특성을 고려하여 다른 연결법과 병행 평가 필요\n\n\n2.2 최장 연결법\nComplete Linkage 두 군집 간 최대 거리(maximum pairwise distance)를 사용한다. \\[\nd_{\\text{complete}}(C_i,C_j)=\\max_{x \\in C_i,, y \\in C_j} d(x,y)\n\\]\n특징: * 군집 내부가 조밀(compact)하게 유지됨 (각 군집 내 데이터 간 최대 거리를 고려하기 때문) * 이상치와 잡음의 영향을 Single linkage 대비 상대적으로 덜 받음 (균형 잡힌 군집 구조(Balanced Cluster Structure) 생성) * 덴드로그램 상에서 병합 높이가 일정하게 유지되어 구조가 시각적으로 균형 있게 나타남\n실무적 고려: * 분류 경계가 명확해야 하는 경우 유용 * 군집 간 거리 기준이 엄격하여, 너무 작은 군집이 과도하게 분리되는 경우 주의 필요\n\n\n2.3 평균 연결법\nAverage Linkage / UPGMA 군집 간 모든 개체 쌍의 거리 평균을 사용한다.\n\\[\nd_{\\text{average}}(C_i,C_j)\n= \\frac{1}{|C_i|\\cdot |C_j|}\n\\sum_{x\\in C_i}\\sum_{y\\in C_j} d(x,y)\n\\]\n특징: * 군집 간 전체적 거리 구조를 반영 (단일 연결법의 사슬 현상 + 최장 연결법의 지나친 조밀화를 완화) * 극단적 이상치에 대한 민감도가 단일/최장 연결법보다 낮음 * 평균 기반 병합으로 군집 내 구조를 보다 세밀하게 반영 * 모든 데이터 쌍의 평균 거리 기반으로 병합이 이루어짐 * 병합 높이가 극단적으로 치우치지 않고, 일정한 간격을 유지하며 균형 있는 시각적 구조를 보여줌\n\n\n2.4 중심 연결법\nCentroid Linkage 각 군집의 중심(centroid)을 계산한 뒤 중심 간 거리로 측정.\n\\[\n\\text{군집} C_i \\text{의 중심}: \\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i} x\n\\]\n\\[\n\\text{군집 간 거리}: d_{\\text{centroid}}(C_i,C_j)=|\\mu_i-\\mu_j|\n\\]\n특징: * 중심 계산과 거리 계산이 행렬 연산으로 처리 가능하여 구현 용이 * 군집이 선형적으로 분리되거나 중심 기반 구조가 뚜렷한 경우 성능이 우수 * 중심만 계산하면 되므로, 반복 연산이 많은 대규모 데이터에서 계산이 비교적 효율적 * 중심 이동으로 인해 Single/Complete/Average보다 덴드로그램의 구조가 덜 안정적일 수 있음 * 역병합(Reversal) 발생 가능성 높음 (군집 병합 후 새로운 중심이 기존 거리 구조를 뒤흔들어 덴드로그램 높이가 역전되는 비단조성(non-monotonicity) 문제가 발생하기 쉬움) * 컷(cut) 기준의 주관성 (덴드로그램의 높이가 단조 증가하지 않아, 군집 수 결정 시 절단 시점 판단이 더 주관적일 수 있음)\n\n\n2.5 중위수 연결법\nMedian Linkage 두 군집 중심의 중위값(median)을 기반으로 정의하며, Centroid linkage와 유사하나 중심 계산 방법이 다르다.\n\\[\n\\text{병합 후 새로운 중심}: \\mu_{new}=\\frac{1}{2}(\\mu_i+\\mu_j)\n\\]\n특징: * 중위수 사용으로 극단값의 영향을 평균보다 적게 받음 (단, 전체 군집 구조 안정성 문제를 해결할 수준의 강인성(robustness)은 아님) * 중위수 기반이므로 계산 과정은 상대적으로 단순하고 구현 난이도도 낮음\n\n역병합(Reversal) 발생 가능성 높음 (Centroid와 마찬가지로 병합 후 새 중위수 위치가 기존 거리 구조를 비단조적으로 변화시켜 덴드로그램 높이 역전(Non-monotonicity)이 발생할 수 있음)\n여러 연구에서 Centroid와 유사하게 군집 구조가 불안정하다는 보고가 존재 (특히 군집 분리 기준이 덴드로그램에서 명확하지 않은 경우가 잦음)\n데이터가 비정상적 분포(heavy-tailed)거나 극단값이 많은 경우 평균 기반보다 중위수 기반이 유리할 수 있음 (그러나 덴드로그램 해석의 비단조성 문제와 불안정성 때문에 Single, Complete, Average, Ward 방식보다 권장 빈도가 현저히 낮음)\n따라서 실무·통계 패키지에서 기본 옵션으로 잘 사용되지 않으며, 실증 연구에서도 활용 빈도가 다른 연결법 대비 매우 낮음\n\n\n\n2.6 Ward의 방법\nWard’s Minimum Variance Method 군집 병합 시 전체 군집 내 오류제곱합(SSE, Within-Cluster Sum of Squares)의 증가를 최소화하는 방법.\n\\[\n\\text{군집} C \\text{의} SSE: \\quad SSE(C)=\\sum_{x\\in C}|x-\\mu_C|^2\n\\]\nWard 방식은 다음을 최소화한다: \\[\n\\Delta = SSE(C_i \\cup C_j) - (SSE(C_i)+SSE(C_j))\n\\]\n특징: * 가장 널리 사용되는 연결법 * 군집이 구형(spherical) 형태로 형성됨 (분산 최소화 원리로 인해 밀집되고 균형 잡힌 구형 구조를 만들기 쉬움) * 단일·최장 연결법보다 이상치 영향이 적고 안정적인 군집 구조를 형성 (병합 높이(height)가 비교적 균일하게 증가 → 매우 안정적이고 해석이 쉬운 덴드로그램 구조를 제공 군집 간 병합 폭이 일정해 분기(branch)가 균형적으로 나타남) * 군집 내 제곱합 기반이라 군집 간 차이가 직관적으로 해석 가능 * k-means와 유사한 알고리즘적 성향 (분산 최소화) (둘 다 군집 내 변동을 최소화하는 방향으로 동작 → 결과 군집 형태가 유사해지는 경향.)\n실무적 고려: * 데이터가 연속형이며, 군집이 구형에 가까운 구조일 때 가장 적합 * 고차원 데이터에서도 안정적이지만, 분산 계산 특성상 변수 스케일링(표준화)이 필수 * 비구형 구조(long, chain-like cluster)를 가진 데이터에서는 과도하게 조밀한 군집이 생성될 수 있음"
  },
  {
    "objectID": "da/dap/dap_02.html#실무적-고려-사항",
    "href": "da/dap/dap_02.html#실무적-고려-사항",
    "title": "Clustering",
    "section": "3. 실무적 고려 사항",
    "text": "3. 실무적 고려 사항\n계층적 군집 분석은 다음과 같은 실무적 특성이 존재한다.\n\n연산 복잡도\n\n\n거리 행렬 계산: \\(O(n^2)\\)\n전체 병합 과정: \\(O(n^3)\\) (일반적 구현 기준)\n\n→ 데이터가 많아지면 실무에서 수천 개 이상은 사실상 불가능 → 샘플링, 차원 축소 병행 필요\n\n데이터 전처리 필요성\n\n\n거리 기반이므로 표준화(Standardization) 필수 \\[\nx'=\\frac{x-\\mu}{\\sigma}\n\\]\n이상치(outlier)에 매우 민감 → 사전 처리 필요\n고차원 데이터에서는 차원의 저주로 성능 저하 → PCA 필요\n\n\n군집 수 결정\n\n\n덴드로그램의 컷 높이(cut height)\n비일관성 계수(inconsistency coefficient)\n코페네틱 상관계수(cophenetic correlation coefficient) 등 고려\n\n\n주관성 존재\n\n\n덴드로그램 컷(cut height) 결정, 연결법 선택,\n최종 군집 수 결정은 분석자의 경험과 목적에 크게 의존\n\n\n실무 활용\n\n\n단위가 다른 변수는 반드시 표준화 필요\n통계적 솔루션만 적용하면 데이터의 감성적 패턴 반영 부족\n마케팅/금융에서 고객 세그멘테이션, 1년 단위 갱신 등 경험적 기준 적용\n\n\n알고리즘 관점\n\n\n최단, 최장, 평균, 중심 연결법 결과는 병합 순서나 군집 모양에서 차이가 발생하지만, 최종 그룹화 자체는 모두 유사\n관점 차이일 뿐, 전체적인 군집 구조 탐색에는 유용함"
  },
  {
    "objectID": "da/dap/dap_02.html#k-평균-군집의-목적-함수",
    "href": "da/dap/dap_02.html#k-평균-군집의-목적-함수",
    "title": "Clustering",
    "section": "1. K-평균 군집의 목적 함수",
    "text": "1. K-평균 군집의 목적 함수\nK-평균 알고리즘은 다음의 목적 함수(Objective Function)를 최소화하는 문제로 정의된다.\n\\[\n\\min_{C_1,\\dots,C_K} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} | x_i - \\mu_k |^2\n\\]\n\n\\(C_k\\): k번째 군집\n\\(\\mu_k\\): k번째 군집의 중심(centroid)\n\\(x_i\\): 군집에 속한 데이터 포인트\n\\(|x_i - \\mu_k|^2\\): 유클리드 거리의 제곱\n목적: 군집 내 제곱합(WSS, Within-Cluster Sum of Squares)의 최소화\n\n이 함수가 최소화될 때, 각 군집은 내부적으로 가장 조밀하며, 군집 간 분리는 상대적으로 크다."
  },
  {
    "objectID": "da/dap/dap_02.html#k-평균-알고리즘-절차",
    "href": "da/dap/dap_02.html#k-평균-알고리즘-절차",
    "title": "Clustering",
    "section": "2. K-평균 알고리즘 절차",
    "text": "2. K-평균 알고리즘 절차\nK-평균 알고리즘의 표준 절차는 다음과 같다.\n\n군집 수(K)의 결정 K는 사전에 사용자가 결정해야 하는 하이퍼파라미터이다. 일반적으로 엘보우 기법(Elbow method), 실루엣 계수(Silhouette coefficient), Gap Statistic 등으로 후보값을 선정한다.\n\n특히 엘보우 기법은 실무에서 가장 흔하게 사용된다.\n\n초기 중심 선택 Initial Centroid 초기 중심은 임의로 선택하거나, K-means++ 등 보다 안정적인 방법으로 선정할 수 있다. 초기값에 따라 최종 군집 결과가 달라질 수 있어 초기 중심 선택은 중요한 단계이다.\n개체의 군집 할당 Assignment Step 각 데이터 포인트 \\(x_i\\) 는 가장 가까운 중심 \\(\\mu_k\\) 에 할당된다. 거리 계산은 일반적으로 유클리드 거리로 수행한다: \\[\nd(x_i, \\mu_k) = \\sqrt{\\sum_{j=1}^{p} (x_{ij} - \\mu_{kj})^2}\n\\]\n\n\n\\(p\\): 변수의 개수\n\n\n군집 중심 재계산 Update Step 각 군집에 속한 데이터의 평균 벡터를 이용해 새로운 중심을 계산한다:\n\n\\[\n\\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i\n\\]\n각 개체가 추가될 때마다(혹은 반복적 재계산 과정에서) 중심값이 변동한다.\n\n수렴 조건 충족 시까지 반복 다음 중 하나의 조건이 충족하면 알고리즘은 종료된다.\n\n\n중심 벡터 변화량이 특정 임계값 이하일 때 \\[\n|\\mu_k^{(t+1)} - \\mu_k^{(t)}| &lt; \\epsilon\n\\]\n일정 횟수(T) 이상 반복 수행\nWSS 변화가 미미할 때\n\n이러한 반복 과정으로 K-means는 군집 내 동질성이 가장 높은 상태로 수렴한다."
  },
  {
    "objectID": "da/dap/dap_02.html#엘보우-기법-1",
    "href": "da/dap/dap_02.html#엘보우-기법-1",
    "title": "Clustering",
    "section": "3. 엘보우 기법",
    "text": "3. 엘보우 기법\nElbow 엘보우 기법은 군집 수 K를 선택하기 위한 정형화된 방법이다.\n\nWSS 정의\n\n\\[\nWSS(K) = \\sum_{k=1}^{K}\\sum_{x_i\\in C_k}|x_i - \\mu_k|^2\n\\]\n군집 수 K가 증가할수록 WSS는 감소한다. 이는 군집이 나뉠수록 각 군집이 더 조밀해지기 때문이다.\n\n엘보우의 해석\n\n\nWSS는 K가 증가할수록 급격히 감소하다가,\n어느 지점에서 감소 폭이 완만해지는 시점이 나타난다.\n이 지점이 “엘보우(elbow)”이며 적절한 K의 후보로 간주된다.\n\n단, 엘보우는 절대적인 정답이 아니며, 실루엣 계수 등 다른 지표와 병행해야 한다."
  },
  {
    "objectID": "da/dap/dap_02.html#데이터-전처리와-표준화",
    "href": "da/dap/dap_02.html#데이터-전처리와-표준화",
    "title": "Clustering",
    "section": "4. 데이터 전처리와 표준화",
    "text": "4. 데이터 전처리와 표준화\nK-평균은 거리 기반 모델이므로 변수의 단위 차이가 큰 경우 오차가 발생한다. 예: 나이(20 ~ 80)와 수입(1,000 ~ 1억)이 함께 있을 경우 수입 변수가 거리 계산을 압도한다.\n따라서 다음과 같은 표준화가 필요하다.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n표준화는 거리 측정의 공정성을 확보하지만, “정보 손실”이 있는 것은 아니며 변수의 스케일 해석이 달라지는 것이 정확한 표현이다."
  },
  {
    "objectID": "da/dap/dap_02.html#차원-축소",
    "href": "da/dap/dap_02.html#차원-축소",
    "title": "Clustering",
    "section": "5. 차원 축소",
    "text": "5. 차원 축소\n대부분의 실무 데이터는 3차원 이상의 다변량 구조를 가진다. K-평균으로 군집을 생성한 뒤, 결과를 2차원 또는 3차원 그래프로 시각화하기 위해 차원 축소 기법(PCA, t-SNE 등)을 사용한다.\n\nPCA를 통해 고차원 공간의 분산을 보존하면서 차원을 축약\n축약된 공간에서 각 군집을 색상(라벨)으로 표시\n이는 군집 구조를 탐색하고 설명하는 데 유용\n\n군집 라벨은 비지도 학습의 결과이며 “예측값”이라기보다 “모델이 발견한 데이터 구조에 대한 할당 결과”에 가깝다."
  },
  {
    "objectID": "da/dap/dap_02.html#실무적-장점과-제약",
    "href": "da/dap/dap_02.html#실무적-장점과-제약",
    "title": "Clustering",
    "section": "6. 실무적 장점과 제약",
    "text": "6. 실무적 장점과 제약\n\n장점\n\n\n계산 효율이 매우 높음\n대규모 데이터에 적합\n구현이 단순하고 결과가 직관적\n마케팅·고객 세그멘테이션·금융 데이터 분석에서 표준처럼 사용됨\n\n\n제약\n\n\n구형(spherical) 군집 가정\n비정형, 길쭉한 군집 구조에서는 부정확\n사전에 K를 결정해야 하는 제약이 존재한다.\nK-means++ 등의 기법을 활용해야 초기값이 안정적\n이상치는 중심값을 크게 왜곡시켜 전체 군집 구조를 변형시킨다."
  },
  {
    "objectID": "da/dap/dap_02.html#실루엣-계수-1",
    "href": "da/dap/dap_02.html#실루엣-계수-1",
    "title": "Clustering",
    "section": "1. 실루엣 계수",
    "text": "1. 실루엣 계수\nSilhouette Coefficient\n\n1.1 수학적 정의\n실루엣 계수는 개별 데이터 포인트가 자신이 속한 군집과 얼마나 응집(Cohesion) 되어 있으며, 동시에 다른 군집과는 얼마나 분리(Separation) 되어 있는지를 정량적으로 측정하는 지표이다.\n특정 데이터 포인트 \\(i\\) 에 대해 다음과 같이 정의한다.\n\n동일 군집 내 거리 평균 \\[\na(i) = \\frac{1}{|C_i|-1}\\sum_{j\\in C_i, j\\neq i} d(i,j)\n\\]\n가장 가까운 외군집과의 평균거리 \\[\nb(i) = \\min_{C_k \\ne C_i} \\left( \\frac{1}{|C_k|} \\sum_{j\\in C_k} d(i,j) \\right)\n\\]\n실루엣 계수 공식 \\[\ns(i)=\\frac{b(i)-a(i)}{\\max(a(i),b(i))}, \\quad \\text{범위}: -1 \\le s(i) \\le 1\n\\]\n\n\n\n1.2 해석 기준\n\n\\(s(i) \\approx 1\\): 군집이 매우 잘 형성됨 (높은 응집 + 높은 분리)\n\\(s(i) \\approx 0\\): 군집 간 경계에 위치\n\\(s(i) &lt; 0\\): 잘못된 군집 배정 가능성이 높음\n\n군집 전체의 실루엣 계수는 다음과 같이 평균으로 계산한다.\n\\[\nS(k) = \\frac{1}{n}\\sum_{i=1}^{n}s(i)\n\\]\n이 값이 최대가 되는 k를 선택하는 것이 대표적 접근이다."
  },
  {
    "objectID": "da/dap/dap_02.html#실무-적용성과-한계",
    "href": "da/dap/dap_02.html#실무-적용성과-한계",
    "title": "Clustering",
    "section": "1.3 실무 적용성과 한계",
    "text": "1.3 실무 적용성과 한계\n장점 * 개별 포인트 단위의 군집 품질 평가 가능 * 모델 비교 가능 (예: K=2~10)\n한계 * 비선형 구조(예: 두 개의 링 모양)에서는 K-means와 함께 비적합 * 고차원 데이터에서 거리 기반 접근의 신뢰도 저하 * 실루엣 계수가 높아도 실무 요구와 맞지 않을 수 있음(예: 비즈니스 세분화 목적이 다른 경우)"
  },
  {
    "objectID": "da/dap/dap_02.html#엘보우-기법-2",
    "href": "da/dap/dap_02.html#엘보우-기법-2",
    "title": "Clustering",
    "section": "2. 엘보우 기법",
    "text": "2. 엘보우 기법\nElbow Method, 군집 내 응집도를 나타내는 군집 내 제곱합(WSS, Within-Cluster Sum of Squares)에 기반한다.\nWSS는 다음과 같이 정의한다.\n\\[\nWSS(k)=\\sum_{i=1}^{k}\\sum_{x \\in C_i} | x - \\mu_i |^2\n\\] * \\(C_i\\): i번째 군집 * \\(\\mu_i\\): 해당 군집의 중심(centroid)\n해석 기준 * WSS(k)는 k가 증가할수록 항상 감소한다. * 감소 곡선에서 기울기 변화가 급격 → 완만으로 바뀌는 지점을 “팔꿈치(Elbow)”라고 한다. * 이 지점이 적절한 군집 수 후보가 된다.\n한계 * 엘보우 지점이 명확히 보이지 않는 경우가 매우 많다. * 시각적 판단 의존도가 높아 객관성이 떨어진다. * 실무에서는 “엘보우처럼 보이는 두세 지점”이 나오는 경우가 흔하며, 이를 전문가가 해석해야 한다."
  },
  {
    "objectID": "da/dap/dap_02.html#갭-통계량",
    "href": "da/dap/dap_02.html#갭-통계량",
    "title": "Clustering",
    "section": "3. 갭 통계량",
    "text": "3. 갭 통계량\nGap Statistic 갭 통계량은 Tibshirani(2001)에 의해 제안되었으며, “관측 데이터의 군집 응집도”와 “참조분포(보통 균일분포) 기반 기대치”를 비교하여 군집의 구조가 통계적으로 의미 있는지를 측정한다.\n\\[\nGap(k)=E^*[\\log(W_k)] - \\log(W_k)\n\\] * \\(W_k\\): k개의 군집으로 분할했을 때의 군집 내 분산 * \\(E^*[\\cdot]\\): 참조 분포에서의 몬테카를로(Monte Carlo) 기준 기대값\n값이 클수록 군집이 “랜덤한 분포보다 더 잘 분리되어 있다”는 의미이다.\n최적 k 선택 규칙 Tibshirani는 다음 조건을 만족하는 최초의 k를 선택하도록 제안하였다.\n\\[\nGap(k) \\ge Gap(k+1) - s_{k+1}\n\\]\n\n\\(s_{k+1}\\): 표준오차(SE)에 기반한 보정 항\n\n장점 * 엘보우 기법보다 객관적 * 내재적 군집 구조를 통계적으로 검증 가능\n단점 * 부트스트랩 반복수가 많을수록 시간 비용이 큼 * 대규모 데이터에서는 사용되지 않는 경우가 많음"
  },
  {
    "objectID": "da/dap/dap_02.html#실무-관점의-종합적-의사결정",
    "href": "da/dap/dap_02.html#실무-관점의-종합적-의사결정",
    "title": "Clustering",
    "section": "4. 실무 관점의 종합적 의사결정",
    "text": "4. 실무 관점의 종합적 의사결정\n\n4.1 지표 간 결과 차이의 존재\n엘보우 기법과 실루엣 계수는 서로 다른 관점에서 k를 선택하기 때문에 서로 다른 결과가 나오기 쉽다. 갭 통계량까지 고려하면 결과는 더욱 다양해진다.\n따라서 한 가지 지표만으로 군집 수를 결정하는 것은 통계적으로 부적절하다.\n\n\n4.2 산업 현장에서의 실제 의사결정 방식\n기업의 고객군 세분화(마스터 세그멘테이션), 금융 리스크 분류, 구좌별 소비자 유형 분류 등에서는 다음의 요소가 함께 고려된다.\n\n지표 값의 통계적 안정성\n군집의 해석 가능성(Interpretability)\n비즈니스 목적에 맞는 분류 구조\n실무 담당자 및 도메인 전문가의 판단\n추후 유지·갱신 가능성\n스케일링(표준화) 여부의 영향\n\n특히, * 변수 단위가 모두 다를 경우 표준화(Z-score normalization)는 필수적이다. * 표준화는 “정보 손실”이 아니라 “스케일 기반 의미가 소거된다는 문제”로 해석하는 것이 정확하다."
  },
  {
    "objectID": "da/dap/dap_02.html#학문성과-해석의-주관성",
    "href": "da/dap/dap_02.html#학문성과-해석의-주관성",
    "title": "Clustering",
    "section": "5. 학문성과 해석의 주관성",
    "text": "5. 학문성과 해석의 주관성\n군집 분석은 지도학습이 아니며, 통계적으로 “가장 좋은 군집”이라는 절대적 기준이 존재하지 않는다. 따라서 해석자, 전문가, 비즈니스 목적에 의해 결과가 달라진다.\n이것이 군집 분석을 최근 연구에서 Exploratory Data Analysis(EDA) 기반 기법으로 분류하는 이유이기도 하다.\n즉, 이론적 수학 모델은 제공된다. 그러나 최종 모델 선택은 이론+실무 목적+해석의 융통성이 결합된 의사결정이다."
  },
  {
    "objectID": "da/dap/dap_02.html#gmm",
    "href": "da/dap/dap_02.html#gmm",
    "title": "Clustering",
    "section": "1. GMM",
    "text": "1. GMM\n다변량 정규분포(Multivariate Gaussian Distribution)의 혼합으로 데이터 분포를 나타낸다. 각 데이터 포인트 \\(x_i \\in \\mathbb{R}^d\\) 는 다음과 같이 확률적으로 군집에 속한다.\n\\[\np(x_i) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\n\\]\n\n\\(K\\) : 군집 수\n\\(\\pi_k\\) : k번째 군집의 혼합 비율 ((_{k=1}^{K} _k = 1))\n\\(\\mu_k\\) : k번째 군집의 평균 벡터\n\\(\\Sigma_k\\) : k번째 군집의 공분산 행렬\n\\(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\) : 다변량 정규분포 확률밀도함수"
  },
  {
    "objectID": "da/dap/dap_02.html#bayesian-gmm",
    "href": "da/dap/dap_02.html#bayesian-gmm",
    "title": "Clustering",
    "section": "2 Bayesian GMM",
    "text": "2 Bayesian GMM\n군집 수 K에 대한 불확실성을 모델링하며, 사전 분포(prior)와 데이터로부터 얻은 우도(likelihood)를 결합하여 후행 분포(posterior)를 계산한다.\n\n혼합 비율 \\(\\pi_k\\) : Dirichlet 사전 분포\n평균 \\(\\mu_k\\) : Gaussian 사전 분포\n공분산 \\(\\Sigma_k\\) : Inverse-Wishart 사전 분포\n\n\\[\np(\\pi, \\mu, \\Sigma \\mid X) \\propto p(X \\mid \\pi, \\mu, \\Sigma) , p(\\pi) , p(\\mu) , p(\\Sigma)\n\\]\nBayesian 추정에서는 EM(Expectation-Maximization)과 유사한 반복적 최적화 또는 변분 추정(Variational Inference)을 사용한다.\n특징 1. 데이터 포인트마다 군집 소속 확률 제공 2. 군집 수가 명확하지 않을 때 사전 분포를 통한 자동 결정 가능성 3. 특이값(outlier)에 민감\n\n이유: 공분산 행렬 추정 시 이상치가 분산을 왜곡하기 때문\n단순 거리 기반이 아니며, 확률적 분포 추정 과정에서 민감"
  },
  {
    "objectID": "da/dap/dap_02.html#군집-안정성-검증",
    "href": "da/dap/dap_02.html#군집-안정성-검증",
    "title": "Clustering",
    "section": "3. 군집 안정성 검증",
    "text": "3. 군집 안정성 검증\n군집 결과가 신뢰할 수 있는지 평가하기 위해 여러 방법이 사용된다.\n\n동일 자료에 다양한 군집 기법 적용\n\n\n같은 데이터셋에 K-means, GMM, 계층적 군집 분석 등 다양한 가정 기반 군집 방법을 적용\n결과 군집이 유사하게 나타나는지 비교하여 안정성을 검증\n\n\n데이터 분할 Cross-validation\n\n\n데이터셋을 임의로 두 부분으로 분할\n각 부분을 독립적으로 군집 분석 수행\n군집 구조가 일관되게 나타나는지 확인\n\n\n변수 제거/추가 Sensitivity Analysis\n\n\n일부 변수를 제거하거나 추가하여 군집 분석 수행\n군집 구조가 어떻게 변화하는지 관찰\n특정 변수에 과도하게 의존하는 군집인지 평가 가능\n\n\n실무 적용\n\n\nBayesian GMM은 다중 패턴이 혼합된 데이터 예: 고객 세분화, 금융 리스크 평가, 헬스케어 환자 그룹 분류에 적합\n안정성 검증 방법은 실무 데이터 분석에서 필수적\n\n데이터 분할, 변수 제거/추가, 다른 알고리즘 비교를 통해 신뢰성 확보\n\n모델 복잡도가 높으므로, 분석 목적, 데이터 특성, 해석 가능성을 함께 고려해야 함\n\n\n파셜 주성분 분석.(교수님 박사 논문)"
  },
  {
    "objectID": "ai-ml-dl.html",
    "href": "ai-ml-dl.html",
    "title": "AI·ML·DL",
    "section": "",
    "text": "인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nDec 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n로지스틱 분류\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n현대 AI 연구\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n현대 AI 연구\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n인공신경망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n단계별 머신러닝 학습\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSVM: 비확률적 마진 기반 분류기\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n의사결정트리 실습\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n경사하강법 (Gradient Descent)\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝: R2D3\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝: 실무 사례\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝: EnjoySport\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Approval\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAI 산업 전망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝 개론\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n트리 알고리즘\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 5, 2024\n\n\n혼자 공부하는 머신러닝+딥러닝\n\n\n\n\n\n\n\n\n\n\n\n\n다양한 분류 알고리즘\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\n혼자 공부하는 머신러닝+딥러닝\n\n\n\n\n\n\n\n\n\n\n\n\n회귀 알고리즘 & 모델 규제\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n혼자 공부하는 머신러닝+딥러닝\n\n\n\n\n\n\n\n\n\n\n\n\n데이터 다루기\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\n혼자 공부하는 머신러닝+딥러닝\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ai/ml_14.html",
    "href": "ai/ml_14.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: November. 12, 2025"
  },
  {
    "objectID": "ai/ml_14.html#문제점",
    "href": "ai/ml_14.html#문제점",
    "title": "인공신경망",
    "section": "2.1 문제점",
    "text": "2.1 문제점\n하나의 가중치만을 따로 떼어 분석하면 전체 네트워크의 상호의존적 구조를 반영하지 못한다는 점이 역전파의 핵심적 난점 중 하나이다.\n\n신경망은 공동 기여 시스템이다\n\n신경망의 출력은 단일 가중치의 효과가 아니라, 모든 입력 ( x_i )와 가중치 ( w_i ), 그리고 여러 층을 거친 비선형 합성 함수의 결과이다. 즉, 각 가중치는 다른 가중치들과의 조합을 통해서만 의미 있는 출력을 만들어낸다.\n이 때문에 하나의 가중치만 고립적으로 평가하면, 다른 가중치들과의 상호작용(interaction)을 무시하게 되어 실제 영향도를 정확히 알 수 없다.\n\n역전파는 ’부분 기여’를 계산하는 과정이다\n\n역전파 알고리즘의 목적은 각 가중치가 전체 손실(Loss)에 얼마나 기여했는가를 계산하는 것이다. 이때 체인룰을 이용해, 출력층에서 발생한 오차가 어떻게 각 가중치 방향으로 퍼져나가는지를 추적한다.\n즉, \\(\\frac{\\partial L}{\\partial w_i}\\) 는 “모든 다른 파라미터들이 고정되어 있을 때, ( w_i )를 미세하게 변화시켰을 때 손실이 얼마나 변하는가”를 의미한다.\n다시 말해, 하나의 가중치 변화가 전체 결과에 미치는 ‘국소적 기여도(local contribution)’를 구하는 것이며, 이는 전체 맥락에서의 상호작용을 미분의 형태로 부분적으로 포착한 것입니다.\n\n그러나 ’전체적 상호작용’은 여전히 남는다\n\n역전파는 개별 가중치의 기울기를 구하되, 그 계산 과정에 이미 모든 다른 가중치와 뉴런의 값이 포함됩니다. 즉, 수학적으로는 고립된 것이 아니라, 계산 그래프(computational graph) 전체를 거쳐 영향을 받아 나온 결과입니다. 그럼에도 불구하고 다음과 같은 한계가 존재합니다.\n\n비선형성(Nonlinearity) 때문에, 다른 가중치가 조금만 달라져도 각 기울기의 상대적 영향이 크게 바뀐다.\n따라서 한 시점의 기울기만으로는 “전체 네트워크에서의 근본적 관계”를 완전히 파악할 수 없다.\n이로 인해 실제 학습에서는 “한 번의 역전파 결과”보다 “다수의 반복(iteration)을 통한 평균적 수렴”이 중요하다. \n\n결론적으로 하나의 가중치를 따로 본다면 신경망의 다차원 상호작용을 제대로 볼 수 없다. 그러나 역전파는 바로 그 문제를 부분 미분의 누적 형태로 해결합니다.\n즉, 각 가중치의 기울기를 “다른 파라미터가 고정된 상태에서의 국소적 영향”으로 계산하고, 이를 전체 그래프를 따라 합성함으로써, 전체 시스템이 함께 조정되도록 하는 것입니다.\n결국, 하나의 가중치는 단독으로는 의미가 없고, 오직 “네트워크 전체에서의 미분적 상호작용” 속에서만 의미를 가집니다.\n이 점이 신경망이 선형 모델과 본질적으로 다른 이유이며, 딥러닝 학습이 단순한 회귀(regression)가 아닌 비선형적 협조 최적화(non-linear cooperative optimization)라는 점을 보여준다."
  },
  {
    "objectID": "ai/ml_14.html#해결-방안",
    "href": "ai/ml_14.html#해결-방안",
    "title": "인공신경망",
    "section": "2.2 해결 방안",
    "text": "2.2 해결 방안\n역전파 + 경사하강법 작동원리\n\n역전파의 순서적 기울기 계산\n\n출력층에서 손실이 계산된 후, 그 오차를 뒤로 거슬러 올라가며(backward) 각 층의 가중치가 손실에 어떤 영향을 미치는지를 따져봅니다.\n즉,\n\n마지막 층부터 오차의 영향을 계산하고,\n그 결과를 이전 층의 가중치로 전달하며,\n각 가중치의 변화 방향(∂L/∂w)을 구합니다.\n\n이게 바로 chain rule을 층별로 적용하는 과정입니다.\n\n경사하강법(Gradient Descent)\n\n각 가중치의 기울기(∂L/∂w)는 “이 방향으로 가면 손실이 증가한다”를 의미합니다. 따라서 반대 방향(−∂L/∂w)으로 이동하면 손실이 감소합니다. 이를 수식으로 표현하면: [ w_{new} = w_{old} - ] 여기서 ()는 학습률(learning rate)로, 이동 속도를 조절합니다.\n즉, 각 가중치는 “에러가 줄어드는 방향으로” 조금씩 움직이며, 이 과정이 전체 네트워크에서 동시에 일어납니다.\n\n순차적 파라미터 갱신\n\n하나의 가중치 ( w_i )를 업데이트할 때는, 그 시점의 다른 파라미터를 임시로 고정한 상태로 취급합니다. 즉, 한 번의 역전파에서는 모든 가중치를 동시에 업데이트하되, 각각은 “현재 다른 값들이 고정되어 있다”는 전제 하에서 계산된 기울기에 따라 움직입니다.\n이 점에서 동시적이면서도 독립적인 최적화 단위가 형성됩니다. 그 후, 전체 네트워크의 파라미터가 한 번에 갱신되며 다음 반복(iteration)으로 넘어갑니다."
  },
  {
    "objectID": "ai/ml_14.html#예제-시그모이드의-연산-흐름-구조",
    "href": "ai/ml_14.html#예제-시그모이드의-연산-흐름-구조",
    "title": "인공신경망",
    "section": "2.3. 예제: 시그모이드의 연산 흐름 구조",
    "text": "2.3. 예제: 시그모이드의 연산 흐름 구조\n시그모이드 함수 \\[\ng(z) = \\frac{1}{1 + e^{-z}}\n\\]\n를 계산 그래프로 풀어쓰면, 다음과 같은 일련의 연산 노드로 표현됩니다.\n\\[\nz → ( * -1 ) → exp → ( +1 ) → ( 1/x ) → g\n\\]\n즉,\n\n입력값 ( z )에\n음수를 곱하고( * -1 ),\n지수함수를 취하고( exp ),\n1을 더하고( +1 ),\n역수를 취하면( 1/x ) 결과적으로 시그모이드 출력 ( g )가 나옵니다.\n\n이처럼 단순한 하나의 수식도 여러 개의 연산 노드로 분해되어, 각 노드에서 미분(기울기)이 계산되고 역전파될 수 있도록 구성됩니다.\n\n복잡한 신경망 = 수식의 확장적 노드화\n\n신경망이 복잡해진다는 것은 결국 “이러한 단순한 연산 노드들이 수천, 수만 개로 연결되어 합성된 형태”를 의미합니다.\n즉,\n\n층이 깊을수록 합성 함수의 깊이가 늘어나고,\n뉴런 수가 많을수록 병렬적인 연산 노드가 많아지며,\n전체 네트워크는 거대한 계산 그래프(computational graph)로 확장된다.\n\n이 그래프 상에서 역전파는 체인룰을 적용하여, 출력 노드에서 입력 노드로 기울기를 노드 단위로 전파(backpropagate) 한다.\n\n직관적으로 보면 신경망의 “복잡성”은 사실 수학적 표현의 압축 정도입니다.\n\n즉,\n\n수식으로는 간단히 적힌 ( g(f(h(x))) ) 같은 표현이,\n실제 계산 단계에서는 수십 개의 노드로 세분화되어 구현됩니다.\n\n따라서, 복잡해 보이는 신경망도 결국 “단순한 기본 연산들의 반복적 조합”이며, 그 조합을 노드 그래프 형태로 펼쳐놓은 것이 바로 신경망 구조입니다.\n\n예 — hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) 같은 표현을 TensorFlow 코드·계산 그래프·TensorBoard 이미지로 웹에서 쉽게 찾을 수 있습니다.\n권장 이미지 유형 코드 스니펫과 간단 다이어그램(활성화 함수 흐름). TensorBoard로 그린 연산 그래프(MatMul → Add(b) → Sigmoid). 계산 그래프의 노드(곱셈·덧셈·비선형 연산) 시각화 예시."
  },
  {
    "objectID": "ai/ml_14.html#해결-방법2",
    "href": "ai/ml_14.html#해결-방법2",
    "title": "인공신경망",
    "section": "2.4 해결 방법2",
    "text": "2.4 해결 방법2\n\n순전파(Forward Propagation)\n\n먼저 신경망은 입력 데이터를 받아 층을 따라 순방향으로 연산을 수행합니다.\n즉, \\[\nx \\rightarrow (W_1, b_1) \\rightarrow L_1 \\rightarrow (W_2, b_2) \\rightarrow L_2 \\rightarrow \\cdots \\rightarrow \\hat{y}\n\\]\n\n\\(\\hat{y}\\): 예측값(hypothesis)\n\n이 단계에서는 단지 “현재 가중치로 계산된 결과”를 내는 것뿐입니다.\n\n역전파(Backpropagation)\n\n출력값이 실제 정답 (y)와 다르면 손실 함수 (L(y, ))가 커집니다. 이때 오차를 역방향으로 전파하여 각 가중치 (w)가 오차에 미친 영향을 계산하고, 그에 따라 가중치를 오차가 줄어드는 방향으로 미세하게 수정합니다.\n즉, \\[\nw := w - \\eta \\frac{\\partial L}{\\partial w}\n\\]\n이 과정을 데이터셋의 각 샘플(혹은 배치)에 대해 반복하면서 학습이 진행됩니다.\n\n대규모 데이터가 필요한 이유\n\n오류를 줄이기 위해서는 다양한 입력 상황을 학습해야 합니다. 데이터가 많을수록 네트워크는\n\n특정 패턴에 과적합되지 않고,\n전체 분포의 일반적 경향을 학습할 수 있습니다.\n\n즉, 대규모 데이터는 가중치가 균형 있게 조정되도록 도와주며, 이는 결국 일반화 능력(generalization)을 향상시킵니다.\n\n가중치 변화 폭의 수렴\n\n학습이 진행될수록\n\n손실이 줄어들고,\n기울기(gradient)의 크기도 점점 작아집니다.\n\n이로 인해 가중치의 변동 폭은 점점 감소하며, 결국 오차가 거의 변하지 않는 수렴 상태에 도달합니다.\n이때 각 가중치의 분포는 무작위 초기화 상태에서 점차 안정된 정규분포 형태로 가까워집니다. 이는 확률론적으로 “많은 데이터 샘플에 의해 평균적으로 조정된 결과”이기 때문입니다.\n\n테스트 데이터로 검증\n\n훈련(Training) 과정에서 사용되지 않은 테스트 데이터(Test set)를 이용해 학습된 모델이 새로운 데이터에서도 잘 작동하는지 검증합니다.\n이 과정을 통해 정확도(accuracy), 손실(loss), 과적합 여부(overfitting) 등을 평가할 수 있습니다."
  },
  {
    "objectID": "ai/ml_12.html",
    "href": "ai/ml_12.html",
    "title": "현대 AI 연구",
    "section": "",
    "text": "인공지능 발전과 대규모 언어 모델의 CoT 통합 구조에 대해 다루고자 한다.\n01 AI의 학문적 발전 인공지능(AI)의 발전은 소프트웨어적 진화와 하드웨어적 진화라는 두 축을 중심으로 병행되어 왔다.\n이 두 축은 독립적으로 발전한 것이 아니라, 서로 상호 보완적 관계 속에서 영향을 주고받으며 오늘날의 AI 연구 쳬계를 형성하였다.\n1 . 소프트웨어적 발전 ① 인과관계 중심 접근\n심볼릭 어프로치, Symbolic AI\nAI 연구의 초창기에는 인간의 사고 과정을 모사하기 위해 명시적 인과관계(explicit causality)를 기반으로 한 접근이 시도되었다.\n이 시기의 대표적 연구 흐름은 Symbolic AI로, 지식공학(knowledge engineering)을 중심으로 발전하였다.\n대표적 사례로는 전문가 시스템(expert system)이 있으며, MYCIN과 DENDRAL이 대표적 예이다.\n이러한 시스템은 인간 전문가의 지식을 ’규칙(rule)’과 ’추론(inference)’의 형태로 체계화하여, 명시적으로 저장하고, 이를 이용해 논리적 결론을 도출하는 구조를 가진다.\n장점 원인과 결과의 관계가 명확하므로, 시스템의 판단 근거를 추적할 수 있어 설명 가능성(explainability)이 높다.\n한계 모든 지식을 사람이 사전에 정의해야 하므로, 현실 세계의 복잡성과 불확실성을 충분히 반영하지 못한다.\n② 상관관계 중심 접근\n애니매틱 어프로치, Connectionist / Statistical AI\n이후 연구자들은 인과관계를 직접 모델링하기 보다는, 데이터로부터 패턴을 학습하는 방향으로 나아갔다.\n이것이 바로 연결주의적 또는 통계적 접근으로, 오늘날의 머신러닝 및 딥러닝의 기반이 된다.\n이 접근법에서는 대량의 데이터를 수집·정제한 뒤, 심층신경망(deep neural networks) 등의 모델을 학습시켜 결과를 도출한다.\n장점 명시적 인과모형 없이도 높은 예측 정확도를 달성하며, 이미지 인식·자연어 처리·음성 인식 등 다양한 분야에서 혁신적 성과를 보였다.\n한계 모델 내부 구조가 복잡하고 가중치에 의존하므로, 판단 과정이 불투명한 블랙박스(black box) 문제가 발생한다.\n즉, 상관관계를 통해 결과를 도출할 수는 있지만, 그 관계가 ‘왜’ 성립하는가에 대한 설명이 어렵다. 이로 인해 실세계 응용에서 신뢰성과 윤리성의 한계가 제기되었다.\n③ 하이브리드 접근 및 설명 가능한 AI\nExplainable AI, XAI OR Neuro-Symbolic AI\n이러한 한계를 극복하기 위한 새로운 방향으로 설명 가능한 인공지능 연구가 등장하였다.\n이는 심볼릭 AI의 논리적 해석력과 연결주의 AI의 학습 능력을 통합하는 하이브리드 AI, 혹은 뉴로-심볼릭 AI의 형태로 발전하고 있다.\n이 접근의 핵심은 예측성과 해석성의 통합에 있다.\n하이브리드 AI의 운영 구조는 다음과 같다.\n예측 단계: 대규모 데이터를 활용하여 통계적 분석과 패턴 인식을 수행함으로써 예측 결과를 생성한다.\n설명 단계: 규칙 및 온톨로지 기반의 인과 추론 체계를 활용하여, 예측의 근거를 논리적으로 해석하고 검증한다. 적용 분야: 의료, 금융, 법률 등 설명 가능성이 필수적인 영역. 현재 한계: 완전한 인과적 추론 수준에는 아직 도달하지 못하였으며, 인과적 설명의 내재화(causal reasoning integration)는 여전히 AI 연구의 핵심 과제로 남아 있다.\n2 . 하드웨어적 발전 Physical AI\nAI의 발전은 알고리즘적 진보를 넘어, 물리적 지능(physical intelligence)의 단계로 확장되었다.\n이를 피지컬 AI(Physical AI)라고 불리며, 단순한 연산 능력 향상이 아니라 실제 환경과의 실시간 상호작용을 가능하게 하는 새로운 패러다임을 의미한다.\n① 철학적 배경\n피지컬 AI의 이론적 기반은 ” 지능은 환경과의 상호작용을 통해 완성된다 ” 는 체화된 인지(embodied cognition) 개념에 있다.\n이는 지능을 단순한 계산 능력이 아니라, 신체적 경험과 감각적 피드백을 포함한 총체적 능력으로 본다.\n② 실제 구현\n이 개념은 자율주행차, 로봇 조작, 드론 제어, 스마트 IoT 기기 등에서 실현되고 있다.\nAI 알고리즘이 센서 및 엑추에이터와 결합함으로써, 기계는 외부 자극을 인식하고 판단하며 행동하는 실시간 자율 시스템으로 발전한다.\n③ 학문적 의미\n단순 기술 확장이 아니라, AI가 스스로 감지, 학습, 행동하는 실체적 지능을 실현하려는 연구 축으로 이해할 수 있다.\n하드웨어적 발전은 단순한 기술적 진보가 아닌, AI의 실체적 존재화(realization)로 이해된다.\n즉, AI가 데이터로만 존재하던 비물질적 지능에서 벗어나, 감지(sensing)–학습(learning)–행동(acting)의 순환 구조를 스스로 수행하는 단계로 진입한 것이다.\n따라서 하드웨어적 진화는 소프트웨어 중심 AI의 한계를 보완하며, 현실 세계 속에서 자율적 판단과 행동 능력을 구현하는 핵심 축으로 평가된다. 이는 향후 지능의 통합적 구현(integrated intelligence)을 실현하기 위한 필수적 토대로 자리 잡고 있다.\n02 CoT 통합 구조 Chain-of-Thought Integrated Architecture\n대규모 언어 모델(LLM, Large Language Model)은 최근 몇 년간 두 가지 방향으로 발전해왔다.\n① 하나는 언어 생성 능력(Language Generation)에 초점을 둔 학습 중심형 모델(Learning-Oriented Model)\n② 다른 하나는 논리적 사고(Logical Reasoning)를 내재화한 추론 중심형 모델(Reasoning-Oriented Model)이다.\n이 구분은 모델의 구조적 형태보다는, 모델이 최적화하는 목적 함수(Objective Function)와 훈련 패러다임(Training Paradigm)의 차이에 의해 정의된다.\n1 . 학습 중심형 모델 Learning-Oriented Model\nGPT-3에서 GPT-4.5로 이어지는 계열은 지도 학습(SL)과 자기회귀 언어 모델링(Autoregressive Language Modeling)을 기반으로 한 대표적 학습형 구조이다.\n이 모델들은 입력된 문맥(Context)에 대해 다음 단어의 조건부 확률을 최대화하도록 학습된다.\n\\[P(w_t | w_1, w_2, \\ldots, w_{t-1}; \\theta)\\]\n(:) 모델의 파라미터 손실 함수: 일반적으로 음의 로그 가능도(Negative Log-Likelihood)로 정의된다. \\[\\mathcal{L}(\\theta) = -\\sum_{t=1}^{T} \\log P(w_t | w_{&lt;t}; \\theta)\\]\n이 접근은 모델이 방대한 언어 데이터를 통계적으로 모사하며, 언어의 문맥적 패턴을 효율적으로 학습하게 만든다.\nGPT-4.5 이하의 모델들은 주로 텍스트 생성(Text Completion), 요약(Summarization), 번역(Translation) 등 언어적 유창성이 필요한 과제에 최적화되어 있다.\n그러나 이러한 모델들은 논리적 추론(logical reasoning)과 같은 고차적 사고를 명시적으로 수행하지 못한다는 한계를 지닌다.\n2 . 추론 중심형 모델 Reasoning-Oriented Model\n이 한계를 보완하기 위해 OpenAI는 2024년 이후 O 시리즈(O1, O1-mini, O3 등)를 개발하였다.\n이들 모델은 단순한 언어 예측이 아닌, 사고 과정(CoT)을 내재화하여 논리적·수학적 추론을 수행하도록 설계된 구조이다.\n즉, 결과를 곧바로 산출하는 대신, 모델 내부에서 일련의 사고 단계를 거쳐 중간 논리 과정(intermediate reasoning steps)을 생성한 후 최종 응답을 도출한다.\n이를 수식적으로 표현하면 다음과 같다.\n\\[\\text{Answer} = f_\\theta(\\text{Prompt}) = g_\\theta(\\text{Chain of Thought Steps})\\]\n(g_:)​ 모델 내부의 사고 전개 과정.\n즉, 모델은 단순히 단어를 예측하는 확률기계가 아니라, 내재적 추론 구조를 가진 사고 시스템으로 진화한 것이다.\n이러한 추론 능력은 별도의 규칙 기반이 아닌, 사전학습(Pretraining)과 인간 피드백 강화학습(RLHF)을 통해 점진적으로 강화된다.\n결과적으로 O 시리즈는 언어의 표현 능력보다 사고의 정확성과 합리성을 우선시하는 추론 중심형(reasoning-oriented) 모델로 분류된다.\n3 . GPT-5: CoT 기반 통합형 모델 Integrative Model\n2025년 발표된 GPT-5는 기존 GPT 계열의 학습 중심형 구조와 O 시리즈의 추론 중심형 구조를 통합한 CoT 기반 하이브리드 LLM 아키텍처로 정의된다.\nGPT-5는 입력의 복잡도에 따라 자동으로 두 가지 처리 모드 중 하나를 선택한다.\nFast Mode: 단순 질의에 대한 신속 응답 Deliberative Mode: 복합 문제에 대한 단계적 사고(CoT 기반)로 자동 전환하는 이중 처리 구조를 갖는다. 이 과정을 수식으로 나타내면 다음과 같다.\n\\[\\text{Output} = \\begin{cases} f_\\theta(x) & \\text{if } x \\in \\text{simple query} \\\\ f_\\theta(g_\\theta(x)) & \\text{if } x \\in \\text{complex reasoning task} \\end{cases}\\]\n(g_(x):) CoT 기반 내부 사고 전개 과정. 이를 통해 GPT-5는 단순 질의응답(Q&A)에서는 빠른 응답을 제공하고, 복합적인 문제(계획 수립, 코드 분석, 수학적 추론 등)에서는 단계적 사고 절차를 자동으로 활성화한다.\n4 . 이론적 통합 관점 학술적 관점에서 보면 GPT-5의 구조적 진화를 요약하면 다음과 같다.\n\\[\\text{L-O GPT (≤4.5)} + \\text{R-O O-Series} \\Rightarrow \\text{GPT-5 (Integrated CoT Model)}\\]\n즉, GPT-5는 단순히 매개변수 규모가 확장된 모델이 아니라, 학습 중심적 언어 처리와 추론 중심적 사고 구조를 결합하여 인공지능의 인식 능력(perception)과 사고 능력(reasoning)을 동시에 고도화한 모델로 정의된다.\nLLM 발전 모델 비교 구분 모델 계열 중심 개념 주요 기능 학습 중심형 GPT-3 ~ GPT-4.5 언어 패턴 학습 및 문맥 예측 언어 생성, 번역, 요약 등 추론 중심형 O1 ~ O3 논리적 사고 및 CoT 전개 논리·수학적 문제 해결 통합형 GPT-5 언어 생성 + 사고 통합 자동 모드 전환, 고차원 추론 수행"
  },
  {
    "objectID": "ai/ml_10.html",
    "href": "ai/ml_10.html",
    "title": "단계별 머신러닝 학습",
    "section": "",
    "text": "시그모이드, 소프트맥스, 다층 퍼셉트론 등에 대해 다루고자 한다.\n01 다중 회귀 Multiple Regression\n앞서 살펴본 단일 입력(단변량, single variable) 회귀모델은 하나의 독립 변수만을 고려하였다.\n그러나 실제 데이터 분석에서는 여러 입력값을 동시에 반영하는 다중 회귀 모델이 보다 현실적이고 효과적으로 활용된다.\n이 모델은 여러 독립 변수(feature)를 기반으로 종속 변수(target)를 예측하며, 복잡한 현실 세계의 관계를 선형적으로 근사할 수 있다.\n다중 회귀의 기본 구조는 단일 회귀와 동일하다. 입력 변수의 개수만 증가하고, 이에 대응하는 가중치(weight) 역시 벡터 형태로 확장된다.\n\\[\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\\]\nw_i: 각 특성(feature)에 대한 가중치, x_i: 입력 변수, b: 절편(bias)\n1 . 벡터와 행렬 기반 연산 입력 변수가 증가하더라도 손실함수의 계산 원리는 변하지 않는다. 다만, 계산의 복잡성이 높아지므로, 가중치와 입력값을 벡터 또는 행렬 형태로 표현된다.\n\\[\\mathbf{X} \\in \\mathbb{R}^{m \\times n}, \\quad \\mathbf{w} \\in \\mathbb{R}^{n \\times 1}, \\quad \\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b}\\]\nm: 데이터 포인트(샘플)의 수 n: 특성(feature)의 수 이러한 행렬 표현을 통해 모든 샘플에 대한 예측을 단일 연산으로 수행할 수 있어 모델 학습 과정의 계산 효율성(computational efficiency)이 크게 향상된다.\n2 . 핵심 선형대수 연산 핵심 연산은 다음과 같다.\n① 내적(dot product)\n두 벡터의 방향과 크기를 비교하는 연산으로, 회귀에서는 예측값 계산 및 유사도 측정((cos θ))에 주로 활용된다.\n② 외적(cross product)\n두 벡터에 수직인 새로운 벡터를 생성하며, (sin θ)를 통해 방향성을 표현한다. 다만 외적은 회귀 분석이나 유사도 계산보다는 물리적 벡터 계산에서 주로 사용된다.\n고차원 연산과 병렬 처리 다중 입력 연산은 수많은 곱셈과 덧셈을 포함하므로, 입력 차원이 커질수록 연산 부하가 기하급수적으로 증가한다. 특히 이미지, 영상, 음성 등 대규모 데이터셋에서는 이 문제가 더욱 두드러진다.\n이러한 연산 복잡도를 해결하기 위해 병렬 연산(parallel computation)기법이 적용되며, 대표적으로 GPU가 사용된다.\nGPU는 수천 개의 코어를 이용해 행렬 및 벡터 연산을 동시에 수행하므로, 3차원 좌표 변환이나 2차원 이미지 처리 등 고속 연산 환경에서 탁월한 성능을 발휘한다.\n또한 CUDA와 같은 GPU 전용 병렬처리 라이브러리 사용 시 AI 모델 학습 과정에서 연산 속도를 획기적으로 향상시킬 수 있다.\n이와 같이 확장된 회귀모델의 기반에는 선형대수(linear algebra)가 자리하고 있으며, 모든 계산은 행렬 곱, 내적, 외적 등 선형대수적 연산 원리에 의해 수행된다.\n3 . 스케일링과 정규화 scaling & normalization\n다중 회귀 모델의 학습 과정에서는 입력 데이터의 스케일 차이가 학습 안정성과 예측 정확도에 큰 영향을 미칠 수 있다.\n이를 방지하기 위해, 각 특성(feature)을 동일한 기준으로 조정하는 스케일링과 정규화 과정이 필수적이다.\n① 스케일링\n각 변수의 단위나 값의 범위 차이로 인해 특정 특성이 모델 학습 과정에서 과도하게 영향을 미치는 문제를 방지한다.\n예: 각 값에 대해 최대값 또는 범위를 기준으로 나누어 0~1 범위로 조정\n② 정규화\nZ-점수 정규화를 적용하여, 데이터를 평균 0, 표준편차 1의 정규분포로 변환하면 특성 간 스케일 불균형을 더욱 효과적으로 완화할 수 있다.\n예: 데이터가 타원형 분포를 가진 경우 각 값을 최대값으로 나누어 원형 형태로 정규화할 수 있다.\n이러한 변환을 수행하면 손실 함수의 등고선(contour)이 타원형에서 원형으로 바뀌어, 경사하강법의 수렴 속도와 안정성이 향상된다.\n즉, 모든 입력값을 동일한 기준으로 인식하도록 조정하여 모델이 보다 균형 잡힌 방식으로 최적화를 수행하게 만든다.\n참고로, 변동계수(CV)는 데이터의 상대적 분산을 평가하는 보조 지표로 활용될 수 있으나, 본 장에서는 필수 적용 사항은 아니다.\n02 분류와 비선형 함수의 개념 Classification & Nonlinear Functions\n앞서 다중 및 다항 회귀를 통해 연속적인 수치값을 예측하는 모델 구조를 살펴보았다.\n그러나 실제 문제의 상당수는 단순한 수치 예측이 아닌, “이것이냐, 저것이냐”와 같은 범주형(class) 결과를 요구한다.\n이처럼 출력값이 명확히 구분되는 문제에서는 회귀(regression) 대신 분류(classification) 개념이 사용된다.\n그중에서도 두 가지 범주를 구분하는 가장 기본적인 형태가 이진 분류(binary classification) 이다.\n대표적인 활용 사례는 다음과 같다.\n스팸 메일 탐지: 스팸 / 정상 소셜 미디어 콘텐츠 노출 결정: 보이기 / 숨기기 신용카드 부정 거래 탐지: 정상 / 이상 MRI 영상 판독: 정상 / 비정상 이와 같이, 분류 문제는 연속적 수치 예측과 달리 출력값이 불연속적이고 범주형이라는 점에서 학습 방식과 손실 함수, 모델 설계 측면에서 특수성을 가진다.\n1 . 회귀에서 분류로의 전환 선형 회귀는 입력값의 선형 결합을 통해 연속적인 출력값을 생성한다.\n그러나 분류 문제에서는 출력이 특정한 범위 내에 속해야 하며, 즉 결과값이 “0 또는 1”, “긍정 혹은 부정”처럼 해석 가능한 확률 형태로 제한될 필요가 있다.\n예를 들어, 종양의 크기에 따라 악성(1)과 양성(0)을 분류하는 문제를 생각해보자.\n데이터에서 “크기 40 이상일 때 악성” 으로 정의하면, 단순 선형 모델은 30 ~ 40 사이에 결정 경계선(decision boundary)을 그어 두 범주를 구분할 수 있다.\n그러나 새로운 데이터에서 70 이상의 값이 등장하면, 기존 경계선이 더 이상 유효하지 않아 새로운 기준선 재설정 문제가 발생한다.\n이러한 한계를 해결하기 위해, 선형 출력값을 0 ~ 1 사이로 압축하여 확률적으로 해석 가능한 비선형 변환 함수(activation function)가 도입된다.\n그중 가장 대표적인 예가 시그모이드 함수이다.\n2 . 시그모이드 함수 Sigmoid Function\n시그모이드 함수는 실수 입력값 (z)를 받아 이를 0 ~ 1 사이의 실수 값으로 변환하는 비선형 함수이다.\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\n여기서 (z)는 이전 장에서 다룬 선형 결합 (WX)에 해당한다. 즉, 회귀모델의 출력값을 시그모이드 함수에 통과시켜 확률(probability) 형태로 변환하는 과정이다.\n확률적 해석 가능\n입력값이 매우 크면 1에 근접 입력값이 매우 작으면 0에 근접 이 함수는 ” 연속적인 선형 출력을 상·하한이 존재하는 구간으로 압축하는 변환 ” 으로 이해할 수 있다.\n부드러운 결정 경계\nS자 곡선(S-shaped curve)을 형성하며, 입력값이 증가에 따라 악성일 확률이 점진적으로 증가한다. 선형 출력이 비선형적으로 압축되어, 부드러운 결정 경계를 제공한다.\n미분 연속성\n미분이 연속적이므로, 경사하강법과 같은 최적화 알고리즘에서 기울기 계산이 원활하다.\n이러한 특성 덕분에 시그모이드 함수는 암 진단(양성/음성), 이메일 스팸 분류(스팸/정상) 등 이진 분류 문제에서 확률 기반 의사결정을 수행하는 핵심 요소로 활용된다.\n3 . 하이퍼볼릭 탄젠트 함수 Hyperbolic tangent Function\n시그모이드 함수와 유사하게, 하이퍼볼릭 탄젠트(tanh) 함수도 입력값을 비선형적으로 변환하는 함수이다.\n이 함수는 다음과 같이 정의된다.\n\\[\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\]\n출력값은 -1 ~ 1 사이에 위치한다. 중심이 0 에 있어, 시그모이드보다 학습 과정에서 안정성이 높다. 예: 감정 분석에서 긍정(+1)과 부정(-1)을 구분, 좋아요/싫어요 같은 양방향 선택 구조를 다룰 때 유용하다.\n단, 음의 출력 범위를 포함하므로 연산적으로 시그모이드보다 약간 복잡하며, 데이터 특성에 따라 선택적으로 적용된다.\n비교표 함수 출력 범위 중심값 주 용도 장점 시그모이드 0 ~ 1 0.5 확률 기반 이진 분류 확률적 해석 용이, 단순 하이퍼볼릭 탄젠트 -1 ~ +1 0 양극적 분류(긍정/부정) 중심 0, 학습 안정성 높음 시그모이드와 tanh는 모두 선형 회귀 결과를 비선형적으로 변환하여 확률 또는 분류 경계를 생성하는 역할을 한다.\n이러한 활성화 함수의 도입은 회귀모델이 “연속 예측”에서 “범주 판별”로 확장되는 전환점이며, 이후의 심층신경망(Deep Neural Network) 구조에서도 핵심적 역할을 수행한다.\n03 다중 입력 확장\n1 . 가설 Hypothesis\n단일 입력 로지스틱 회귀에서처럼, 여러 입력(feature)을 동시에 고려할 경우에도 원리는 동일하다.\n입력값의 선형 결합 (z = WX + b)를 시그모이드 함수에 통과시키면 출력값이 0 ~ 1 사이의 확률로 변환된다.\n\\[h(X) = \\sigma(WX + b) = \\frac{1}{1 + e^{-(WX+b)}}\\]​\n이 확률값은 주어진 입력이 특정 범주에 속할 가능성을 의미하며, 이를 통해 이진 분류 문제에서 직관적이고 안정적인 의사결정이 가능하다.\n2 . 다중 입력 확장 Multivariate Input\n현실 세계의 데이터는 단일 입력보다는 여러 독립 변수(feature)를 동시에 고려해야 하는 경우가 많다.\n다중 입력의 경우에도 선형 결합의 구조는 동일하며, 단지 입력 벡터와 가중치 벡터를 확장하여 계산한다.\n\\[z = W_1 x_1 + W_2 x_2 + \\dots + W_n x_n + b = WX + b\\]\n이렇게 계산된 다중 입력 결과는 소프트맥스(Softmax) 함수를 통해 정규화될 수 있다.\n\\[\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\]\n소프트맥스는 각 범주의 확률 합이 1이 되도록 조정하며, 출력값이 여러 클래스 중 하나로 분류될 수 있도록 확률 기반 해석 제공\n다중 클래스 분류 문제에서 필수적인 역할을 수행한다.\n3 . 다중 클래스(Softmax + 원-핫) 확장 다중 클래스 문제에서는 각 클래스 k에 대해 선형 점수(score)를 계산하고 소프트맥스(Softmax)로 확률분포를 얻는다.\n\\[s_k = w_k^\\top x + b_k,\\qquad p_k=\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\]\n손실은 다중 클래스 크로스엔트로피(원-핫 레이블 y 기준):\n\\[J=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k} y^{(i)}_k \\log p_k^{(i)}\\]​\n구조적으로 퍼셉트론과 유사하며, 신경망(NN)의 전단계로 이해할 수 있다.\n4 . 다층 페셉트론 Multi-Layer Perceptron, MLP\n단일 로지스틱 회귀와 다중 입력 확장을 기반으로, 구조를 여러 층으로 확장하면 다층 퍼셉트론이 된다.\n각 층에서는 선형 변환과 비선형 활성화가 반복 적용되며, 입력 간의 복잡한 상호작용과 패턴을 학습할 수 있다.\nMLP는 심층 신경망(DNN)의 기본 단위로, 다층 구조를 통해 비선형적 결정 경계와 복잡한 데이터까지 패턴 학습이 가능하다."
  },
  {
    "objectID": "ai/ml_07.html",
    "href": "ai/ml_07.html",
    "title": "SVM: 비확률적 마진 기반 분류기",
    "section": "",
    "text": "SVM, 즉 비확률적 마진 기반 분류기에 대해 다루고자 한다.\n수준별 이해도 ① 데이터 정제 및 전처리\n대상: 기존 IT 인력 요구 수준: 기본적인 이해로 충분하며, 실무 적용 중심 내용: 결측치 처리, 이상치 제거, 형식 변환 등 데이터의 품질을 높이는 작업 ② 모델링(연계 결합, 파이프라인 구성)\n대상: 석사 및 박사 수준의 연구자 요구 수준: 심층적인 이해와 설계 능력 필요 내용: 모델의 구조 설계, 학습 및 추론 과정의 최적화, 다양한 알고리즘의 조합 및 실험 ③ 알고리즘 설계\n대상: 석사 및 박사 수준의 연구자 요구 수준: 이론적 배경과 수학적 이해가 필수 내용: 새로운 알고리즘의 개발, 기존 알고리즘의 개선, 수학적 모델링 및 분석 이러한 단계별 구분은 일반적으로 데이터 과학 및 AI 분야에서의 역할 분담과 학습 경로를 반영한 것이다.\n실무에서는 데이터 정제 및 전처리가 핵심적인 역할을 하며, 연구 및 개발 단계에서는 모델링과 알고리즘 설계가 중심이 된다.\n우리는 이번 과정에서 모델링(연계 결합 및 파이프라인 구성) 단계에 초점을 맞춰 배워볼 것이다.\n01 서포트 벡터 머신 Support Vector Machine, SVM\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js 기본적으로 선형 분리(linear separator) 찾는 알고리즘.\n데이터가 선형적으로 구분되지 않을 경우, 커널(kernel)을 이용하여 입력 데이터를 더 높은 차원 공간으로 사상(mapping)한 뒤 선형 판별을 수행한다.\n대표적인 예로 다항식 커널(polynomial kernel) 은 다음과 같이 정의된다.\n\\[K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^\\top \\mathbf{y} + c)^d\\]\n이 커널은 입력 벡터의 고차항 조합을 암시적으로 고려한다.\n또한, 명시적으로 모든 고차항을 계산하지 않아도 커널 트릭(kernel trick) 을 통해 고차원 특징 공간에서의 내적을 효율적으로 대체할 수 있다.\n① 특징(feature) / 속성, 변수\n데이터의 각 관측치는 여러 개의 속성으로 구성되며, 이를 (x_1, x_2, , x_n)​으로 표현한다.\n이때 이 (n)개의 속성을 차원(dimension) 이라고 한다.\n② 차수(degree)\n일반적으로 다항식(polynomial)의 최고 지수(exponent)를 의미한다.\n예: 다차원 입력 ((x_1, x_2)) 에 대해 (x_1^2), (x_1 x_2), (x_2^2) 등 고차항(high-order term) 을 추가하면 모델은 비선형(곡선 형태) 관계를 표현할 수 있게 된다.\n02 데이터의 벡터 표현 데이터는 점과 방향성을 갖는 벡터(vector) 로 표현될 수 있다.\n벡터는 스칼라(scalar)와 달리 크기뿐 아니라 방향을 가지며, 이를 표현하기 위해서는 시작 좌표와 끝 좌표가 필요하다.\n한편, 어떤 점 ((x_1, x_2, , x_n))도 원점을 기준으로 하면 원점에서 그 점까지의 벡터로 간주할 수 있다. 즉, 점과 벡터는 서로 밀접한 개념이며, 데이터는 수학적으로 이러한 벡터 형태로 표현된다.\n자연어 처리(NLP)에서는 문장을 단어(토큰) 단위로 나누고, 각 단어를 하나의 임베딩 벡터(embedding vector) 로 나타낸다.\n이를 다음과 같이 표현할 수 있다.\n\\[\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{id}) \\in \\mathbb{R}^d\\]\n(i:) 문장 내 단어의 인덱스 (d:) 임베딩 공간의 차원(dimension) 모든 단어 벡터는 동일한 차원 (d)를 가지며, 유사한 의미를 가진 단어들은 벡터 공간상에서 가까운 위치에 존재한다. 이러한 벡터 표현은 문장 내 의미적 관계를 수치적으로 분석할 수 있게 해 준다.\n03 토큰 수와 표현의 제약\n1 . 언어 모델의 입력 구조 언어 모델은 입력할 수 있는 토큰(token) 의 수에 제한이 있다. 즉, 모델은 정해진 최대 토큰 길이(max token length) 이내의 입력만 처리할 수 있다.\n또한, 각 토큰은 고정된 차원 (d)의 임베딩 벡터(embedding vector) 로 변환되므로, 모델의 입력 공간 역시 차원이 고정되어 있다고 할 수 있다.\n① 고전적 표현 방식\nBag-of-Words 또는 One-hot Encoding 에서는 문장 내 각 단어의 존재 여부를 0과 1로 표시하여 표현하였다.\n예: 특정 단어가 문장에 포함되어 있으면 1, 포함되지 않으면 0으로 나타내는 방식.\n② 현대적 표현 방식\n임베딩 기반 모델(embedding-based model) 에서는 이진값이 아닌 실수(real-valued) 벡터를 사용하며, 단어의 의미 유사성과 문맥 정보를 함께 반영한다.\n이로써 모델은 단순한 단어 포함 여부를 넘어, 단어 간의 의미적 관계와 문맥적 상호작용까지 학습할 수 있다.\n2 . 문장의 벡터 표현과 유사도 계산 문장은 이러한 단어(토큰)들의 벡터로 구성되며, 전체 문장은 다차원 공간 상의 하나의 점(vector) 으로 표현된다.\n문장 간의 유사도는 두 벡터 간의 거리(distance) 나 코사인 유사도(cosine similarity) 를 통해 측정할 수 있으며, 거리가 가까울수록 의미적으로 유사한 문장으로 해석된다.\n그러나 실제 데이터는 차원이 매우 높고, 대부분의 값이 0인 희소(sparse) 형태를 띤다.\n이러한 고차원 희소 표현은 계산 비용은 크고 메모리 효율은 낮아, 이를 더 작은 차원으로 변환하는 차원 축소(dimensionality reduction) 가 필요하다.\n차원 축소는 본래 의미 구조를 최대한 보존 및 데이터 압축 과정이다.\n대표적인 기법: PCA(주성분분석), SVD(특이값 분해), 워드 임베딩(word embedding) 등.\n이 과정을 통해 얻어진 밀집 벡터(dense vector) 는 정보가 효율적으로 압축된 형태로, 계산 효율이 높고 모델이 의미적 관계를 더욱 잘 학습할 수 있다.\n이러한 벡터는 저차원 공간으로 투영(projection) 되어, 이후의 학습, 분류, 또는 유사도 계산 등의 연산에 활용된다.\n04 SVM의 핵심 개념\nSupport Vector Machines (SVM): An Intuitive Explanation\nEverything you always wanted to know about this powerful supervised ML algorithm\nmedium.com\n출처: Tibrewal (2021), Medium – Support Vector Machines (SVM)\n1 . 결정 경계 Decision Boundary\nSVM은 두 클래스를 분리하는 초평면 (^ + b = 0) 을 찾는다.\n쉽게 말해, 데이터를 두 클래스(예: 빨간 점 vs 파란 점)로 나누는 선(2차원) 또는 초평면(3차원 이상) 을 찾는 것이며, 핵심 목표는 두 클래스 사이의 “간격”을 최대화하는 초평면을 찾는 것이다.\n초평면의 수학적 표현과 조건 결정 경계: (^ + b = 0) 마진 경계: (^ + b = +1) 또는 (^ + b = -1) 서포트 벡터 조건: (y_i(^_i + b) = 1)\n2 . 마진 Margin\n결정 경계와 서포트 벡터 사이의 거리. SVM은 이 마진을 최대화하는 초평면을 찾는다.\n마진이 넓을수록 모델의 일반화 능력이 향상되어, 새로운 데이터가 들어와도 결정 경계가 잘못 분류될 가능성이 줄어든다.\n즉, 마진 크기와 분류 오류 확률은 반비례 관계에 있다.\n\\[P_{\\text{error}} \\propto \\frac{1}{\\gamma_2}\\]\nSVM은 “마진 최대화 문제” 를 풀어 초평면을 결정한다. 수학적으로는 다음 문제를 푼다:\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\quad \\text{s.t.} \\; y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1\\]\n(:) 초평면의 법선 벡터(normal vector) ( b ; : ; ) 편향(bias) (y_i:) 클래스 레이블(+1 또는 -1) (_i:) 각 데이터 점 이 문제는 Lagrange 승수법 및 이중 문제(dual problem)를 통해 풀 수 있으며, 서포트 벡터만으로도 최적의 초평면을 결정할 수 있다.\n3 . 서포트 벡터 Support Vectors\n결정 경계 또는 마진 경계에 가장 가까운 데이터 포인트들. 쉽게 말해, “초평면을 밀었을 때 점들이 가장 먼저 닿는 위치”이다.\n이 점들은 초평면의 위치와 방향을 결정하며, 나머지 점들은 보통 결정 경계에 영향을 주지 않는다.\n(단, soft margin SVM에서는 일부 내부 점이 영향을 줄 수도 있다.)\n05 SVM의 최적화 문제 앞서 설명한 마진 최대화 문제에서, 이상치가 존재하거나 데이터가 완전히 선형적으로 분리되지 않는 경우 슬랙 변수 (_i)를 도입한다.\n이는 약간의 분류 오류를 허용하면서 마진 최대화가 가능해, 각 데이터 포인트의 결정 경계에서의 거리를 고려하는 힌지 손실 함수로 최적화 문제를 정의한다.\nSVM의 최적화 문제는 다음과 같이 표현된다:\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i\\]\n단, 제약 조건은 다음과 같다:\n\\[y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\quad \\forall i\\]\n(_i:) 슬랙 변수(허용 오차) (C:) 정규화 파라미터\n1 . 슬랙 변수 & 힌지 손실 Slack Variable & Hinge Loss\n슬랙 변수 (_i)는 각 데이터 포인트가 결정 경계에서 얼마나 벗어났는지를 나타낸다.\n이는 힌지 손실 함수와 동일한 역할을 한다:\n\\[L(y_i, f(\\mathbf{x}_i)) = \\max(0, 1 - y_i f(\\mathbf{x}_i))\\]\n여기서 (f(_i) = ^_i + b)이다. 이 함수는 다음과 같은 경우에 대해 다르게 동작한다:\n(_i)가 정확하게 분류됨, 마진을 만족하는 경우: 손실 0 (_i)가 정확하게 분류됨, 마진 내에 있는 경우: 양의 손실 (_i)가 잘못 분류됨: 큰 손실 이러한 손실은 모델이 마진을 넓히고 오차를 최소화하도록 유도한다.\n2 . 비선형 분류를 위한 커널 방법 비선형 데이터의 경우, SVM은 커널 함수를 사용하여 고차원 특성 공간으로 데이터를 매핑한다. 이는 선형적으로 분리 불가능한 데이터를 선형적으로 분리할 수 있게 된다.\n커널 함수는 다음과 같은 형태를 가진다:\n\\[K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\\]\n(())는 특성 공간으로의 매핑 함수(basis function, kernel)로, 커널 트릭을 사용하면 내적 계산만으로 고차원 공간에서의 계산을 효율적으로 수행할 수 있다.\n현실 데이터에서는 선형 관계보다는 비선형 관계가 더 흔하다.\n예: AND, OR 게이트는 선형적으로 분리 가능하지만 XOR 게이트는 선형 경계로 구분할 수 없으며, 이를 해결하려면 2차원 이상의 고차원 공간으로 매핑해야 직선(초평면)으로 구분 가능하다.\n따라서 XOR 문제와 같이 선형 분리가 어려운 문제를 표현하기 위해 AI 연구자들은 원 공간(original space)에서 고차원 매핑 공간(mapping space)으로 데이터를 변환하는 개념을 도입했다.\n이때 사용되는 함수들을 커널 함수(kernel function)라 부르며, 매핑된 공간(mapping space)에서는 비선형 데이터가 선형적으로 분리 가능해진다.\n대표적인 커널 함수: 선형(linear), Gaussian (RBF), 다항식(polynomial), sigmoid 등.\n한편, 데이터 처리 과정에서 발생하는 노이즈는 측정 오류나 무작위 변동에 의해 생기는 것으로 일반적으로 제거를 고려한다.\n반면 이상치는 데이터 패턴에서 벗어난 극단적인 값으로 유의미한 정보를 포함할 수 있어 분석에 포함되기도 한다.\n노이즈와 이상치를 구분하는 기준은 보통 도메인 전문가의 판단에 따라 결정된다.\n4 . 최적화 문제의 이중 문제 Dual Problem\nSVM의 최적화 문제는 이중 문제로 변환할 수 있으며, Lagrange 승수 (_i)를 도입하여 다음과 같이 표현된다:\n\\[\\max_{\\alpha} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\\]\n단, 제약 조건은 다음과 같다:\n\\[0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0\\]\n이 이중 문제를 풀면 최적의 (_i)를 얻을 수 있으며, 이를 통해 초평면의 계수 ()와 (b)를 계산할 수 있다.\n06 모델 일반화와 조정\n1 . 하이퍼파라미터 상수 (C)는 슬랙 변수에 부여되는 패널티 강도(regularization strength)를 조절한다.\n(C)가 크면: 오차를 최소화하려는 경향이 강해져 과적합 위험 증가 (C)가 작으면: 마진 증가 및 일부 오차를 허용하여 일반화 성능 향상 즉, (C) 값은 결정 경계의 유연성을 결정하는 핵심 요인이다. 이 값을 조정하는 과정을 하이퍼파라미터 튜닝(Hyperparameter Tuning) 또는 파인튜닝(Fine-Tuning)이라 한다.\n2 . 전이학습 Transfer Learning\n모델이 학습 데이터에는 잘 맞지만, 새로운 데이터(테스트 데이터)에서는 성능이 저하되는 경우를 과적합이라 한다.\n이를 방지하려면 다양한 데이터에 대해 잘 작동하는 일반화 능력이 필요하다.\n딥러닝에서는 이러한 일반화를 위해 이미 대규모 데이터(예: 기사, 위키피디아 등)로 학습된 사전 학습 모델(Pre-trained Model)을 기반으로 하는 전이학습(Transfer Learning)이 활용된다.\n전이학습은 기존 지식을 새로운 도메인에 적용하는 과정이며, 특정 분야의 데이터로 추가 학습하여 세부 조정을 수행하는 단계를 파인튜닝이라 한다.\n결국, SVM에서의 (C) 조정과 같이 전이학습 및 파인튜닝 역시 “기존 모델의 일반화 능력을 유지하면서 특정 목적에 맞게 조정하는 과정”이라는 공통 원리를 가진다."
  },
  {
    "objectID": "ai/ml_04.html",
    "href": "ai/ml_04.html",
    "title": "머신러닝: 실무 사례",
    "section": "",
    "text": "반도체 HBM 공정과 스마트 CCTV를 중심으로 머신러닝의 실제 활용에 대해 다루고자 한다.\n01 제조 공정 혁신\n1 . 과거 제조업 현장 오랜 기간 아날로그 방식의 공정 관리에 의존해 왔으나 글로벌 경쟁 심화와 공정 복잡성 증가로 인해 단순 수작업이나 경험 중심의 운영만으로는 품질과 효율성을 확보하기 어려워졌다.\n이에 따라 디지털 전환(DX) 과 인공지능 전환(AX) 은 제조업 경쟁력 강화를 위한 핵심 과제로 부상하였다.\nDX와 AI 트랜스포메이션의 개념을 정리하고, 실제 사례로서 반도체 HBM 적층 공정의 기술적 차이를 분석한다.\n2 . Digital Transformation (DX) 기존의 아날로그 및 비체계적 공정을 시스템화된 디지털 관리 체계와 데이터베이스 기반 운영으로 전환하는 과정이다.\n공정, 설비, 품질 관리 정보를 실시간으로 수집·저장 운영 데이터의 추적성(traceability) 확보 표준화된 관리 체계 구축을 통해 오류 및 낭비 최소화 제조업에서 DX는 스마트 팩토리로의 진입을 위한 기초 단계라 할 수 있다.\n3 . AI Transformation (AX) DX 위에 구축된 데이터 인프라는 AI 분석을 통한 고도화로 확장된다. 이것은 다음 단계들을 포함한다.\n센서 및 IoT 장치를 통한 빅데이터 수집 실시간 데이터 시각화 및 공정 모니터링 이상치(anomaly) 탐지 및 자동 알림 데이터 기반 원인 분석 및 해결 방안 제시 공정 최적화 및 예측 유지보수 이러한 체계는 설비 수준이 낮은 공장에서도 점진적 도입이 가능하며, 데이터 기반 의사결정을 통해 전사적 품질 및 생산성 개선 효과를 제공한다.\n4 . 사례 분석: HBM 적층 공정 고대역폭 메모리(HBM)는 여러 개의 DRAM 다이를 수직으로 적층하여 초고속 데이터 전송을 가능케 하는 핵심 기술이다. 하지만 적층 단수가 증가할수록 수율(yield) 저하가 주요 문제로 지적된다.\n① SK하이닉스\nMR-MUF 공정을 활용하여 적층 단수 증가에도 수율 저하를 최소화한 것으로 평가된다.\n일부 보고에 따르면 12단 적층에서도 비교적 안정적인 수율 확보가 가능하다.\n② 삼성전자\nTC-NCF 기반 공정을 채택했으며, 접합 안정성 확보 및 void 제어 문제로 인해 수율 측면에서 난제를 겪고 있는 것으로 알려져 있다. 다만, 최근에는 Cu-to-Cu 본딩 등 새로운 기술을 병행 도입하고 있다.\n공개된 산업 보고 및 전문가 분석에 따르면, “삼성은 수율이 70% 미만, SK는 상대적으로 높은 수율 확보”라는 평가가 존재하나, 이는 공식 수치가 아닌 시장 리서치 기반 추정에 가깝다.\n‘HBM’ 골든타임 노리는 엇갈린 시선···삼전 ‘포괄적’ vs SK하닉 ‘트렌드’\n[이뉴스투데이 김진영 기자] AI 반도체 시장이 급속도로 확대되는 가운데 고대역폭메모리(HBM)의 중요성이 커지고 있다. 삼성전자와 SK하이닉스는 모두 AI 생태계 확장을 목표로 하고 있지만 HBM을\nwww.enewstoday.co.kr\nAI 시대의 새로운 심장: 삼성전자와 SK하이닉스, HBM과 차세대 메모리 기술로 펼치는 반도체 패권\nAI 시대의 HBM 기술 혁명! 삼성전자와 SK하이닉스의 패권 경쟁, 차세대 메모리의 미래를 탐구합니다. 한국 반도체의 도약을 확인하세요!\nskywork.ai 따라서 기술적 차이와 경향성은 확인되지만, 구체적인 수치는 신뢰도 높은 자료가 부족하므로 주의가 필요하다.\n5 . 결론 DX 와 AX 는 제조업 혁신의 핵심 경로로, 단순한 디지털화에서 나아가 AI 기반 예측·최적화 체계로의 전환을 의미한다.\n이는 설비 수준과 무관하게 단계적으로 도입이 가능하며, 스마트 팩토리 구현의 기반이 된다.\nHBM 사례는 AX 의 필요성을 잘 보여준다. 고난도의 적층 공정에서는 공정 변수와 결함 데이터를 정밀하게 수집·분석해야 수율을 안정적으로 확보할 수 있다. SK와 삼성의 공정 차이는 이러한 데이터 기반 접근의 중요성을 방증한다.\n따라서 제조업의 경쟁력 확보를 위해서는 DX → AX → 스마트 팩토리로 이어지는 단계적 전략이 필수적이며, 이는 단순한 시스템 도입을 넘어 데이터와 AI 중심의 운영 철학 전환을 요구한다.\n02 서울시 스마트 CCTV 시스템 서울시는 2020년부터 지능형 CCTV를 도입하여 교통 흐름 분석, 범죄 예방, 실시간 교통 신호 제어 등을 시도하였으나, 초기 도입 단계에서 AI 모델이 실제 현장 환경에서 기대 이하의 성능을 보였다.\n이는 모델이 주로 실내 환경이나 정형화된 데이터에만 학습되어, 다양한 날씨, 시간대, 사람의 행동 패턴 등 실제 환경의 변수를 충분히 반영하지 못했기 때문이다.\n서울시는 이러한 문제를 해결하기 위해 산·학·연과 자치구가 참여하는 ‘지능형 CCTV 활성화 계획’ 을 수립하고, 맞춤형 이벤트 설정, 오탐 데이터 학습, 사물·사람 구분 학습 등을 추진하였다.\n지능형CCTV로 만드는 디지털 안전도시 서울\n서울시가 시민 안전 강화 및 범죄 등 예방을 위해 올해 AI기반 지능형 CCTV를 대폭 늘리고, 시민들의 정보접근성을 높여줄 공공와이파이를 확대할 계획입니다. 서울시는 또 유동인구가 많은 곳 등\nscpm.seoul.go.kr\n서울시, ’지능형 CCTV’로 지자체 ICT 우수사례 대통령상 수상\n서울시가 ’제30회 지방자치단체 정보통신 우수사례 발표대회’에서 지능형 CCTV 오탐지 문제 해결로 대통령상인 최우수상을 수상했다. 관제 효율과 시민 안전망 강화가 높이 평가됐다. 서울시는\nwww.etnews.com 그 결과, 지능형 CCTV의 상황 판별 정확도는 36% → 71%로 향상되었고, 관제요원의 이벤트 확인률도 37% → 82%로 높아졌다.\n불필요한 탐지 건수는 월 454만 → 53만 건으로 줄어들며 약 8.8배 감소하였다. 이러한 개선은 사건 처리 건수를 이전보다 6배 이상 증가된 성과를 가져왔다.\n또한, 서울시는 2026년부터 지능형 CCTV에 생성형 AI를 접목하는 시범사업을 추진하여, 기존 CCTV가 단순히 ‘이상 유무’ 만을 판별하는 수준에서 ‘왜 이상한지, 어떤 맥락인지’ 를 설명할 수 있는 단계로 진화할 계획이다.\n이러한 사례는 AI 기반 CCTV 시스템의 효용성을 높이기 위해서는 다양한 상황을 반영한 데이터 수집과 모델 훈련이 필수적임을 보여준다.\n정상 상황뿐 아니라 예외적 상황까지 포함하는 데이터 기반의 모델 훈련이 가장 중요한 요소로 고려되어야 한다."
  },
  {
    "objectID": "ai/ml_02.html",
    "href": "ai/ml_02.html",
    "title": "머신러닝 개론",
    "section": "",
    "text": "머신러닝의 전반적인 개론에 대해 다루고자 한다.\n\n01 머신러닝\nMachine Learning\n데이터로부터 패턴을 학습하여 예측이나 의사결정을 수행하는 인공지능의 한 분야이다.\n머신러닝은 데이터의 라벨(Label) 존재 여부에 따라 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)으로 구분된다. 이를 이해하기 위해 대표적인 예시인 분류와 군집화를 비교하도록 한다.\n1 . 분류(Classification) 지도학습의 대표적인 기법으로, 라벨이 주어진 데이터를 학습하여 새로운 데이터의 정답을 예측한다.\n결과값은 이산형 범주(예: 스팸/일반메일)이며, 대표 알고리즘으로 로지스틱 회귀, 의사결정나무, SVM 등이 있다.\n2 . 군집화(Clustering) 비지도학습에 속하며, 라벨이 없는 데이터를 유사도에 따라 그룹화하여 내재된 구조를 발견한다.\n실제 라벨이 없으므로 손실함수 대신 클러스터 내의 응집도(cohesion)와 클러스터 간 분리도(separation)와 같은 목적함수를 사용하여 데이터의 구조를 평가한다. [출처]\n대표 알고리즘으로는 K-Means, DBSCAN, 계층적 군집화가 있으며, 고객 세분화나 이미지 분석 등에 활용된다. [출처1], [출처2]\n반지도학습(Semi-supervised Learning)\n일부 데이터에만 라벨이 존재하는 경우, 라벨이 없는 데이터를 함께 활용해 학습 성능을 향상시키는 방법이다. 이는 라벨링 비용이 높은 실제 환경에서 자주 사용된다.\n02 손실함수 Loss function\n앞서 살펴본 지도학습은 라벨이 존재하는 데이터를 기반으로 예측 모델을 학습한다. 이러한 학습의 핵심은 예측값과 실제값의 차이를 최소화하는 것으로, 이를 위해 손실함수를 정의한다.\n예측과 정답의 오차(error)를 수치화하며, 학습의 목표는 이 손실을 최소화하는 것이다. [출처]\n이 과정에서 사용되는 가중치(weight) 는 입력 변수의 중요도를 나타내는 모델의 매개변수이며, 학습은 손실함수를 최소화하는 방향으로 가중치를 조정하는 절차이다.\n기울기(gradient)는 손실함수에 대한 가중치의 변화율을 의미하며, 이를 이용해 가중치를 반복적으로 업데이트한다.\n최종적으로 학습이 완료되면 성능 지표(evaluation metric)를 통해 모델의 예측력이 평가된다. [출처]\n강화학습, Reinforcement Learning\n앞서 지도학습이 정답 데이터를 통해 모델을 학습했다면, 강화학습은 보상을 통해 스스로 최적의 행동을 학습한다.\n예를 들어, 로봇이 공의 궤적을 예측하고 어떤 타격 동작을 해야 가장 좋은 보상을 얻는지를 학습하는 방식이다.\n계층적 구조를 적용해, 상위 수준에서는 언제 어떤 자세로 칠지 등의 전략을 결정하고, 하위 수준에서는 실제 관절 제어와 동작 실행을 담당한다.\n결과적으로 로봇이 사람처럼 스핀·속도에 대응하며 탁구를 치는 것이 가능해진다.\n03 차원축소 Dimensionality reduction\n고차원 데이터에서 중요한 정보를 보존한 채 저차원으로 표현하는 기법이다.\n숨겨진 구조나 패턴을 더 명확히 드러내고 데이터의 시각화 및 분석 효율을 높이는 데 활용된다. [출처]\n이들은 차원을 줄이면서도 정보 손실을 최소화하는 것을 목표로 한다.\n이러한 차원 축소 알고리즘들은 학습 방식에 따라 선형과 비선형으로 구분된다.\n1 . 선형 기법 데이터의 분산이나 경계선을 직선(또는 평면) 형태로 설명하는 방식이다.\n공분산 행렬의 고유벡터나 선형 결합 계수를 구하며, 경우에 따라 가중치나 파라미터를 학습하지만 반드시 절편(intercept)이 포함되지는 않는다.\n대표적인 기법으로는 PCA(주성분분석), LDA(선형판별분석)이 있다.\n2 . 비선형 기법 복잡하게 휘어진 데이터 구조를 그대로 보존하려는 방식이다.\n고차원 공간에서의 유사성 확률을 저차원에서도 유지하도록 하며, 전통적인 가중치나 절편 개념을 사용하지 않는다.\n대표적인 기법으로는 t-SNE가 있다.\n3 . 특징의 의미 머신러닝에서 특징(feature)은 관측된 입력 변수로서, 목표값 y를 예측하는 데 사용되는 정보이다. [출처]\n이러한 변수들은 모두 같은 역할을 하지는 않으며, 모델은 학습을 통해 변수의 기여도나 중요도를 반영한다. [출처]\n선형 모델에서는 가중치(coefficient)가 크면 특정 변수의 영향이 크다는 의미가 될 수 있지만, 변수 간 상관관계나 스케일 차이로 단순 비교는 주의가 필요하다. [출처]\n그러나 모든 모델이 가중치를 명확히 제공하는 것은 아니다.\n특히 비선형 구조나 트리 기반 모델에서는 변수의 영향력을 직접 해석하기 어려우므로,\nfeature importance, SHAP, LIME 등의 기법을 이용해 각 변수의 기여도를 시각화한다.\n04 최적화 알고리즘 파라미터 공간(parameter space)을 탐색하여 더 나은 해를 찾는 절차라는 점에서 탐색(search)의 성격을 가진다.\n하지만 전통적인 그래프, 경로 탐색 알고리즘과는 달리, 연속적인 공간에서 목적 함수를 기준으로 한 최적 해를 찾아가는 과정이다.\n비선형(non-convex) 문제나 딥러닝 모델의 경우, 전역 최적해(global optimum)에 도달할 수 있는지,\n아니면 지역 최적해(local optimum)나 안장점(saddle point)에 머무를지에 대한 이론적 증명은 매우 복잡하다.\n특히 Adam, AMSGrad 같은 최적화 기법의 수렴(convergence)을 보장하는 연구에서도, 하이퍼파라미터 설정이나 학습률 조건 등이 중요한 변수가 되며, 수렴 증명 자체가 쉽지 않다는 점이 강조된다.\n또한 최적화 과정은 많은 반복(iteration)과 대규모 데이터, 복잡한 모델 구조가 결합될 경우 계산 비용(computation cost)과 시간적 비용이 매우 크다.\n이러한 이유로 실무에서는 학습 과정이 고비용·고시간 소모적이라는 점이 자주 지적된다.\n05 실무 사용처\n1 . 지자체 분야 지자체는 교통 흐름 예측, 범죄 예측, 환경 모니터링 등 다양한 분야에서 머신러닝 알고리즘을 활용한다.\n예: 교통 혼잡도 예측, 재난 발생 가능성 예측\n2 . 제조 분야 제조업체는 센서 데이터를 분석하여 장비 고장을 예측하고, 이를 통해 유지보수 비용을 절감하는 예지 보수(Predictive Maintenance)를 한다.\n생산 라인에서 발생하는 결함을 실시간으로 감지 및 품질 향상을 위해 머신러닝 알고리즘이 사용된다.\nKAMP, 인공지능제조플랫폼\n스마트 대한민국 구현의 허브!제조 AI 강국으로의 도약 KAMP가 함께합니다.\nwww.kamp-ai.kr\n3 . 금융 분야 금융 기관은 고객의 신용도 평가 및 대출 리스크 관리를 위해 머신러닝 모델을 활용한다.\n거래 패턴을 분석하여 이상 거래를 실시간으로 감지 및 사기 예방에 사용된다.\n06 데이터 분석 파이프라인\n1 . 수집, Collection DB 연계(Repository), 웹 크롤링, 에이전트 설치, 서버 로그, IoT 센서 등 다양한 채널을 통해 데이터를 확보한다.\n이때 수집된 데이터에는 정상치(normal data), 비정상치(outlier 또는 anomaly), 그리고 노이즈(noise) 가 함께 포함될 수 있으며, 이 세 요소의 비율과 품질은 분석 성능에 직접적인 영향을 미친다.\n다만 이러한 비율은 고정된 규칙이 있는 것은 아니며, 데이터의 특성과 도메인(예: 제조, 금융, 의료 등)에 따라 경험적으로 조정·판단되어야 한다.\n핵심은 각 요소를 정확히 구분하고 적절히 처리하는 것이다.\n2 . 정제, Cleaning 수집된 데이터의 품질을 향상시켜 학습 알고리즘이 효율적으로 동작하도록 한다.\n이 단계에서는 먼저 추가·삭제·대체 등의 과정을 통해 누락값(missing values)을 처리하거나 불필요한 데이터를 제거한다.\n또한, 로그 변환이나 스케일 조정 등 변환(Transformation) 작업을 수행하고, 범주형 데이터를 수치형으로 바꾸는 인코딩(Encoding) 과정을 거친다.\n변수 간의 범위를 맞추기 위해 스케일링(Scaling) 또는 정규화(Normalization) 를 적용하며, 이상치(outlier)는 탐지 후 제거하거나 조정하여 데이터의 왜곡을 방지한다.\n한편, 이상치 또는 희소 클래스(sparse class)에 대한 증강(augmentation) 은 전통적 전처리(cleaning) 절차에는 포함되지 않지만,\n불균형(class imbalance) 문제를 완화하거나 특정 도메인(예: 이미지, 텍스트)에서 모델의 일반화 능력 향상을 위해 합성(over-sampling), 생성모델 기반 증강, 혼합 확장(mixup) 등 증강 기법으로 활용된다.\n이러한 방식은 소수 클래스의 표현을 늘려 분포의 대표성을 높이는 전략으로 연구되고 있다.\n예를 들어, SMOTE(Synthetic Minority Over-sampling Technique)는 소수 클래스 데이터 간 보간(interpolation) 방식을 통해 합성 샘플을 생성함으로써 클래스 불균형을 완화하는 대표적인 증강 기법이다.\n또한, GAN(Generative Adversarial Network) 기반 증강 기법(BAGAN 등)은 소수 클래스를 생성하여 불균형 문제를 해결하려는 연구도 있다.\n3 . 저장, Storage 수집 및 정제된 데이터를 효율적으로 보관하고, 분석과 학습에 활용할 수 있도록 관리한다.\n데이터는 세 가지 유형으로 나뉘며, 각 유형에 따라 저장 기술이 다르다.\n① 정형 데이터, Structured Data\n고정된 스키마를 가진 테이블 형식 데이터로, 관계형 DB(RDBMS)에 저장된다.\n예: MES(Manufacturing Execution System)에서 생성되는 생산 이력, IoT 센서 데이터 등\n대표 기술: MySQL, MariaDB, PostgreSQL, SAP HANA(인메모리 DB), 데이터 웨어하우스(Amazon Redshift, Google BigQuery)\n② 비정형 데이터, Unstructured Data\n사전 정의된 스키마가 없으며, 텍스트, 이미지, 영상, 오디오 등 다양한 형식을 포함한다.\n예: CCTV 영상, 소셜 미디어 게시물, 고객 피드백\n대표 기술: MongoDB, Cassandra, HBase(NoSQL DB), Hadoop HDFS, 데이터 레이크(Amazon S3, Azure Data Lake)\n③ 반정형 데이터, Semi-structured Data\n일부 구조적 요소를 포함하지만 완전한 스키마는 없는 데이터\n예: JSON, XML, 로그 파일, IoT 센서 데이터\n대표 기술: MongoDB, Couchbase, 데이터 레이크(Amazon S3, Azure Data Lake)\n참고 사항 Schema-on-write: 데이터를 저장할 때 스키마를 정의하는 방식 (RDBMS)\nSchema-on-read: 데이터를 읽을 때 스키마를 적용하는 방식 (NoSQL, 데이터 레이크)\n4 . 시각화, Visualization 분석 결과를 직관적으로 이해 및 전달하기 위한 단계이다. 특히 대시보드(Dashboard)는 여러 시각화 요소를 통합하여 웹이나 애플리케이션 상에서 상호작용 가능하게 제공한다.\n① 주요 시각화 도구\nPlotly: 웹 기반 인터랙티브 시각화 라이브러리로, 2D·3D 플롯과 대시보드 제작에 적합하다. Dash: Plotly를 기반으로 한 Python 웹 프레임워크로, 코드만으로 대시보드를 구현 가능하다. Matplotlib: 기본 2D 시각화에 강점을 가지며, 세밀한 커스터마이징이 가능하다. Seaborn: Matplotlib 기반으로 통계적 시각화에 특화되어 있으며, 간결한 문법과 미려한 디자인을 제공한다. ② 주요 시각화 유형\n2D/3D 플롯: 데이터 분포 및 변수 간 관계를 표현 바 차트, 파이 차트: 범주형 데이터의 분포를 시각화 히트맵(Heatmap): 데이터 간 상관관계 또는 밀도를 색상으로 표현 맵(Map): 지리적 데이터를 시각화하여 공간적 패턴 분석\n5 . 분석, Analysis 수집·정제·저장된 데이터를 기반으로 의미 있는 인사이트를 도출한다. 빅데이터와 AI 환경에서 머신러닝은 핵심적인 역할을 하며, 네 가지의 주요 기법이 있다.\n① 분류, Classification\n지도학습(Supervised Learning)에 속하며, 입력 데이터에 대해 미리 정의된 레이블을 예측한다.\n활용 예: 스팸 이메일 필터링, 질병 진단 등\n② 군집화, Clustering\n비지도학습(Unsupervised Learning)에 속하며, 라벨 없이 데이터 내 유사한 특성을 가진 그룹을 식별한다.\n활용 예: 고객 세분화, 시장 분석\n③ 예측, Prediction/Regression\n연속형 목표값을 예측하며, 과거 데이터 기반으로 미래 값을 추정한다.\n④ 추천, Recommendation\n사용자 행동이나 선호도를 분석하여 개인화된 추천을 제공한다.\n활용 예: 전자상거래, 콘텐츠 플랫폼"
  },
  {
    "objectID": "ai/ml_01.html",
    "href": "ai/ml_01.html",
    "title": "AI 산업 전망",
    "section": "",
    "text": "AI 산업 전망에 대해 다루고자 한다.\n01 팬데믹\n1 . 온라인 전환 및 IT 수요 급증 팬데믹으로 인해 오프라인 중심이던 기업들이 온라인으로 빠르게 전환되었다.\nMS의 CEO인 Satya Nadella는 2020년 4월 30일에 발표된 분기 실적 보고서에서 다음과 같이 언급하였다:\n” 우리는 두 달 만에 2년 분량의 디지털 전환을 경험했다. ” [출처] 재택근무와 원격교육이 보편화됨에 따라 화상회의, 실시간 번역, 맞춤형 배경 등 다양한 비대면 기술이 빠르게 발전했으며, 이로 인해 협업 솔루션 시장은 급격히 확대되었다.\n연도 시장 규모 추정치 출처/비고 2019 약 9.878 십억 달러 Allied Market Research 2020 약 15.25 십억 달러 Fortune Business Insights 2021 추정치 계산 불가 COVID-19 효과가 강해, 일관된 비교가 어렵다는 한계 있음. 2022 약 27.4081 십억 달러 Grand View Research 2023 약 21.79 십억 달러 Fortune Business Insights 2024 약 36.1142 십억 달러 Grand View Research 미국의 온라인 쇼핑 매출은 2019년 5,712억 달러에서 2020년 8,154억 달러로 전년 대비 43% 증가했다. [출처]\n유럽에서도 2020년 4월, 전체 소매 판매는 큰 폭으로 감소했지만, 온라인 구매 비중은 2019년 4월 19.1%에서 30.7%로 급증했다. [출처]\n각국의 락다운이 시행되면서 화상회의, 원격근무, 온라인 교육 등의 증가로 인터넷 트래픽이 1주일 이내에 15 ~ 20% 증가했다. [출처]\n2 . 팬데믹 종료 이후 이전 수준으로 회귀 여부 소비 회복의 불균형 가능성이 언급되었다.\n팬데믹으로 대면 서비스 수요는 급감했지만, 팬데믹 이후 일부 행태는 유지될 가능성이 컸다. [출처]\n또한, 많은 오프라인 매장의 폐업, 상업용 부동산 공실 증가 등은 온라인 전환이 영구적인 변화였음을 시사한다.\n1 . 오프라인 회복, 여전히 주요 비중 유지 2019년 디지털 판매 성장률이 53.5%였던 반면, 2022년 오프라인 매출은 전체 성장의 78.1%를 차지하며 오프라인 매장이 여전히 매출 성장의 주요 원천임을 보여준다. [출처]\n실제 매장 개방 시 디지털 매출이 평균 6.9% 증가하지만, 매장을 닫으면 온라인 매출은 11.5% 줄어든다는 연구도 있다. 이는 오프라인 매장의 존재가 디지털 매출에도 긍정적 영향을 준다는 점을 시사한다. [출처]\n2 . 오프라인이 다시 강세를 보이는 추세 팬데믹 중 급성장한 전자상거래는 이후 이전 예상 수준으로 회귀하는 경향이 있는 반면, 오프라인 소매업은 예상보다 더 높은 회복세를 보였다. 오프라인 소매 비중이 온라인보다 더욱 빠르게 반등했다는 분석도 있다. [출처]\n3 . 장기적인 변화는 지속 중 “다시는 이전으로 돌아가지 않을 것”이라는 표현이 있을 만큼, 디지털화된 경제 구조는 팬데믹 이후에도 지속될 가능성이 크다는 분석도 존재한다. 변화된 생산 및 소비 방식이 장기적으로 유지될 것이라는 견해이다.\n02 GPT\n1 . GPT 탄생, 챗봇 혁명 OpenAI는 2022년 11월 30일, GPT-3.5 기반의 ChatGPT를 공개적인 “연구 미리보기(research preview)” 형태로 출범시켰다.\n이 발표 직후 비약적인 성장세를 보였으며, 출시 두 달 만에 1억 명 이상의 사용자를 확보하는 등 빠른 이용자 확산을 기록했다.\n이 모델은 텍스트 기반 응답뿐만 아니라, 코드 생성 및 코드 완성이 가능한 능력을 갖추고 있어 많은 개발자가 실제로 활용 중이다.\n2 . 저렴한 수준의 AI 코딩 서비스 OpenAI가 제공하는 최상위 구독 요금제인 ChatGPT Pro는 월 $200에 무제한 GPT-4o Pro 접근권, 고급 음성 기능 등을 포함하며, 이는 약 50만 원 수준에 해당한다. [출처]\n구글도 마찬가지로 AI Ultra 요금제를 발표했으며, 가격은 $249.99/월로, Deep Think reasoning 모드 등 고급 기능을 탑재한 모델 및 AI 툴 세트를 제공한다. [출처]\n위 요금제는 고급 모델을 사용하려는 개인 혹은 연구자에게 적합하며, 초급 개발자를 비용 대비 대체하는 수준의 AI 기능을 충분히 포함하고 있다.\n3 . 월 10 ~ 20만 원대의 AI 코딩 도구 보다 현실적인 예시로, 다음과 같은 AI 코딩 어시스턴트도 존재한다: [출처]\n구분 비용 및 특징 GitHub Copilot 약 $10/월 , GPT-4 기반 코드 완성 기능 제공 Cursor Pro 약 $20/월, 다양한 모델 액세스 및 IDE 통합 Tabnine Pro 및 유사 도구들 $12–20/월 이들 도구는 초급 개발자 수준의 코딩 지원에 필요한 기능을 충분히 제공하며, 월 약 10 ~ 20만 원의 비용으로 사용 가능하다.\n03 경제 둔화\n1 . 물가 상승 → 금리 인상 중앙은행은 높은 인플레이션을 억제하기 위해 기준금리를 인상한다. 이는 통화정책의 기본 방향이며, 비교적 일반적인 경제 수단이다.\n테일러 룰(Taylor Rule)에 따르면, 인플레이션이 1% 상승할 때마다 명목금리는 그 이상을 인상해야 실제 금리가 상승하여 경제를 안정시키는 효과가 생긴다.\n2 . 금리 인상 → 고용 감소 물가 상승을 막기 위한 금리 인상은 기업의 비용 부담을 높여 고용 축소로 이어질 수 있으며 일부 계층(예: 여성, 소수민족 등)에 더 큰 영향을 줄 수 있다. [출처] 실제로 금리 인상 후 경기 침체(리세션) 즉 고용 시장이 위축되는 경향이 있다.\n3 . 고용 감소 → 소비 위축 고용이 줄면 소득 및 소비 여력이 감소되어, 결과적으로 경제활동 전반에 부정적 영향을 미친다. 이는 중앙은행이 다시 경기 부양을 위한 금리 인하를 고려하게 만드는 요인 중 하나이다. [출처]\n4 . 소득 감소 및 불확실성 → 투자 여력 축소 고용 불안정과 소비 위축은 기업의 투자 여력도 감소시킨다. 이는 재정의 불안정과 기업 활동 위축으로 이어질 수 있다는 일반적 경제 논리와 맞닿아 있다.\n04 고령화\n1 . 기술직 내 세대별 노동력 변화 대형 테크 기업에서 직원 평균 연령이 약 3년 이상 상승했다. 이는 밀레니얼 세대 이상이 IT 업계 내에서 지배적인 위치를 차지하게 되었음을 보여준다. 동시에, 디지털에 익숙한 젊은 세대(Gen Z)의 비중이 감소하고 있는 것으로 해석된다. [출처]\n이러한 경향은 “신입보다 시니어 인력이 주류가 되는 현상”과 연관시켜 볼 수 있다.\n2 . 고령화된 노동층의 도전 및 기회 AI 도입 과정에서 고령 노동자를 배제하지 않고 포함할 수 있도록 하는 전략 즉 ’age-proofing AI’의 필요성을 강조한다.\n이는 인력 다양성과 포용성을 위한 조치이며, 고령자 중심 구조가 불가피한 사회 변화로 다가올 수 있음을 시사한다. [출처]\n55–64세 연령층의 고용률은 학력 수준에 따라 크게 차이가 있으며, 고학력자 중심으로 고령 노동 시장에서의 참여가 증가하고 있다. [출처]\n3 . 시니어 중심 노동 시장에 대한 사회적 우려 45세 이상의 IT 종사자 중 70%가 연령 차별을 경험하거나 목격한 적이 있다고 응답했다.\n이 사실은 고령 인력이 노동 시장에서 고립될 수 있다는 구조적 위험도 내포한다. [출처]\n중국의 “curse of 35” (35세 저주) 사례도 주목할 만한다. 일부 IT 기업들은 30대 중반 이상 직원을 비용 부담과 에너지 부족을 이유로 선호하지 않는 경향이 있으며, 이는 고령 인력의 지위가 위태로울 수 있음을 나타낸다. [출처]\n4 . 기업 내 핵심 지식과 경험의 상실 U.S. 기업들의 경우, 베이비붐 세대가 은퇴하면서 조직의 핵심 지식이 사라지고 경험이 축적되지 않는 상황에 직면하고 있다. 이는 기업의 효율성과 혁신 역량 저하로 이어질 수 있다. [출처]\n고령 직원의 은퇴는 생산성 및 리더십 공백을 초래할 수 있어, 이를 대비한 체계적인 승계 계획과 멘토링 프로그램 수립이 필수적이라고 강조한다. [출처]\nNASA의 아폴로 사업 경험처럼, 핵심 인력 퇴직 후 해당 지식을 대체하지 못한 사례들도 있다. [출처] 2005년 Accenture 조사에서는 많은 기업들이 은퇴자 지식 이전을 위한 계획조차 없었음을 보고했다. [출처]\n5 . 고령 노동 비중 증가 추세 2022년 대비 2032년까지, 65세 이상 노동자의 노동 참여율이 6.6%에서 8.6%로 상승하며, 전체 노동 인구 증가분 중 57%가 고령층에 해당할 것으로 전망된다. 이는 고령층 노동자 비중이 높아지는 구조적 변화를 가리킨다. [출처]\nベイビーブーマー 세대의 대규모 은퇴로, Gen X, 밀레니얼, Z세대가 노동 시장의 주요 역량으로 부상하며, ’Silver Tsunami’로도 불리는 전환기가 도래하고 있다. [출처]\n05 산업 변곡점 1998년, NIPA(한국 IT산업진흥원)이 설립되어 대한민국의 IT 산업 육성을 위한 중추적 역할을 해 왔다.\n1 . 한국 정부의 AI 전략 집중 2025년 8월, 한국 정부는 AI 및 혁신 프로젝트 30개 추진을 골자로 한 AI 중심 경제 전략을 발표했다. 로봇, 자동차, 반도체, 드론 등 산업 전반을 포함하며, 국가 성장 펀드 규모는 100조 원 규모이다. [출처]\n또한 국가 AI 전략위원회를 설립하여 AI 정책 및 전략을 대통령 직속으로 관리하고 있다.\n“한국 정부는 AI를 국가 전략 산업으로 설정했고, 이제 본격적인 출발점에 서 있다.” 결과적으로, 기존 IT는 계속 존재하지만 정책·투자·혁신 관점에서는 AI가 1순위, 전통 IT는 2순위라는 구조적 변화라고 볼 수 있다.\nAI 산업에서의 경쟁 우위 확보는 주요 국정 과제로 꼽힌다. 이를 위해서는 데이터 확보 및 AI 인프라가 중요한 전략으로 강조된다. [출처]\n2 . 플랫폼을 통한 데이터 확보 한국에서는 네이버가 검색 엔진 시장의 절반 이상을 차지하고 있으며, 외국 Big Tech의 점유율은 상대적으로 낮게 유지되고 있다는 보고가 있다. [출처]\n특히, 한국은 지리정보 데이터와 같은 특정 데이터에 대해 엄격한 규제를 두고 있으며, 이로 인해 Google Maps 같은 서비스 기능이 완전히 작동하지 못하는 경우가 있다.\n또, Google은 한국의 앱스토어 수수료 및 자사 앱 우선 노출 등 관련하여 공정거래위원회 또는 규제 대상이 된 사례가 있으며, 이는 국내 플랫폼 보호와 경쟁 환경의 복합적 모습을 보여준다. [출처]\n한국 내 검색·광고 시장 분석에서는 네이버가 국내 사용자 특성, 언어, 콘텐츠 생태계와의 적합성 덕분에 강한 지위를 유지하고 있다는 언급이 있다. [출처]\n네이버와 카카오가 AI·콘텐츠 강화 전략을 내세우며 “자체 생태계 확장”을 추진 중이라는 보도도 있다. [출처]\n3 . 일본의 플랫폼 부재 일본은 전통적으로 내수 중심 플랫폼 기업이 한국·중국 등에 비해 적다는 평가가 있다.\n언어 및 문화, 법률·규제 측면에서 외국 기술 및 플랫폼 기업에 의존하는 경향이 있었고, 이에 따라 일본 정부나 대기업이 데이터 주권, 플랫폼 통합성 확보에 더 민감할 가능성 있다.\n일본과 유럽 간 데이터 공간(data space) 구현 방식을 비교한 연구에서는, 일본이 거버넌스 체계나 인증 프레임워크, 기술 표준 면에서 유럽보다 상호 운용성 확보에 어려움을 겪고 있다는 분석이 나온다. [출처]\n저작권 및 개인정보 보호 규제 측면에서, 일본은 AI 학습용 데이터 활용에 있어 일부 유연한 접근을 허용한 사례도 있으나, 여전히 데이터 접근과 보안 규제의 균형이 과제로 남아 있다. [출처]\n특허 기반 연구에서도 일본 기업들이 기술 복잡성 측면에서는 대응 가능하지만, 신생 플랫폼 기업의 민첩성과 확장성에서는 경쟁국에 비해 약점이 있다는 해석이 나온다. [출처]\n① 생성형 AI는 대규모 데이터와 학습용 데이터의 접근성이 핵심이다.\n일본이 거버넌스 체계, 인증 프레임워크, 기술 표준 면에서 유럽보다 상호 운용성 확보가 어렵다면, 다양한 출처의 데이터를 통합·활용하는 데 제약이 생긴다.\n결과적으로 AI 모델 학습 속도와 성능 향상에 제한이 발생할 수 있으며, 저작권·개인정보 보호 규제의 균형 문제도 데이터 확보와 활용에 직접적 영향을 준다.\n② AI 시스템 개발에는 소프트웨어 설계, 최적화, 확장성이 필수적이다.\n일본의 소프트웨어 산업이 플랫폼 경쟁력 확보에서 취약하다면, 혁신적 AI 서비스 개발이나 글로벌 경쟁에서 불리할 수 있다.\n특허 기반 연구에서는 기술 복잡성 대응은 가능하지만, 신생 플랫폼 기업의 민첩성 및 확장성 부족은 시장 적응과 혁신 속도에 제한을 줄 수 있다.\n따라서 한국도 국내 플랫폼이 강력한 우위를 가진다고 방심할 수 없다. 데이터 확보, 기술 표준화, 소프트웨어 역량 강화, 규제 환경의 유연화 등 다각적인 대비가 필요하며, 지속적인 혁신과 인프라 확충을 통해 경쟁 우위를 유지해야 한다.\n06 AI의 전망\n1 . AI 산업의 급성장 MS는 AI 인프라 공급업체인 Nebius와 최대 200억 달러 규모의 5년 계약을 체결하였다. 이는 AI 모델 학습과 추론을 위한 GPU 리소스 확보를 위한 전략의 일환이다. [출처]\nNVIDIA는 차세대 AI 칩인 ’Rubin CPX’를 2026년 말 출시할 예정이며, 이는 고해상도 비디오 생성 및 AI 기반 SW 개발에 최적화되어 있다. [출처]\n데이터 센터의 전력 수요가 2027년까지 50%, 2030년까지 165% 증가할 것으로 전망하고 있다. [출처]\n2 . 데이터와 GPU의 중요성 IDTechEx는 2025년 GPU 배치량이 기하급수적으로 증가할 것으로 예상하며, 이는 AI 모델 훈련과 추론에 필수적인 요소로 작용하고 있다. [출처]\n데이터 센터 GPU 시장이 2025년 1,199억 7,000만 달러에서 2030년까지 2,280억 4,000만 달러에 이를 것으로 예상하고 있다. [출처]\n3 . AI 관련 인력 수요 Databricks는 AI 제품에 대한 수요 증가로 연간 수익이 40억 달러에 이를 것으로 예상하며, 이는 AI 관련 인력의 수요 증가를 시사한다. [출처]\n4 . AI 산업의 지속 가능성 NVIDIA는 AI 시장이 연평균 성장률(CAGR) 26.6%로 성장할 것으로 전망하며, 이는 AI 산업의 지속 가능성을 뒷받침한다. [출처]\n현재 엔비디아(NVDA)의 주가는 170.76달러로, 전일 대비 2.58달러(1.53%) 상승하였다. 이는 AI 산업의 성장과 엔비디아의 시장 지배력을 반영하는 지표로 볼 수 있다."
  },
  {
    "objectID": "ai/ml_03.html",
    "href": "ai/ml_03.html",
    "title": "머신러닝: R2D3",
    "section": "",
    "text": "모델 학습과 R2D3 를 활용한 실무 적용 방법에 대해 다루고자 한다.\n01 신경망 Neural Network, NN\n현대 딥러닝 모델의 대부분은 신경망 구조에서 발전하였으며, 데이터의 특성(공간적·순차적 구조)과 문제 유형에 따라 다양한 형태로 변형되어 사용된다.\n1 . 인공 신경망 Artificial Neural Network\n인공 신경망은 딥러닝의 기본 구조로, 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 구성된다.\n각 뉴런은 입력값에 가중치(weight)와 편향(bias)을 적용한 뒤, 활성화 함수(activation function)를 통해 비선형 출력을 생성한다.\n딥러닝(DL)은 이러한 인공 신경망을 다층화(심층화)하여 복잡한 패턴과 비선형 관계를 학습할 수 있도록 확장한 구조이다.\n2 . 합성곱 신경망 Convolutional Neural Network, CNN\n이미지와 같이 격자형 구조를 가진 데이터의 공간적 패턴(spatial pattern)을 학습하는 데 특화되어 있다.\n합성곱(convolution) 연산을 통해 국소적인 특징(local feature)을 추출하며, 계층이 깊어질수록 점차 추상적인 특징을 학습한다.\n최근에는 Vision Transformer(ViT) 등과 같은 트랜스포머 기반 모델이 이미지 분석에도 적용되며, CNN과 함께 시각 분야의 핵심 구조로 활용되고 있다.\n3 . 순환 신경망 Recurrent Neural Network, RNN\n시간적 또는 순차적 데이터를 처리하기 위해 설계된 구조로, 이전 단계의 은닉 상태(hidden state)를 현재 입력과 함께 사용하여 시점 간 의존성(temporal dependency)을 학습한다.\n다만, 기본 RNN은 긴 시퀀스 처리 시 기울기 소실(vanishing gradient) 문제가 발생할 수 있다.\n이를 개선하기 위해 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit)와 같은 변형 구조가 널리 사용된다.\n4 . 트랜스포머 Transformer\n순환 구조 없이 시퀀스 데이터를 병렬적으로 처리할 수 있도록 고안된 모델로, 자기어텐션(Self-Attention) 메커니즘을 통해 입력 간 상관관계를 학습한다.\n이 구조는 RNN보다 장거리 의존성(Long-range dependency)을 효과적으로 포착하며, 자연어처리(NLP)를 넘어 이미지, 음성, 멀티모달 학습 등 다양한 분야로 확장되고 있다.\n요약하자면,\nCNN은 “공간적 패턴 추출”에, RNN은 “순차적 패턴 학습”에, Transformer는 “병렬적·전역적 패턴 학습”에 강점을 지닌다. 이들은 모두 “신경망”이라는 공통 기반 위에서 발전해 온 핵심적인 딥러닝 구조이다.\n02 사이킷런 이러한 신경망 기반 접근 외에도, 통계적 학습 이론을 바탕으로 한 전통적 머신러닝 기법이 존재한다.\n그 대표적인 라이브러리가 Scikit-learn이며, 파이썬 기반의 대표적인 머신러닝 라이브러리로 회귀, 분류, 클러스터링, 차원 축소 등 다양한 알고리즘을 일관된 인터페이스로 제공한다.\n이 라이브러리는 심층 신경망 구조인 CNN, RNN 등을 기본적으로 포함하지 않으며, 이러한 구조는 TensorFlow, Keras, PyTorch 와 같은 딥러닝 프레임워크에서 주로 구현된다.\n이 알고리즘은 본질적으로 데이터의 구역을 단순 분할이 아닌, 입력 변수(feature), 목표 변수(target) 간의 관계를 함수적 형태로 모델링하는 방식이다.\n일반적으로 데이터는 ” 행(sample) × 열(feature) ” 형태의 구조를 가지며, 각 행은 개별 관측치를, 각 열은 설명 변수를 나타낸다.\n모델은 이러한 특징들을 바탕으로 목표 변수를 예측하는 함수를 학습한다.\n예: 아파트 가격 예측 문제에서 면적, 교통, 교육 환경 등 다양한 요인을 특징으로 사용 시 데이터는 고차원 공간에 분포하게 된다.\n이때 데이터가 3차원에서 도넛 형태와 같이 복잡한 분포를 보일 수 있으며, 단순한 선형 경계로는 구분하기 어려운 경우가 많다.\n이러한 상황에서는 PCA, t-SNE, UMAP 과 같은 차원 축소 기법을 활용하여 고차원 데이터를 저차원 공간으로 투영함으로써 데이터의 패턴과 구조를 보다 명확하게 시각화할 수 있다.\n03 고도와 주택 가격 R2D3의 시각적 기계학습 입문 글에서는 샌프란시스코와 맨해튼의 주택 가격 데이터를 활용하여, 머신러닝 모델이 입력(feature)과 목표(target) 간 관계를 학습하는 과정을 시각적으로 설명한다.\nA visual introduction to machine learning\nWhat is machine learning? See how it works with our animated data visualization.\nwww.r2d3.us\n샌프란시스코는 언덕 지형이 많아 고도(elevation)가 주택 가격에 영향을 미치는 중요한 요인이 된다.\n반면 맨해튼은 대체로 평지 구조이므로 고도 요인의 영향이 미미하거나 다르게 나타난다.\n이처럼 고도는 미국 부동산 시장에서 elevation 이라는 변수로 다뤄지며, 조망(view), 프라이버시, 도시 스카이라인 등과 함께 가격 형성에 작용할 수 있다.\n한국의 경우, 언덕 위 아파트는 접근성 저하, 급경사 도로, 기반 시설 확충 비용 등의 이유로 가격이 낮게 평가되는 경향이 있다.\n반대로 바닷가 조망이 있는 주택은 오히려 낮은 가격을 받는 경우도 있다. 이는 고도 자체보다 조망성, 접근성, 지형 조건 등 복합 요인들이 작용한 결과라 볼 수 있다.\n이러한 차이는 머신러닝에서 하나의 입력 변수(feature)가 지역적·사회적 맥락에 따라 서로 다른 패턴을 학습하게 되는 과정을 잘 보여준다.\n입력 변수(feature)를 2D 평면에 점으로 시각화하고, 이 점들을 기준으로 분류 경계(decision boundary)를 학습하는 과정을 설명한다.\n이와 유사하게, 부동산 시장에서 고도(elevation)를 하나의 특징(feature)로 입력하면, 모델은 고도와 가격 패턴 사이의 관계를 학습하여 경계 또는 함수 형태로 변환할 수 있다.\n예를 들어, 한국에서는 고도 증가가 가격 하락으로 이어지는 함수적 경향이, 미국 일부 지역에서는 고도 증가가 가격 상승과 연결된 함수적 경향이 학습될 수 있다.\n즉, 고도(elevation)는 부동산 가격 모델링에서 하나의 중요한 입력 변수이며, 지역별 맥락과 다른 특징들과의 상호작용을 고려해야 한다.\n이러한 지역별 차이를 반영하여, 머신러닝 모델은 학습 데이터 기반으로 최적의 예측 함수를 찾아내게 된다.\n전체 데이터를 학습용(train), 검증용(validation), 테스트용(test) 으로 분할한다.\n예를 들어, 전체의 70 %를 학습용, 30 %를 테스트용으로 나눈 뒤, 학습용 중 일부(예: 10–20 %)를 검증용으로 재분할한다.\n학습 중에 매 epoch마다 학습 정확도와 검증 정확도를 기록하여 학습 상태를 모니터링한다. 검증 정확도가 점진적으로 상승하면 정상적인 학습 경향이다.\n하지만 검증 정확도가 자주 들쭉날쭉하거나, 학습 정확도만 계속 상승하고 검증 정확도는 정체되면 과적합 또는 학습 불안정성을 의심해야 한다. 이때 하이퍼파라미터 조정, 정규화(regularization) 적용, 또는 데이터 분할 재설계 등을 고려한다.\nR2D3 예제에 따르면,\n학습 정확도(training accuracy): 100% 검증 정확도(validation accuracy): 89.7% 구체적으로,\n뉴욕 주택: 112개 중 100개 정확 분류 샌프란시스코 주택: 130개 중 117개 정확 분류 이는 모델이 학습 데이터에 과적합되어 있으며, 현실적 적용에는 제한적임을 의미한다. 실무 적용 기준으로는 최소 95% 이상의 검증 정확도가 요구되므로, 현 시점에서의 모델 적용은 신중을 요한다.\n04 결정 트리 학습과 과적합 관리\n1 . 단일 변수 기반 분류의 한계 의사결정트리는 각 노드에서 조건문(if-then)을 통해 데이터를 두 그룹으로 분할하며, 경계를 설정하여 분류를 수행한다.\n그러나 단일 변수(예: elevation)만으로는 고도와 주택 가격 간의 비선형적 관계, 지역별 특성 등을 충분히 반영할 수 없어 데이터 간 명확한 구분이 어렵다.\nR2D3 시각화 예시에서도 나타나듯, 고도 하나만으로 주택 가격 범주를 정확히 분류하기 어려우므로, 면적, 교통 접근성, 조망 등 여러 변수(feature)를 고려한 다변량 분할이 필요하다.\n2 . 최적 분할과 분할 기준 트리는 각 노드에서 모든 후보 특성(feature)과 임계값(threshold)을 조합하여 가능한 분할(split candidates)을 탐색한다.\n각 후보 분할은 엔트로피(entropy), 정보 이득(information gain), 지니 불순도(Gini impurity) 등 불순도 지표를 통해 평가되며, 가장 데이터가 균질하게 나뉘는 최적 분할(best split)이 선택된다.\n이 과정을 반복하면, 학습 데이터가 점차 균질한 소그룹으로 세분화되어 목표 변수(target)에 대한 예측 성능이 향상된다.\n3 . 과적합의 발생과 영향 트리 깊이가 증가하면 모델이 학습 데이터에 과도하게 적합(overfit)될 수 있다.\n학습 데이터에서는 높은 정확도를 나타내지만, 테스트 데이터에서는 일반화 성능이 저하되는 과적합 현상이 발생한다.\nR2D3 예시에서는 단일 변수 분할을 반복하여 분류 경계를 지나치게 세분화함으로써 일부 데이터 포인트가 잘못 분류되는 사례가 확인된다.\n이러한 과적합은 모델이 학습 데이터의 불필요한 변동과 노이즈까지 학습하게 만들어, 실무에서의 모델 신뢰도를 떨어뜨리는 부작용을 초래한다.\n4 . 과적합 방지 전략 과적합을 방지하기 위해서는 여러 전략을 병행할 수 있다.\n먼저, 모델 제약을 통해 트리의 최대 깊이(max depth)나 최소 샘플 수(min samples split)와 같은 파라미터를 제한하면, 지나치게 세분화되는 것을 방지할 수 있다.\n또한, 가지치기(pruning)를 통해 불필요하게 세분화된 분할을 사후적으로 제거하며 모델 단순화 및 일반화 성능을 향상할 수 있다.\n마지막으로, 교차검증(cross-validation)을 활용하여 학습과 검증을 반복하면, 모델이 특정 데이터셋에만 과적합되는 것을 방지하고 안정적인 성능을 확보할 수 있다."
  },
  {
    "objectID": "ai/ml_06.html",
    "href": "ai/ml_06.html",
    "title": "Credit Approval",
    "section": "",
    "text": "의사결정트리 기반 신용평가 분석에 대해 다루고자 한다\n01 Credit Approval UCI의 Credit Approval 데이터셋의 일부 속성(attribute)들을 보고, 어떤 속성을 학습 가설(feature set)에 포함하는 것이 더 좋은지를 판단하고자 한다.\n참고자료\n\n기계 학습의 기초 2-1~2-5 &gt; Machine Learning in Korean (1) | 카이스트 응용인공지능 연구실\n\naai.kaist.ac.kr 본 장에서는 KAIST 인공지능연구원(AAI)에서 제공한 강의 자료 기계 학습의 기초 3강, 4강을 참고하였다.\nUCI Machine Learning Repository\nA1: b, a. A2: continuous. A3: continuous. A4: u, y, l, t. A5: g, p, gg. A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff. A7: v, h, bb, j, n, z, dd, ff, o. A8: continuous. A9: t, f. A10: t, f. A11: continuous. A12: t, f. A13: g, p, s. A14: continuous. A15:\narchive.ics.uci.edu # pip install ucimlrepo # ucimlrepo 파이썬 패키지를 통해 간편히 불러올 수도 있다.\nfrom ucimlrepo import fetch_ucirepo import pandas as pd\n\n1. 데이터 불러오기 (Credit Approval, ID = 27)\ncredit = fetch_ucirepo(id=27)\n\n\n2. 피처와 타깃 결합\ndf = pd.concat([credit.data.features, credit.data.targets], axis=1) df\n데이터셋 개요\n전체 인스턴스 수: 690개 샘플(instances): 690개 피처(features): 15개 타깃(target, 승인 여부): 1개 일부 피처들이 연속형(continuous), 일부 명목형(categorical).\n결측값(missing values)이 존재함 (UCI 리포지토리에 “Has Missing Values? Yes” 라고 명시됨). [출처]\n결측이 표기된 방식이 ‘?’ 문자열 (특히 categorical 또는 일부 continuous 피처)임. [출처]\n여기서 A1, A9 두 속성이 각각 긍정/부정 클래스를 얼마나 잘 분리할 수 있는지를 비교하고 있고, 어떤 속성이 더 좋은 속성인지 판별하고자 한다.\n\n\n각 컬럼의 고유값이 시리즈 형태로 반환\nunique_values = df[[‘A1’, ‘A9’, ‘A16’]].apply(lambda x: x.unique()) unique_values\nA1에 결측치 확인\nA1, A9 속성 기준 승인/거절 빈도 및 퍼센트 요약표 생성\nimport pandas as pd\n\n\nA1 컬럼의 NaN 값을 문자열 ’?’로 변환 (결측치 표시)\ndf[‘A1_filled’] = df[‘A1’].fillna(‘?’)\n\n\n전체 클래스 분포\nclass_dist = df[‘A16’].value_counts().rename_axis(‘A16’).reset_index(name=‘count’) total_positive = class_dist.loc[class_dist[‘A16’]==‘+’,‘count’].values[0] total_negative = class_dist.loc[class_dist[‘A16’]==‘-’,‘count’].values[0]\n\n\nA1 교차표 (결측치 포함)\ncross_A1 = pd.crosstab(df[‘A1_filled’], df[‘A16’]).reset_index() cross_A1.insert(0, ‘속성’, ‘A1’) cross_A1 = cross_A1.rename(columns={‘A1_filled’: ‘값’, ‘+’: ‘승인(+)’, ‘-’: ‘거절(-)’})\n\n\n퍼센트 컬럼 추가\ncross_A1[‘승인(%)’] = (cross_A1[‘승인(+)’] / total_positive * 100).round(1) cross_A1[‘거절(%)’] = (cross_A1[‘거절(-)’] / total_negative * 100).round(1)\n\n\nA9 교차표 (결측치 없음)\ncross_A9 = pd.crosstab(df[‘A9’], df[‘A16’]).reset_index() cross_A9.insert(0, ‘속성’, ‘A9’) cross_A9 = cross_A9.rename(columns={‘A9’: ‘값’, ‘+’: ‘승인(+)’, ‘-’: ‘거절(-)’}) cross_A9[‘승인(%)’] = (cross_A9[‘승인(+)’] / total_positive * 100).round(1) cross_A9[‘거절(%)’] = (cross_A9[‘거절(-)’] / total_negative * 100).round(1)\n\n\n전체 클래스 분포를 표 형태로 변환\noverall = pd.DataFrame({ ‘속성’: [‘전체’], ‘값’: [‘-’], ‘승인(+)’: [total_positive], ‘거절(-)’: [total_negative], ‘승인(%)’: [100.0], ‘거절(%)’: [100.0] })\n\n\n데이터 병합\ncombined_df = pd.concat([overall, cross_A1, cross_A9], ignore_index=True) combined_df\n1 . 속성 선택 기준 머신러닝 / 개념 학습 이론에서 좋은 속성(feature)을 선택하는 기준은 다음과 같다:\n정보 이득(IG) 혹은 엔트로피 감소 지니 계수 (Gini impurity) 감소 분류 순도 (Purity) 잡음 민감성 — 속성이 얼마나 레이블 노이즈에 영향을 덜 받는가 이 속성 선택 기준들은 결정 트리 알고리즘(C4.5, CART 등)에서 흔히 쓰인다.\n즉, 어떤 속성이 클래스 라벨(+)과 (–)를 더 잘 구분할 수 있을지, 엔트로피(불확실성)를 많이 줄일 수 있을지를 보는 것이다.\n2 . A1 vs A9 비교\n기계 학습의 기초 3강 pdf 자료 A9 속성의 Positive/Negative 비율을 보면, t일 경우 약 79%가 Positive, f일 경우 약 93%가 Negative로 나타나, 클래스 구분력이 매우 높음을 알 수 있다.\n반면 A1 속성은 a와 b 사이의 Positive 비율 차이가 약 3~4% 수준으로 미미하여, 클래스 구분 신호가 약하다.\n따라서 (A9)를 포함하는 학습 가설 모델은 더 높은 성능을 기대할 수 있다.\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n3 . 주의할 점 / 한계 제시된 것은 빈도수 기반 단순 비교일 뿐이고, 실제 속성의 복합 상관관계나 다른 속성과의 상호작용은 고려하지 않는다.\n결측치(?) 처리가 중요하다. 제시된 A1의 경우 “?” 값이 일부 존재하므로, 이를 어떻게 처리하는가가 결과에 영향을 준다.\n노이즈(잘못된 레이블, 입력 오류 등)가 존재할 수 있고, 속성 선택이 그 노이즈에 민감할 수 있다.\n속성 하나만으로 모든 분류가 가능하지 않으므로 여러 속성을 조합하는 가설이 필요하다.\n02 암맹처리 UCI의 Credit Approval 데이터셋에서 결측치(missing values, 암맹처리 또는 ‘?’ 표기됨) 처리를 하는 이유에 대해 설명하고자 한다.\n1 . 분류 모델/알고리즘의 단점 대부분의 ML 라이브러리(예: scikit-learn)의 분류/회귀 알고리즘은 입력 피처(feature)의 값이 모두 수치(numeric), 범주형(categorical) 변환이 완료된 상태여야 한다.\n‘?’ 같은 문자열은 수치 연산, 비교, 통계계산, 분기(split) 등이 불가능하거나 오류를 발생시키기 쉬우며, 평균(mean), 엔트로피 계산, 불순도 계산 시 오류가 발생한다.\n따라서 ’?’를 NaN 또는 동일한 결측치 포맷으로 바꾸고, 이를 채우거나 해당 샘플/속성을 제거해야 한다.\n2 . 모델의 성능 및 일반화 결측치를 처리하는 핵심 목적 중 하나가 편향(bias) 제거이다.\n결측치 미처리 시 모델이 학습 중 일부 피처를 무시하거나, 잘못된 피처 분할(split)로 과적합(overfitting) 또는 편향(bias)가 커질 수 있기 때문이다.\n또한 학습 데이터와 테스트 데이터 간의 일관성이 깨질 수 있다.\n학습에는 특정 방식으로 결측치 처리를 했는데, 실제 예측 시에는 결측치 형태가 다르면 예측이 잘 안 되는 경우\n3 . 통계적 분석의 정확성 확보 결측이 있는 상태로 단순히 평균/분포/상관관계(correlation) 계산 등을 하면, 해당 피처가 가진 정보가 왜곡될 수 있다.\n예: 연속형 피처에 결측치가 많으면 그 피처의 평균/표준편차 등의 계산이 왜곡됨 → 전체 엔트로피나 지니, 정보 이득(IG) 계산 등에 영향을 끼침.\n4 . 데이터의 규모와 손실 최소화 전체 데이터 중 결측이 있는 샘플이 많지 않다면(dropna), 이들을 제거하는 것이 하나의 방법임.\n이 데이터셋에서는 약 37개의 행(samples) — 전체의 약 5% 정도가 결측값을 포함함. [출처]\n하지만 결측치가 많은 피처가 있다면 단순 제거는 정보 손실이 큼. 이런 경우 imputation(대체값 채우기) 방법(평균, 최빈값, 예측 모델 등)이 사용된다.\n5 . 실제 적용 예시 여러 프로젝트/분석에서, 이 데이터셋의 결측치는 ’?’로 표시되어 있어 먼저 이를 NaN으로 변환함. [출처]\n연속형 피처에 대해서는 평균(mean) 또는 중앙값(median)으로 채움(imputation). 명목형(categorical) 피처의 경우 최빈값(mode)으로 채우는 사례가 많음. [출처]\n또는 결측이 있는 샘플을 통째로 제거(drop)하는 방법이 사용됨. 예: 37개의 결측 샘플을 제거한 후 분석 진행한 연구 있음. [출처]\n결과적으로, 암맹처리는 단순히 오류를 피하기 위한 것뿐만 아니라 모델의 공정성(fairness)과 일반화 성능을 높이는 중요한 단계이다.\n03 ID3 의사결정나무 분할 전략 ID3(Iterative Dichotomiser 3) 알고리즘은 의사결정나무 모델에서 데이터를 분할할 특성을 선택할 때 정보 이득(IG)을 기준으로 삼는다.\n정보 이득은 특정 속성으로 데이터를 분할했을 때 엔트로피가 얼마나 감소하는지를 측정하는 지표로, 엔트로피는 데이터의 불순도 또는 무질서를 나타낸다.\n즉, 엔트로피 감소가 클수록 해당 속성은 데이터를 더 잘 분리한다고 판단되어 우선적으로 선택된다. [출처].\n엔트로피 계산 예제\n1 . 전체 엔트로피 H(Y) (P(+) = 307/690 ,P(-) = 383/690 ) (H(Y) = - (0.445 _2 0.445 + 0.555 _2 0.555) )\n2 . A1로 분할 후 엔트로피 H(Y|A1) 각 그룹 엔트로피 계산 후 가중 평균:\n\\[H(Y|A1) = \\frac{210}{690} H(a) + \\frac{468}{690} H(b) + \\frac{12}{690} H(?)\\]\n각 그룹 엔트로피:\n(a: (H(a) = - (98/210 _2 (98/210) + 112/210 _2 (112/210)) ) ) (b: (H(b) = - (206/468 _2 (206/468) + 262/468 _2 (262/468)) ) ) (?: (H(?) = - (3/12 _2 (3/12) + 9/12 _2 (9/12)) ) ) 가중 평균:\n\\[H(Y|A1) \\approx \\frac{210}{690}\\cdot0.998 + \\frac{468}{690}\\cdot0.993 + \\frac{12}{690}\\cdot0.811 \\approx 0.994\\]\n3 . A9로 분할 후 엔트로피 H(Y|A9) ( t: (H(t) = - (284/361 _2(284/361) + 77/361 _2(77/361)) )) ( f: (H(f) = - (23/329 _2(23/329) + 306/329 _2(306/329)) ) ) 가중 평균:\n\\[H(Y|A9) \\approx \\frac{361}{690}\\cdot0.722 + \\frac{329}{690}\\cdot0.164 \\approx 0.455\\]\n4 . 정보이득 계산 \\[IG(Y, A1) = H(Y) - H(Y|A1) \\approx 0.99 - 0.994 = -0.004 \\quad (\\text{사실상 0})\\]\n\\[IG(Y, A9) = H(Y) - H(Y|A9) \\approx 0.99 - 0.455 = 0.535\\]\nA9가 정보이득이 훨씬 크므로, ID3에서는 A9를 루트 노드(최초 분할 기준)로 선택한다. A1은 거의 정보이득이 없어 의미 있는 분할이 되지 않는다.\n따라서, A9가 먼저 선택되어 분할 기준으로 사용되며, 이는 모델의 성능 향상에 기여할 수 있다.\n04 의사결정 트리의 한계 의사결정 트리(Decision Tree)는 구조가 직관적이고 해석이 용이하다는 장점이 있으나, 현실의 데이터가 갖는 노이즈와 불일치성에 매우 민감하다는 단점이 있다.\n데이터에 포함된 잡음이나 이상치가 많을 경우, 트리는 그 불완전한 패턴까지 학습하여 과적합(overfitting) 이 발생하거나 작은 데이터 변화에도 트리 구조가 크게 달라지는 불안정성(variance) 을 보인다.\n이러한 이유로, 단일 트리 모델은 실제 업무 환경에서의 예측 신뢰성이 낮을 수 있다.\n이러한 문제를 보완하기 위해 도입된 방법이 앙상블 기법이다.\n1 . 앙상블 기법 Ensemble\n여러 개의 학습기(base learners)를 조합하여, 단일 모델보다 더 안정적이고 일반화 성능이 높은 예측 결과를 도출하는 방법.\n이는 음악의 ’오케스트라’처럼 개별 모델의 예측을 조화롭게 결합하여 전체적인 조화(화음)를 이루는 방식으로 비유할 수 있다.\n앙상블 기법은 일반적으로 두 가지로 구분된다.\n2 . Bagging Bootstrap Aggregating\n데이터로부터 복원추출(bootstrap sampling)된 여러 하위 표본을 생성하여, 각 표본마다 독립된 트리를 학습시킨 후 결과를 평균(또는 투표)하는 방식이다.\n대표적인 알고리즘은 랜덤 포레스트(Random Forest) 로, 각 트리 학습 시 특성(feature)의 일부만 무작위로 선택하여 트리 간의 상관성 및 분산을 효과적으로 감소한다.\n이러한 무작위성과 집계(aggregation) 과정을 통해 단일 트리에 비해 일반화 능력과 예측 안정성이 향상된다.\n3 . Boosting 약한 학습기(weak learner)를 순차적으로 학습시키는 방식으로, 이전 모델이 잘못 예측한 데이터에 더 큰 가중치를 부여하여 점진적으로 오차를 보정한다.\n이 방식은 편향(bias) 을 줄이는 데 효과적이며, 대표적인 알고리즘으로 AdaBoost, Gradient Boosting, XGBoost 등이 있다.\n다만, 데이터에 노이즈가 많을 경우 과적합 위험이 존재하므로, 적절한 규제(regularization)와 학습률 조절이 필요하다.\n4 . 효과와 한계 일반적으로 단일 트리 모델보다 2 ~ 5% 내외의 성능 향상을 보이는 것으로 보고되지만, 향상 폭은 데이터의 특성과 모델 설계에 따라 달라질 수 있다.\n또한, 앙상블은 여러 모델을 결합하므로 해석력이 낮아지고 계산 비용이 증가하는 단점도 존재한다."
  },
  {
    "objectID": "ai/ml_08.html",
    "href": "ai/ml_08.html",
    "title": "의사결정트리 실습",
    "section": "",
    "text": "의사결정트리 실습에 대해 다루고자 한다.\n01 커널 서포트 벡터 머신 Kernel Support Vector Machine\n1 . 제목2 %matplotlib inline import numpy as np import matplotlib.pyplot as plt\n\n랜덤 시드 설정\nnp.random.seed(0)\n\n\nXOR 데이터 생성\nX_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0) y_xor = np.where(y_xor, 1, 0)\n\n\n클래스별 산점도\nplt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c=‘b’, marker=‘o’, label=‘class 1’, s=50) plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c=‘r’, marker=‘s’, label=‘class 0’, s=50)\n\n\n그래프 옵션\nplt.legend() plt.xlabel(“x1”) plt.ylabel(“x2”) plt.title(“XOR Problem”) plt.show()\n%matplotlib inline import numpy as np import matplotlib.pyplot as plt from sklearn.svm import SVC import matplotlib as mpl\n\n\nXOR 데이터 생성\nnp.random.seed(0) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0) y_xor = np.where(y_xor, 1, 0)\n\n\nXOR plot 함수 정의\ndef plot_xor_flipped(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid( np.arange(xmin, xmax, (xmax-xmin)/100), np.arange(ymin, ymax, (ymax-ymin)/100) ) ZZ = np.reshape( model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape ) # 색상 반전 ZZ = 1 - ZZ\nplt.contourf(XX, YY, ZZ, alpha=0.5, cmap=mpl.cm.Paired)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='class 1', s=50)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='class 0', s=50)\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.title(title)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\n\n\n결정 경계 시각화\nsvc = SVC(kernel=‘linear’) svc.fit(X_xor, y_xor) plot_xor_flipped(X_xor, y_xor, svc, “Classification Result by Linear SVC XOR”) plt.show()\nimport numpy as np from sklearn.preprocessing import FunctionTransformer\n\n\n기저 함수 정의\ndef basis(X): return np.vstack([X[:, 0]2, np.sqrt(2)X[:, 0]X[:, 1], X[:, 1]2]).T\n\n\n테스트용 배열 생성\nx = np.arange(8).reshape(4, 2) x\nFunctionTransformer(basis).fit_transform(x)\nimport matplotlib.pyplot as plt from sklearn.preprocessing import FunctionTransformer\n\n\n기저 함수 변환\nX_xor2 = FunctionTransformer(basis).fit_transform(X_xor)\n\n\n변환된 특징으로 산점도\nplt.scatter(X_xor2[y_xor == 1, 0], X_xor2[y_xor == 1, 1], c=“b”, s=50, label=“class 1”) plt.scatter(X_xor2[y_xor == 0, 0], X_xor2[y_xor == 0, 1], c=“r”, s=50, label=“class 0”) plt.legend() plt.show()\n02 ㄷ SVM with various Kernels\nimport matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import FunctionTransformer from sklearn.svm import SVC\n\n\n— 한글 폰트 설정 —\nplt.rc(‘font’, family=‘Malgun Gothic’) # Windows: ‘Malgun Gothic’, Mac: ‘AppleGothic’, Linux: ‘NanumGothic’ plt.rcParams[‘axes.unicode_minus’] = False # 마이너스 기호 깨짐 방지\ndef plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid( np.arange(xmin, xmax, (xmax-xmin)/100), np.arange(ymin, ymax, (ymax-ymin)/100) ) ZZ = np.reshape( model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape )\n# 색상 반전\nZZ = 1 - ZZ  \n\nplt.contourf(XX, YY, ZZ, alpha=0.5, cmap=plt.cm.Paired)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='class 1', s=50)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='class 0', s=50)\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.title(title)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\n\n\n적용\nplot_xor(X_xor, y_xor, basismodel, “기저함수 SVC 모형을 사용한 XOR 분류 결과”) plt.show()\n\n\n시그모이드 커널 SVC 학습\nsigmoidsvc = SVC(kernel=“sigmoid”, gamma=2, coef0=2) sigmoidsvc.fit(X_xor, y_xor)\n\n\nXOR 데이터 결정 경계 시각화\nplot_xor(X_xor, y_xor, sigmoidsvc, “Sigmoid SVC”) plt.show()\nrbfsvc = SVC(kernel=“rbf”).fit(X_xor, y_xor) plot_xor(X_xor, y_xor, rbfsvc, “RBF SVC”) plt.show()\nplot_xor(X_xor, y_xor, SVC(kernel=“rbf”, gamma=2). fit(X_xor, y_xor), “RBF SVC (gamma=2)”) plt.show()\nplot_xor(X_xor, y_xor, SVC(kernel=“rbf”, gamma=50). fit(X_xor, y_xor), “RBF SVC (gamma=50)”) plt.show()"
  },
  {
    "objectID": "ai/ml_11.html",
    "href": "ai/ml_11.html",
    "title": "로지스틱 분류",
    "section": "",
    "text": "[주제] 에 대해 다루고자 한다.\n01 최적화 알고리즘\nAlec Radford’s animations for optimization algorithms\nAlec Radford has created some great animations comparing optimization algorithms SGD , Momentum , NAG , Adagrad , Adadelta , RMSprop (unfo…\nwww.denizyuret.com 기본 SGD 외에 다음과 같은 변형을 자주 사용한다.\nSGD (확률적 경사하강) — 큰 데이터셋에 효율적 Momentum — 관성 도입으로 수렴 가속 및 진동 완화 AdaGrad — 좌표별 학습률 적응 Adam — 1차·2차 모멘트를 사용한 적응형 방법 (실무에서 널리 사용; 성능·안정성 우수)\n\n모델 검증과 일반화 7.1 K-Fold 교차검증 모델 성능 평가를 위해 데이터를 K개의 폴드(fold)로 나누고, 순차적으로 하나의 폴드를 검증셋으로, 나머지를 학습셋으로 사용한다.\n\n데이터를 K등분 (예: 5-fold → 5등분) 각 iteration마다 서로 다른 폴드를 검증셋으로 지정 K번 반복 후 평균 성능 산출 특수 사례: LOOCV(Leave-One-Out CV)\n데이터 포인트 하나를 검증셋으로, 나머지를 학습셋으로 사용 데이터가 적을 때 모델 평가를 정밀하게 할 수 있음 7.2 학습률(Learning Rate)과 데이터 전처리 학습률은 경사하강법의 파라미터 갱신 속도를 조절하며, 과도하면 발산, 너무 작으면 수렴 속도가 느려진다. 데이터 전처리는 모델 안정성과 성능 향상에 중요하며, 일반적으로 정규화(normalization), 스케일링(scaling), 결측치 처리 등을 포함한다. 7.3 오버피팅 방지 모델이 학습 데이터에만 과적합되는 것을 방지하기 위해 다양한 전략을 사용한다.\n데이터 증강(Data Augmentation): 학습 데이터 변형을 통해 다양성을 확보 특징 수 축소(Feature Selection/Reduction): 불필요한 입력 제거 정규화(Regularization): L1/L2 정규화, 드롭아웃(Dropout) 등 이 과정을 통해 모델은 학습 데이터뿐 아니라 미지의 데이터에서도 일반화 성능을 발휘할 수 있다.\n03 퍼셉트론과 다층 신경망 Perceptron &Multi-Layer Neural Network\n1 . 신경망의 구조와 활성화 함수 인공신경망(NN)은 생물학적 뉴런을 모사하여 입력 신호를 처리한다.\n입력 신호(x0,x1,…x_0, x_1, ​,x1​,…): 다른 뉴런(axon)으로부터 전달되는 신호 가중치(w0,w1,…w_0, w_1, ​,w1​,…): 각 신호의 중요도를 나타내며, 시냅스(synapse)와 유사 가중합(dendrite): 각 입력에 가중치를 곱한 후 합산 \\[z = \\sum_i w_i x_i + b\\]\n활성화 함수(Activation Function, f): 뉴런의 세포체(cell body)가 총합 입력을 해석하여 출력 신호 생성 \\[y = f(z) = f\\Big(\\sum_i w_i x_i + b\\Big)\\]\n출력 y는 다음 뉴런(axon)으로 전달되며, 이 신호가 0인지 1인지 또는 확률값인지 결정한다. 이 구조를 MLP이라고 하며, 다층 구성으로 비선형 문제를 해결할 수 있다.\n2 . 역사적 배경 Frank Rosenblatt (1957): 퍼셉트론(Perceptron) 제안 — 단층 신경망 기반의 이진 분류기 Widrow & Hoff (1960): ADALINE(Adaptive Linear Neuron) / MADALINE — 가중치 적응 학습 규칙 제안 Widrow-Hoff Rule (Delta Rule) 가중치 학습 과정은 다음과 같다.\n가중치 초기화Wi(0)을 임의값으로 설정W_i(0) Wi​(0)을 임의값으로 설정 입력 패턴과 목표 출력 제시 출력 계산 (Hard Limiter) \\[y(t) = f_h\\Big(\\sum_{i=0}^{n-1} w_i(t) X_i(t) - \\epsilon\\Big)\\]\n: 임계치 f_h​: 하드리미터 함수 가중치 갱신 \\[W_i(t+1) = W_i(t) + \\alpha \\,[\\,d(t) - y(t)\\,] X_i(t), \\quad 0 \\le i \\le n-1\\]\n: 학습률, 0 &lt; &lt; 1 d(t): 목표 출력값 실제 출력과 목표 출력이 일치하면 가중치는 변하지 않음 반복 수행 출력이 목표에 도달할 때까지 2~4단계 반복\n3 . 퍼셉트론을 통한 논리 연산 퍼셉트론은 기본적인 논리 게이트 연산을 구현할 수 있다.\nAND, OR 연산: 입력값의 가중합이 임계치(θ)를 초과하면 출력 1, 아니면 0 XOR 연산: 단층 퍼셉트론으로는 직선 결정경계로 구분 불가능 — 다층 퍼셉트론 필요 입력 X_0, X_1 ​AND OR XOR 0, 0 0 0 0 0, 1 0 1 1 1, 0 0 1 1 1, 1 1 1 0 AND: 위쪽 위치, 직선 결정 가능 OR: 아래쪽 위치, 직선 결정 가능 XOR: 직선 결정 불가, 곡선 또는 다층 구조 필요\n03 다층 퍼셉트론과 역전파 Backpropagation\n\n초기 비관과 연구의 침체기 1969년, Marvin Minsky 교수와 Seymour Papert는 저서 Perceptrons에서 다음과 같이 지적했다.\n\n“멀티 레이어 퍼셉트론(Multi-Layer Perceptron)으로 구성하면 문제를 해결할 수는 있지만, 실제로 구현하는 사람은 아무도 없을 것이다.”\n이로 인해 퍼셉트론 연구는 첫 번째 머신러닝 침체기(Artificial Intelligence Winter)를 맞이하게 되었다.\n단층 퍼셉트론으로 XOR와 같은 비선형 문제를 해결할 수 없다는 한계 다층 구조의 학습 방법 부재 2. 역전파(Backpropagation)의 등장 1974년과 1982년, Paul Werbos는 다층 신경망의 학습 문제를 해결할 방법을 제안했으며, 1986년, Geoffrey Hinton 등이 이를 체계화하였다.\n핵심 아이디어: 순방향(forward)으로 출력과 실제값의 차이(오차)를 계산 출력층에서 입력층 방향으로 오차를 역전파(backpropagation) 하여 각 가중치를 조정 이로써 단층 퍼셉트론으로 해결할 수 없었던 XOR 등 비선형 문제도 학습 가능하게 되었다.\n\n수학적 원리 다층 퍼셉트론에서 각 뉴런의 출력은 다음과 같이 정의된다.\n\nf=wx+bf = w x + bf=wx+b\n또는 단계적으로,\ng=wx,f=g+bg = w x, f = g + bg=wx,f=g+b\n체인룰(Chain Rule)과 편미분을 이용하여, 출력 오차를 각 층의 가중치에 대해 분배한다. 미분의 기본 정의: ddxf(x)=lim⁡Δx→0f(x+Δx)−f(x)Δx f(x) = _{x } dxd​f(x)=Δx→0lim​Δxf(x+Δx)−f(x)​\n예제: 2차원 입력 x1,x2x_1, x_2x1​,x2​와 가중치 w=[5,−7,−11]w = [5, -7, -11]w=[5,−7,−11]일 경우 출력 [y1,y2]=[1,0,0][y_1, y_2] = [1,0,0][y1​,y2​]=[1,0,0]을 얻을 수 있으며, 각 가중치에 대한 기울기를 계산해 오차를 감소시키는 방향으로 업데이트한다. 4. 의미와 의의 역전파 알고리즘은 신경망 학습의 근간이 되었으며, 다층 퍼셉트론 구조는 XOR와 같은 비선형 문제 해결 가능 현재의 딥러닝 모델(Convolutional Neural Network, Transformer 등)도 이 원리를 확장한 것 즉, 단층 퍼셉트론에서 시작한 연구가 다층 구조와 역전파를 통해 현대 신경망의 기초로 발전하게 된 역사적 과정이다.\n①②③④⑤⑥⑦⑧⑨ ⋅ ⌎\n₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ⁱ ⁿ / ¹ ² ³ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ₋ / α β δ θ ε π μ σ Ω φ ω\n− ± × ∑ ∴ ≥ ≤ ≒ ≓ ⋯ ⋮ / ⇨ ←→↑↓↔︎↕ / ℉ ℃\n[출처]"
  },
  {
    "objectID": "ai/ml_13.html",
    "href": "ai/ml_13.html",
    "title": "현대 AI 연구",
    "section": "",
    "text": "Reporting Date: November. 5, 2025\n\n엔비디아(NVIDIA)는 인공지능 연산을 위한 GPU 시장의 절대적 선도 기업으로, 최근에는 하드웨어 중심의 성능 향상을 넘어 AI 생태계 전체를 아우르는 플랫폼 전략으로 진화하고 있다. 대표적으로 ‘코스모스(Cosmos) Simulation Model’과 ‘옴니버스(Omniverse)’ 플랫폼은 3D 모델링, 물리 기반 시뮬레이션, 렌더링을 통합하여 현실과 유사한 디지털 트윈(Digital Twin) 환경을 구현하는 기술적 기반을 제공한다. 이러한 시뮬레이션 시스템은 실제 물리 세계의 상호작용을 정밀하게 재현함으로써, AI 에이전트(agent)의 학습, 검증, 최적화 실험을 위한 실증적 환경을 제공한다.\n2025년 기준으로 인공지능 연구의 핵심 개념은 에이전트(Agent)이다. 이는 인간의 인지적 판단과 행동을 모사하거나 대리 수행할 수 있는 자율적 지능 시스템을 의미한다.\n과거의 대화형 AI는 명시적으로 프로그래밍된 규칙에 의존했으나, 현재는 자연어 프롬프트(prompt)를 통해 고차원적 명령을 직접 해석하고 실행할 수 있다. 이러한 고도화된 시스템의 중심에는 대규모 언어모델(LLM, Large Language Model)이 있으며, 이는 언어적 지식뿐 아니라 시각·음성·문서 등 다양한 데이터 모달리티(modality)를 통합한 멀티모달 구조로 발전하고 있다.\nLLM의 기술적 진화는 ‘멀티모달 학습(Multimodal Learning)’ 역량을 중심으로 전개된다. 텍스트, 이미지, 오디오, 비디오 등의 이질적 데이터를 통합적으로 해석하고 상호 연관성을 학습하는 것이 핵심이며, 이는 각 데이터 형식별로 고유한 입출력 구조와 통신 프로토콜을 요구한다.\n현재의 LLM은 외부 데이터를 분석하고 가공할 수 있으나, 인간 수준의 자율적 시각 생성 능력에는 아직 도달하지 못한 상태다.\nAI의 실질적 응용의 종착점은 ’피지컬 AI(Physical AI)’이다. 이는 로봇, 제조, 물류, 스마트 팩토리 등 물리적 환경에서 실시간으로 인지·판단·행동을 수행할 수 있는 지능형 시스템을 의미한다. 미국은 첨단 반도체 설계 능력을 보유하고 있으나, 제조 기반의 약화로 인해 피지컬 AI의 산업적 적용에는 한계가 존재한다. 반면, 중국은 제조 기술과 산업 자동화 인프라 측면에서 강점을 보유하고 있으나, 미·중 기술 패권 경쟁이 양국 간 협력의 제약 요인으로 작용한다.\n이러한 상황 속에서 블랙록(BlackRock), 오픈AI(OpenAI)의 샘 알트먼(Sam Altman), 앤스로픽(Anthropic)의 다리오 아모데이(Dario Amodei), 엔비디아의 젠슨 황(Jensen Huang) 등 AI 생태계를 주도하는 주요 인물들은 한국을 차세대 전략적 거점으로 평가하고 있다. 이는 반도체 제조 분야에서 삼성전자와 SK하이닉스가 가진 기술적 우위와 글로벌 공급망 내 핵심적 위치 때문이다.\n한국의 반도체 산업은 엔비디아와 같은 AI 중심 기업의 연산 가속화를 위한 필수적 기반으로 작용하며, 양측의 기술적 상호의존 관계는 AI 반도체 생산 효율을 높이고, 피지컬 AI 산업화의 촉진을 가능하게 하는 협력 생태계를 형성하고 있다.\n\n인공지능의 이론적 기반은 크게 두 가지로 구분된다.\n첫째, 인과관계 기반의 지식 모델은 인간의 논리적 추론과 유사한 방식으로 동작하며, 둘째, 상관관계 기반의 데이터 모델은 통계적 학습을 통해 패턴을 식별한다.\n즉, 학습 기반 AI의 이해 연결주의(Connectionism) – 데이터에 대한 학습 능력을 이용하여 지능 구현\n후자의 대표적 구현이 머신러닝(ML)과 딥러닝(DL)이며, 인공신경망(ANN) 및 생성형 AI 모델이 이에 포함된다.\n\nANN Artificial Neural Network 인간의 신경세포(뉴런) 구조를 모사한 계산 모델이다.\n뉴런은 수상돌기(다중 입력), 세포핵(통합 중심), 축삭돌기(단일 출력)로 구성되며, 시냅스(synapse)를 통해 연결된다. 인간의 뇌는 약 1000억 개의 뉴런과 100조 개 이상의 시냅스로 이루어져 있으며, 각 뉴런은 평균 1000개 이상의 시냅스를 형성한다.\n입력 신호는 수상돌기를 통해 수집되어 세포핵에서 통합·처리되며, 각 입력의 영향력은 가중치(weight)로 조정된다.\n신호가 일정 임계치를 초과하면 활성화 함수(activation function)에 의해 출력으로 변환된다.\n이때 주로 사용되는 함수는 역치 함수(threshold function)와 시그모이드(sigmoid) 함수이며, 이는 입력 자극의 강도와 출력 반응의 비선형 관계를 수학적으로 모델링한다.\n출력 신호는 축삭돌기를 따라 전도되며, 말단부에서는 아세틸콜린 등의 신경전달물질을 통해 다른 뉴런으로 전파된다. 학습 과정에서 형성되는 시냅스의 가중치 변화가 곧 기억과 학습의 수학적 표현이다.\n시각 정보를 예로 들면, 인간의 눈은 약 700만 개의 원추세포와 1억 2000만 개의 막대세포로 구성되어 색상과 명암을 인식한다. 인공신경망에서는 이러한 시각 입력을 디지털 이미지의 픽셀로 표현하며, 예를 들어 28×28 흑백 필기체 데이터는 784차원 벡터로 변환되어 입력층에 주어진다.\n다층 퍼셉트론(MLP, Multi-Layer Perceptron)은 이 벡터 데이터를 여러 은닉층(hidden layer)을 통해 비선형적으로 변환하며, 각 층의 노드 수와 활성화 방식에 따라 인식 정확도가 달라진다.\n출력층(output layer)에는 가중치 대신 확률 분포를 계산하는 소프트맥스(Softmax) 함수가 사용된다. 이를 통해 예측 결과를 확률적으로 해석할 수 있으며, TOP-1 또는 TOP-3 정확도를 기준으로 분류 성능을 평가한다.\n하지만 모델의 복잡도가 과도하면 과적합(overfitting)이 발생할 수 있으며, 이는 학습 데이터의 특성에 과도하게 종속된 결과를 초래한다. 따라서 모델 구조의 단순화나 규제화(regularization) 기법을 통해 이를 완화한다.\n음성 데이터는 또 다른 모달리티를 구성한다. 파형, 주파수, 진동의 패턴을 학습하여 화자 인식, 음성 인식, 다자 음성 분리 등으로 응용된다. 철도나 엘리베이터의 작동음 분석을 통해 고장 여부를 조기 진단하는 산업적 활용도 이에 해당한다.\n결국 인식(recognition)은 곧 분류(classification)이다.\n신경망이 입력을 해석하고, 그 결과를 특정 클래스에 매핑하는 과정은 본질적으로 확률적 분류 문제이다. 이러한 학습의 핵심은 오차(error)를 줄이는 것이며, 과거에는 복잡한 가중치 구조로 인해 학습이 어려웠으나, 역전파(backpropagation) 알고리즘의 도입으로 이를 해결하였다.\n이 혁신으로 인해 인공신경망은 1980년대 후반의 혹한기를 극복하고, 현대 딥러닝의 토대를 구축하게 되었다."
  },
  {
    "objectID": "ai/ml_15.html",
    "href": "ai/ml_15.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: November. 19, 2025\n머신러닝에서 최적화(optimization)는 곧 고차원 매개변수 공간(parameter space)에서 최소점을 탐색하는 문제이며, 이 관점에서 학습 알고리즘은 본질적으로 탐색 알고리즘이다.\n인공신경망의 맥락에서는 이를 학습 알고리즘이라 부르며, 가장 대표적인 접근이 확률적 경사하강법(Stochastic Gradient Descent, SGD)이다.\nSGD는 손실 함수(loss function)의 기울기를 소량의 미니배치(mini-batch)로 근사하여 계산하기 때문에 계산 비용이 낮고 대규모 데이터셋에서도 효율적이다. 그러나 이러한 근사적 기울기는 통계적 분산이 크기 때문에 갱신 경로가 불안정하게 진동한다.\n이 문제를 완화하기 위해 도입된 개념이 모멘텀(momentum)이다. 이는 물리학적 관점에서 관성(inertia)과 마찰력(friction)을 도입한 것으로, 갱신 벡터를 단순히 현재 기울기만으로 결정하지 않고 이전 갱신의 방향성을 누적해 속도 벡터를 형성한다.\n결과적으로 최적화 경로는 비평면적(local curvature)의 영향을 덜 받고 안정적으로 수렴하며, 협곡형 지형(ravine)에서 진동을 줄이는 데 효과적이다."
  },
  {
    "objectID": "ai/ml_15.html#적응형-학습률",
    "href": "ai/ml_15.html#적응형-학습률",
    "title": "인공신경망",
    "section": "적응형 학습률",
    "text": "적응형 학습률\nadaptive learning rate AdaGrad는 학습 과정에서 각 파라미터의 변화량에 따라 학습률을 자동 조정한다. 기울기의 제곱 누적합을 기반으로 학습률을 축소하기 때문에 자주 변하는 파라미터는 더 빠르게 학습률이 감소하고, 드물게 변하는 파라미터는 학습률이 상대적으로 유지된다. 이는 데이터의 기하학적 구조에 따라 비등방적(anisotropic) 최적화를 수행하는 효과가 있다.\nAdam(Adaptive Moment Estimation)은 모멘텀과 AdaGrad의 장점을 결합한 비선형 최적화 알고리즘으로, 1차 및 2차 모멘트(기울기의 평균과 분산)를 동시에 추정한다.\nAdaGrad처럼 학습률이 급격히 감소하여 학습이 조기 정지되는 문제를 완화하고, 모멘텀 기반 탐색보다 진동이 적으며 빠른 초기 수렴 속도를 갖는다. 실험적으로 Adam은 다양한 비정상적(non-stationary) 목적 함수에서 강건하며, 딥러닝 모델 전반에서 사실상 표준으로 사용된다.\n최적화와 독립적으로, 모델의 일반화 성능을 향상하기 위해 정규화 기법이 도입되며 대표적인 것이 L2 정규화(weight decay)이다. 이는 가중치를 작게 유지함으로써 오버피팅을 방지하고, 매개변수 공간에서 불필요한 자유도를 억제함으로써 더 안정적인 표현 학습을 유도한다. 이는 통계학적 관점에서는 리지 회귀(ridge regression)의 페널티 항과 동일한 역할을 한다."
  },
  {
    "objectID": "ai/ml_15.html#가중치-초기화",
    "href": "ai/ml_15.html#가중치-초기화",
    "title": "인공신경망",
    "section": "가중치 초기화",
    "text": "가중치 초기화\nweight initialization 학습 안정성을 결정하는 중요한 요소.\nXavier 초기화는 시그모이드 함수와 같은 대칭적 활성화 함수에서, 전방/역방 전파 시 분산이 일정하게 유지되도록 설계된 방식으로 표준편차를 (1/)에 비례하도록 설정한다.\nReLU 계열 함수에서는 활성 뉴런 비율이 달라지기 때문에 분산 유지 조건이 다르게 정의되며, 이에 기반한 He 초기화는 표준편차를 ()로 확장하여 표현력을 높이고 초기 활성화의 비선형 왜곡을 방지한다.\n초기화가 중요한 이유는 깊은 신경망에서 발생하는 기울기 소실(vanishing gradient) 때문이다. 이는 활성화 함수의 포화 구간(saturation region)에서 기울기가 거의 0에 가까워지는 현상으로, 특히 시그모이드 함수는 입력이 조금만 크거나 작아도 기울기가 소멸한다. 이로 인해 초기 레이어는 거의 학습되지 않으며, 네트워크 전체가 비효율적으로 수렴한다. ReLU 함수는 양의 영역에서 기울기가 일정하게 유지되기 때문에 기울기 소실 문제를 근본적으로 완화하고, 깊은 모델의 학습을 가능하게 만든 핵심 요인이다.\n딥러닝 분야에서 비교 실험은 필수적이며, 다양한 최적화 알고리즘과 초기화 전략을 CIFAR-10과 같은 벤치마크 데이터셋에서 체계적으로 검증하는 과정은 모델 성능 평가의 표준 절차다. 이러한 데이터셋은 역사적으로 많은 연구자(예: 마빈 민스키와 관련된 초기 신경망 논쟁 이후의 연구 흐름)에 의해 발전해 왔으며, 현대적 딥러닝 모델의 성능 비교와 구조적 혁신을 검증하는 중요한 실험 환경을 제공한다."
  },
  {
    "objectID": "da/dap/dap_03.html",
    "href": "da/dap/dap_03.html",
    "title": "주성분 분석(PCA)",
    "section": "",
    "text": "Principal Component Analysis, PCA 상관관계가 존재할 수 있는 (p)개의 관찰 변수를 선형 변환하여 서로 상관관계가 없는 새로운 인공 변수(주성분)를 생성하는 통계적 기법이다.\n고차원 데이터에서는 변수 간 상관 관계가 분석과 해석을 복잡하게 만들 수 있으므로, PCA를 통해 데이터 구조를 단순화하고, 차원 축소를 수행함으로써 데이터 시각화, 노이즈 제거, 변수 간 다중공선성 문제 해결이 가능하다."
  },
  {
    "objectID": "da/dap/dap_03.html#주성분-생성",
    "href": "da/dap/dap_03.html#주성분-생성",
    "title": "주성분 분석(PCA)",
    "section": "1. 주성분 생성",
    "text": "1. 주성분 생성\n주성분 분석의 목적은 다음과 같이 정의할 수 있다:\n\n상관관계가 존재할 수 있는 \\(p\\) 개의 원본 관찰 변수 \\(X = [x_1, x_2, \\dots, x_p]\\) 를 선형 변환(linear transformation)하여, 서로 상관관계가 없는 새로운 변수 집합 \\(Z = [z_1, z_2, \\dots, z_p]\\) 를 생성한다.\n변환된 변수 \\(Z\\) 는 서로 직교(orthogonal)하므로 상관성이 제거되며, 이를 통해 차원 축소와 데이터 구조 파악을 보다 효과적으로 수행할 수 있다.\n각 주성분 \\(z_k\\) 는 원자료의 분산을 최대한 보존하도록 선택되며, 주성분의 순서는 설명하는 분산의 크기에 따라 결정된다."
  },
  {
    "objectID": "da/dap/dap_03.html#표준화",
    "href": "da/dap/dap_03.html#표준화",
    "title": "주성분 분석(PCA)",
    "section": "2 표준화",
    "text": "2 표준화\nStandardization PCA 수행 시 변수의 단위와 스케일이 다를 경우, 표준화를 먼저 수행해야 한다. 표준화 방법:\n\\[\nx'_i = \\frac{x_i - \\bar{x}}{s_x}, \\quad i = 1,2,\\dots,n\n\\]\n\n\\(x_i\\) : 원 변수 값\n\\(\\bar{x}\\) : 변수의 평균\n\\(s_x\\) : 변수의 표준편차\n\n표준화 후 변수는 평균 0, 분산 1을 갖게 되어 PCA에서 각 변수의 영향력이 균등하게 반영된다.\n데이터 행렬은 다음과 같이 표현한다.\n\n원본 데이터 행렬: \\(X \\in \\mathbb{R}^{p \\times n}\\)\n\n\\(p\\): 관찰 변수의 수\n\\(n\\): 관측치(샘플)의 수"
  },
  {
    "objectID": "da/dap/dap_03.html#선형-변환",
    "href": "da/dap/dap_03.html#선형-변환",
    "title": "주성분 분석(PCA)",
    "section": "3. 선형 변환",
    "text": "3. 선형 변환\n주성분 \\(z_k\\) 는 원본 변수의 선형 결합으로 정의되며 다음과 같이 표현된다.\n\\[\nz_k = a_{1k} x_1 + a_{2k} x_2 + \\dots + a_{pk} x_p\n= \\mathbf{a}_k^T \\mathbf{x}, \\quad k = 1, 2, \\dots, p\n\\]\n\n\\(\\mathbf{x} \\in \\mathbb{R}^p\\): 원본 데이터 벡터\n\\(\\mathbf{a}_k \\in \\mathbb{R}^p\\): k번째 주성분의 로딩(loading) 벡터\n\\(\\mathbf{a}_k^T \\mathbf{a}_j = 0 ( k \\neq j )\\): 주성분 간 직교성\n\n\n예시: 첫 번째 관측치 (n_1)의 첫 번째 주성분 좌표 (z_{1,1} = x_1^T v_1 = -0.2152) → (n_1)이 첫 번째 주성분 축 (v_1) 상에서 어디에 위치하는지를 나타내는 스칼라 값이다."
  },
  {
    "objectID": "da/dap/dap_03.html#분산-최대화",
    "href": "da/dap/dap_03.html#분산-최대화",
    "title": "주성분 분석(PCA)",
    "section": "4. 분산 최대화",
    "text": "4. 분산 최대화\n첫 번째 주성분 \\(z_1\\): 전체 분산을 최대한 설명하는 방향이 되도록 선택. 두 번째 주성분 \\(z_2\\): \\(z_1\\) 과 직교한다는 제약 아래 남아 있는 분산을 최대한 설명하는 방향으로 정의됨. 이러한 방식으로 각 주성분은 계층적으로 결정된다.\n따라서 k번째 주성분의 로딩 벡터 \\(\\mathbf{a}_k\\) 는 다음 최적화 문제를 통해 구해진다.\n일반식 (k번째 주성분)\n\\[\n\\mathbf{a}*k\n= \\arg\\max*{\\mathbf{a}}\n\\mathrm{Var}(\\mathbf{a}^T \\mathbf{x}),\n\\quad\n\\text{subject to }\n\\mathbf{a}^T \\mathbf{a} = 1,;\n\\mathbf{a}^T \\mathbf{a}_j = 0,; j &lt; k\n\\]\n특수식 (첫 번째 주성분) 첫 번째 주성분은 이전 주성분이 없으므로 직교 조건이 필요 없습니다.\n\\[\n\\mathbf{a}_1\n= \\arg\\max_{\\mathbf{a}}\n\\mathrm{Var}(\\mathbf{a}^T \\mathbf{x}),\n\\quad\n\\text{subject to }\n\\mathbf{a}^T \\mathbf{a} = 1\n\\]\n\n\\(|\\mathbf{a}|^2 = \\mathbf{a}^T \\mathbf{a} = 1\\): Lagrange 승수법 적용을 위한 제약 조건"
  },
  {
    "objectID": "da/dap/dap_03.html#공분산-행렬과-고유값-분해",
    "href": "da/dap/dap_03.html#공분산-행렬과-고유값-분해",
    "title": "주성분 분석(PCA)",
    "section": "5. 공분산 행렬과 고유값 분해",
    "text": "5. 공분산 행렬과 고유값 분해\n\nLagrange 승수법 적용 → 고유값 문제 도출\n\n첫 번째 주성분에 대해 Lagrange 함수를 구성한다.\n\\[\nL(\\mathbf{a},\\lambda)\n= \\mathbf{a}^T \\Sigma \\mathbf{a} - \\lambda (\\mathbf{a}^T\\mathbf{a}-1)\n\\]\n편미분하여 최적 조건을 구하면, 다음 고유값 문제가 얻어진다.\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{a}} = 2\\Sigma\\mathbf{a} - 2\\lambda \\mathbf{a} = 0\n\\quad \\Rightarrow \\quad \\Sigma \\mathbf{a} = \\lambda \\mathbf{a}\n\\]\n즉, 주성분 로딩 벡터는 공분산 행렬의 고유벡터이다. * 추가: 여기서 \\(|\\mathbf{a}|^2 = 1\\) 조건은 단위벡터(normalized vector)로 만들어 주성분 크기 비교가 가능하게 하는 역할을 한다.\n\n공분산 행렬 계산\n\n데이터 행렬 \\(X \\in \\mathbb{R}^{p \\times n}\\) 에 대해 공분산 행렬은 다음으로 정의된다. \\(\\Sigma\\)는 실대칭 행렬이므로 고유값 분해가 가능하다.\n\\[\n\\Sigma = \\frac{1}{n-1} XX^T\n\\]\n\n추가\n\n\\(X \\in \\mathbb{R}^{p \\times n}\\) 가 아닌 \\(n \\times p\\) 형태라면 \\(\\Sigma = \\frac{1}{n-1} X^T X\\) 로 계산해야 함. → 구현 시 데이터 행렬 차원 주의\n\\(\\Sigma\\) 는 대칭 행렬(Symmetric matrix) → 고유벡터는 서로 직교(orthogonal)하며 정규화 가능\n\n\n\n고유값·고유벡터 해석 → 주성분의 분산\n\n고유값 분해에서 얻어진 값:\n\\[\n\\Sigma \\mathbf{a}_k = \\lambda_k \\mathbf{a}_k\n\\]\n\n\\(\\mathbf{a}_k\\): k번째 주성분 방향(loading vector)\n\\(\\lambda_k\\): 해당 주성분이 설명하는 분산\n\n주성분 순서: 고유값 크기 순서대로 정렬 → \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p\\)\n다음과 같이 분산과 연결된다: \\[\n\\mathrm{Var}(z_k) = \\lambda_k\n\\]"
  },
  {
    "objectID": "da/dap/dap_03.html#차원-축소",
    "href": "da/dap/dap_03.html#차원-축소",
    "title": "주성분 분석(PCA)",
    "section": "6. 차원 축소",
    "text": "6. 차원 축소\nDimensionality Reduction PCA는 고차원 데이터에서 분산을 많이 설명하는 주성분만 선택하여 차원을 축소하는 기법이다. 원래의 \\(p\\) 차원 데이터 중 정보를 가장 많이 보존하는 상위 \\(m\\) 개의 주성분( \\(m &lt; p\\) )을 선택한다.\n\n설명되는 분산 비율 EVR, Explained Variance Ratio 공분산 행렬의 고유값 \\(\\lambda_k\\) 는 k번째 주성분이 설명하는 분산을 의미한다. 따라서 EVR은 다음과 같이 정의된다.\n\n\\[\n\\text{EVR}_k\n= \\frac{\\lambda_k}{\\sum_{i=1}^p \\lambda_i},\n\\quad k = 1,2,\\dots,p\n\\]\n\n누적 설명 분산 비율 Cumulative EVR 차원 축소의 선택 기준은 누적 EVR이다.\n\n\\[\n\\text{Cumulative EVR}_m\n= \\sum*{k=1}^m \\text{EVR}_k\n\\]\n일반적으로 전체 분산의 80~90%를 설명하는 m개의 주성분을 선택하면 정보 손실 없이 효과적인 차원 축소가 가능하다.\n\n새로운 좌표계로의 사영 projection 선택된 상위 \\(m\\) 개의 주성분 벡터\n\n\\[\nA_m = [\\mathbf{a}_1, \\dots, \\mathbf{a}_m] \\in \\mathbb{R}^{p \\times m}\n\\]\n사용하여 원본 데이터 \\(X \\in \\mathbb{R}^{p \\times n}\\) 를 새로운 좌표계(주성분 축)로 투사(projection)한다. 각 관측치 \\(x_i \\in \\mathbb{R}^p\\) (i번째 열) 는 다음과 같이 \\(m\\) 차원 좌표 \\(z_i \\in \\mathbb{R}^m\\) 로 변환된다:\n\\[\nz_i = A_m^T x_i\n\\]\n\n\\(m\\) : 선택된 주성분 수 (차원 축소 후)\n\\(z_i\\) : 관측치 (i)의 주성분 좌표\n예: 5개의 주성분을 선택하면 각 관측치는 5차원 좌표를 가지게 됨\n\n\\[\nZ_m = A_m^T X \\in \\mathbb{R}^{m \\times n}\n\\]\n\n\\(A_m = [\\mathbf{a}_1, \\dots, \\mathbf{a}_m]\\): 상위 m개의 주성분 로딩 벡터\n\\(Z_m\\): 축소된 차원에서의 데이터 표현\n\n각 주성분은 원본 데이터의 분산을 최대한 유지하며 서로 직교하기 때문에, \\(Z_m\\) 은 상호 독립적인 새로운 좌표계에서의 데이터가 된다.\n\n해석\n\n\n각 주성분은 원본 변수들의 선형 결합으로 구성되므로, 가중치(로딩) 벡터 \\(\\mathbf{a}_k\\) 를 확인하면 해당 주성분이 어떤 변수에 의해 형성되었는지 해석 가능하다.\n예: \\(\\mathbf{a}_1 = [0.5, 0.5, -0.3, \\dots]\\) → 첫 번째 주성분은 첫 두 변수의 기여도가 높음을 의미\n필요 시, 시각화를 통해 각 주성분과 원자료 변수의 관계를 분석 가능.\n(z_{i,k})는 관측치 (i)를 주성분 (k) 축에 사영한 값\n시각화 시, 좌표 값의 크기는 축 방향으로 관측치가 얼마나 멀리 위치하는지를 나타냄\n고유벡터 성분의 부호와 크기는 원 변수들이 주성분에 미치는 영향도를 나타냄\n부호는 방향성을 의미하며, 절대 크기를 통해 상대적 기여도를 판단 가능\n각 관측치의 (z_{i,k}) 값은 주성분 축 상의 위치를 나타냄\n2차원 또는 3차원 시각화 시, 좌표 값은 관측치가 새로운 축 상에서 가지는 위치와 변동성을 직관적으로 보여줌\n예: (z_{1,1})을 x축으로, (z_{1,2})를 y축으로 하여 관측치를 점으로 표현하면, 주성분 상에서의 데이터 분포를 시각화 가능\n\n\n실무 활용\n\n\nPCA는 고차원 데이터의 시각화, 노이즈 제거, 변수 간 다중공선성 해결, 머신러닝 전처리에 활용됨\n실무 적용 시 유의사항:\n\n원자료 변수 단위가 서로 다른 경우 표준화(Standardization) 필요\nPCA는 비지도 학습이므로 목적 변수(y)를 고려하지 않음\n차원 축소 후 선택된 주성분이 실제 업무 의미와 일치하는지 확인 필요\n주성분 해석 시, 원본 변수와의 관계를 반드시 확인하여 의미 있는 인사이트를 확보\n\n\n\n전처리 차원 축소 vs PCA 차원 축소\n\n\n\n\n\n\n\n\n\n구분\n전처리 차원 축소\nPCA 차원 축소\n\n\n\n\n목적\n불필요 변수 제거, 단위 조정 등\n분산 최대 보존, 상관관계 제거, 통계적 차원 축소\n\n\n방식\n경험적/규칙적\n선형 변환 기반, 수학적 최적화\n\n\n결과\n원 변수 일부 제거\n주성분으로 변환된 새로운 좌표\n\n\n활용\n데이터 정제, 단순화\n시각화, 피처 선택, 노이즈 제거, 모델 전처리"
  },
  {
    "objectID": "da/dap/dap_03.html#데이터가-적으므로-데이터-증강을-통해-정규분포를-따르는-노이즈-생성한다.-비율-서로-알맞게-설정.-강건성을-보기-위해서.-여러개-중에-비교하여-월드의-효용성-입증.-2로도-한-번-해보기-어떤-패턴이-나타나는지-가우시안-메소드-베이지안-메소드.약간-효능감이-떨어진다-가중치중심점가-0.5이상인-것만-표시함-k-mean-가우시안-베이지안의-혼동행렬을-출력.-해석-방식이-조금-달라짐.-결과적으로-k-mean-가우시안이-보다-효능감-있게-분리한다.-여기서-차이가-발생하는-데-이-이유는-각각-중심기준으로-원형태-타원-형태로-그-기준이-달라서-차이가-나는-것임.-그러나-베이지안이-너무-디테일하게-한다고-특성을-무시하면서-억지로-그룹을-맞추려고-하는-것은-그러면-안된다.",
    "href": "da/dap/dap_03.html#데이터가-적으므로-데이터-증강을-통해-정규분포를-따르는-노이즈-생성한다.-비율-서로-알맞게-설정.-강건성을-보기-위해서.-여러개-중에-비교하여-월드의-효용성-입증.-2로도-한-번-해보기-어떤-패턴이-나타나는지-가우시안-메소드-베이지안-메소드.약간-효능감이-떨어진다-가중치중심점가-0.5이상인-것만-표시함-k-mean-가우시안-베이지안의-혼동행렬을-출력.-해석-방식이-조금-달라짐.-결과적으로-k-mean-가우시안이-보다-효능감-있게-분리한다.-여기서-차이가-발생하는-데-이-이유는-각각-중심기준으로-원형태-타원-형태로-그-기준이-달라서-차이가-나는-것임.-그러나-베이지안이-너무-디테일하게-한다고-특성을-무시하면서-억지로-그룹을-맞추려고-하는-것은-그러면-안된다.",
    "title": "주성분 분석(PCA)",
    "section": "데이터가 적으므로 데이터 증강을 통해 정규분포를 따르는 노이즈 생성한다. 비율 서로 알맞게 설정. 강건성을 보기 위해서. 여러개 중에 비교하여 월드의 효용성 입증. 2로도 한 번 해보기 어떤 패턴이 나타나는지?? 가우시안 메소드, 베이지안 메소드.(약간 효능감이 떨어진다) 가중치(중심점)가 0.5이상인 것만 표시함 k-mean, 가우시안, 베이지안의 혼동행렬을 출력. 해석 방식이 조금 달라짐. 결과적으로 k-mean, 가우시안이 보다 효능감 있게 분리한다. 여기서 차이가 발생하는 데 이 이유는 각각 중심기준으로 원형태, 타원 형태로 그 기준이 달라서 차이가 나는 것임. 그러나 베이지안이 너무 디테일하게 한다고 특성을 무시하면서 억지로 그룹을 맞추려고 하는 것은 그러면 안된다.",
    "text": "데이터가 적으므로 데이터 증강을 통해 정규분포를 따르는 노이즈 생성한다. 비율 서로 알맞게 설정. 강건성을 보기 위해서. 여러개 중에 비교하여 월드의 효용성 입증. 2로도 한 번 해보기 어떤 패턴이 나타나는지?? 가우시안 메소드, 베이지안 메소드.(약간 효능감이 떨어진다) 가중치(중심점)가 0.5이상인 것만 표시함 k-mean, 가우시안, 베이지안의 혼동행렬을 출력. 해석 방식이 조금 달라짐. 결과적으로 k-mean, 가우시안이 보다 효능감 있게 분리한다. 여기서 차이가 발생하는 데 이 이유는 각각 중심기준으로 원형태, 타원 형태로 그 기준이 달라서 차이가 나는 것임. 그러나 베이지안이 너무 디테일하게 한다고 특성을 무시하면서 억지로 그룹을 맞추려고 하는 것은 그러면 안된다."
  },
  {
    "objectID": "da/dap/dap_03.html#시나리오-할수-있다-없다를-떠나서.-ceo관점에서-뭐가-궁금한가-가설을-여러개-세우고-묶는다.-그리고-단계별로-가지치기하면서-한다.-뭐가-빠져있지-뭘-붙이지-어떤-식을-세워야-하는가",
    "href": "da/dap/dap_03.html#시나리오-할수-있다-없다를-떠나서.-ceo관점에서-뭐가-궁금한가-가설을-여러개-세우고-묶는다.-그리고-단계별로-가지치기하면서-한다.-뭐가-빠져있지-뭘-붙이지-어떤-식을-세워야-하는가",
    "title": "주성분 분석(PCA)",
    "section": "시나리오, 할수 있다 없다를 떠나서. CEO관점에서 뭐가 궁금한가? 가설을 여러개 세우고 묶는다. 그리고 단계별로 가지치기하면서 한다. 뭐가 빠져있지? 뭘 붙이지? 어떤 식을 세워야 하는가?",
    "text": "시나리오, 할수 있다 없다를 떠나서. CEO관점에서 뭐가 궁금한가? 가설을 여러개 세우고 묶는다. 그리고 단계별로 가지치기하면서 한다. 뭐가 빠져있지? 뭘 붙이지? 어떤 식을 세워야 하는가?\n종이나 변수에 대해 자세히 설명하기 갭통계량에서는 2를 할 이유는 없다, 적당히 큰 것이 좋기 때문. 기울기가 기준이 아님.\n미국 대학의 최신의 데이터를 가져와서 쓰기. 증강한 데이터 패턴. 보여주기, 원데이터의 기초통계량과의 차이 등. 미적분학, 행렬, 벡터. 몇 백만, 십만개로 해야 됨.\n아이디어. 데이터 주입. 그럼 분석결과와 시각화 나옴. 클릭하면 파이썬 코드 나옴.\nSVM에서도 라그랑주 승수법으로 풀어주는 기법이 있다. RBF커널도 사용. 가장 성능이 좋다고 나옴."
  },
  {
    "objectID": "cs/bn_01.html",
    "href": "cs/bn_01.html",
    "title": "네트워크 개론",
    "section": "",
    "text": "네트워크 개론에 대해 다루고자 한다.\n01 통신 두 개 이상의 주체 간 정보의 송수신과 이해를 포함하는 상호작용 과정으로 정의된다.\n통신 주체는 인간 또는 기계, IoT 시스템과 같은 장치가 될 수 있으며, 전달되는 정보는 언어, 데이터, 음성, SNS 메시지 등 다양한 형태를 갖는다.\n효율적 통신을 위해서는 일정한 규약과 전송 시점, 데이터 인코딩 방식 등 명확한 방법이 요구되며, 이는 프로토콜의 형태로 구현된다.\n통신 과정은 단순한 정보 전달에 그치지 않고, 수신자가 해당 정보를 흡수하고 의미를 해석하는 과정을 포함한다. 아울러 제한된 자원을 어떻게 효율적으로 활용할 것인가는 통신 설계와 운영에서 핵심적 고려 사항이다.\n1 . 기본요소 통신은 정보를 주고받는 과정에서 다섯 가지 기본 요소로 구성된다. 먼저 송신기는 정보를 생성하고 전송하는 주체로, 사람, 컴퓨터, IoT 장치 등이 될 수 있다.\n메시지는 송신기가 전달하고자 하는 실제 정보로, 언어, 데이터, 음성, 영상 등 다양한 형태를 가진다.\n전송 매체는 메시지가 송신기에서 수신기로 전달되는 물리적 또는 논리적 경로를 의미하며, 유선, 무선, 광섬유 등 다양한 방식이 존재한다.\n수신기는 전달된 메시지를 받아 해석하고 이해하는 주체로, 송신기와 마찬가지로 사람이나 기계가 될 수 있다.\n마지막으로 프로토콜은 송신기와 수신기 간의 통신을 원활하게 수행하기 위해 적용되는 규약이나 약속으로, 메시지 형식, 전송 순서, 오류 처리 방법 등을 정의한다.\n이 다섯 요소는 상호 유기적으로 작용하여 신뢰성 있고 효율적인 통신을 가능하게 한다.\n2 . 유튜브 영상 재생 과정 사용자가 폰에서 영상을 재생하면, 폰은 HTTP나 HTTPS와 같은 프로토콜을 통해 유튜브 서버에 요청 패킷을 전송한다.\n이 과정에서 IP 주소를 기반으로 목적지를 지정하고, TCP 를 통해 전송 신뢰성을 확보하며 데이터가 순서대로 전달되도록 제어한다.\n유튜브 서버는 요청을 수신한 후, 해당 영상 데이터를 폰으로 전송한다. 전송된 영상 데이터는 일반적으로 RTP 와 같은 스트리밍 프로토콜을 통해 재생되며, 폰은 이를 받아 디코딩하고 화면에 출력한다.\n이러한 일련의 과정은 사용자가 요청한 영상이 안정적이고 실시간으로 재생되도록 설계되어 있다.\n3 . 통신의 종류 음성 통신은 사람 간 실시간 대화를 목적으로 하는 통신으로, 전통적인 전화선과 이동통신망을 통해 이루어진다.\n대표적으로 GSM, VoLTE 와 같은 이동통신 기술이 있으며, SIP 과 같은 신호 프로토콜을 통해 통화 연결과 종료, 세션 관리를 수행한다.\n데이터 통신은 문자, 파일, 인터넷 서비스 등 디지털 데이터를 전달하는 통신을 의미한다. Wi-Fi, LTE, 5G 와 같은 네트워크를 통해 TCP/IP 기반의 신뢰성 있는 전송이 이루어지며, IoT 환경에서는 MQTT와 같은 경량 메시지 프로토콜이 사용되기도 한다.\n방송 통신은 라디오, TV, 위성, 케이블과 같이 단방향 전송을 특징으로 하며, 다수의 수신자가 동시에 정보를 받을 수 있도록 설계되어 있다. 사용자는 송신자의 신호를 수신만 할 수 있으며, 피드백은 제한적이다.\n사물 인터넷(IoT) 통신은 다양한 장치와 센서가 네트워크에 연결되어 데이터를 주고받는 통신을 의미한다. Wi-Fi, Zigbee, LoRa와 같은 무선 기술을 사용하며, CoAP, MQTT 등 경량화된 프로토콜을 통해 제한된 자원에서도 효율적인 데이터 전송과 관리가 가능하다.\n4 . 통신의 역사와 발전 과정 고대·중세 초기 벽화, 연기 신호, 문자를 통한 기본적인 정보 전달. 15 ~ 18세기 인쇄술의 발명으로 정보의 대량 복제와 확산 가능. 세마포어 신호기(망루에서 깃발, 팔 신호로 메시지 전달). 19세기 전신기(모스 부호) 발명으로 장거리 신속 통신 실현. 전화기의 등장으로 음성 전달 가능. 무선 통신 기술 개발 시작. 20세기 라디오 방송 보급 → 대중 매체로 자리잡음. 인공위성(스푸트니크, 통신 위성)으로 전 지구적 통신 가능. 현대 인터넷 시대 ARPANET에서 시작 → TCP/IP 프로토콜 확립. WWW(월드 와이드 웹) 등장으로 정보 공유 혁신. 모바일 인터넷 확산으로 휴대 기기 중심 통신 발전. 최신 세대 5G: 초고속, 초저지연, 초연결을 특징으로 하여 IoT, 자율주행, 메타버스 등 새로운 서비스 기반 제공.\n02 인터넷\n1 . 작동 원리 인터넷은 패킷 교환 방식을 기반으로 동작한다.\n전송하고자 하는 데이터는 일정한 크기의 패킷 단위로 분할되며, 각 패킷에는 출발지와 목적지의 주소 정보가 포함된다.\n이 패킷들은 네트워크 상에서 최적의 경로를 따라 독립적으로 전송되고, 목적지에 도착한 후 원래의 데이터로 재조립된다.\n이 과정에서 핵심 역할을 하는 것이 TCP/IP 프로토콜이다.\nTCP 는 데이터의 신뢰성 있는 전송을 보장하고, 패킷이 순서대로 도착하도록 제어한다. IP 는 각 패킷이 정확한 목적지로 전달되도록 주소 지정과 경로 설정을 담당한다.\n또한, 인터넷에서는 사람이 기억하기 쉬운 도메인 이름을 사용한다. 이를 실제 통신에 필요한 IP 주소로 변환하는 과정이 DNS 이며, 사용자가 도메인 이름을 입력하면 DNS 서버가 해당 이름에 대응하는 IP 주소를 찾아 반환한다.\n이 과정을 통해 최종적으로 사용자의 요청이 해당 서버로 전달되고, 서버는 응답 데이터를 다시 패킷 형태로 전송한다.\n03 네트워크 기반 서비스 구조\n1 . 서버-클라이언트 구조 (중앙집중형) 클라이언트(PC, 브라우저, 앱)가 인터넷을 통해 중앙 서버에 요청을 보내고, 서버는 웹 애플리케이션 및 DB를 통해 응답을 제공. 예: AWS, Azure, GCP\n장점:\n데이터와 서비스를 통합적으로 관리 가능 중앙 통제 용이 스케일 업(서버 성능 확장) 또는 스케일 아웃(서버 추가)으로 확장성 확보 단점:\n서버 장애 시 전체 서비스가 중단될 위험 서버 의존도가 높아 운영 비용 및 리스크 집중\n2 . P2P 구조 (분산형) 모든 노드가 동등한 지위를 가지며, 직접 연결을 통해 데이터를 공유하고 처리. 예: IPFS, 토렌트, 블록체인\n장점:\n분산 구조로 인한 고가용성 확보 뛰어난 확장성 중앙 서버 비용 절감 단점:\n보안 관리가 상대적으로 어려움 운영·유지 관리 복잡성 데이터 동기화 지연 발생 가능\n3 . 하이브리드 구조 서버-클라이언트 구조와 P2P 구조를 혼합한 형태.\n특징:\n핵심 데이터나 보안이 중요한 부분은 중앙 서버에서 관리 대용량 파일 전송, 분산 저장, 연산 등은 P2P 네트워크 활용 중앙 집중 관리와 분산 구조의 장점을 모두 살리면서 단점을 보완 가능\n04 네트워크 기본 개념\n1 . 기본 구성 요소 노드(Node): 네트워크에 연결된 모든 장치(PC, 서버, 라우터, 스마트폰 등). 링크(Link): 노드 간 데이터를 전달하는 경로(유선 케이블, 무선 전파 등). 대역폭(Bandwidth): 네트워크가 단위 시간당 전송할 수 있는 데이터의 최대량, 즉 네트워크의 ‘용량’.\n2 . 네트워크 주소 체계 IP 주소: 네트워크에서 장치를 구분하는 논리적 주소 (IPv4, IPv6). MAC 주소: 네트워크 인터페이스 카드(NIC)에 부여된 물리적 주소, 전 세계적으로 유일. DNS: 사람이 이해하기 쉬운 도메인 이름을 IP 주소로 변환해주는 시스템.\n3 . 통신 방향 분류 (비용 증가 순) 단방향(Simplex): 한쪽 방향으로만 데이터 전송 가능 (예: 라디오, TV). 반이중(Half Duplex): 양방향 통신 가능하지만 동시에 불가능 (예: 무전기). 전이중(Full Duplex): 양방향 동시 통신 가능 (예: 전화, 현대 네트워크).\n4 . 거리 기준 네트워크 분류 PAN: 개인 영역 네트워크 (블루투스, 개인 기기 간 연결). LAN: 근거리 네트워크 (가정, 사무실). MAN: 도시 규모 네트워크. WAN: 광역 네트워크, 인터넷 포함.\n5 . 계층화 개념 네트워크는 복잡성을 줄이고 유지보수를 용이하게 하기 위해 계층적으로 설계됨. 각 계층은 독립적으로 동작하며, 모듈화된 구조로 상호 의존성을 최소화.\n비유: 비행기 서비스 과정\n매표소 계층 → 티켓 발권 수화물 계층 → 짐 위탁 게이트 계층 → 탑승 절차 활주로/비행기 계층 → 실제 이동 수행\nOSI 7계층 모델은 네트워크 통신을 7개의 층으로 나누어 각각의 역할과 기능을 정의한 개념 모델이다.\n1 . 물리층 Physical Layer\n역할: 실제 데이터 전송 매체를 통해 0과 1의 비트 신호를 전기적·광학적 신호로 변환하여 전송. 주요 기능: 케이블, 허브, 리피터 등 하드웨어 장치와 비트 전송. 예시: UTP 케이블, 광섬유, RJ-45 커넥터, 전송 속도(100Mbps, 1Gbps 등).\n2 . 데이터 링크층 Data Link Layer\n역할: 물리적 주소(MAC 주소)를 기반으로 신뢰성 있는 프레임 전송. 주요 기능: 에러 검출(CRC), 흐름 제어, 프레임화, 스위치 동작. 예시: Ethernet, Wi-Fi, 스위치, 브리지, MAC 주소.\n3 . 네트워크층 Network Layer\n역할: 서로 다른 네트워크 간의 데이터 전달 및 경로 선택(Routing). 주요 기능: 논리 주소(IP), 라우팅, 패킷 분할/조립. 예시: IP, ICMP, 라우터, 서브넷.\n4 . 전송층 Transport Layer\n역할: 종단 간(end-to-end) 신뢰성 있는 데이터 전송 제공. 주요 기능: 세그먼트 분할, 오류 제어, 흐름 제어, 연결 관리. 예시: TCP(연결형, 신뢰성), UDP(비연결형, 빠름), 포트 번호.\n5 . 세션층 Session Layer\n역할: 통신 세션(연결)을 설정·관리·종료. 주요 기능: 세션 동기화, 체크포인트, 재연결 지원. 예시: NetBIOS, RPC(Remote Procedure Call).\n6 . 표현층 Presentation Layer\n역할: 데이터 표현 형식과 인코딩 관리. 주요 기능: 암호화/복호화, 압축/복원, 문자 코드 변환. 예시: JPEG, MPEG, SSL/TLS, ASCII ↔︎ Unicode 변환.\n7 . 응용층 Application Layer\n역할: 사용자와 직접 상호작용하며 네트워크 서비스를 제공. 주요 기능: 메일 전송, 파일 전송, 웹 서비스 등. 예시: HTTP, FTP, SMTP, DNS, Telnet, Web Browser, Email Client.\nTCP/IP 모델은 OSI 7계층을 단순화하여 4 ~ 5층 정도로 나눈 실무 중심의 네트워크 모델이다.\n1 . 네트워크 인터페이스층 Network Interface / Link Layer\n역할: 물리적인 네트워크 연결과 데이터 전송 담당. 주요 기능: 프레임 전송, MAC 주소 기반 통신, 에러 검출. 장치/프로토콜 예시: Ethernet, Wi-Fi, 스위치, NIC.\n2 . 인터넷층 Internet Layer\n역할: 서로 다른 네트워크 간 데이터 전달과 경로 선택(Routing). 주요 기능: 논리 주소(IP) 기반 패킷 전달, 라우팅, 주소 지정. 프로토콜 예시: IP, ICMP, ARP, IPv4/IPv6.\n3 . 전송층 Transport Layer\n역할: 종단 간(end-to-end) 신뢰성 있는 데이터 전송 제공. 주요 기능: 포트 번호, 오류 제어, 흐름 제어, 연결 관리. 프로토콜 예시: TCP(신뢰성, 연결형), UDP(비연결형, 빠름).\n4 . 응용층 Application Layer\n역할: 사용자와 직접 상호작용하며 네트워크 서비스를 제공. 주요 기능: 데이터 포맷 처리, 응용 서비스 제공. 프로토콜 예시: HTTP, FTP, SMTP, DNS, Telnet, SSH.\n07 개발 방법론\n1 . Waterfall 방식 단계별(분석 → 설계 → 개발 → 테스트 → 배포) 순차적으로 진행되는 전통적 방법론. 장점: 체계적이고 관리가 용이, 문서 기반으로 요구사항 추적 가능. 단점: 변경에 취약, 초기 요구사항이 명확하지 않으면 리스크 증가.\n2 . Agile 방식 짧은 개발 주기(Iteration, Sprint)마다 요구사항을 반영하고 점진적으로 개선. 장점: 유연성, 빠른 피드백, 고객 중심 개발 가능. 단점: 문서화 부족 시 유지보수 어려움, 팀 역량 의존도 큼.\n3 . 개발 조직 및 역할 개발 조직(회사): 실제 시스템을 설계·구현·테스트하는 주체. 사업자: 서비스나 제품을 최종적으로 운영·제공하는 주체. 요구사항 주체(ISP): 사업 목표에 따른 요구사항을 도출하고 정리. APO(기술 영업): 사업자 요구와 기술적 가능성을 조율, 고객과 개발팀 간 가교 역할. OPO(다수 배치 가능): 각 세부 기능/모듈 단위 책임, 애자일 Scrum 팀 내에서 구체적 백로그 관리.\n4 . Scrum 및 협업 방식 애자일 프레임워크인 Scrum에서는 짧은 주기(Sprint) 단위로 개발. 일반적으로 3 주 단위 Sprint를 운영하며, 반복되는 사이클을 통해 요구사항을 점진적으로 구현.\nSprint 계획 회의 (목표 설정) Daily Scrum (매일 15분 점검) Sprint Review (성과 검토) Sprint Retrospective (개선점 도출)"
  },
  {
    "objectID": "da/tm/tm_06_0.html",
    "href": "da/tm/tm_06_0.html",
    "title": "6장: 감성 분석",
    "section": "",
    "text": "감성분석에 대해 다루고자 한다.\n01 제목1 [ID 사용하기] 감성 사전, 말에는 감성이 있다. 단어에 관한 감성 분석, 태도, 성향, 의견 한 개인의 감정이라도 이것도 많아지면 사람들이 생각하는 한 방향이 됨\n극성, 과학에서 말하는 극성\n어휘 기반은 수동으로 구축, 도메인(산업군, 어떤 카테고리 또는 영역인가?) 같은 언어, 단어가 도메인에 따라 다른 감성을 가질 수 있다.\n사전 기반은 이미 누군가가 만들어 놓은 감성 사전을 사용하는 것\n한국어 사전, 한국어, 국문학을 전공한 사람들이 참여했을 것이다. 리커트 척도? 보통이라는 의미를 담는 3점을 고른 생각이 같지 않을 것이라는 것이다. 정량적인 분석이 가능해야 리커트 척도라고 볼 수 있다. 즉, 각 등간 간격이 모두 동일해야 한다는 것이다.\n디테일하게 보고 싶으면 에뮬렉스를 사용.\n1 . 제목2 감성 분석을 위해 군산대학교 감성사전 웹사이트를 참고한다.\nKNU 한국어 감성사전\ndilab.kunsan.ac.kr\n또는 깃허브로 바로 이동해도 된다.\nGitHub - park1200656/KnuSentiLex: KNU(케이앤유) 한국어 감성사전\nKNU(케이앤유) 한국어 감성사전. Contribute to park1200656/KnuSentiLex development by creating an account on GitHub.\ngithub.com\n이 사이트는 다음과 같은 이유로 감성 분석에 유용하다.\nKNUSL 감성 사전의 특징 국내에서 구축한 한글 감성 어휘 사전으로, 한국어 감성 분석에 특화되어 있다. 각 단어에 감성 점수(긍정/부정/중립 등급)를 부여할 수 있다. 다양한 도메인(예: 리뷰, 뉴스 등)에 맞춘 감성 어휘 제공한다. 데이터는 연구 및 학습 목적으로 자유롭게 다운로드 가능 (단, 출처 명시 필요)\nDownload ZIP 클릭.\n다운로드 된 파일 압축 풀기.\ndata 폴더로 들어간 다음 SentiWord_info 파일 경로 복사하기\n감성 사전 JSON 파일을 읽어서 단어만 추출해보기.\nimport json\nfile_path = r’C:-master-master_info.json’\nwith open(file_path, ‘r’, encoding=‘utf-8’) as f: json_data = json.load(f)\n\n단어 리스트 생성\nword_list = [] for item in json_data: # 리스트 반복 word_txt = item[‘word’] word_list.append(word_txt)\n\n\n결과 출력 (앞 10개만 보기)\nprint(word_list[:10])\nKOSAC 감성사전 다운로드\nGitHub - mrlee23/KoreanSentimentAnalyzer: 한국어 감성 분석기\n한국어 감성 분석기. Contribute to mrlee23/KoreanSentimentAnalyzer development by creating an account on GitHub.\ngithub.com\n\n\nvalues 긍정부정중립\n01 제목1 [ID 사용하기]\n1 . 제목2\nNRC Emotion Lexicon\nImpact Some notable ways in which the NRC Emotion Lexicon has made impact include: First of its kind: It was the first word-emotion association lexicon, with entries for eight basic emotions as well as positive and negative sentiment. It still remains the\nsaifmohammad.com"
  },
  {
    "objectID": "da/tm/tm_04_0.html",
    "href": "da/tm/tm_04_0.html",
    "title": "4장: 크롤링 데이터 전처리",
    "section": "",
    "text": "크롤링 데이터의 통합 및 전처리에 대해 다루고자 한다.\n너저문한 데이터를 정리\n개행문자는 스페이스바를 눌렸기에 생기는 것임. 이를 처리하는 것이 전처리 과정임.\n의미있는 인사이트를 얻기 위해서 정리\n원데이터, 쿠팡에서 들어오는 데이터는 트랜젝션데이터(거래 데이터) 어떤 고객이 가입, 비가입, 어떤 카드로 결제, 회원가입 정보(최소한의 정보), 배송을 위한 성명, 휴대폰 번호, 본인인증 정도, - 고객의 프로파일링, 또는 데모그래픽 데이터 품목명, 가격명, 시간대, 카드 정보 – 정형데이터 이러한 필드로 저장됨, 댓글 정보 – 비정형 데이터, 사람마다 쓰는 댓글 양이 다름 고객의 반응을 보기 위해 전처리를 시도함, 이는 IT팀, 마케팅팀, MD가 사용\n성별을 구별하는 방법\n룰세팅을 하여 전처리 필요 없이 데이터를 뽑아서 써야 됨 여기선 가공 변수를 만드는 것이 필요함(예: 특정한 시간대에서 발생되는 매출)\n품목별 페이지에 대한 로그분석, 리뷰 데이터, 댓글의 패턴, 쿠키로 데이터 가져오기 SKT 전화요금제,만 있을 경우, 전화거래량 패턴 분석 정도 밖에 할 수 없음. 이름으로 성별을 판별 –\n추정을 하는 것임\n02 정규표현식\nimport re text = ‘core core883core’ re.findall(r’, text)\n\n단어 중간에 있는디\nre.findall(r’’, text)\nre.findall(r’1’, text)\ntext = ‘12 month 365 days 2023?’ re.findall(r’, text)\ntext = ‘12 month 365 days 2023?’ re.findall(r’’, text) # 숫자에 해당되는 걸 다 가져와\nre.findall(r’+’, text) # 숫자를 제외한 모든 것\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text # 특수문자를 제외한 모든 문자출력\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text # 특수문자까지 포함하여 출력\n03 데이터 합치기\n\n라이브러리 불러오기\n\nimport pandas as pd import pickle import os import re\n\n데이터 병합하기\n\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n\n\npkl 파일 로드 함수\ndef pklopen(text): f = open(file_path + ‘{}.pkl’.format(text),“rb”) a = pickle.load(f) f.close() return a\n\n\n수집된 데이터\ndata1 = pklopen(‘노인부양blog’) data2 = pklopen(‘노인부양cafe’) data3 = pklopen(‘노인부양cafe2’)\n\n\n데이터 결합\n\n\n행(row) 방향으로 데이터를\n\n\n밑으로 합치는(concatenate) 방식\ndata = pd.concat([data1, data2, data3])\n\n\n수집된 데이터가 모두 같은 컬럼 구조를 갖는다면,\n\n\n위에서 아래로 이어붙이는 방식으로 결합된다.\n\n\n각 채널 사이즈 확인\ndata.groupby([‘ch’, ‘ch2’]).size()\n\n인덱스 재설정하기\n\n\n\n인덱스를 0, 1, 2, …로 초기화하고,\n\n\n기존 인덱스는 새로운 열로 남기지 않도록 하는 명령어.\n\n\n주로 데이터 정제 후 인덱스를 깔끔하게 맞출 때 사용된다.\ndata = data.reset_index(drop=True) data\n채널별 수집한 데이터의 병합 결과\n04 데이터 전처리\n\n한글화\n\n정제, 정규화, 토큰화의 3단계를 거친다. 비정형 데이터일 경우,\n문서 날리기\n100 정열\nf = open(file_path + ‘노인부양병합’, ‘wb’) pickle.dump(data, f) f.close()\nf = open(file_path + ‘노인부양병합’, ‘rb’) docs = pickle.load(f) f.close() docs\n\n\n병합된 데이터를 피클 파일로 저장 및 출력한 것으로\n\n\n아래와 같이 파일이 저장된 것을 확인할 수 있다.\n\n\n제목, 본문, 댓글의 한글화 및 특수문자 제거\nfor i in range(len(docs)): docs.loc[i, ‘title’] = re.sub( r”[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]“,”“, str(data.loc[i, ‘title’]))\ndocs.loc[i, 'doc'] = re.sub(\n    r\"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(data.loc[i, 'doc']))\n\ndocs.loc[i, 'comment_cnt'] = re.sub(\n    r\"[^0-9]\", \"\", str(docs.loc[i, 'comment_cnt']))\n\ndocs.loc[i, 'comment_list'] = re.sub(\n    r\"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(data.loc[i, 'comment_list']))\ndocs\n\n\n조건에 맞는 row만 남기기\ndocs = docs[ ~( (docs[‘doc’].str.len() &lt; 2) | (docs[‘doc’].str.isspace()) )].reset_index(drop=True) 안에 ^를 넣을 때 한글만 가져오기\n\n문자형으로는 카우트 할 수 없으므로\n\n\n\nlike, comment_cnt, img, div의 데이터 타입을 숫자로 변환\n\n\n변환 중 에러 발생 시 NaN으로 처리\ndocs[‘like’] = pd.to_numeric(docs[‘like’], errors=‘coerce’).astype(‘Int64’) docs[‘comment_cnt’] = pd.to_numeric(docs[‘comment_cnt’], errors=‘coerce’).astype(‘Int64’) docs[‘img’] = pd.to_numeric(docs[‘img’], errors=‘coerce’).astype(‘Int64’) docs[‘div’] = pd.to_numeric(docs[‘div’], errors=‘coerce’).astype(‘Int64’)\ndocs\n원데이터와 값이 일치하는 지 확인하기.\n\n숫자형 결측치로 단어, 또는 문장, 형태소 분석 어간, 어근 어조 어미, 단순 띄어쓰기만으로는 힘듦, 형태소 분석을 쓰는 것임\n\n자립 형태소, 의존형태소\nOkt, 메캅, 코모란, 한나눔, 꼬꼬마\nfrom tqdm import tqdm # 진행상황 시각화 from konlpy.tag import Komoran\n\n\nKomoran 형태소 분석기 초기화\nkomoran = Komoran() # 클래스의 인스턴스 지정 # 이는 형태소 분석기 하나를 준비해서 계속 쓰기 위함이다.\n\n형태소 분석을 하고 명사들을 리스트로 저장\n\n이름은 단순하게 지정해도 상관없지만 너무 이름이 단순하면 이를 구별하거나 변수를 이해하기 어려으므로 다른 사람도 알아볼 수 있도록 객관적으로 판단하여 룰 세팅을 하는 것이 좋음\ntitle_token_list = [] # 제목의 형태소를 담아낼 리스트 title_token_noun = [] # 제목의 명사를 담아낼 리스트\nfor i in tqdm(range(len(docs))): # for문 - :\n# komoran.pos() 메서드를 사용하여 형태소 분석 실시\npos = komoran.pos(str(docs['title'][i]))\n\n# komoran.nouns() 메서드를 사용하여 추출하고 리스트에 저장\nnoun = [term for term in komoran.nouns( # 명사만 추출하며,\n    str(docs['title'][i])) if len(term) &gt; 1] # 명사의 길이는 2 이상이어야 한다.\n\ntitle_token_list.append(pos)\ntitle_token_noun.append(noun)\n리스트 이름구조 형태내용\ntitle_token_list [[(‘단어’, ‘품사’), …], …] 모든 형태소와 품사 정보 title_token_noun [[‘명사’, ‘명사’], …] 2글자 이상 명사만\n\n본문 토큰화 # 본문 형태소 및 명사 리스트 doc_token_list = [] doc_token_noun = []\n\nfor i in tqdm(range(len(docs))):\npos = komoran.pos(u'{}'.format(docs['doc'][i]))\n\nnoun = [term for term in komoran.nouns(\n    u'{}'.format(docs['doc'][i])) if len(term) &gt; 1]\n\ndoc_token_list.append(pos)\ndoc_token_noun.append(noun)\n\n\n댓글 형태소 및 명사 리스트\ncomment_token_list = [] comment_token_noun = []\nfor i in tqdm(range(len(docs))):\npos = komoran.pos(u'{}'.format(docs['comment_list'][i]))\n\nnoun = [term for term in komoran.nouns(\n    u'{}'.format(docs['comment_list'][i])) if len(term) &gt; 1]\n\ncomment_token_list.append(pos)\ncomment_token_noun.append(noun)\n형태소 분석만으로는 전처리가 끝났다고 볼 수 없다 아래 쓸대없는 불용어 때문에 찾고자 하는 문맥을 못 볼 수 있다.\n예를 들면, 광고글이 있는데 이는 광고글을 쓴 자가 정성스럽게 알맞은 단어 (의미없는 개행 문자 등만을 나열하지 않는) 말 그대로의 정돈된 글이기 떄문에 이를 제거하려면 일일이 광고글을 제거해야 한다.\n그러므로, 불용어 처리까지 해야 한다.\n다만, 불용어 처리에도 의미가 있는 명사를 제거하지 않도록 주의해야 한다. 예를 들어, ‘la’ 라는 문자만 본다면 의미없는 불용어라고 착각할 수 있다. 그러나 이는 LA를 의미하며, 미국 현지에서는 la, La, lA, LA와 같이 다양하게 사용되는 것으로 나타났다.\n따라서 이러한 불용어 처리 전에는 혹은 처리 중에 이러한 용어들이 나온다면 즉시 문서나 원데이터를 들여다봐서 실제로 그 값이 어떤 문맥상에서 어떤 의미를 지니는지를 확인해야 한다.\n어휘에 대한 이해를 할 수 있어야 한다.\n\n불용어 사전 다운받기\n\nstopwords-ko/stopwords-ko.txt at master · stopwords-iso/stopwords-ko\nKorean stopwords collection. Contribute to stopwords-iso/stopwords-ko development by creating an account on GitHub.\ngithub.com # 불용어 사전 기반 불용어 리스트 정리 f = open( file_path + “stopwords-ko.txt”, “r”, encoding=“UTF-8”) # UTF-8 인코딩으로 불용어 파일 열기\nst = f.readlines() # 한 줄씩 읽어서 리스트에 저장 f.close()\n\n\n줄 끝 개행 문자 제거\nst = [word.strip() for word in st] st\n\n불용어 사전 깔끔하게 만들기 stw = [word.strip() for word in st if word.strip() != ’’] stw\n나만의 불용어 사전 만들기 # 사용자가 정의한 불용어 추가 # 목적: 순수한 노인부양과 관련된 이야기 수집 # 광고글을 제외하기 위한 사용자 지정 불용어 사전 # 사용자가 정의한 불용어 추가 user_stopwords = [ ‘노인’, ‘부양’, ‘무자’, ‘양의’, ‘기초’, ‘노인학’, ‘계급’, ‘보험’, ‘고령’, ‘경제’, ‘바탕’, ‘국가’, ‘어르신’,‘지역’, ‘생각’, ‘포함’, ‘사업’, ‘한부모’, ‘일상생활’, ‘국민’, ‘확인’, ‘우리나라’, ‘적용’, ‘위해’, ‘기본’, ‘수준’, ‘예방’, ‘방법’, ‘주택’, ‘가능’, ‘방안’, ‘진행’, ‘행위’, ‘등의’, ‘대한민국’, ‘내년’, ‘개념’, ‘모집’, ‘개선’, ‘자격증’, ‘대상자’, ‘자격’, ‘과제’, ‘토론’, ‘청주’, ‘감소’, ‘증가’, ‘대의’, ‘추천’, ‘자부’, ‘경우’, ‘게시판’, ‘자금’, ‘본인’, ‘사람’, ‘연령’, ‘등급’, ‘활동’, ‘정부’, ‘평균’, ‘일반’, ‘파일’, ‘자의’, ‘더보’, ‘주간’, ‘기대’, ‘결과’, ‘통해’, ‘인가’, ‘자료’, ‘두레’, ‘포트’, ‘사이트’, ‘회원’, ‘다운’, ‘추가’, ‘완성’, ‘포인트’, ‘다운로드’, ‘충전’, ‘신규’, ‘제휴’, ‘작성’, ‘이벤트’, ‘저도’, ‘바우’, ‘해주’, ‘아래’, ‘링크’, ‘자가’, ‘해주시’, ‘등록’, ‘특례’, ‘네이버’, ‘구부’, ‘다이’, ‘이얼’, ‘마나’, ‘한일’, ‘서로’, ‘이다’, ‘현재’, ‘해서’, ‘댓글’, ‘하기’, ‘니다’, ‘이하’, ‘안녕하세요’, ‘해도’, ‘오늘’, ‘하면’, ‘키메’, ‘고맙습니다’, ‘이고’, ‘제가’, ‘내세’, ‘가요’, ‘만세’, ‘이노’, ‘때문’, ‘블로그’, ‘블로거’, ‘카페’, ‘만원’, ‘보내기’, ‘질문’, ‘재가’, ‘한국’, ‘세계’, ‘사회’, ‘가족’, ‘기준’, ‘서비스’, ‘장기’]\n\n\n\n불용어 리스트 확장\nstw.extend(user_stopwords)\n\n\n불용어 리스트 CSV 파일로 저장\nimport csv\nwith open(‘불용어 리스트’, “w”) as file: writer = csv.writer(file) writer.writerow(stw)\n\n정리된 불용어를 각문서의 제목, 본문, 댓글에서 제거 for word in stw: for i in range(len(title_token_noun)): # 제목에서 불용어 제거 while word in title_token_noun[i]: title_token_noun[i].remove(word)\n# 본문에서 불용어 제거\nwhile word in doc_token_noun[i]:\n    doc_token_noun[i].remove(word)\n\n# 댓글에서 불용어 제거\nwhile word in comment_token_noun[i]:\n    comment_token_noun[i].remove(word)\n\n\n\n문서파일 docs에 적용\ndocs[‘title_token_noun’] = title_token_noun # 제목 명사 리스트 docs[‘title_token_list_pos’] = title_token_list # 형태소+품사 리스트\ndocs[‘doc_token_noun’] = doc_token_noun # 본문 명사 리스트 docs[‘doc_token_list_pos’] = doc_token_list # 형태소+품사 리스트\ndocs[‘comment_token_noun’] = comment_token_noun # 본문 명사 리스트 docs[‘comment_token_list_pos’] = comment_token_list # 형태소+품사 리스트\n\n불용어를 제거한 최종 파일 저장 및 불러오기 # pickle로 저장 (최초 1회만 실시) import pickle with open(file_path + “total_doc.pkl”, “wb”) as f: pickle.dump(docs, f)\n\n\n\npickle로 다시 불러오기\nwith open(file_path + “total_doc.pkl”, “rb”) as f: data = pickle.load(f)"
  },
  {
    "objectID": "da/tm/tm_03_0.html",
    "href": "da/tm/tm_03_0.html",
    "title": "3장: 네이버 블로그 크롤링",
    "section": "",
    "text": "동적 크롤링을 위한 준비 및 네이버 블로그 크롤링 실습에 대해 다루고자 한다.\n01 자바 설치 방법\n1 . 파이썬과 자바의 관계 일반적으로 파이썬은 자바 없이 독립적으로 실행할 수 있다.\n하지만, 특정 라이브러리(예: JPype, PySpark, Jython 등)는 자바(Java)를 필요로 한다.\n따라서 사용하려는 기능이 자바 기반이라면, 먼저 자바가 설치되어 있어야 한다.\n2 . 자바 설치 여부 확인 Anaconda 프롬프트 실행한 다음 명령어 입력 후 실행.\n\n자바가 설치되어 있다면 버전 정보가 출력됨.\n\n\n“java is not recognized…” 오류가 발생하면 자바가 설치되지 않은 것임.\njava -version\n3 . 내 컴퓨터에 자바 설치하기 Oracle 공식 홈페이지에서 JDK 다운로드 설치 후, 환경 변수를 설정해야 한다.\nDownload the Latest Java LTS Free\nSubscribe to Java SE and get the most comprehensive Java support available, with 24/7 global access to the experts.\nwww.oracle.com\n환경 변수 설정 (Windows 기준)\n제어판 → 시스템 및 보안 → 시스템 → 고급 시스템 설정 고급 탭 → 환경 변수 버튼 클릭 시스템 변수에서 “새로 만들기” 클릭 변수 이름: JAVA_HOME 변수 값: C:Files-XX.X.X (설치된 JDK 경로 입력) Path 변수 편집 → ;%JAVA_HOME%추가\nAnaconda 프롬프트 또는 명령 프롬프트에서 다시 입력하여 정상적으로 출력되는지 확인한다.\n02 Selenium을 사용한 동적 크롤링\n1 . Selenium 설치 웹 브라우저에서 동적 크롤링 시 가장 많이 사용하는 패키지.\n과거에는 웹드라이버 버전에 맞는 경로를 지정해줘야 했지만, 현재는 패키지의 새버전에 의해 자동적으로 맞춰진다.\npip install selenium\n2 . 웹드라이버 다운로드 사용하는 브라우저에 맞는 WebDriver를 다운로드해야 한다.\nChrome 다운로드 및 설치 - 컴퓨터 - Google Chrome 고객센터\n도움이 되었나요? 어떻게 하면 개선할 수 있을까요? 예아니요\nsupport.google.com 다운로드한 WebDriver를 실행 파일 경로에 두거나, Python 코드에서 직접 경로를 지정해야 한다.\n03 네이버 블로그 크롤링\n\n라이브러리 불러오기.\n\nfrom selenium import webdriver # 웹 브라우저 자동화 from bs4 import BeautifulSoup as BS # HTML 및 XML 파싱\nimport pandas as pd # 데이터 조작 및 분석 import requests # HTTP 요청을 보내기 위한 모듈 import datetime # 날짜 및 시간 연산 import pickle # 파이썬 객체 직렬화 import time # 코드 실행 간격 조절 import re # 정규 표현식을 사용하여 문자열 처리\n\n\nSelenium에서 다양한 방법으로 HTML 요소를 찾기\nfrom selenium.webdriver.common.by import By\n\nSelenium을 이용한 네이버 블로그 검색 자동화.\n\n\n\n크롬 드라이버 실행\ndriver = webdriver.Chrome()\n\n\n네이버 블로그 검색 페이지로 이동\n\n\n검색할 키워드 지정 및 데이터 수집기간 설정한 뒤\n\n\n복사한 URL을 붙여 넣으면 되며, 아래 코드는 가독성을 위해 일부러 줄바꿈을 시도함\ndriver.get(’’’ https://search.naver.com/search.naver? ssc=tab.blog.all&query=%EB%85%B8%EC%9D%B8%20%EB%B6%80%EC%96%91 &sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20240301to20240325’’’.replace(“”, ““))\nURL 가져오는 방법\n실행 화면\n\n웹 페이지 자동 스크롤 함수.\n\ndef doScrollDown(whileSeconds): start = datetime.datetime.now() # 스크롤 다운 시작 시간 설정 end = start + datetime.timedelta(seconds=whileSeconds) # 스크롤 다운 종료 시간\nwhile True:\n    # 페이지 맨 아래로 스크롤 다운\n    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n    time.sleep(1) # 1초 대기\n\n    # 종료 시간에 도달하면 반복 종료\n    if datetime.datetime.now() &gt; end:\n        break\n        \ndoScrollDown(2) # 스크롤 다운 시간 설정\n\n웹 페이지에서 제목과 URL 추출하기.\n\n\n\n제목과 URL을 저장할 리스트 초기화\ntitle_list = [] url_list = []\n\n\n현재 페이지에서 클래스명이 ’title_link’인 요소들을 찾음\ntitles = driver.find_elements(By.CLASS_NAME, ‘title_link’)\nfor i, title_element in enumerate(titles): try: # 요소에서 제목을 추출하여 title_list에 추가 title_list.append(title_element.text) # 요소에서 URL을 추출하여 url_list에 추가 url_list.append(title_element.get_attribute(‘href’)) except: print(“오류 발생”) # 예외 발생 시 출력 continue # 오류가 발생해도 다음 요소 처리 계속 진행\n# 10번째 항목마다 진행 상황 출력\nif (i + 1) % 10 == 0:\n    print(f\"{i + 1}개 수집 완료\")\n\n블로그 본문 및 메타데이터 크롤링 자동화.\n\n\n\n1] 크롤링 데이터 저장 리스트 초기화\nnew_doc, like_cnt, comment_cnt, comment_list, img_cnt, div_cnt = [], [], [], [], [], []\n\n\n2] 블로그 본문 크롤링\nfor i in range(len(url_list)): url_path = url_list[i] # URL 불러오기 driver.switch_to.window(driver.window_handles[0]) # 첫 번째 탭으로 이동 driver.execute_script(“window.open(‘{}’)”.format(url_path)) # 새 탭 열기(URL 실행) driver.switch_to.window(driver.window_handles[1]) # 두 번째 탭으로 이동\ntime.sleep(1)  # 1초 대기\ntry:\n    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n    d = ''     # 댓글 변수 초기화\n\n    # 댓글 영역의 HTML 코드 가져오기\n    if len(iframes) &gt; 0:            # iframes의 존재 확인\n        driver.switch_to.frame(0)       # 첫 번째 iframe으로 전환 및 내용 가져옴\n        html = driver.page_source       # HTML 코드 가져와 변수 저장\n        soup = BS(html, \"html.parser\")  # 저장된 코드 파싱 및 soup 생성\n\n        # 3] 블로그 본문 추출\n        try:\n            a = soup.find(\"div\", class_=\"se-main-container\").get_text()\n        except: # 블로그 본문을 찾지 못할 경우\n            a = soup.find(\"div\", id=\"postListBody\")     # 일반 블로그에 경우\n            a = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(a)) # 정규표현식 -&gt; 한글만 남김\n\n        # 4] 좋아요 수 추출\n        try:\n            b = soup.find(\"em\", class_=\"u_cnt_count\").get_text()\n        except:\n            b = \"null\"\n\n        # 5] 댓글 수 추출\n        try:\n            c = soup.find(\"em\", id=\"commentCount\").get_text()\n        except:\n            c = \"null\"\n\n        # 6] 댓글 추출\n        try: # 댓글을 모두 보기 위해 버튼 클릭\n            comment = driver.find_elements(By.CLASS_NAME, \"btn_arr\")\n            comment[-1].click()  # 마지막 댓글 버튼 클릭\n            time.sleep(1)\n            commentLen = len(driver.find_elements(By.CLASS_NAME, \"u_cbox_page\"))\n            d = \"\\n\".join([comment.text for comment in driver.find_elements(By.CLASS_NAME, \"u_cbox_text_wrap\")])\n        except:\n            d = \"null\"\n\n        # 7] 이미지 및 영상 수 추출\n        e = len(soup.find_all(\"img\", class_=\"se-image-resource egjs-visible\"))\n        f = len(soup.find_all(\"div\", class_=\"pzp-ui-dimmed pzp-dimmed pzp-pc__dimmed\"))\n\n        # 8] 데이터 리스트에 추가\n        new_doc.append(a)\n        like_cnt.append(b)\n        comment_cnt.append(c)\n        comment_list.append(d)\n        img_cnt.append(e)\n        div_cnt.append(f)\n\n        driver.switch_to.default_content()  # 기본 콘텐츠로 전환\n    else:\n        # 데이터가 없을 경우 빈 값 추가\n        new_doc.append(' ')\n        like_cnt.append(' ')\n        comment_cnt.append(' ')\n        comment_list.append(' ')\n        img_cnt.append(' ')\n        div_cnt.append(' ')\n\nexcept Exception as e:\n    # 예외 발생 시 에러 메시지와 함께 빈 값 추가\n    print(f\"Error at {url_path}: {e}\")\n    new_doc.append(' ')\n    like_cnt.append(' ')\n    comment_cnt.append(' ')\n    comment_list.append(' ')\n    img_cnt.append(' ')\n    div_cnt.append(' ')\n\ndriver.close()  # 현재 탭 닫기\ntime.sleep(0.3)  # 0.3초 대기\n\n# 매 10번마다 진행 상황 출력\nif (i+1) % 10 == 0:\n    print(f\"진행 상황: {i+1}/{len(url_list)}\")\n\n데이터프레임으로 변환.\n\n\n\n크롤링한 데이터를 데이터프레임으로 변환\nraw_data = pd.DataFrame({ “title”: title_list, “doc”: new_doc, “like”: like_cnt, “comment_cnt”: comment_cnt, “commnet_list”: comment_list, “img”: img_cnt, “div”: div_cnt, “ch”: “naver”, “ch2”: “blog” })\n\n\n데이터프레임을 pickle 파일로 저장\nfile_path = “C:/Users/jkl12/텍스트마이닝/” # 슬래시 사용 with open(file_path + “노인부양blog.pkl”, “wb”) as f: pickle.dump(raw_data, f)\n\n\n크롬 드라이버 종료\ndriver.quit()\n\n\n저장된 pickle 파일을 불러오기\nwith open(file_path + “노인부양blog.pkl”, “rb”) as f: temp_file = pickle.load(f)\n\n\n데이터프레임을 CSV 파일로 저장\ntemp_file.to_csv(file_path + “노인부양blog.csv”, index=False, encoding=“utf-8-sig”) # 파일 경로 지정 file_path = r”C:.csv”\n\n\nCSV 파일 불러오기\ndf = pd.read_csv(file_path, encoding=“utf-8-sig”) df"
  },
  {
    "objectID": "cybersec.html",
    "href": "cybersec.html",
    "title": "Cyber-Security",
    "section": "",
    "text": "제목\n\n\n\n1\n\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n제목\n\n\n\n1\n\n\n\n\n\n\n\n\n\nDec 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n전송계층\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n서브넷 마스크\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIP 프로토콜 및 QoS\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n라우팅(Routing)\n\n\n\n1\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n네트워크계층의이해\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n무선통신시스템의이해\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n데이터링크계층의핵심구조\n\n\n\n1\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n데이터 링크\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n유선 통신망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n무선 통신망\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n네트워크 개론\n\n\n\n1\n\n\n\n\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cs/bn_12.html",
    "href": "cs/bn_12.html",
    "title": "제목",
    "section": "",
    "text": "OSI 모델의 표현 계층(Presentation Layer)은 데이터의 변환, 암호화, 압축 등 상위 애플리케이션 간 데이터 호환성을 보장하는 역할을 수행한다. 이를 통해 하드웨어 및 운영체제 독립성을 유지하고, 다양한 표현 방식을 표준화할 수 있다.\n\n\n응용 계층은 최종 사용자와 직접 상호작용하며, 데이터의 구조화, 처리, 전송을 담당한다.\n\n\n\nHTML: 웹 콘텐츠 구조화, 확장성 및 호환성 제공\nHTML5: 멀티미디어 통합, Canvas API, 반응형 디자인, 로컬 스토리지, WebSocket 지원으로 현대 웹의 표준화 구현\n\n\n\n\n\n플랫폼 독립적이며 언어 제약이 없음\n요청 시마다 프로세스 생성으로 성능 저하 발생 가능\n유지보수 어려움과 보안 취약점 존재\n\n\n\n\n\n구조화된 데이터 저장 및 전송에 최적화\n사용자 정의 태그 지원, 가독성 우수, 플랫폼 독립성 확보"
  },
  {
    "objectID": "cs/bn_12.html#osi-표현-계층과-응용-계층",
    "href": "cs/bn_12.html#osi-표현-계층과-응용-계층",
    "title": "제목",
    "section": "",
    "text": "OSI 모델의 표현 계층(Presentation Layer)은 데이터의 변환, 암호화, 압축 등 상위 애플리케이션 간 데이터 호환성을 보장하는 역할을 수행한다. 이를 통해 하드웨어 및 운영체제 독립성을 유지하고, 다양한 표현 방식을 표준화할 수 있다.\n\n\n응용 계층은 최종 사용자와 직접 상호작용하며, 데이터의 구조화, 처리, 전송을 담당한다.\n\n\n\nHTML: 웹 콘텐츠 구조화, 확장성 및 호환성 제공\nHTML5: 멀티미디어 통합, Canvas API, 반응형 디자인, 로컬 스토리지, WebSocket 지원으로 현대 웹의 표준화 구현\n\n\n\n\n\n플랫폼 독립적이며 언어 제약이 없음\n요청 시마다 프로세스 생성으로 성능 저하 발생 가능\n유지보수 어려움과 보안 취약점 존재\n\n\n\n\n\n구조화된 데이터 저장 및 전송에 최적화\n사용자 정의 태그 지원, 가독성 우수, 플랫폼 독립성 확보"
  },
  {
    "objectID": "cs/bn_12.html#웹-시스템-구조와-아키텍처",
    "href": "cs/bn_12.html#웹-시스템-구조와-아키텍처",
    "title": "제목",
    "section": "웹 시스템 구조와 아키텍처",
    "text": "웹 시스템 구조와 아키텍처\n웹 시스템의 주요 목표는 데이터 처리 효율, 확장성 확보, 보안 및 안정성 유지이다.\n\n2. 핵심 구성 요소\n\n클라이언트(Client): 사용자 요청 생성 및 화면 표시\n웹 서버(Web Server): HTTP 요청 처리, 정적 콘텐츠 제공\n애플리케이션 서버(App Server): 비즈니스 로직 처리\n데이터베이스(DB): 데이터 저장 및 관리\n\n\n\n2.1 확장 및 최적화 구성 요소\n\n로드 밸런서(Load Balancer): 서버 부하 분산\n캐시 서버(Cache Server): 반복 데이터 빠른 접근 제공\nCDN(Content Delivery Network): 글로벌 데이터 전송 최적화\n보안 시스템(Security Layer): 접근 제어 및 위협 방어\n\n\n\n2.2 아키텍처 진화\n단일 계층 → 2계층 → 3계층 → 마이크로서비스 아키텍처로 발전, 모듈화 및 확장성 강화"
  },
  {
    "objectID": "cs/bn_12.html#웹-요청-및-응답-흐름",
    "href": "cs/bn_12.html#웹-요청-및-응답-흐름",
    "title": "제목",
    "section": "웹 요청 및 응답 흐름",
    "text": "웹 요청 및 응답 흐름\n\n3. DNS(Domain Name System)\n\n필요성: 기억하기 어려운 IP 주소를 도메인 이름으로 변환, 유연한 변경, 효율적 운영\n구조: 최상위 루트 → TLD → 세컨드 레벨 도메인 → 서브 도메인 → 호스트, 계층적 트리 구조\n쿼리 방식: Recursive, Iterative\n활용 사례: 웹 브라우징, 이메일, CDN, 부하 분산, 보안"
  },
  {
    "objectID": "cs/bn_12.html#이메일과-ftp-프로토콜",
    "href": "cs/bn_12.html#이메일과-ftp-프로토콜",
    "title": "제목",
    "section": "이메일과 FTP 프로토콜",
    "text": "이메일과 FTP 프로토콜\n\n4. 이메일 시스템\n\n흐름: 메일 작성 → 송신 → 라우팅 → 저장 → 수신\n프로토콜: SMTP(송신), POP3/IMAP(수신)\nMIME(Multipurpose Internet Mail Extensions): 다양한 언어, 이미지, 동영상 지원\n\n\n\n5. POP3 vs IMAP\n\nPOP3: 서버에서 메일 다운로드 후 로컬 저장, 단순 구조\nIMAP: 서버에서 메일 동기화 및 관리, 멀티 디바이스 지원\n\n\n\n6. FTP(File Transfer Protocol)\n\n동작 모드: 액티브 모드, 패시브 모드\n보안 강화 프로토콜: FTPS, SFTP, FTP over SSL/TLS 등 안전한 파일 전송 지원"
  },
  {
    "objectID": "cs/bn_10.html",
    "href": "cs/bn_10.html",
    "title": "서브넷 마스크",
    "section": "",
    "text": "IP 주소에서 네트워크와 호스트 영역을 명확히 구분하기 위한 핵심적인 비트 마스크이다. 이를 통해 네트워크 식별, 호스트 구분, 라우팅 효율화가 가능하며, IPv4 주소 체계에서 필수적인 구성 요소로 작동한다.\n\n\n\nIPv4 주소는 32비트로 구성\n‘1’: 네트워크 영역\n‘0’: 호스트 영역\n서브넷팅(Subnetting):\n기존 네트워크를 보다 작은 서브넷으로 분할하여 네트워크 관리 효율과 보안을 향상시키고, 라우팅 최적화를 달성한다.\n네트워크 비트 수 증가 → 서브넷 수 증가, 호스트 수 감소.\n슈퍼넷팅(Supernetting):\n다수의 작은 네트워크를 하나의 큰 네트워크로 결합하여\nCIDR(Classless Inter-Domain Routing) 구현 및 라우팅 테이블 축소 효과를 달성한다.\n\n\n\n\nHierarchical Routing 라우팅 테이블의 항목 수를 최소화하여 라우터 부하를 줄이고, 네트워크 관리 효율성을 높인다. 이를 통해 네트워크 범위 식별, 라우팅 영역 구분, 상위 네트워크 및 외부 네트워크와의 효율적인 데이터 전달이 가능하다.\n장점: 주소 활용 효율화, 보안 강화, 트래픽 최적화, 계층적 설계. 단점: 설계 복잡성 증가, 오류 가능성, 관리 부담 증가.\n\n\n\n게이트웨이, DNS, 라우팅 테이블과 밀접히 연관되어 네트워크 통합 관리와 안정적 운영을 지원한다."
  },
  {
    "objectID": "cs/bn_10.html#네트워크-분할",
    "href": "cs/bn_10.html#네트워크-분할",
    "title": "서브넷 마스크",
    "section": "",
    "text": "IPv4 주소는 32비트로 구성\n‘1’: 네트워크 영역\n‘0’: 호스트 영역\n서브넷팅(Subnetting):\n기존 네트워크를 보다 작은 서브넷으로 분할하여 네트워크 관리 효율과 보안을 향상시키고, 라우팅 최적화를 달성한다.\n네트워크 비트 수 증가 → 서브넷 수 증가, 호스트 수 감소.\n슈퍼넷팅(Supernetting):\n다수의 작은 네트워크를 하나의 큰 네트워크로 결합하여\nCIDR(Classless Inter-Domain Routing) 구현 및 라우팅 테이블 축소 효과를 달성한다."
  },
  {
    "objectID": "cs/bn_10.html#계층적-라우팅",
    "href": "cs/bn_10.html#계층적-라우팅",
    "title": "서브넷 마스크",
    "section": "",
    "text": "Hierarchical Routing 라우팅 테이블의 항목 수를 최소화하여 라우터 부하를 줄이고, 네트워크 관리 효율성을 높인다. 이를 통해 네트워크 범위 식별, 라우팅 영역 구분, 상위 네트워크 및 외부 네트워크와의 효율적인 데이터 전달이 가능하다.\n장점: 주소 활용 효율화, 보안 강화, 트래픽 최적화, 계층적 설계. 단점: 설계 복잡성 증가, 오류 가능성, 관리 부담 증가."
  },
  {
    "objectID": "cs/bn_10.html#연계-요소",
    "href": "cs/bn_10.html#연계-요소",
    "title": "서브넷 마스크",
    "section": "",
    "text": "게이트웨이, DNS, 라우팅 테이블과 밀접히 연관되어 네트워크 통합 관리와 안정적 운영을 지원한다."
  },
  {
    "objectID": "cs/bn_10.html#arp",
    "href": "cs/bn_10.html#arp",
    "title": "서브넷 마스크",
    "section": "1. ARP",
    "text": "1. ARP\nAddress Resolution Protocol\n\nIPv4 환경에서 IP 주소를 MAC 주소로 변환.\n송신자 확인 → 브로드캐스트 요청 → 유니캐스트 응답 → 캐시 저장.\n특징: 브로드캐스트-유니캐스트 혼합 통신, 일정 기간 IP-MAC 매핑 캐시, 편리하지만 보안 취약점 존재."
  },
  {
    "objectID": "cs/bn_10.html#rarp-bootp-dhcp",
    "href": "cs/bn_10.html#rarp-bootp-dhcp",
    "title": "서브넷 마스크",
    "section": "2. RARP, BOOTP, DHCP",
    "text": "2. RARP, BOOTP, DHCP\n\nRARP: MAC 주소를 기반으로 IP 주소 요청.\nBOOTP: IP 주소가 없는 장치에 초기 IP 할당.\nDHCP: BOOTP 기능을 확장하여 IP, 서브넷 마스크, 게이트웨이, DNS 등 자동 제공.\n동작 순서: Discover → Offer → Request → Acknowledge.\n일반 가정용 및 기업 환경에서 널리 사용."
  },
  {
    "objectID": "cs/bn_10.html#기타-네트워크-프로토콜",
    "href": "cs/bn_10.html#기타-네트워크-프로토콜",
    "title": "서브넷 마스크",
    "section": "03 기타 네트워크 프로토콜",
    "text": "03 기타 네트워크 프로토콜"
  },
  {
    "objectID": "cs/bn_10.html#icmp",
    "href": "cs/bn_10.html#icmp",
    "title": "서브넷 마스크",
    "section": "1. ICMP",
    "text": "1. ICMP\nInternet Control Message Protocol\n\nIP 네트워크에서 오류 보고와 제어 메시지 전달용 보조 프로토콜.\nIP 자체에는 오류 보고 기능이 없으므로, ICMP를 통해 안정적 네트워크 동작을 지원.\n주요 메시지: 오류 보고, 상태 진단, 경로 추적 등."
  },
  {
    "objectID": "cs/bn_10.html#igmp",
    "href": "cs/bn_10.html#igmp",
    "title": "서브넷 마스크",
    "section": "2. IGMP",
    "text": "2. IGMP\nInternet Group Management Protocol\n\nIPv4에서 멀티캐스트 그룹 관리. IPv6에서는 MLD 사용.\n필요성: 멀티캐스트 트래픽 효율적 관리, 실시간 서비스 지원.\n동작: 가입 요청 → 라우터 정보 유지 → 주기적 멤버 확인 → 탈퇴 알림.\n버전별 특징: v1 기본 가입 기능, v2 빠른 탈퇴 지원, v3 SSM(Source-Specific Multicast) 지원.\n적용 사례: 유튜브 라이브 스트리밍, 금융 거래 서비스."
  },
  {
    "objectID": "cs/bn_10.html#라우팅-프로토콜",
    "href": "cs/bn_10.html#라우팅-프로토콜",
    "title": "서브넷 마스크",
    "section": "3. 라우팅 프로토콜",
    "text": "3. 라우팅 프로토콜\n\n내부 라우팅 프로토콜: OSPF(Link State 기반), 빠른 수렴, 계층적 네트워크 구조에 적합.\n외부 라우팅 프로토콜: BGP(자율 시스템 간 경로 선택), 인터넷 백본 및 대규모 네트워크에서 표준으로 사용."
  },
  {
    "objectID": "cs/bn_08.html",
    "href": "cs/bn_08.html",
    "title": "라우팅(Routing)",
    "section": "",
    "text": "네트워크 계층(Network Layer)의 핵심 기능으로, 데이터 패킷이 출발지에서 목적지까지 도달하기 위한 최적의 경로를 결정하는 과정을 의미한다.\n이 과정은 네트워크의 토폴로지 변화, 트래픽 부하, 링크 상태 등에 따라 동적으로 변화하며, 효율성과 안정성 간의 균형이 중요하다.\n라우팅은 크게 정적(Static)과 동적(Dynamic) 방식으로 구분된다.\n\n정적 라우팅 (Static Routing):\n관리자가 수동으로 경로를 지정.\n소규모 네트워크에서 안정적이지만, 링크 장애나 토폴로지 변화에 대응 불가.\n동적 라우팅 (Dynamic Routing):\n라우터가 인접 노드들과 정보를 교환하며 최적 경로를 실시간 계산.\n자율성과 적응력이 높으나, 연산 부하와 제어 메시지 비용이 증가."
  },
  {
    "objectID": "cs/bn_08.html#애자일-개발-방법론-전략적-관점",
    "href": "cs/bn_08.html#애자일-개발-방법론-전략적-관점",
    "title": "라우팅(Routing)",
    "section": "애자일 개발 방법론: 전략적 관점",
    "text": "애자일 개발 방법론: 전략적 관점\n\n1. 개발 방법론의 분류 및 특성\n소프트웨어 개발 프로젝트는 전통적으로 폭포수(Waterfall) 방식과 애자일(Agile) 방식으로 구분된다. 폭포수 방식은 요구사항 정의 → 분석 → 설계 → 구현 → 테스트 → 배포로 이어지는 선형적 단계 모델을 따르며, 각 단계가 완료되어야 다음 단계로 진행된다. 이 접근법은 명확한 요구사항과 엄격한 일정 관리가 필요할 때 효과적이지만, 요구사항 변경이나 시장 변화에 유연하게 대응하기 어렵다는 한계가 있다.\n반면 애자일 방식은 반복적·점진적 개발(iterative and incremental development)을 기반으로 한다. 핵심 원칙은 고객 요구사항의 변화에 민첩하게 대응하며, 기능 단위의 점진적 납품을 통해 피드백을 신속하게 반영하는 것이다. 프로젝트 관리에서는 WBS 기반의 일정 중심 관리보다 작업(Task) 중심 관리가 강조되며, 각 개발 주기를 스프린트(Sprint)로 정의한다. 일반적으로 스프린트는 1~4주 단위로 반복되며, 각 사이클 종료 시 실제 동작 가능한 소프트웨어를 제공하여 고객 검증과 요구사항 반영을 동시에 달성한다.\n\n\n2. 애자일의 전략적 활용\n\n고객 중심성: 초기 버전부터 고객과 지속적으로 상호작용하여 요구사항을 점진적으로 명확화한다.\n적응적 계획(Adaptive Planning): 시장 변화나 기술적 요건에 맞춰 스프린트 계획을 유연하게 조정한다.\n위험 관리: 단기 목표 기반 반복 개발로 프로젝트 실패 가능성을 조기에 탐지하고 완화한다.\n팀 자율성: 크로스 기능적 팀(Cross-functional Team)이 스스로 우선순위를 판단하고 작업을 수행한다.\n\n\n\n3. 폭포수 방식 대비 애자일의 장점과 한계\n\n\n\n구분\n폭포수(Waterfall)\n애자일(Agile)\n\n\n\n\n개발 진행\n선형, 단계별\n반복적, 점진적\n\n\n요구사항 변경 대응\n어려움\n용이, 적응적\n\n\n일정 관리\nWBS 중심, 고정\n스프린트 중심, 유연\n\n\n고객 참여\n제한적\n지속적 피드백 포함\n\n\n위험 관리\n후기 발견\n조기 탐지 및 완화\n\n\n문서화\n상세 문서 중심\n최소 문서, 실행 중심\n\n\n\n애자일은 무제한적 유연성을 의미하지 않는다. 프로젝트 규모, 조직 구조, 규제 환경, 기술적 복잡성에 따라 적용 전략을 신중히 설계해야 하며, 과도한 변화 반영은 일정 지연과 품질 저하를 초래할 수 있다.\n\n\n4. 적용 사례 및 실무 전략\n\n스타트업 제품 개발: 빠른 시장 반응과 경쟁력 확보를 위해 애자일 방식이 적합하다.\n대기업 내부 시스템 개발: 일부 핵심 모듈은 폭포수 방식으로 안정성을 확보하고, 신규 기능 모듈은 애자일 방식으로 점진적 개발을 수행한다.\n혼합 접근(Hybrid Approach): 전통적 개발 프로세스에 애자일 스프린트를 통합하여 안정성과 민첩성 간 균형을 확보한다.\n\n\n\n5. 결론\n애자일은 단순한 개발 방법론이 아니라, 조직의 전략적 의사결정, 프로젝트 관리 체계, 고객 가치 창출을 통합적으로 고려한 개발 패러다임이다. 이를 통해 변화하는 요구사항과 시장 환경 속에서도 효과적인 소프트웨어 개발과 관리가 가능하다."
  },
  {
    "objectID": "cs/bn_06.html",
    "href": "cs/bn_06.html",
    "title": "무선통신시스템의이해",
    "section": "",
    "text": "IEEE 802.11 기술 유선 LAN 형태 이너넷 단점보완을 위해 기기가 AP와 연동해서 AC 또는 라우터를 통해서 인터넷 망을 주고 받는다. 연결 교모별 BSS, AP 없음 ESS, 다중 AP 연결 유형별 Ad Hoc, Infrastructure, 기술 발전의 세부규격들의 기준은 대부분 속도이다.\n6G로 도약이 어려운 이유 중 하나는 현재 한국은 4G와 5G가 혼용된 형태의 환경인데 6G는 대부분이 5G로 되어 있어야 설치 가능한 환경이 만들어진다. 그러려면 대용량 데이터 전송이 필요한 매체(AR) 등의 플랫폼이 있어야 하는데 아직까지 또 그러지도 않아서 기술의 발전이 더디는 상황이다.\nCSMA/CA 동작원리 4가지(IEEE 802.11 관련 기술) 채널감지, 대기, 프레임 전송, 수신확인\nIEEE 802.15와의 차이점\n블루투스 기호의 유래: 스칸다비아의 문자 두개를 합쳐 탄생 전파 간섭 문제가 자주 있으며 인원이 많아지면 보통 일어난다. pairing, BLE 2가지 기술용도로 씀, 그래서 유용하면서도 보완에는 상대적으로 취약에서 범용사용에는 국한된다. 블루투스 비콘의 등장배경: 2010 코인 배터리 등장 스마트폰 중에서도 애플의 도움으로 이것이 실현하게된 가장 큰 요인이 됨. 사실상 휴대폰의 확장 기능.\nZigbee 개요: 간단한 기능 사용. 짧은 보고 시간 + 낮은 에너지 사용 + 높은 배터리(수개월-수년) =&gt; 전기 및 도시 가스 계량기, 거리 조명 등 사용 이런 장치의 보완 문제를 해결하는 것이 주요 문제, 그 이유는 앞으로는 이런 종류의 장치가 많아질 것이라는 보고가 있기 때문 이동통신망 사용이 필요치 않는 곳에서 사용할 가능성이 있음. 동시에 사업자 입장에서 인원 단축 및 편리성을 위해 이는 우리나라보단 미국이나 동남아 처럼 국가의 크기 규모가 큰 나라에서 더 적용될 가능성이 높음 중국이 한국처럼 카드를 사용하지 않고 QR로 사용하듯 나라마다 필요한 기술이 다르다. 구성: Coordinator &gt;Router &gt; Device\nREID: 버스, 지하철, 시설 출입증 카드, 톨게이트 이것은 수동 소자이며, 전원을 공급하는 형태의 카드는 REIC이다. 구조는 태그,(데이터-에너지), 안테나, 리더, 호스트 무선 상태에서 얼마나 데이터 손실없이 잘 받아들이는 지가 주요 관점 현재의 대중화와 달리 초기엔 잘 작동하지 않았음. 효과미미\nNFC: 모바일 결정, 교통카드(FeliCa, MIFARE), 출입통제\nRFID와 NFC의 차이점 비교하기\n\n무선 LAN 에서 신뢰성 있는 통신을 위해 고려해야 할 부분이 있다면 어떤것이 있을까 ?\n저속통신 방식을 통해 향 후 실생활에서 적용이 가능한 범위와 사례는무엇이 있을까 ?"
  },
  {
    "objectID": "cs/bn_04.html",
    "href": "cs/bn_04.html",
    "title": "데이터 링크",
    "section": "",
    "text": "데이터 링크가 왜 필요한가? 1. 물리계층의 한계 신호의 시작과 끝, 신호의 왜곡, 유실\n\n노드 간 신뢰성 확보 노드 간의 충돌 최소화(CSMA/CD), 오류 제어, 백오프(충돌 시 지연) 문제를 예방해야 상위 쪽에서의 노력이 덜 들어가는 편의가 있음 이를 해결하는 방법은 매우 다양함\n\n그 중 기본적인 것을 배우고자 함 데이터 링크 계층의 5가지 핵심 역할 프레임화, 주소 지정, 오류 제어, 프름 제어 링크 제어\n프레임의 정의 문자 프레임(과거) 대 비트 프레임(현대) 예시: 이더넷 프레임 구조\n보통의 형태는 동기화 형태로 데이터를 보내며 여기서 preamble 가 사용되며 SFD가 프레임 시작을 알린다.\n과거의 통신 장비를 바로 최신 기종으로 바꾸지 않는 이유는 호환성 문제가 가장 크다. 바꿀 때에도 백워드 컴퓨터를 잘 고려해야 한다.\n이태원 지역은 하웨이 장비 계열의 통신 장비(BTS) 등을 사용하지 못한다. 미군이 살고 있어 해킹의 위험이 있기 때문.\n\n데이터 전송 오류 유형 단순 비트 오류(한 비트만 변경), 버스트 오류(연속적으로), 프레임 손실(수신측 미도착), 프레임 중복(동일 프레임 중복 수신) 오류 검출 방식 패리티 비트, 체크섬, CRC 단순 오류 검출, 데이터 단위 합산, 다항식 기반 검출 등 짝수 패리티(전체를 짝수로), 홀수 패리티(전체를 홀수로) 그러나 이 방법으로는 2개 이상의 비트 변경은 오류를 알기 어렵다. 체크섬은 더 발전된 형태, 각각을 16비트 진수 형태로 만들어 검증 그러나 내부가 완전히 깨진 형태는 검출하기 어렵다. CRC은 더 발전된 형태, 다항식 계산을 통해 거의 완전하게 검출함 그러나 이는 곧 다른 방법에 비해선 시간과 비용이 들어간다는 것 때문에 보안의 중요도에 따라서 다르게 사용할 수 있다.\n오류 정정 코드 해밍 코드, 패리티를 최소 몇 개를 붙일 것인지 공식으로 계산 FEC 실무적용사례: 이더넷 LAN, 와이파이, 5G 등 슬라이딩 윈도우 프로토콜, 윈도우 개념, 흐름제어, 부분 재전송 stop-and-Wait, timeout 시간 동안 기다리고 유실 등으로 수신층이 데이터를 보내지 않으면 다시 보냄. 다만 과거엔 개발자 위주 였으나 최근은 사용자 관점에서 편의성을 갖추어야 하므로 기다리는 방식이 오히려 불편할 수도 있음 go-back N 중간에 못 받은 게 있으면 이후 데이터는 받더라도 무시하고 다시 그 지점부터 보냄. 불필요한 로스들이 발생할 수 있다는 단점이 있음 Selective Repeat 위 단점을 해소하기 위한 해결 방안, 이후 데이터는 받으면 버퍼링을 돌리고 이전에 못 받은 걸 다시 받으면 지금까지 받을 걸 잘 정리 및 합쳐서 상위계층으로 올림 다만 상황에 따라서는 오히려 비효율적일 수도 있다. 그래도 대부분 데이터 유실은 개별 단위 보다는 전체가 유실되거나 파괴되는 경우가 더 많다.\n\n데이터 링크계층에서의 신뢰성 확보가 중요한 이유는 무엇일까 ?\n프레임 전송시 슬라이딩 원도우 방식이 나오게 된 배경과 좀 더 효율적으로 개선이 필요한부분이 있다면 어떠한 부분이 있을까 ?\n\n슬라이딩 윈도우 프로토콜 슬라이딩 윈도우 프로토콜은 송신자가 여러 프레임을 연속적으로 전송하고, 수신자가 ACK를 통해 수신 확인을 하는 방식입니다. 이 방식은 흐름 제어와 오류 제어를 동시에 수행할 수 있습니다. 이것이 언제 어디에서 처음 개발 되었는가? 이것은 현대까지 포함해서 어떻게 발전되었는지 웹에서 찾아서 자세하게 말해주세요"
  },
  {
    "objectID": "cs/bn_02.html",
    "href": "cs/bn_02.html",
    "title": "무선 통신망",
    "section": "",
    "text": "물리계층 1. 물리적 연결 보장, 2. 데이터 전송 담당\n비트에서 신호로 변환 0과1의 물리적 신호로 변환 전압, 빛, 전파의 유무\n아날로그, 정보 저장 공간 많음 복제 시 품질 이 저하, 수정 변경 어려움 시간이 지나면 품질에 영향, 선명하고 세밀한 표현\n디지털은 반대, 마그네틱이 있음\n전송매체의 종류, 유선: UTP 케이블, 동축 케이블, 광섬유 케이블, MDMI/USB, 전력선(철탑) 무선: 와이파이, 블루투스, LTE, 5G, (주파수대역이 높아질수록 기지국을 더 많이 세워야 함.)\n전송 방식의 분류 1. 직렬&병렬 2. 동기&비동기 3. 아날로그&디지털\n물리적 특정 정의 최대 케이블 길이 100m 전송 속도 1Gbps 신호전압 5V\n택배 지유로 이해하기 물리계층 = 로도, 택배상자 = 데이터\n\n신호, 파형: 주파수(시간), 파장(거리), 진폭, 위상 소리 관점으로 본 특징: 1. 고음/ 저음 Hz 2. 크고/ 작음 dB (0은 상대적 개념 0이라고 아예 안들리는 건 아님) 가청주파수 대역, 아날로그-디지털로의 변화 샘플링, 양자화/부호화, 압축 용어 정리, 감쇠, 간섭, 지터, 신호대 잡음비\n무선 통신, 주파수가 높을 떄와 낮을 때의 특징들(직진성, 투과성, 정보량, 안테나 크기, 울림) 기본성질, 굴절, 반사, 회절, 감쇠, 산란 변조, 신호를 다른 종류로 변환 1. 상대방에게 전달 용이하도록 2. 허가 받은 곳으로 전달하려고 진폭 변조 대 주파수 변조 사람의 목소리 - Carrier 교류 신호(발진기에서 발생) - AM, FM 변조 그 이외 ASK, PSK, FSK QAM의 등장 제한된 대역폭에서 전송 효율을 높이기 위함 위상과 진폭의 개수 기준에 따라 데이터를 구분\n다중 접속, 특정 주파수 대역에서 여러 명이 동시에 해당 대역을 사용하기 위함 FDMA(1세대), TDMA, CDMA - 복잡성/효율성, Generation, 주요기술, 수용가입자, Handover 으로 구분\n\n유선 통신 또는 무선 통신에서 개선이 필요한 사항\n향후 AI에 통신을 어떻게 적용할 건지"
  },
  {
    "objectID": "cs/bn_03.html",
    "href": "cs/bn_03.html",
    "title": "유선 통신망",
    "section": "",
    "text": "01 유선망 예: 이더넷·전용회선\n1 . 제목2 무선보다 지연·지터가 작고 예측 가능성이 높음 — 물리적 접속·스위치 경로가 고정적이기 때문입니다.\n광섬유는 단일 링크에서 Tbps급(실험·상용 사례: 수십~수백 Tbps 기록)으로 확장 가능 — DWDM 등 기술로 용량을 극대화합니다. (nict.go.jp)\n도청 위험은 상대적으로 낮다. 광탭은 구리보다 어렵고 탐지도 까다롭지만 완전히 불가능한 것은 아니므로 중요 트래픽은 종단암호화 등 추가 보호가 필요합니다. (VIAVI Perspectives)\n매체·용도 정리: 데스크/사무실 단거리 연결은 구리(Cat5e/6)·PoE가 일반적, 빌딩 간·IDC/DCI·백본은 주로 광섬유 사용 — 요구 대역폭·거리·보안·운영비용에 따라 선택합니다. (실무적 근거: 광섬유의 고용량·장거리 특성). (Corning)\n1 . 회선교환 통신 시작 시 전용 경로(회선)를 설정 → 대역폭 예약·순서 보장·지연 안정(예: 전통 PSTN). (위키백과)\n2 . 패킷교환 데이터를 패킷으로 분할해 라우터가 최적 경로로 전달 → 경로·지연이 패킷별로 달라질 수 있어 효율적이나 순서 뒤바뀜(재정렬)·지터가 발생할 수 있음. (Obkio)\n보완(현대적 관점): MPLS·가상회선(ATM 등)처럼 패킷망 위에서 회선형(예약/QoS) 성격을 흉내내는 기술이 널리 사용됩니다 — 따라서 “회선교환=항상 더 단순/우수”라는 이분법은 현실을 반영하지 않습니다. (Cisco)\n초기 경로 설정 필요 · 경로 고정(전용 자원 예약)\n회선교환은 통신 시작 시 종단 간 회선을 설정하고 그 회선의 자원을 전용으로 예약합니다 — 설정된 경로가 통신 기간 동안 유지됩니다. (TechTarget)\n순서적 수신(정렬 보장)\n물리적 전용 회로를 쓰므로 데이터(음성 샘플)는 순서가 보장되어 전송됩니다(정해진 지연·순서 장점). (Simon Fraser University) 경로 공유 불가(전용) — 기본적으로 맞음 회선이 설정되면 그 회선의 대역폭은 다른 통신에 재할당되지 않습니다(효율성 측면에서는 비효율). (TechTarget) 경로 상 장비 오류 시 — ’우회 불가’는 부분적 진술 단일 확립된 회로 내에서는 경로상 고장이 발생하면 그 회로 연결은 유지되지 못함(콜이 끊김). 다만 현대 전화망·통신사업자 인프라는 중복 경로·스위치 레벨의 재라우팅·트렁크 다양화로 장애를 회피하거나 콜을 다른 경로로 재설정하는 메커니즘을 갖추고 있으므로 “우회 경로 전혀 없음”은 과도한 단정입니다. (NIST CSRC) 과금 방식(시간 기반) — 역사적/일반적 사실 전통 PSTN/회선 기반 서비스는 통화 시간 단위 과금이 일반적이었으나(또는 전용회선은 월정액 등), 사업자·서비스 유형에 따라 과금 모델은 다를 수 있습니다. (위키백과) 패킷교환이 ’전부 반대’라는 표현의 문제점 패킷교환은 일반적으로 패킷별 라우팅(경로 가변), 통신 자원의 통계적 다중화(공유), 순서 뒤바뀜·지연 변동 가능성, 링크 장애 시 라우팅 프로토콜에 의한 우회 등이 특징입니다 — 그러나 패킷망에도 가상회선(VC: X.25, Frame Relay, ATM, MPLS 등) 같은 연결지향 모드가 있어 회선형 특성을 흉내낼 수 있습니다. 따라서 “전부 정반대”로 단순화하면 정확하지 않습니다. (위키백과) 실무적 함의(요점)\n음성·실시간 제어 등 지연·순서 보장이 중요한 트래픽은 전용 회선 또는 패킷망 위의 QoS/가상회선(MPLS, SR-TE 등)으로 보장한다. (위키백과) 신뢰성 설계 시에는 물리적 중복·트렁크 다양화·신호 레벨 재설정 정책을 고려해야 함(단일 링크 고장으로 서비스 전체 중단 방지). (NIST CSRC) 원하시면 위 근거(논문·교재·운영자 문서)를 근거별로 요약해 표로 정리해 드리겠습니다.\n\n통신망의 다양화와 발전에 영향을 준 부분과 그것의 산물은 어떠한 것이 있는가 ?\n향 후 미래 통신에서 더 중점적으로 고려할 부분은 어떠한 것이 있을까 ?"
  },
  {
    "objectID": "cs/bn_05.html",
    "href": "cs/bn_05.html",
    "title": "데이터링크계층의핵심구조",
    "section": "",
    "text": "LAN의 특징, (범위, 속도, 낮은 지연, 구조) IEEE 802가 LAN의 표준\nLLC, MAC 세부구조, OSI 레이어 2개로 이뤄짐. 802.3, 11, 15 은 주로 쓰는 것.\nLLC: 802.2 표준 MAC: 3, 5, 11\n오류제어 정리 MAC 오류검출만 끝남, CRC, 체크섬 하드웨어 프레임 단위, 오류 시 프레임 폐기\nLLC 복구까지, ARQ SW/논리링크 단위, 오류시 재전송 요청\n802.3 대표적 표준 기술 접근 방식(CSMA/CD), 속도 발전, 토폴로지(버스형, 스타형)\n이더넷: 현대LAN 의 표준 초기 공유 매체, 스위치 전화, 현대 기능\n\nToken-Ring 802.5 토큰 (획득, 반환, 순환) 보내는 쪽이 없앱, 받는 쪽이 다수일 수 있으므로 장점: 충돌 데이터 없음, Qos 보장 용이, 고정 대역폭 제공 단점: 장치 하나의 장애로 전체 마비, 설치 및 유지보수 복잡하여 높은 비용 그래서 현대에서는 거의 쓰지 않음 이더넷은 상대적으로 고속 전송, 저비용, 표준화, 유연한 확장성을 가졌기 때문. 프레임 분석 도구(wireshark, tcpdump, SPAN 포트 등)\nHDLC 구조 Flag, 주소, 컨트롤(I, S, U프레임), 정보, FCS, Flag 공통점: 데이터 링크 계층 사용되는 프로토콜 차이점은 WAN에서 주로 사용, 상대적으로 느림\n\n데이터 링크계층를 세부 LLC와 MAC sublayer로 구분 지은 이유는 무엇이고 각 sublayer에서의 역할에 대하여 설명해 보세요\nIEEE 802에서 MAC sublayer를 세분화하여 표준으로 선정한 이유와 대표적인 MAC 표준을 찾아보고 해당 내용에 대하여 설명해 보세요"
  },
  {
    "objectID": "cs/bn_07.html",
    "href": "cs/bn_07.html",
    "title": "네트워크계층의이해",
    "section": "",
    "text": "네트워크 계층의 핵심 기능 주소지정, 라우팅, 패킷 전달, 단편화와 재조립\n필수 설정 4가지 요소 IP 주소, Subnet Mask, Default Gateway, DNS Server\n\nIP 주소 논리적 주소의 고유 번호, 네트워크 계층(3계층 해당), 2가지 버전(IPv4, IPv6)\n\nIPv4 클레스 체계(약 42억개, E는 연구원 용도이므로 실제 사용 가능한 건 더 적음) 보통은 A~E 중 C를 사용함\nA 클레스의 경우 할당 받는 주소 만큼 실제 사용 안 함 망의 증가, 축소 시 IP주소의 재배치가 경직됨. 해결: Subnet, CIDR 도입, 결과적으로 클래스 타입 IP에서 클레스 네트워크로 발전\nIPv6), 매우 많은 수의 주소\n\nSubnet Mask IP주소의 네트워크(1), 호스트(0) 부분을 나눈다.\nDefault Gateway 내부 네트워크, 게이트 웨이(라우터도 포함하는 큰 개념), 외부 네트워크 대표 사례: 무선공유기 네트워크 입장: 다른 네트워크로 나가는 출구, 입구 데이터 입장: 통로, 호스트 기기 입장: 반드시 도달해야 하는 접속 지점\n\n역할: 프로토콜 반환, 네크워크 연결, 데이터 경로 설정, 보안 종류: 기본 게이트웨이, 인터넷 게이트 웨이, 단반향 게이트웨이\n\nDNS Server 도메인 입력, 질의, 응답, 서버 통신 구성 요소: root, TLD, Authoritative, 캐시 서버 레코드 타입: A, AAAA, MX 레코드 / CNAME\n\nVPN 가상사설망 1. 공중 퍼블릭 인터넷 / 2. 전용선 설치 원격 근무자가 회사 내부 네트워크에 접속하기 위함 가상회선, 패킷 교환망에서 논리적으로 전용 회선처럼 동작하는 경로를 미리 설정 경로대로 순서대로 전달.\n가상회선과 데이터그램의 차이., 표 작성\n\n게이트웨이, 방화벽 그리고 라우터의 용도와 차이는 무엇인가?\n가상 사설망이 사용되는 예와 장점은 무엇인가 ?"
  },
  {
    "objectID": "cs/bn_09.html",
    "href": "cs/bn_09.html",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "IP, Internet Protocol 네트워크 계층에서 논리적 주소 지정과 패킷 전달을 담당하는 핵심 프로토콜이다. IP는 비연결성(connectionless)과 비신뢰성(unreliable)을 특징으로 하며, 최선형 전달(best-effort delivery)을 제공한다.\n즉, 패킷을 가능한 한 손실 없이, 가능한 빠르게 전달하는 것이 주 목표이다.\n\n\n\n논리 주소 지정: 호스트 식별 및 라우팅 경로 선택에 사용되는 IP 주소 체계\nIP 버전:\n\n\n\n\n\n\n\n\n\n특징\nIPv4\nIPv6\n\n\n\n\n주소 길이\n32비트\n128비트\n\n\n기본 필드\n고정 20바이트 + 옵션\n고정 40바이트 + 확장 헤더\n\n\n옵션 처리\n헤더 내 포함\n확장 헤더로 분리, 간소화\n\n\n패킷 분할\n라우터 단편화 가능\n송신지 단편화만 가능, 라우터 단편화 불가\n\n\n기타\n주소 부족 문제, 옵션 포함\n128비트 주소, 확장 헤더로 유연성 확보\n\n\n\n\n\n\n\n혼합 제어(Mixed Control)\n패킷 단편화(Fragmentation) 및 재조립(Reassembly)\n터널링(Tunneling): 데이터 패킷 캡슐화 → 전송 → 목적지에서 디캡슐화 (VPN, MPLS)\nQoS 지원 및 트래픽 관리"
  },
  {
    "objectID": "cs/bn_09.html#논리-주소와-버전-관리",
    "href": "cs/bn_09.html#논리-주소와-버전-관리",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "논리 주소 지정: 호스트 식별 및 라우팅 경로 선택에 사용되는 IP 주소 체계\nIP 버전:\n\n\n\n\n\n\n\n\n\n특징\nIPv4\nIPv6\n\n\n\n\n주소 길이\n32비트\n128비트\n\n\n기본 필드\n고정 20바이트 + 옵션\n고정 40바이트 + 확장 헤더\n\n\n옵션 처리\n헤더 내 포함\n확장 헤더로 분리, 간소화\n\n\n패킷 분할\n라우터 단편화 가능\n송신지 단편화만 가능, 라우터 단편화 불가\n\n\n기타\n주소 부족 문제, 옵션 포함\n128비트 주소, 확장 헤더로 유연성 확보"
  },
  {
    "objectID": "cs/bn_09.html#ip-패킷-처리-주요-기능",
    "href": "cs/bn_09.html#ip-패킷-처리-주요-기능",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "혼합 제어(Mixed Control)\n패킷 단편화(Fragmentation) 및 재조립(Reassembly)\n터널링(Tunneling): 데이터 패킷 캡슐화 → 전송 → 목적지에서 디캡슐화 (VPN, MPLS)\nQoS 지원 및 트래픽 관리"
  },
  {
    "objectID": "cs/bn_09.html#배경-및-필요성",
    "href": "cs/bn_09.html#배경-및-필요성",
    "title": "IP 프로토콜 및 QoS",
    "section": "2.1 배경 및 필요성",
    "text": "2.1 배경 및 필요성\n\n네트워크 서비스 품질 저하 문제 심화\n인프라 확장 한계로 효율적 자원 관리 필요\n비용 절감 및 성능 보장 요구 증가\n특정 애플리케이션 기능 및 성능 보장 필요"
  },
  {
    "objectID": "cs/bn_09.html#qos-4대-요소",
    "href": "cs/bn_09.html#qos-4대-요소",
    "title": "IP 프로토콜 및 QoS",
    "section": "2.2 QoS 4대 요소",
    "text": "2.2 QoS 4대 요소\n\n대역폭(Bandwidth): 충분한 전송 용량 확보\n지연(Latency): 실시간 서비스의 낮은 지연 확보\n지터(Jitter): 패킷 간 전송 간격 일정 유지\n손실 제어(Packet Loss): 패킷 손실률 최소화"
  },
  {
    "objectID": "cs/bn_09.html#중간-지점-기반-qos-기술",
    "href": "cs/bn_09.html#중간-지점-기반-qos-기술",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.1 중간 지점 기반 QoS 기술",
    "text": "3.1 중간 지점 기반 QoS 기술\n\nQueue 관리: 패킷 순서 및 처리 우선순위 제어, 대표 5가지 기법\nTraffic Shaping: 전송률 제한 및 버스트 제어, 대표 3가지 기법\n사전 패킷 폐기: 혼잡 상황 시 우선순위 낮은 패킷 폐기, 대표 3가지 기법\nQoS 보장 기술: 자원 예약 및 클래스별 관리, 대표 2가지 기법"
  },
  {
    "objectID": "cs/bn_09.html#홉-제어-개념",
    "href": "cs/bn_09.html#홉-제어-개념",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.2 홉-제어 개념",
    "text": "3.2 홉-제어 개념\nHop-by-Hop Congestion Control * 각 라우터(홉)에서 혼잡을 감지하고 대응하는 기술 * 목적: 혼잡 완화 → 지연 감소, 패킷 손실 최소화, 공정한 대역폭 사용\n즉, 패킷이 목적지까지 가는 동안 각 홉에서 처리 방식을 결정"
  },
  {
    "objectID": "cs/bn_09.html#포함되는-요소와-대응-기법",
    "href": "cs/bn_09.html#포함되는-요소와-대응-기법",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.2 포함되는 요소와 대응 기법",
    "text": "3.2 포함되는 요소와 대응 기법\n\n\n\n\n\n\n\n\n요소\n대응 기법\n설명\n\n\n\n\n대역폭\nQueuing, Shaping, Policing\n각 홉에서 전송 우선순위, 송신률 제한, 초과 트래픽 차단\n\n\n패킷 손실\nQueuing, Policing, Early Drop\n혼잡 시 패킷 폐기, 우선순위 기반 손실 최소화\n\n\n지연 및 지터\nQueuing, Shaping\n패킷 순서·처리 지연 관리, 트래픽 평탄화로 지터 감소"
  },
  {
    "objectID": "cs/bn_09.html#임의-우선순위-예시",
    "href": "cs/bn_09.html#임의-우선순위-예시",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.3 임의 우선순위 예시",
    "text": "3.3 임의 우선순위 예시\n\n이메일 &lt; 일반 애플리케이션 &lt; 대용량 데이터/영상 &lt; 미션 크리티컬 &lt; 음성"
  },
  {
    "objectID": "cs/bn_11.html",
    "href": "cs/bn_11.html",
    "title": "전송계층",
    "section": "",
    "text": "01 Transport Layer\n전송계층은 신뢰성 있는 데이터 전송과 효율적인 네트워크 통신을 담당하는 계층으로, 다음 네 가지 핵심 역할을 수행한다:\n\n세그먼트화(Segmentation)\n전송 제어(Transmission Control)\n흐름 제어(Flow Control)\n다중화(Multiplexing)\n\n\n1. 세그먼트화(Segmentation)\n세그먼트화는 상위 계층에서 전송되는 데이터를 작은 단위로 나누고, 수신 측에서 이를 재조립하는 과정이다.\n\n송신 측: 큰 데이터 스트림을 효율적 전송을 위해 작은 세그먼트로 분할\n수신 측: 세그먼트를 원래 데이터 스트림으로 재조립하여 상위 계층에 전달\n\n목적: 전송 효율성 증가, 오류 복구 용이, 신뢰성 있는 데이터 전달 보장\n\n\n2. 전송 제어(Transmission Control)\n전송 제어는 송신자와 수신자 간의 신뢰성 있는 데이터 전송을 보장한다.\n\n오류 제어(Error Control): 전송 중 발생한 데이터 손상이나 손실을 검출하고, 재전송 요청을 통해 데이터 무결성을 확보\n혼잡 제어(Congestion Control): 네트워크 과부하 시 전송 속도를 조절하여 혼잡을 방지하고 성능 저하 최소화\n\n\n\n3. 흐름 제어(Flow Control)\n흐름 제어는 데이터 전송 속도를 조절하여 수신 측 버퍼 오버플로를 방지하고 네트워크 혼잡을 줄인다.\n\nStop-and-Wait 방식: 송신 측에서 한 패킷 전송 후 수신 측 확인 응답을 기다린 후 다음 패킷 전송\n슬라이딩 윈도우(Sliding Window) 방식: 여러 패킷을 연속 전송하고 수신 측의 확인 응답을 통해 전송을 관리\n\n\n\n4. 다중화(Multiplexing)\n다중화는 여러 애플리케이션 계층 데이터를 하나의 전송 채널로 통합하는 과정이다. 각 세그먼트에는 식별용 포트 번호를 포함하여 수신 측에서 올바른 애플리케이션으로 전달된다.\n\n포트(Port): IP 주소 내 논리적 통신 지점, 애플리케이션 식별 및 데이터 분배 역할\n소켓(Socket): 실제 데이터 송수신 통로 제공. 서버와 클라이언트 간 세션 연결 관리\n\n\n\n5. 전송 계층 프로토콜\n\n5.1 TCP(Transmission Control Protocol)\n\n연결 지향형 프로토콜\n신뢰성 있는 데이터 전달 및 순서 유지, 흐름 제어 및 혼잡 제어 지원\n3-way handshake를 통한 연결 설정\n사용 사례: 웹, 이메일, 파일 전송 등\n헤더 구조: 복잡하지만 신뢰성 확보에 필수적\n\n\n\n5.2 UDP(User Datagram Protocol)\n\n비연결형 프로토콜\n빠른 전송 속도, 최소 헤더 구조, 신뢰성 보장 없음\n사용 사례: 스트리밍, 실시간 게임\n헤더 구조: 송수신 포트, 길이, 체크섬 등 최소 정보 포함\n\n\n\n\n6. TCP 연결 설정 과정\nTCP 연결은 3-way handshake를 통해 이루어진다:\n\nSYN: 클라이언트 → 서버, 연결 요청\nSYN-ACK: 서버 → 클라이언트, 연결 수락 및 응답\nACK: 클라이언트 → 서버, 연결 확정\n\n각 단계에서 시퀀스 번호(Seq)와 확인 응답 번호(Ack)를 사용하여 신뢰성을 유지하며, 데이터 흐름과 재전송 관리를 지원한다."
  },
  {
    "objectID": "cs/bn_13.html",
    "href": "cs/bn_13.html",
    "title": "제목",
    "section": "",
    "text": "네트워크 보안의 핵심은 세 가지 요소로 정의된다: 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)\n\n\n\n정의: 민감 정보가 권한 없는 주체에게 노출되지 않도록 보호\n기술적 수단: 데이터 암호화, 사용자 인증, 접근 제어, 방화벽\n목표: 데이터 전송 및 저장 과정에서 정보 유출 방지\n\n\n\n\n\n정의: 데이터가 전송 또는 저장 중 변경되거나 훼손되지 않음을 보장\n기술적 수단: 접근 제어, 암호화, 로그 기록 및 감시, 해시 함수 활용\n목표: 위변조 방지 및 신뢰성 확보\n\n\n\n\n\n정의: 사용자가 필요할 때 언제든지 네트워크와 서비스를 이용할 수 있도록 보장\n기술적 수단: 백업 시스템, 네트워크 이중화, DDoS 방어, 실시간 시스템 모니터링, 보안 패치 및 취약점 관리\n목표: 서비스 연속성과 장애 회복력 강화"
  },
  {
    "objectID": "cs/bn_13.html#네트워크-보안network-security-심화-분석",
    "href": "cs/bn_13.html#네트워크-보안network-security-심화-분석",
    "title": "제목",
    "section": "",
    "text": "네트워크 보안의 핵심은 세 가지 요소로 정의된다: 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)\n\n\n\n정의: 민감 정보가 권한 없는 주체에게 노출되지 않도록 보호\n기술적 수단: 데이터 암호화, 사용자 인증, 접근 제어, 방화벽\n목표: 데이터 전송 및 저장 과정에서 정보 유출 방지\n\n\n\n\n\n정의: 데이터가 전송 또는 저장 중 변경되거나 훼손되지 않음을 보장\n기술적 수단: 접근 제어, 암호화, 로그 기록 및 감시, 해시 함수 활용\n목표: 위변조 방지 및 신뢰성 확보\n\n\n\n\n\n정의: 사용자가 필요할 때 언제든지 네트워크와 서비스를 이용할 수 있도록 보장\n기술적 수단: 백업 시스템, 네트워크 이중화, DDoS 방어, 실시간 시스템 모니터링, 보안 패치 및 취약점 관리\n목표: 서비스 연속성과 장애 회복력 강화"
  },
  {
    "objectID": "cs/bn_13.html#주요-사이버-위협",
    "href": "cs/bn_13.html#주요-사이버-위협",
    "title": "제목",
    "section": "주요 사이버 위협",
    "text": "주요 사이버 위협\n\n1. 피싱(Phishing)\n\n원인 분석: 국내 금융 및 카드 서비스 발달로 인해 공격 타겟 관계망이 촘촘함\n유형: 이메일 피싱, 스피어 피싱, 스미싱, 파밍\n대응 전략: URL 검증, 2단계 인증, 출처 확인, 보안 솔루션 활용\n제한 사항: 파밍 공격은 개인 차원에서 예방이 어려움\n\n\n\n2. DDoS 공격(Distributed Denial of Service)\n\n특징: 볼륨 기반, 프로토콜 기반, 애플리케이션 계층 공격을 통해 서비스 마비\n대응: 트래픽 분석, 방화벽 규칙, CDN 및 부하 분산 활용\n\n\n\n3. 악성 소프트웨어(Malware)\n\n종류: 바이러스, 웜, 트로이 목마, 랜섬웨어, 스파이웨어\n대응: 실시간 탐지, 정기적 업데이트, 침입 방지 시스템 적용"
  },
  {
    "objectID": "cs/bn_13.html#암호화-기술",
    "href": "cs/bn_13.html#암호화-기술",
    "title": "제목",
    "section": "암호화 기술",
    "text": "암호화 기술\n\n대칭키 암호화(Symmetric Encryption): 동일 키로 암호화·복호화\n비대칭키 암호화(Asymmetric Encryption): 공개키-개인키 구조 사용\n기타 암호화: 치환 암호, 코드북 암호 등\n응용: VPN, SSL/TLS, 데이터 전송 보호, 클라우드 암호화"
  },
  {
    "objectID": "cs/bn_13.html#네트워크-보안-기술",
    "href": "cs/bn_13.html#네트워크-보안-기술",
    "title": "제목",
    "section": "네트워크 보안 기술",
    "text": "네트워크 보안 기술\n\n기술적 보안: 방화벽, WAF(Web Application Firewall), 제로 트러스트(Zero Trust), AI 기반 위협 분석\n관리적 보안: 보안 교육, 정책 수립\n실시간 모니터링: 네트워크 트래픽 분석, 침입 탐지 및 대응"
  },
  {
    "objectID": "cs/bn_13.html#사이버-보안-관리-체계cso",
    "href": "cs/bn_13.html#사이버-보안-관리-체계cso",
    "title": "제목",
    "section": "사이버 보안 관리 체계(CSO)",
    "text": "사이버 보안 관리 체계(CSO)\n\n범위: 네트워크, 클라우드, IoT, 데이터, 애플리케이션, 원격 접속 엔드포인트\n운영 요소: 인력, 프로세스, 기술\n기술 적용 사례: 제로 트러스트 아키텍처, 동작 기반 이상 탐지, 침입 방지 시스템, 클라우드 데이터 암호화"
  },
  {
    "objectID": "da/tm/tm_02_0.html",
    "href": "da/tm/tm_02_0.html",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "",
    "text": "웹 크롤링 개념 및 정적 크롤링 실습에 대해 다루고자 한다."
  },
  {
    "objectID": "da/tm/tm_02_0.html#정형-데이터",
    "href": "da/tm/tm_02_0.html#정형-데이터",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "1 . 정형 데이터",
    "text": "1 . 정형 데이터\n(Structured Data)\n일정한 형식을 갖춘 데이터로, 데이터베이스의 테이블처럼 행과 열로 정리된다.\n예: 엑셀, SQL 데이터베이스, 고객 정보(이름, 나이, 주소 등).\n룰세팅(Rule Setting) 데이터를 저장할 때 고정된 형식(테이블, 행/열 구조, 스키마 등)을 미리 정의하는 것.\n2 . 비정형 데이터 (Unstructured Data)\n형식이 일정하지 않아 체계적으로 저장하기 어려운 데이터.\n예: 텍스트(SNS 게시글, 이메일), 이미지, 동영상, 음성 데이터.\n주로 자연어 처리(NLP)나 텍스트 마이닝 등의 기법을 활용해 분석.\n3 . 반정형 데이터 (Semi-Structured Data)\n일정한 구조를 가지지만 완전히 정형화되지 않은 데이터. 태그나 특정한 형식(XML, JSON 등)을 포함하여 구조화가 가능하다.\n예: HTML, JSON, XML 파일, 로그 데이터.\n02 크롤링\n1 . 정적 크롤링 웹 페이지의 HTML 소스 코드를 직접 가져와서 필요한 데이터를 추출하는 방식.\n기본적으로, requests 라이브러리로 웹 페이지 HTML을 가져와 BeautifulSoup으로 데이터 추출한다.\n페이지 로딩 속도가 빠르고, 서버 부하가 적으나 JavaScript로 생성되는 데이터는 가져올 수 없다.\nHTML만으로 필요한 정보를 얻을 수 있다면 → 정적 크롤링이 유리하다.\n2 . 동적 크롤링 웹 브라우저를 실제로 실행하여 JavaScript로 로드되는 데이터까지 가져오는 방식.\n기본적으로, Selenium이나 Playwright 같은 브라우저 자동화 도구 사용한다.\nJavaScript 렌더링된 데이터를 포함하여 크롤링 가능하나, 속도가 느리고, 서버 부하가 높다.\nJavaScript로 데이터가 동적으로 로딩된다면 → 동적 크롤링이 필요하다.\n03 라이브러리 Jupyter Notebook에서 실행하는 명령어는 기본적으로 일반적인 Python 실행 환경에서도 동일하게 사용할 수 있다.\n(예: 터미널, 명령 프롬프트, 다른 IDE)\n1 . requests 파이썬에서 HTTP 요청을 보내고 응답을 받을 수 있는 라이브러리로, 주로 웹에서 데이터를 가져오거나 서버에 데이터를 전송하는 데 사용된다.\n웹사이트와 데이터를 주고받는 과정에서 사용되는 HTTP 프로토콜을 쉽게 다룰 수 있도록 도와준다.\n① GET 요청 - requests.get() 웹 페이지의 정보를 가져올 때 사용된다. 이는 브라우저에서 주소를 입력하고 페이지를 여는 것과 같은 동작이다.\n② POST 요청 - requests.post() 서버에 데이터를 전송할 때 사용된다. 회원가입, 로그인, 데이터 저장 등의 작업에서 활용된다.\n③ JSON 응답 처리 - response.json() 서버에서 JSON 형식의 데이터를 받으면, .json() 메서드를 사용하여 딕셔너리로 변환할 수 있다.\n2 . BeautifulSoup4 HTML/XML 문서를 파싱하여 원하는 데이터를 추출하는 라이브러리로, 문서를 구성하는 요소를 개별적인 구조(태그, 속성, 텍스트 등)로 나눈다.\n먼저, HTML 문서를 파싱하여 태그 간의 계층을 이해할 수 있는 트리 구조로 변환한다.\n이를 통해 특정 태그나 클래스에 접근할 수 있으며, CSS 선택자를 활용하여 원하는 요소를 쉽게 선택할 수 있다.\n또한, get_text() 메서드를 사용하면 태그 내부의 텍스트만 추출할 수 있어 데이터 정제 작업이 용이하다.\n3 . selenium 웹 브라우저를 자동으로 제어하는 라이브러리로, 클릭, 입력, 스크롤 등의 동작을 수행할 수 있다.\nJavaScript로 동적으로 변경되는 웹 페이지의 데이터도 가져올 수 있어 정적인 크롤링 방식보다 더 유연하다.\n이를 사용하려면 Chrome, Firefox 등 웹 브라우저에 맞는 드라이버가 필요하며, 이를 통해 실제 브라우저를 실행하고 조작할 수 있다.\n과거에는 웹 브라우저와 드라이버의 버전이 맞아야 했지만, 현재는 자동 업데이트 기능 덕분에 큰 문제가 없다.\n4 . pandas 데이터 분석 및 처리를 위한 필수 라이브러리로, CSV, Excel, JSON 등의 다양한 형식의 데이터를 데이터프레임으로 불러와 조작할 수 있다.\n또한, 결측값을 처리하거나 특정 조건에 따라 데이터를 필터링하고 정렬하는 등 정리 작업이 가능하다.\n뿐만 아니라, 데이터를 그룹화하여 분석할 수 있는 groupby() 기능, 기초 통계를 확인할 수 있는 describe() 메서드,\n특정 연산을 적용할 수 있는 apply() 메서드 등을 제공하여, 보다 효과적인 데이터 분석을 지원한다.\n04 정적 크롤링 다음은 네이버 뉴스 기사에 대해 정적 크롤링을 수행하는 코드이다.\n\n설치된 라이브러리를 불러오는 과정."
  },
  {
    "objectID": "da/tm/tm_03_1.html",
    "href": "da/tm/tm_03_1.html",
    "title": "3장: 네이버 카페 크롤링",
    "section": "",
    "text": "네이버 카페 크롤랑에 대해 다루고자 한다.\n합칠려면 모든 변수가 동일하게 들어가야 한다.\nfrom selenium import webdriver # 브라우저 자동화 from bs4 import BeautifulSoup as BS # html 내용 파싱 from selenium.webdriver.common.by import By # 다양한 방법으로 엘리먼트를 찾기 from selenium.webdriver.common.keys import Keys # Keys 클래스 가져오기(키보드 입력 제어)\nimport pandas as pd # 데이터 조작 및 분석 import datetime # 날짜와 시간 연산 import requests # Http 요청을 보내기 import pickle # 파이썬 객체 직렬화 import time # 코드 실행 속도 조절 import re # 정규 표현식 사용\n\n\n\ndriver = webdriver.Chrome() driver.get(‘https://search.naver.com/search.naver?ssc=tab.cafe.all&sm=tab_jum&query=%EB%85%B8%EC%9D%B8+%EB%B6%80%EC%96%91&nso=so%3Ar%2Cp%3Afrom20240301to20240325’)\n\n\n\ntitle_list = [] url_list = []\n\n검색 결과에서 모든 제목 링크 요소 가져오기 (스크롤 다운 포함)\nfor _ in range(2): # 5번 스크롤 내리기 (필요에 따라 조절 가능) driver.execute_script(“window.scrollTo(0, document.body.scrollHeight);”) # 스크롤 맨 아래로 이동 time.sleep(1) # 데이터 로딩을 기다리기 위해 1초 대기\ntitles = driver.find_elements(By.XPATH, “//*[@id='main_pack']/section/div[1]/ul/li/div/div[2]/div[2]/a”)\nfor i, title_element in enumerate(titles, start=1): # 1부터 카운트 시작 try: title_list.append(title_element.text) # 제목 추가 url_list.append(title_element.get_attribute(“href”)) # URL 추가\nexcept Exception as e:\n    print(f\"오류 발생: {e}\")  # 오류 메시지 출력\n\nif i % 10 == 0:  # 진행 상황 출력 (10개 단위)\n    print(f\"진행 중: {i}개 완료\")\nprint(“데이터 수집 완료!”) # 최종 완료 메시지 출력\n\n\n\n\n\n본문, 좋아요 수, 댓글 수, 댓글, 이미지 수, 영상 수를 저장할 리스트 초기화\nnew_doc = []\nlike_cnt = []\ncomment_cnt = []\ncomment_list = []\nimg_cnt = []\ndiv_cnt = []\n\n\n카페 글 크롤링\nfor i in range(len(url_list)): url_path = url_list[i] # URL 불러오기 driver.switch_to.window(driver.window_handles[0]) # 첫 번째 탭으로 이동 driver.execute_script(f”window.open(‘{url_path}’)“) # 새 탭에서 URL 실행 driver.switch_to.window(driver.window_handles[1]) # 두 번째 탭으로 이동\ntime.sleep(2)  # 2초 대기\n\ntry:\n    iframes = driver.find_elements(By.TAG_NAME, 'iframe')  # 카페 iframe 찾기\n    \n    if len(iframes) &gt; 0:\n        # iframe 전환\n        driver.switch_to.frame('cafe_main') # ifame의 첫부분\n        html = driver.page_source           # html 가져오고\n        soup = BS(html, 'html.parser')      # html 파싱하라\n\n        # 본문 추출\n        try:\n            a = soup.find('div', class_='article_viewer').get_text() # 값을 가져와라\n        except:\n            # 본문을 찾지 못할 경우\n            a = 'null'\n\n        # 좋아요 수 추출\n        try:\n            b = soup.find('em', class_='u_cnt _count').get_text()\n        except:\n            b = 'null'\n\n        # 댓글 수 추출\n        try:\n            c = soup.find('strong', class_='num').get_text()\n        except:\n            c = 'null'\n\n        # 댓글 추출\n        try:\n            d = \"\\n\".join([t.get_text() for t in soup.find_all('span', class_='text_comment')])\n        except:\n            d = 'null'\n\n        # 이미지 수 추출\n        e = len(soup.find_all('img', class_='se-image-resource'))\n\n        # 영상 수 추출\n        f = len(soup.find_all('div', class_='pzp-ui-dimmed pzp-dimmed pzp-pc_dimmed'))\n\n        # iframe에서 기본 컨텐츠로 전환\n        driver.switch_to.default_content()\n    else:\n        a, b, c, d, e, f = 'null', 'null', 'null', 'null', 0, 0  # iframe이 없을 경우 기본값\n\n    # 데이터 저장\n    new_doc.append(a)\n    like_cnt.append(b)\n    comment_cnt.append(c)\n    comment_list.append(d)\n    img_cnt.append(e)\n    div_cnt.append(f)\n\nexcept Exception as e:\n    # 오류 발생 시 기본값 저장\n    new_doc.append('null')\n    like_cnt.append('null')\n    comment_cnt.append('null')\n    comment_list.append('null')\n    img_cnt.append(0)\n    div_cnt.append(0)\n    print(f\"Error occurred at index {i}\")\n\nfinally:\n    # 현재 열린 탭 닫기\n    driver.close()\n    time.sleep(0.3)  # 0.3초 대기\n    driver.switch_to.window(driver.window_handles[0])  # 첫 번째 탭으로 복귀\n\n# 매 10번째 URL마다 진행 상황 출력\nif (i + 1) % 10 == 0:\n    print(f\"진행 상황: {i + 1}/{len(url_list)}\")\n\n\n\n\n\n크롤링 데이터를 데이터프레임으로 변환\nraw_data = pd.DataFrame() # 초기화 raw_data[‘title’] = title_list # 제목 리스트 raw_data[‘doc’] = new_doc # 본문 리스트 raw_data[‘like’] = like_cnt # 좋아요 수 리스트 raw_data[‘comment_cnt’] = comment_cnt # 댓글 수 리스트 raw_data[‘comment_list’] = comment_list # 댓글 리스트 raw_data[‘img’] = img_cnt # 이미지 수 리스트 raw_data[‘div’] = div_cnt # 영상 수 리스트 raw_data[‘ch’] = ‘naver’ # 채널 정보 raw_data[‘ch2’] = ‘cafe’ # 채널 정보 (세부)\n\n\n데이터프레임을 pickle 파일로 저장\nfile_path = “C:/Users/jkl12/텍스트마이닝/” # 슬래시 사용 with open(file_path + “노인부양cafe.pkl”, “wb”) as f: pickle.dump(raw_data, f)\n\n\n크롬 드라이버 종료\ndriver.quit()\n\n\n저장된 pickle 파일을 불러오기\nwith open(file_path + “노인부양cafe.pkl”, “rb”) as f: temp_file = pickle.load(f)\n\n\n데이터프레임을 CSV 파일로 저장\ntemp_file.to_csv(file_path + “노인부양cafe.csv”, index=False, encoding=“utf-8-sig”)"
  },
  {
    "objectID": "da/tm/tm_05_0.html",
    "href": "da/tm/tm_05_0.html",
    "title": "5장: 텍스트 데이터 마이닝",
    "section": "",
    "text": "텍스트 데이터 마이닝에 대해 다루고자 한다.\n\n01 텍스트 데이터 마이닝\n광도들이 보석을 캐는 과정.\n노인 부양에 관한 가설 세우기.\n텍스트 데이터 전처리\n텍스트 마이닝의 핵심적인 시작 단계로, 데이터의 품질을 높이기 위한 여러 과정으로 구성된다.\n먼저, 데이터 수집 후에는 한글화, 결측치 처리, 단어 및 형태소 분석 등의 전처리를 진행합니다.\n한글화는 텍스트에서 한글 이외의 문자를 제거하거나 블랭크 처리하여 분석에 적합한 형태로 만드는 과정입니다.\n이때 특수기호는 유지하며 한글만 남기는 방식으로 필터링한다.\n이렇게 정제된 데이터는 피클(pickle) 파일 형태로 저장하며, 작업 시에는 파일 경로와 파일명을 명확히 지정해야 한다.\n예를 들어, 보험연수원에서 제공한 연금 관련 텍스트 데이터를 수년간 6개 채널에서 크롤링해 5개의 피클 파일로 저장한 사례가 있다.\n이 파일들은 병합한 후 인덱스를 지정해 다시 저장하며, 저장 경로는 작업 환경에 맞춰 지정해야 한다.\nimport pickle import pandas as pd import itertools import os import re\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n이후 분석을 위해 저장된 피클 파일을 다시 로드하여 활용한다.\nf = open(file_path + ‘total_doc.pkl’, “rb”) # 데이터 불러오기 data = pickle.load(f) f.close()\ndata # 문서 전체의 명사 리스트 확보\n\n단어들의 빈도 데이터 정제 과정에서는 불필요한 기호나 단어를 제거하고, 결측값은 일괄 삭제하며 인덱스를 재정비합니다. 예컨대 ‘샵’, ’펀드’와 같은 특정 요소는 정제 대상이 되며, 본문 일부 삭제 시 데이터의 일관성을 유지하기 위해 인덱스를 재조정합니다. 정제는 원본을 복사한 후 진행하는 것이 안전합니다.\n\n형태소 분석은 한글 데이터 분석에 필수적인 과정이며, 이는 텍스트를 의미 단위로 나누어주는 작업입니다. 형태소 분석을 위해서는 Java 설치와 버전 확인, 인터넷 환경 설정이 필요하며, 대표적으로 사용하는 라이브러리는 코모란(Komoran)입니다. 코모란은 GitHub에서 설치 가능하며, 설치 후 환경 변수 설정 및 보안 설정 등을 완료한 후 사용합니다.\n형태소 분석을 통해 본문에서 추출된 단어들은 토큰화 과정을 거쳐 리스트 형태로 정리됩니다. 이때 불용어(의미 없는 단어)를 제거하기 위해 스탑워드 리스트를 활용하며, 불용어와 일치하는 형태소는 제외합니다. 최종적으로 정제된 단어 리스트와 형태소 리스트는 데이터프레임 형태로 저장하고, 이를 다시 파일로 변환하여 보관합니다.\nimport itertools\n\n\n제목 리스트 언패킹\ntitle_noun = list(itertools.chain(*data[‘title_token_noun’])) print(title_noun[:15]) # 앞에서 5개 요소 출력\n\n\n본문 리스트 언패킹\ndoc_noun = list(itertools.chain(*data[‘doc_token_noun’])) print(doc_noun[:15])\n\n\n댓글 리스트 언패킹\ncomment_noun = list(itertools.chain(*data[‘comment_token_noun’])) print(comment_noun[:15])\n\n제목, 본문, 댓글 데이터 빈도\n\n\n\n빈도를 카운트하는 라이브러리\nfrom collections import Counter\ntitle_count = Counter(title_noun) # 리스트 원소의 개수가 계산됨 title_top = dict(title_count.most_common(100)) # 상위 100개 출력하기 title_top\n#—\ndoc_count = Counter(doc_noun) # 리스트 원소의 개수가 계산됨 doc_top = dict(doc_count.most_common(100)) # 상위 100개 출력하기 doc_top\n\n\n—\ncomment_count = Counter(comment_noun) # 리스트 원소의 개수가 계산됨 comment_top = dict(comment_count.most_common(100)) # 상위 100개 출력하기 comment_top\n\nimport csv\n\n\n\n제목별 빈도수 저장\nwith open(file_path + ‘\\title_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in title_top.items(): w.writerow([k, v]) # k, v -&gt; 딕셔너리의 key, value # 즉, 단어와 빈도\n\n\n본문별 빈도수 저장\nwith open(file_path + ‘\\doc_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in doc_top.items(): w.writerow([k, v])\n\n\n댓글별 빈도수 저장\nwith open(file_path + ‘\\comment_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in comment_top.items(): w.writerow([k, v])\n4 . 워드 클라우드 정제된 단어들을 기반으로 워드 클라우드를 그릴 수 있다.\n워드 클라우드는 단어의 빈도를 시각적으로 표현하는 기법으로, 가장 자주 등장한 단어를 강조하는 방식으로 표현된다.\n이때 Counter의 most_common 함수를 이용해 빈도를 계산하고, 원하는 형태의 마스크 이미지(예: 사람 모양, 네모형 등)를 적용해 시각화할 수 있다.\n백그라운드 설정, 폰트 다운로드, 컬러 맵 지정 등 세부 설정도 가능하며, 정보 전달력이 높은 네모 형태를 권장한다.\nimport matplotlib.pyplot as plt from wordcloud import WordCloud\nfont_path = r”C:.otf”\n\n\n워드클라우드 생성\nwordcloud = WordCloud( font_path=font_path, background_color=‘white’, colormap=“Accent”, width=600, height=400 ).generate_from_frequencies(doc_top)\nplt.figure(figsize=(8, 10)) plt.imshow(wordcloud) plt.axis(‘off’) plt.show()\nTF - IDF 결과 텀은 단어, 단어들의 빈도, 워드클라우드도 사용 이것이 결합된 형태가 TF - IDF임\n단어 뿐만아니라 문서 전체도 고려 한다.\n\n워드 클라우드의 문제점, 본문에서 자주 등장하는 것이 높은 가중치를 문맥에 따라서는 다른 의미(완전히 다른 단어)를 가질 수 있음 또한, 그 단어는 낮은 가중치일 수도 있음\n\n즉, 데이터가 진짜 말하고자 하는 것을 찾는 인사이트에서는 부적합할 수 있음.\n단순한 빈도만으로는 판별하기 애매하다.\n\n문서의 길이의 따라 사용 어휘의 중요도가 바뀔 수 있다.\n\n특정 단어가 포함된 문서가 몇 개가 되느냐? d=문서, t=단어, 특정 단어가 나타나는 문서수의 역수(역수의 로그를 취해준 개념)\n각 문서에 포함된 단어 카운트 - DTM 행렬\n이는 특정 문서에서만 많이 나오지만 전체 문서에서는 적은 단어와 전체적으로 많이 나오게 분포하지만 개별 문서에서는 적게 나오는 단어 2가지가 존재하고 그 중 후자가 더 중요한 가중치를 가진다.\n로그를 취해서 소수점으로 나오고 TF-IDF를 곱한다. 이떄 여기저기 많이 나오면 상대적인 가중치가 비슷하고 낮게 나옴\n먼저, 단어들을 문자열로 만들어 주어야 한다. 명사들의 문자열 리스트 만들고, sklearn 가져오기.\n\n\n명사들의 문자열 구성\ndoc_noun = [] for i in range(0, len(data[“doc_token_noun”])): doc_noun.append(’ ’.join(data[‘doc_token_noun’][i])) # 각 문서의 명사들을 str로 연결\n너무 희박한 것들은 제외할 수도 있다.(최소치, 최대치)\n\n\n텍스트 문서 모음을 단어 tf-idf 행렬로 변환\nfrom sklearn.feature_extraction.text import TfidfVectorizer vec_y = TfidfVectorizer(min_df=0.01, max_df=0.95)\n\n\n문서의 1% ~ 95%로 나타나는 단어들을 고려\nY = vec_y.fit_transform(doc_noun) print(Y)\n10번째 문서 21번째 단어이다. +1\nk개의 평균을 갖는다는 것. 비지도, 타겟X 타겟이 있어, 예측을 시도하는 지도학습과는 달리 어떤 패턴을 가진 그룹이 있는지를 보려는 것.\n구조화, 군집 분석을 시도하는 것.\n임의의 k개의 중심점을 지정, 각각의 개별 데이터를 가장 가까운 곳으로 할당시킴 이 거리를 유클리드의 거리를 한다.\n그 그룹이 생성되면 그 그룹 안에서 새로운 중심점을 찾음 그 중심점을 가지고 위의 일련의 과정을 더 이상 중심점이 움직이지 않을 때까지 반복한다.\n합리적인 k를 찾는 방법 - 대표적으로 엘보우 기법 팔굽치 처럼 꺾이는 지점을 k값으로 정하는 것.\n2개에서 6개 정도가 타당하다 너무 적거나 많으면 의미가 없음.\n거리에 대한 SSE 손실함수 구하는 과정 10번 반복\nimport os os.environ[“OMP_NUM_THREADS”] = “2” # 선택 사항\nimport matplotlib.pyplot as plt from sklearn.cluster import KMeans\ndef elbow(X): sse = []\nfor i in range(1, 10):\n    km = KMeans(n_clusters=i, n_init=10, \n                algorithm='lloyd', random_state=0)\n    km.fit(X)\n    sse.append(km.inertia_)\n    print(i)\n\nplt.plot(range(1, 10), sse, marker='o')\nplt.xlabel('K')\nplt.ylabel('SSE')\nplt.xticks(range(1, 10))\nplt.show()\nelbow(X)\nconda install -c conda-forge pyldavis\nmodel_y = KMeans(n_clusters=2, algorithm=‘lloyd’, random_state=0) # 모델 정의 model_y.fit(Y) # 모델 학습\nprint(“Doc Top terms for each cluster”) order_centroids = model_y.cluster_centers_.argsort()[:, ::-1] # 클러스터 중심 정렬 terms_y = vec_y.get_feature_names_out() # 단어 목록\nfor i in range(2): # 두 개의 클러스터에 대해 반복 print(“Cluster %d:” % i) for ind in order_centroids[i, :50]: # 각 클러스터의 상위 50개 단어 출력 print(‘%s’ % terms_y[ind]) print(‘’)\n데이터 프레임의 형식\nimport pandas as pd\n\n\n클러스터 중심에서 가장 중요한 단어 인덱스 정렬\norder_centroids = model_y.cluster_centers_.argsort()[:, ::-1] terms_y = vec_y.get_feature_names_out()\n\n\n각 클러스터의 상위 50개 단어 수집\ntop_terms = {}\nfor i in range(2): # 클러스터 수만큼 반복 top_terms[f’Cluster {i}’] = [terms_y[ind] for ind in order_centroids[i, :50]]\n\n\nDataFrame으로 변환\ndf_top_terms = pd.DataFrame(top_terms) df_top_terms\n1 . 제목2 더 나아가 워드 클러스터링과 토픽 모델링을 통해 텍스트의 의미 구조를 분석할 수 있다.\n워드 클러스터링은 문서 내 단어 빈도를 기반으로 단어들을 군집화하는 방법으로, TF (Term Frequency) 및 IDF (Inverse Document Frequency) 값을 활용해 중요 단어를 판단합니다.\n이후 유클리디안 거리 기반의 K-means와 같은 알고리즘으로 최적의 군집을 형성합니다.\n토픽 모델링은 문서 집합에서 주제를 추출하는 기법으로, LDA(Latent Dirichlet Allocation) 같은 확률 기반 모델을 활용합니다.\n혼잡도 그래프와 일관성 지표 등을 통해 토픽 수를 결정하고, 각 토픽의 특징을 평가합니다. 이를 통해 시스템화된 분석 체계를 구축할 수 있습니다."
  },
  {
    "objectID": "da/tm/tm_06_1.html",
    "href": "da/tm/tm_06_1.html",
    "title": "6장: 감성 분석",
    "section": "",
    "text": "감성분석에 대해 다루고자 한다.\n01 감성 사전 다운\n1 . KNU\nGitHub - park1200656/KnuSentiLex: KNU(케이앤유) 한국어 감성사전\nKNU(케이앤유) 한국어 감성사전. Contribute to park1200656/KnuSentiLex development by creating an account on GitHub.\ngithub.com import json\nfile_path = r’C:-master-master_info.json’\nwith open(file_path, ‘r’, encoding=‘utf-8’) as f: json_data = json.load(f)\n\n단어 리스트 생성\nword_list = [] for item in json_data: # 리스트 반복 word_txt = item[‘word’] word_list.append(word_txt)\n\n\n결과 출력 (앞 10개만 보기)\nprint(word_list[:10])\n2 . KoreanSentimentAnalyzer\nGitHub - mrlee23/KoreanSentimentAnalyzer: 한국어 감성 분석기\n한국어 감성 분석기. Contribute to mrlee23/KoreanSentimentAnalyzer development by creating an account on GitHub.\ngithub.com import pandas as pd\n\n\n파일 경로\nfile_path = r’C:-master-master.csv’\n\n\nCSV 파일 읽기\nsenti_df = pd.read_csv(file_path, encoding=‘utf-8’) # 또는 encoding=‘cp949’ senti_df.head()\nimport pickle\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n\n\ndoc_topic과 comment_topic이 포함된 파일\nf = open(file_path + ‘topic_doc.pkl’, “rb”) # 데이터 불러오기 data = pickle.load(f) f.close()\ndata # 문서 전체의 명사 리스트 확보\nKNU 감성사전을 이용해서 텍스트 데이터에 감정 점수를 부여하는 Python 스크립트입니다. 각 텍스트가 긍정적인지, 부정적인지, 중립적인지를 파악하기 위해 사용됩니다.\nimport json import pandas as pd from tqdm import tqdm\n\n\n감정분석 JSON 데이터 (KNU 감성사전) 불러오기\nfile_path = r’C:-master-master’\nwith open(file_path + r’_info.json’, encoding=‘UTF-8’) as json_file: sentiword = json.load(json_file)\n\n\n감성 단어 리스트 및 점수 초기화\ns_word = [] values = [] score = []\n\n\n평균 계산 함수\ndef average(lst): return sum(lst) / len(lst)\n텍스트에서 감성 단어 찾고 점수 계산\n\n\n감성 점수 계산\nfor word in tqdm(data[‘doc’]): temp_s_word = [] # 본문에서 가져옴 temp_value = []\nfor s in sentiword:\n    if s['word'] in word:\n        temp_s_word.append(s['word'])\n        temp_value.append(int(s['polarity']))\n\ns_word.append(temp_s_word)\nvalues.append(temp_value)\n\nif len(temp_value) &gt; 0:\n    score.append(average(temp_value))\nelse:\n    score.append(0)\n\n\n결과 삽입\ndata = data.assign(sentiword=s_word, values=values, score=score) data\n가상 공간 안에서만 있는 것, 이를 저장 함.\nimport pickle import pandas as pd\n\n\n저장\nfile_path = r’C:\\’ with open(file_path + “total_docs_KNU.pkl”, “wb”) as f: pickle.dump(data, f)\n\n\n불러오기\nwith open(file_path + “total_docs_KNU.pkl”, “rb”) as f: ff = pickle.load(f)\n\n\n데이터프레임 복원\ntotal_docs = pd.DataFrame() total_docs[‘doc’] = ff[‘doc’] total_docs[‘doc_token_noun’] = ff[‘doc_token_noun’] total_docs[‘doc_topic’] = ff[‘doc_topic’] total_docs[‘comment_topic’] = ff[‘comment_topic’] total_docs[‘sentiword’] = ff[‘sentiword’] total_docs[‘values’] = ff[‘values’] total_docs[‘score’] = ff[‘score’]\ntotal_docs\ndoc_token_noun의 모든 단어가 감성 단어가 아니다. 그들 중 감성 단어를 sentiword로 불러온 것.\nfrom wordcloud import WordCloud import matplotlib.pyplot as plt\n\n\n폰트 경로 (Windows용 예시 - 나눔고딕)\nfont_path = r”C:.otf”\n\n\n토픽 개수만큼 반복\nnum_topics = total_docs[‘doc_topic’].nunique()\nfor topic_num in range(num_topics): # 해당 토픽의 문서 필터링 topic_docs = total_docs[total_docs[‘doc_topic’] == topic_num]\n# 토큰 리스트를 하나로 합치기 (flatten)\nall_tokens = sum(topic_docs['doc_token_noun'], [])\n\n# 문자열로 변환 (공백으로 연결)\ntext = ' '.join(all_tokens)\n\n# 워드클라우드 생성\nwordcloud = WordCloud(font_path=font_path, background_color='white', width=800, height=400).generate(text)\n\n# 시각화\nplt.figure(figsize=(10, 8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(f\"Topic {topic_num} Word Cloud\", fontsize=16)\n이 코드는 감성 점수(score)를 기준으로 각 토픽(doc_topic)에 대해 감정 분포를 분류하고 있습니다.\nif score &gt; 0.3: # 긍정 elif -0.3 &lt;= score &lt;= 0.3: # 중립 else: # 부정 여기서 “0.3”과 “-0.3”이라는 기준은 사용자가 임의로 정한 값 (threshold)입니다. → 즉, 이 기준이 정해진 절대값이 아니라, → 분석 목적에 따라 조정해야 하는 값이에요.\n🧠 사용자가 결정해야 할 것들 score의 값 범위가 어떻게 구성되어 있는가? 감성 점수가 -2 ~ 2인지, -1 ~ 1인지, -5 ~ 5인지 먼저 확인해야 합니다. 0.3이 의미 있는 경계값인가? 감성 점수의 분포가 대부분 -0.1 ~ 0.1이라면, 0.3은 너무 높은 기준일 수 있습니다. 반대로 감성 점수 범위가 크다면, 0.3은 너무 낮은 기준일 수 있죠. 목표에 따라 기준이 달라질 수 있음 예를 들어: 마케팅 분석이라면 조금만 긍정적이어도 긍정으로 간주 감정 민감도 분석이라면 더 엄격한 기준 적용 필요\n코드에서 senti_0 = [0, 0, 0, 0, 0, 0] 의미 이 리스트는 특정 토픽(topic 0, topic 1 등)에 대한 감성 점수 분포를 저장하려고 만든 것입니다. 즉, 각 인덱스가 감정의 강도나 구간을 나타냅니다.\n0 매우 긍정 1 긍정 2 중립 3 부정 4 매우 부정 5 기타 / 미정 (혹은 사용자 정의) 그런데 본문 코드에서는 실제로는 0, 2, 4만 사용되고 있죠? 이건 중간 단계일 가능성도 있고, 나중에 더 구체적인 등급으로 확장하기 위해 미리 6칸 확보해둔 것일 수도 있어요.\n🧠 핵심 포인트 senti_0, senti_1은 토픽별 감정 분포를 담기 위한 그릇입니다. 토픽이 더 늘어나면? → senti_2, senti_3, … 식으로 계속 만들어야겠죠. 또는 다음처럼 딕셔너리 구조로 더 깔끔하게 관리할 수도 있어요: senti_dict = {i: [0, 0, 0, 0, 0, 0] for i in total_docs[‘doc_topic’].unique()} ✅ 요약 항목 설명\nsenti_0, senti_1 토픽별 감정 분포 저장용 리스트 길이 6 감정 강도 6단계로 분류하려는 구조 (0~5) 사용자 정의 가능 목적에 따라 개수, 의미를 직접 설정 필요하시면 이 구조를 자동화해서 n개의 토픽에 대해 감정 분포를 계산하는 코드도 만들어드릴 수 있어요. 원하시나요?\n긍정 / 중립 / 부정의 빈도와 비율을 토픽별로 확인하려는 거군요.\n각 토픽(topic)마다 긍정: score &gt; 0.3 중립: -0.3 ≤ score ≤ 0.3 부정: score &lt; -0.3 해당 범주의 빈도수와 비율(%)을 구하기\n감정 빈도 doc_topic negative neutral positive\n0 12 56 32 1 8 30 62\n감정 비율 (%) doc_topic negative neutral positive\n0 12.0 56.0 32.0 1 8.0 30.0 62.0\n\n\n토픽별 감성 점수 분류 리스트 초기화 (긍정, 중립, 부정)\nsenti_0 = [0, 0, 0, 0, 0, 0] senti_1 = [0, 0, 0, 0, 0, 0] senti_2 = [0, 0, 0, 0, 0, 0] senti_3 = [0, 0, 0, 0, 0, 0]\nfor i in range(len(total_docs)): topic = total_docs[‘doc_topic’].iloc[i] score = total_docs[‘score’].iloc[i]\nif topic == 0:\n    if score &gt; 0.3:\n        senti_0[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_0[2] += 1\n    else:\n        senti_0[4] += 1\n\nelif topic == 1:\n    if score &gt; 0.3:\n        senti_1[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_1[2] += 1\n    else:\n        senti_1[4] += 1\nfor i in range(len(total_docs)): topic = total_docs[‘comment_topic’].iloc[i] score = total_docs[‘score’].iloc[i]\nif topic == 0:\n    if score &gt; 0.3:\n        senti_2[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_2[2] += 1\n    else:\n        senti_2[4] += 1\n\nelif topic == 1:\n    if score &gt; 0.3:\n        senti_3[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_3[2] += 1\n    else:\n        senti_3[4] += 1\n지금 작성하신 코드는 토픽별 감정 분포 리스트에서 비율(%)을 1칸씩 띄워서 저장하는 방식입니다.\n📦 구조 요약 senti_0 = [긍정_빈도, 긍정_비율, 중립_빈도, 중립_비율, 부정_빈도, 부정_비율]\n인덱스 0, 2, 4: 빈도수 (count) 인덱스 1, 3, 5: 비율 (ratio, 혹은 percentage) 🔁 반복문 설명 for i in range(1, 7, 2): # i는 1, 3, 5 i-1 → 현재 비율을 계산할 빈도 인덱스 i → 비율을 저장할 인덱스 분모는 전체 감정의 합: 긍정 + 중립 + 부정 즉, 예를 들어:\nsenti_0[1] = senti_0[0] / (senti_0[0] + senti_0[2] + senti_0[4]) 이건 긍정 비율, 그다음 senti_0[3]은 중립 비율, senti_0[5]는 부정 비율이 되는 식입니다.\n\n\n감성 클래스별 비율 계산 (분모가 0일 경우 예외 처리 추가)\nfor i in range(1, 7, 2): if (senti_0[0] + senti_0[2] + senti_0[4]) != 0: senti_0[i] = senti_0[i-1] / (senti_0[0] + senti_0[2] + senti_0[4]) else: senti_0[i] = 0 # 분모가 0이면 비율을 0으로 설정\nif (senti_1[0] + senti_1[2] + senti_1[4]) != 0:\n    senti_1[i] = senti_1[i-1] / (senti_1[0] + senti_1[2] + senti_1[4])\nelse:\n    senti_1[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\nif (senti_2[0] + senti_2[2] + senti_2[4]) != 0:\n    senti_2[i] = senti_2[i-1] / (senti_2[0] + senti_2[2] + senti_2[4])\nelse:\n    senti_2[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\nif (senti_3[0] + senti_3[2] + senti_3[4]) != 0:\n    senti_3[i] = senti_3[i-1] / (senti_3[0] + senti_3[2] + senti_3[4])\nelse:\n    senti_3[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\n\n토픽별 감성 비율 데이터프레임 생성\ngraph = pd.DataFrame( [senti_0, senti_1, senti_2, senti_3], index=[‘topic1’, ‘topic2’, ‘topic3’, ‘topic4’], columns=[[‘긍정’, ‘긍정’, ‘중립’, ‘중립’, ‘부정’, ‘부정’], [‘빈도’, ‘비율’, ‘빈도’, ‘비율’, ‘빈도’, ‘비율’]] )\ngraph\n🔍 왜 워드클라우드와 감정 점수(혹은 분류) 결과가 다를 수 있는가? 1. 워드클라우드는 감성 단어 필터 없이 모든 단어 사용 일반적으로 워드클라우드는 특정 토픽에서 자주 등장한 단어의 빈도만을 시각화합니다. 이 과정에서 감성 사전에 없는 중립 단어, 불용어(의미 없는 단어)들도 포함될 수 있습니다. 따라서 시각적으로 중요한 단어처럼 보여도 감성 점수 계산에서는 무시될 수 있습니다. 2. KNU 감성사전 기반 감정 점수는 ’등록된 감성 단어’만 사용 예: 좋다, 싫다, 기쁘다, 화나다 등만 감성 점수로 환산됨. 감성 사전에 없는 단어는 아무리 많이 나와도 score에 기여하지 않음. 3. 토픽의 특성과 감성 단어 간 연관성 결여 예를 들어, 주제는 부정적인 사건이라도 직접적으로 부정 단어(예: “나쁘다”, “불편하다”)가 없을 수 있음. 이 경우 토픽 자체는 부정적으로 보이지만, 감성 점수는 중립 혹은 긍정이 나올 수 있습니다. 4. 토픽 내 감성 단어 비율이 낮은 경우 감성 점수를 계산할 때 사용하는 감성 단어 수가 전체 단어에 비해 매우 적다면, score의 분포도 좁거나 왜곡될 수 있습니다. 이로 인해 score는 0 근처로 몰리거나, 예외적으로 높은 감성 단어 하나에 과도하게 영향받을 수 있습니다. ✅ 요약 요소특징감성 점수에 반영됨? 워드클라우드 주요 단어 빈도가 높은 모든 단어 ❌ 감성 단어만 반영됨 감정 점수(score) 감성사전에 있는 단어 기반 ✅ 해당 단어만 반영됨 감정 판단 정확도 단어 수, 감성 단어 존재 여부에 민감 상황에 따라 다름\n💡 개선 팁 워드클라우드 만들 때 감성 단어만 필터링해서 시각화할 수도 있습니다. python 복사편집 sentiment_words = [s[‘word’] for s in sentiword] topic_words = [word for word in topic_docs if word in sentiment_words] 감성 점수 외에도 TF-IDF 기반 상위 감성 단어 추출도 좋은 방법입니다. 감성 점수 분포와 함께 워드클라우드 결과를 비교 분석하면 더 풍부한 인사이트를 얻을 수 있습니다.\n01 감성 사전 다운\n1 . NRC\nNRC Emotion Lexicon\nImpact Some notable ways in which the NRC Emotion Lexicon has made impact include: First of its kind: It was the first word-emotion association lexicon, with entries for eight basic emotions as well as positive and negative sentiment. It still remains the\nsaifmohammad.com\n이 코드는 NRC 감성사전 (Korean NRC Emotion Lexicon)을 기반으로 각 문서의 감정값을 계산하는 과정입니다. 간단히 말하면, 문서에 등장하는 감성 단어를 찾아서 해당 감정 점수를 누적하는 구조입니다.\n아래에 코드의 의미를 단계별로 설명드리겠습니다:\n🔢 코드 설명 for i in range(1, len(nrc)): # NRC 감성사전의 각 단어에 대해 반복 nrc: NRC 감성사전을 담은 DataFrame입니다. nrc[‘Korean Word’]: 감성사전에 있는 한국어 단어. range(1, len(nrc)): 아마 첫 번째 행(헤더 또는 불필요한 데이터)을 생략하고자 1부터 시작한 것 같습니다. if nrc[‘Korean Word’][i] in word: 현재 문서(word)에 감성사전의 단어가 포함되어 있는지 확인. if len(nrc[‘Korean Word’][i]) &gt; 1: 글자 수가 1자인 경우(ex. “다”, “게”)는 보통 의미가 불분명하거나 너무 일반적이라 제외. 따라서 두 글자 이상인 감성 단어만 사용. temp_s_word.append(nrc[‘Korean Word’][i]) 해당 감성 단어를 temp_s_word 리스트에 저장 (이 문서에서 발견된 감성 단어 목록). b = list(map(int, nrc.iloc[i, 1:11].tolist())) nrc.iloc[i, 1:11]: 해당 단어에 대한 감정 점수들 (예: 긍정, 부정, 분노, 기쁨 등 10가지 감정). map(int, …): 감정 점수들이 문자열로 되어 있다면 정수로 변환. 결과적으로 b는 해당 단어의 10개 감정 점수 리스트. temp_value = [x + y for x, y in zip(temp_value, b)] temp_value: 현재 문서에서 감정 점수를 누적하는 리스트. b를 더해가며 문서 전체의 감정 점수를 계산. 🔍 이 코드의 목적 NRC 감성사전 기반 다중 감정 분석입니다. 단순히 긍·부정 점수만 계산하는 것이 아니라, 여러 감정 카테고리(기쁨, 슬픔, 분노 등)의 누적 점수를 구해서 문서의 감정 프로파일을 생성합니다."
  },
  {
    "objectID": "cg-hw.html",
    "href": "cg-hw.html",
    "title": "cg-hw",
    "section": "",
    "text": "DB 개론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n운영체제(OS)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n통신 기술의 기초\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n컴퓨터 그래픽스\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n팅커캐드 사용법\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDB 개론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 5, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cs/bn/bn_12.html",
    "href": "cs/bn/bn_12.html",
    "title": "제목",
    "section": "",
    "text": "OSI 모델의 표현 계층(Presentation Layer)은 데이터의 변환, 암호화, 압축 등 상위 애플리케이션 간 데이터 호환성을 보장하는 역할을 수행한다. 이를 통해 하드웨어 및 운영체제 독립성을 유지하고, 다양한 표현 방식을 표준화할 수 있다.\n\n\n응용 계층은 최종 사용자와 직접 상호작용하며, 데이터의 구조화, 처리, 전송을 담당한다.\n\n\n\nHTML: 웹 콘텐츠 구조화, 확장성 및 호환성 제공\nHTML5: 멀티미디어 통합, Canvas API, 반응형 디자인, 로컬 스토리지, WebSocket 지원으로 현대 웹의 표준화 구현\n\n\n\n\n\n플랫폼 독립적이며 언어 제약이 없음\n요청 시마다 프로세스 생성으로 성능 저하 발생 가능\n유지보수 어려움과 보안 취약점 존재\n\n\n\n\n\n구조화된 데이터 저장 및 전송에 최적화\n사용자 정의 태그 지원, 가독성 우수, 플랫폼 독립성 확보"
  },
  {
    "objectID": "cs/bn/bn_12.html#osi-표현-계층과-응용-계층",
    "href": "cs/bn/bn_12.html#osi-표현-계층과-응용-계층",
    "title": "제목",
    "section": "",
    "text": "OSI 모델의 표현 계층(Presentation Layer)은 데이터의 변환, 암호화, 압축 등 상위 애플리케이션 간 데이터 호환성을 보장하는 역할을 수행한다. 이를 통해 하드웨어 및 운영체제 독립성을 유지하고, 다양한 표현 방식을 표준화할 수 있다.\n\n\n응용 계층은 최종 사용자와 직접 상호작용하며, 데이터의 구조화, 처리, 전송을 담당한다.\n\n\n\nHTML: 웹 콘텐츠 구조화, 확장성 및 호환성 제공\nHTML5: 멀티미디어 통합, Canvas API, 반응형 디자인, 로컬 스토리지, WebSocket 지원으로 현대 웹의 표준화 구현\n\n\n\n\n\n플랫폼 독립적이며 언어 제약이 없음\n요청 시마다 프로세스 생성으로 성능 저하 발생 가능\n유지보수 어려움과 보안 취약점 존재\n\n\n\n\n\n구조화된 데이터 저장 및 전송에 최적화\n사용자 정의 태그 지원, 가독성 우수, 플랫폼 독립성 확보"
  },
  {
    "objectID": "cs/bn/bn_12.html#웹-시스템-구조와-아키텍처",
    "href": "cs/bn/bn_12.html#웹-시스템-구조와-아키텍처",
    "title": "제목",
    "section": "웹 시스템 구조와 아키텍처",
    "text": "웹 시스템 구조와 아키텍처\n웹 시스템의 주요 목표는 데이터 처리 효율, 확장성 확보, 보안 및 안정성 유지이다.\n\n2. 핵심 구성 요소\n\n클라이언트(Client): 사용자 요청 생성 및 화면 표시\n웹 서버(Web Server): HTTP 요청 처리, 정적 콘텐츠 제공\n애플리케이션 서버(App Server): 비즈니스 로직 처리\n데이터베이스(DB): 데이터 저장 및 관리\n\n\n\n2.1 확장 및 최적화 구성 요소\n\n로드 밸런서(Load Balancer): 서버 부하 분산\n캐시 서버(Cache Server): 반복 데이터 빠른 접근 제공\nCDN(Content Delivery Network): 글로벌 데이터 전송 최적화\n보안 시스템(Security Layer): 접근 제어 및 위협 방어\n\n\n\n2.2 아키텍처 진화\n단일 계층 → 2계층 → 3계층 → 마이크로서비스 아키텍처로 발전, 모듈화 및 확장성 강화"
  },
  {
    "objectID": "cs/bn/bn_12.html#웹-요청-및-응답-흐름",
    "href": "cs/bn/bn_12.html#웹-요청-및-응답-흐름",
    "title": "제목",
    "section": "웹 요청 및 응답 흐름",
    "text": "웹 요청 및 응답 흐름\n\n3. DNS(Domain Name System)\n\n필요성: 기억하기 어려운 IP 주소를 도메인 이름으로 변환, 유연한 변경, 효율적 운영\n구조: 최상위 루트 → TLD → 세컨드 레벨 도메인 → 서브 도메인 → 호스트, 계층적 트리 구조\n쿼리 방식: Recursive, Iterative\n활용 사례: 웹 브라우징, 이메일, CDN, 부하 분산, 보안"
  },
  {
    "objectID": "cs/bn/bn_12.html#이메일과-ftp-프로토콜",
    "href": "cs/bn/bn_12.html#이메일과-ftp-프로토콜",
    "title": "제목",
    "section": "이메일과 FTP 프로토콜",
    "text": "이메일과 FTP 프로토콜\n\n4. 이메일 시스템\n\n흐름: 메일 작성 → 송신 → 라우팅 → 저장 → 수신\n프로토콜: SMTP(송신), POP3/IMAP(수신)\nMIME(Multipurpose Internet Mail Extensions): 다양한 언어, 이미지, 동영상 지원\n\n\n\n5. POP3 vs IMAP\n\nPOP3: 서버에서 메일 다운로드 후 로컬 저장, 단순 구조\nIMAP: 서버에서 메일 동기화 및 관리, 멀티 디바이스 지원\n\n\n\n6. FTP(File Transfer Protocol)\n\n동작 모드: 액티브 모드, 패시브 모드\n보안 강화 프로토콜: FTPS, SFTP, FTP over SSL/TLS 등 안전한 파일 전송 지원"
  },
  {
    "objectID": "cs/bn/bn_10.html",
    "href": "cs/bn/bn_10.html",
    "title": "서브넷 마스크",
    "section": "",
    "text": "IP 주소에서 네트워크와 호스트 영역을 명확히 구분하기 위한 핵심적인 비트 마스크이다. 이를 통해 네트워크 식별, 호스트 구분, 라우팅 효율화가 가능하며, IPv4 주소 체계에서 필수적인 구성 요소로 작동한다.\n\n\n\nIPv4 주소는 32비트로 구성\n‘1’: 네트워크 영역\n‘0’: 호스트 영역\n서브넷팅(Subnetting):\n기존 네트워크를 보다 작은 서브넷으로 분할하여 네트워크 관리 효율과 보안을 향상시키고, 라우팅 최적화를 달성한다.\n네트워크 비트 수 증가 → 서브넷 수 증가, 호스트 수 감소.\n슈퍼넷팅(Supernetting):\n다수의 작은 네트워크를 하나의 큰 네트워크로 결합하여\nCIDR(Classless Inter-Domain Routing) 구현 및 라우팅 테이블 축소 효과를 달성한다.\n\n\n\n\nHierarchical Routing 라우팅 테이블의 항목 수를 최소화하여 라우터 부하를 줄이고, 네트워크 관리 효율성을 높인다. 이를 통해 네트워크 범위 식별, 라우팅 영역 구분, 상위 네트워크 및 외부 네트워크와의 효율적인 데이터 전달이 가능하다.\n장점: 주소 활용 효율화, 보안 강화, 트래픽 최적화, 계층적 설계. 단점: 설계 복잡성 증가, 오류 가능성, 관리 부담 증가.\n\n\n\n게이트웨이, DNS, 라우팅 테이블과 밀접히 연관되어 네트워크 통합 관리와 안정적 운영을 지원한다."
  },
  {
    "objectID": "cs/bn/bn_10.html#네트워크-분할",
    "href": "cs/bn/bn_10.html#네트워크-분할",
    "title": "서브넷 마스크",
    "section": "",
    "text": "IPv4 주소는 32비트로 구성\n‘1’: 네트워크 영역\n‘0’: 호스트 영역\n서브넷팅(Subnetting):\n기존 네트워크를 보다 작은 서브넷으로 분할하여 네트워크 관리 효율과 보안을 향상시키고, 라우팅 최적화를 달성한다.\n네트워크 비트 수 증가 → 서브넷 수 증가, 호스트 수 감소.\n슈퍼넷팅(Supernetting):\n다수의 작은 네트워크를 하나의 큰 네트워크로 결합하여\nCIDR(Classless Inter-Domain Routing) 구현 및 라우팅 테이블 축소 효과를 달성한다."
  },
  {
    "objectID": "cs/bn/bn_10.html#계층적-라우팅",
    "href": "cs/bn/bn_10.html#계층적-라우팅",
    "title": "서브넷 마스크",
    "section": "",
    "text": "Hierarchical Routing 라우팅 테이블의 항목 수를 최소화하여 라우터 부하를 줄이고, 네트워크 관리 효율성을 높인다. 이를 통해 네트워크 범위 식별, 라우팅 영역 구분, 상위 네트워크 및 외부 네트워크와의 효율적인 데이터 전달이 가능하다.\n장점: 주소 활용 효율화, 보안 강화, 트래픽 최적화, 계층적 설계. 단점: 설계 복잡성 증가, 오류 가능성, 관리 부담 증가."
  },
  {
    "objectID": "cs/bn/bn_10.html#연계-요소",
    "href": "cs/bn/bn_10.html#연계-요소",
    "title": "서브넷 마스크",
    "section": "",
    "text": "게이트웨이, DNS, 라우팅 테이블과 밀접히 연관되어 네트워크 통합 관리와 안정적 운영을 지원한다."
  },
  {
    "objectID": "cs/bn/bn_10.html#arp",
    "href": "cs/bn/bn_10.html#arp",
    "title": "서브넷 마스크",
    "section": "1. ARP",
    "text": "1. ARP\nAddress Resolution Protocol\n\nIPv4 환경에서 IP 주소를 MAC 주소로 변환.\n송신자 확인 → 브로드캐스트 요청 → 유니캐스트 응답 → 캐시 저장.\n특징: 브로드캐스트-유니캐스트 혼합 통신, 일정 기간 IP-MAC 매핑 캐시, 편리하지만 보안 취약점 존재."
  },
  {
    "objectID": "cs/bn/bn_10.html#rarp-bootp-dhcp",
    "href": "cs/bn/bn_10.html#rarp-bootp-dhcp",
    "title": "서브넷 마스크",
    "section": "2. RARP, BOOTP, DHCP",
    "text": "2. RARP, BOOTP, DHCP\n\nRARP: MAC 주소를 기반으로 IP 주소 요청.\nBOOTP: IP 주소가 없는 장치에 초기 IP 할당.\nDHCP: BOOTP 기능을 확장하여 IP, 서브넷 마스크, 게이트웨이, DNS 등 자동 제공.\n동작 순서: Discover → Offer → Request → Acknowledge.\n일반 가정용 및 기업 환경에서 널리 사용."
  },
  {
    "objectID": "cs/bn/bn_10.html#기타-네트워크-프로토콜",
    "href": "cs/bn/bn_10.html#기타-네트워크-프로토콜",
    "title": "서브넷 마스크",
    "section": "03 기타 네트워크 프로토콜",
    "text": "03 기타 네트워크 프로토콜"
  },
  {
    "objectID": "cs/bn/bn_10.html#icmp",
    "href": "cs/bn/bn_10.html#icmp",
    "title": "서브넷 마스크",
    "section": "1. ICMP",
    "text": "1. ICMP\nInternet Control Message Protocol\n\nIP 네트워크에서 오류 보고와 제어 메시지 전달용 보조 프로토콜.\nIP 자체에는 오류 보고 기능이 없으므로, ICMP를 통해 안정적 네트워크 동작을 지원.\n주요 메시지: 오류 보고, 상태 진단, 경로 추적 등."
  },
  {
    "objectID": "cs/bn/bn_10.html#igmp",
    "href": "cs/bn/bn_10.html#igmp",
    "title": "서브넷 마스크",
    "section": "2. IGMP",
    "text": "2. IGMP\nInternet Group Management Protocol\n\nIPv4에서 멀티캐스트 그룹 관리. IPv6에서는 MLD 사용.\n필요성: 멀티캐스트 트래픽 효율적 관리, 실시간 서비스 지원.\n동작: 가입 요청 → 라우터 정보 유지 → 주기적 멤버 확인 → 탈퇴 알림.\n버전별 특징: v1 기본 가입 기능, v2 빠른 탈퇴 지원, v3 SSM(Source-Specific Multicast) 지원.\n적용 사례: 유튜브 라이브 스트리밍, 금융 거래 서비스."
  },
  {
    "objectID": "cs/bn/bn_10.html#라우팅-프로토콜",
    "href": "cs/bn/bn_10.html#라우팅-프로토콜",
    "title": "서브넷 마스크",
    "section": "3. 라우팅 프로토콜",
    "text": "3. 라우팅 프로토콜\n\n내부 라우팅 프로토콜: OSPF(Link State 기반), 빠른 수렴, 계층적 네트워크 구조에 적합.\n외부 라우팅 프로토콜: BGP(자율 시스템 간 경로 선택), 인터넷 백본 및 대규모 네트워크에서 표준으로 사용."
  },
  {
    "objectID": "cs/bn/bn_08.html",
    "href": "cs/bn/bn_08.html",
    "title": "라우팅(Routing)",
    "section": "",
    "text": "네트워크 계층(Network Layer)의 핵심 기능으로, 데이터 패킷이 출발지에서 목적지까지 도달하기 위한 최적의 경로를 결정하는 과정을 의미한다.\n이 과정은 네트워크의 토폴로지 변화, 트래픽 부하, 링크 상태 등에 따라 동적으로 변화하며, 효율성과 안정성 간의 균형이 중요하다.\n라우팅은 크게 정적(Static)과 동적(Dynamic) 방식으로 구분된다.\n\n정적 라우팅 (Static Routing):\n관리자가 수동으로 경로를 지정.\n소규모 네트워크에서 안정적이지만, 링크 장애나 토폴로지 변화에 대응 불가.\n동적 라우팅 (Dynamic Routing):\n라우터가 인접 노드들과 정보를 교환하며 최적 경로를 실시간 계산.\n자율성과 적응력이 높으나, 연산 부하와 제어 메시지 비용이 증가."
  },
  {
    "objectID": "cs/bn/bn_08.html#애자일-개발-방법론-전략적-관점",
    "href": "cs/bn/bn_08.html#애자일-개발-방법론-전략적-관점",
    "title": "라우팅(Routing)",
    "section": "애자일 개발 방법론: 전략적 관점",
    "text": "애자일 개발 방법론: 전략적 관점\n\n1. 개발 방법론의 분류 및 특성\n소프트웨어 개발 프로젝트는 전통적으로 폭포수(Waterfall) 방식과 애자일(Agile) 방식으로 구분된다. 폭포수 방식은 요구사항 정의 → 분석 → 설계 → 구현 → 테스트 → 배포로 이어지는 선형적 단계 모델을 따르며, 각 단계가 완료되어야 다음 단계로 진행된다. 이 접근법은 명확한 요구사항과 엄격한 일정 관리가 필요할 때 효과적이지만, 요구사항 변경이나 시장 변화에 유연하게 대응하기 어렵다는 한계가 있다.\n반면 애자일 방식은 반복적·점진적 개발(iterative and incremental development)을 기반으로 한다. 핵심 원칙은 고객 요구사항의 변화에 민첩하게 대응하며, 기능 단위의 점진적 납품을 통해 피드백을 신속하게 반영하는 것이다. 프로젝트 관리에서는 WBS 기반의 일정 중심 관리보다 작업(Task) 중심 관리가 강조되며, 각 개발 주기를 스프린트(Sprint)로 정의한다. 일반적으로 스프린트는 1~4주 단위로 반복되며, 각 사이클 종료 시 실제 동작 가능한 소프트웨어를 제공하여 고객 검증과 요구사항 반영을 동시에 달성한다.\n\n\n2. 애자일의 전략적 활용\n\n고객 중심성: 초기 버전부터 고객과 지속적으로 상호작용하여 요구사항을 점진적으로 명확화한다.\n적응적 계획(Adaptive Planning): 시장 변화나 기술적 요건에 맞춰 스프린트 계획을 유연하게 조정한다.\n위험 관리: 단기 목표 기반 반복 개발로 프로젝트 실패 가능성을 조기에 탐지하고 완화한다.\n팀 자율성: 크로스 기능적 팀(Cross-functional Team)이 스스로 우선순위를 판단하고 작업을 수행한다.\n\n\n\n3. 폭포수 방식 대비 애자일의 장점과 한계\n\n\n\n구분\n폭포수(Waterfall)\n애자일(Agile)\n\n\n\n\n개발 진행\n선형, 단계별\n반복적, 점진적\n\n\n요구사항 변경 대응\n어려움\n용이, 적응적\n\n\n일정 관리\nWBS 중심, 고정\n스프린트 중심, 유연\n\n\n고객 참여\n제한적\n지속적 피드백 포함\n\n\n위험 관리\n후기 발견\n조기 탐지 및 완화\n\n\n문서화\n상세 문서 중심\n최소 문서, 실행 중심\n\n\n\n애자일은 무제한적 유연성을 의미하지 않는다. 프로젝트 규모, 조직 구조, 규제 환경, 기술적 복잡성에 따라 적용 전략을 신중히 설계해야 하며, 과도한 변화 반영은 일정 지연과 품질 저하를 초래할 수 있다.\n\n\n4. 적용 사례 및 실무 전략\n\n스타트업 제품 개발: 빠른 시장 반응과 경쟁력 확보를 위해 애자일 방식이 적합하다.\n대기업 내부 시스템 개발: 일부 핵심 모듈은 폭포수 방식으로 안정성을 확보하고, 신규 기능 모듈은 애자일 방식으로 점진적 개발을 수행한다.\n혼합 접근(Hybrid Approach): 전통적 개발 프로세스에 애자일 스프린트를 통합하여 안정성과 민첩성 간 균형을 확보한다.\n\n\n\n5. 결론\n애자일은 단순한 개발 방법론이 아니라, 조직의 전략적 의사결정, 프로젝트 관리 체계, 고객 가치 창출을 통합적으로 고려한 개발 패러다임이다. 이를 통해 변화하는 요구사항과 시장 환경 속에서도 효과적인 소프트웨어 개발과 관리가 가능하다."
  },
  {
    "objectID": "cs/bn/bn_06.html",
    "href": "cs/bn/bn_06.html",
    "title": "무선통신시스템의이해",
    "section": "",
    "text": "IEEE 802.11 기술 유선 LAN 형태 이너넷 단점보완을 위해 기기가 AP와 연동해서 AC 또는 라우터를 통해서 인터넷 망을 주고 받는다. 연결 교모별 BSS, AP 없음 ESS, 다중 AP 연결 유형별 Ad Hoc, Infrastructure, 기술 발전의 세부규격들의 기준은 대부분 속도이다.\n6G로 도약이 어려운 이유 중 하나는 현재 한국은 4G와 5G가 혼용된 형태의 환경인데 6G는 대부분이 5G로 되어 있어야 설치 가능한 환경이 만들어진다. 그러려면 대용량 데이터 전송이 필요한 매체(AR) 등의 플랫폼이 있어야 하는데 아직까지 또 그러지도 않아서 기술의 발전이 더디는 상황이다.\nCSMA/CA 동작원리 4가지(IEEE 802.11 관련 기술) 채널감지, 대기, 프레임 전송, 수신확인\nIEEE 802.15와의 차이점\n블루투스 기호의 유래: 스칸다비아의 문자 두개를 합쳐 탄생 전파 간섭 문제가 자주 있으며 인원이 많아지면 보통 일어난다. pairing, BLE 2가지 기술용도로 씀, 그래서 유용하면서도 보완에는 상대적으로 취약에서 범용사용에는 국한된다. 블루투스 비콘의 등장배경: 2010 코인 배터리 등장 스마트폰 중에서도 애플의 도움으로 이것이 실현하게된 가장 큰 요인이 됨. 사실상 휴대폰의 확장 기능.\nZigbee 개요: 간단한 기능 사용. 짧은 보고 시간 + 낮은 에너지 사용 + 높은 배터리(수개월-수년) =&gt; 전기 및 도시 가스 계량기, 거리 조명 등 사용 이런 장치의 보완 문제를 해결하는 것이 주요 문제, 그 이유는 앞으로는 이런 종류의 장치가 많아질 것이라는 보고가 있기 때문 이동통신망 사용이 필요치 않는 곳에서 사용할 가능성이 있음. 동시에 사업자 입장에서 인원 단축 및 편리성을 위해 이는 우리나라보단 미국이나 동남아 처럼 국가의 크기 규모가 큰 나라에서 더 적용될 가능성이 높음 중국이 한국처럼 카드를 사용하지 않고 QR로 사용하듯 나라마다 필요한 기술이 다르다. 구성: Coordinator &gt;Router &gt; Device\nREID: 버스, 지하철, 시설 출입증 카드, 톨게이트 이것은 수동 소자이며, 전원을 공급하는 형태의 카드는 REIC이다. 구조는 태그,(데이터-에너지), 안테나, 리더, 호스트 무선 상태에서 얼마나 데이터 손실없이 잘 받아들이는 지가 주요 관점 현재의 대중화와 달리 초기엔 잘 작동하지 않았음. 효과미미\nNFC: 모바일 결정, 교통카드(FeliCa, MIFARE), 출입통제\nRFID와 NFC의 차이점 비교하기\n\n무선 LAN 에서 신뢰성 있는 통신을 위해 고려해야 할 부분이 있다면 어떤것이 있을까 ?\n저속통신 방식을 통해 향 후 실생활에서 적용이 가능한 범위와 사례는무엇이 있을까 ?"
  },
  {
    "objectID": "cs/bn/bn_04.html",
    "href": "cs/bn/bn_04.html",
    "title": "데이터 링크",
    "section": "",
    "text": "데이터 링크가 왜 필요한가? 1. 물리계층의 한계 신호의 시작과 끝, 신호의 왜곡, 유실\n\n노드 간 신뢰성 확보 노드 간의 충돌 최소화(CSMA/CD), 오류 제어, 백오프(충돌 시 지연) 문제를 예방해야 상위 쪽에서의 노력이 덜 들어가는 편의가 있음 이를 해결하는 방법은 매우 다양함\n\n그 중 기본적인 것을 배우고자 함 데이터 링크 계층의 5가지 핵심 역할 프레임화, 주소 지정, 오류 제어, 프름 제어 링크 제어\n프레임의 정의 문자 프레임(과거) 대 비트 프레임(현대) 예시: 이더넷 프레임 구조\n보통의 형태는 동기화 형태로 데이터를 보내며 여기서 preamble 가 사용되며 SFD가 프레임 시작을 알린다.\n과거의 통신 장비를 바로 최신 기종으로 바꾸지 않는 이유는 호환성 문제가 가장 크다. 바꿀 때에도 백워드 컴퓨터를 잘 고려해야 한다.\n이태원 지역은 하웨이 장비 계열의 통신 장비(BTS) 등을 사용하지 못한다. 미군이 살고 있어 해킹의 위험이 있기 때문.\n\n데이터 전송 오류 유형 단순 비트 오류(한 비트만 변경), 버스트 오류(연속적으로), 프레임 손실(수신측 미도착), 프레임 중복(동일 프레임 중복 수신) 오류 검출 방식 패리티 비트, 체크섬, CRC 단순 오류 검출, 데이터 단위 합산, 다항식 기반 검출 등 짝수 패리티(전체를 짝수로), 홀수 패리티(전체를 홀수로) 그러나 이 방법으로는 2개 이상의 비트 변경은 오류를 알기 어렵다. 체크섬은 더 발전된 형태, 각각을 16비트 진수 형태로 만들어 검증 그러나 내부가 완전히 깨진 형태는 검출하기 어렵다. CRC은 더 발전된 형태, 다항식 계산을 통해 거의 완전하게 검출함 그러나 이는 곧 다른 방법에 비해선 시간과 비용이 들어간다는 것 때문에 보안의 중요도에 따라서 다르게 사용할 수 있다.\n오류 정정 코드 해밍 코드, 패리티를 최소 몇 개를 붙일 것인지 공식으로 계산 FEC 실무적용사례: 이더넷 LAN, 와이파이, 5G 등 슬라이딩 윈도우 프로토콜, 윈도우 개념, 흐름제어, 부분 재전송 stop-and-Wait, timeout 시간 동안 기다리고 유실 등으로 수신층이 데이터를 보내지 않으면 다시 보냄. 다만 과거엔 개발자 위주 였으나 최근은 사용자 관점에서 편의성을 갖추어야 하므로 기다리는 방식이 오히려 불편할 수도 있음 go-back N 중간에 못 받은 게 있으면 이후 데이터는 받더라도 무시하고 다시 그 지점부터 보냄. 불필요한 로스들이 발생할 수 있다는 단점이 있음 Selective Repeat 위 단점을 해소하기 위한 해결 방안, 이후 데이터는 받으면 버퍼링을 돌리고 이전에 못 받은 걸 다시 받으면 지금까지 받을 걸 잘 정리 및 합쳐서 상위계층으로 올림 다만 상황에 따라서는 오히려 비효율적일 수도 있다. 그래도 대부분 데이터 유실은 개별 단위 보다는 전체가 유실되거나 파괴되는 경우가 더 많다.\n\n데이터 링크계층에서의 신뢰성 확보가 중요한 이유는 무엇일까 ?\n프레임 전송시 슬라이딩 원도우 방식이 나오게 된 배경과 좀 더 효율적으로 개선이 필요한부분이 있다면 어떠한 부분이 있을까 ?\n\n슬라이딩 윈도우 프로토콜 슬라이딩 윈도우 프로토콜은 송신자가 여러 프레임을 연속적으로 전송하고, 수신자가 ACK를 통해 수신 확인을 하는 방식입니다. 이 방식은 흐름 제어와 오류 제어를 동시에 수행할 수 있습니다. 이것이 언제 어디에서 처음 개발 되었는가? 이것은 현대까지 포함해서 어떻게 발전되었는지 웹에서 찾아서 자세하게 말해주세요"
  },
  {
    "objectID": "cs/bn/bn_02.html",
    "href": "cs/bn/bn_02.html",
    "title": "무선 통신망",
    "section": "",
    "text": "물리계층 1. 물리적 연결 보장, 2. 데이터 전송 담당\n비트에서 신호로 변환 0과1의 물리적 신호로 변환 전압, 빛, 전파의 유무\n아날로그, 정보 저장 공간 많음 복제 시 품질 이 저하, 수정 변경 어려움 시간이 지나면 품질에 영향, 선명하고 세밀한 표현\n디지털은 반대, 마그네틱이 있음\n전송매체의 종류, 유선: UTP 케이블, 동축 케이블, 광섬유 케이블, MDMI/USB, 전력선(철탑) 무선: 와이파이, 블루투스, LTE, 5G, (주파수대역이 높아질수록 기지국을 더 많이 세워야 함.)\n전송 방식의 분류 1. 직렬&병렬 2. 동기&비동기 3. 아날로그&디지털\n물리적 특정 정의 최대 케이블 길이 100m 전송 속도 1Gbps 신호전압 5V\n택배 지유로 이해하기 물리계층 = 로도, 택배상자 = 데이터\n\n신호, 파형: 주파수(시간), 파장(거리), 진폭, 위상 소리 관점으로 본 특징: 1. 고음/ 저음 Hz 2. 크고/ 작음 dB (0은 상대적 개념 0이라고 아예 안들리는 건 아님) 가청주파수 대역, 아날로그-디지털로의 변화 샘플링, 양자화/부호화, 압축 용어 정리, 감쇠, 간섭, 지터, 신호대 잡음비\n무선 통신, 주파수가 높을 떄와 낮을 때의 특징들(직진성, 투과성, 정보량, 안테나 크기, 울림) 기본성질, 굴절, 반사, 회절, 감쇠, 산란 변조, 신호를 다른 종류로 변환 1. 상대방에게 전달 용이하도록 2. 허가 받은 곳으로 전달하려고 진폭 변조 대 주파수 변조 사람의 목소리 - Carrier 교류 신호(발진기에서 발생) - AM, FM 변조 그 이외 ASK, PSK, FSK QAM의 등장 제한된 대역폭에서 전송 효율을 높이기 위함 위상과 진폭의 개수 기준에 따라 데이터를 구분\n다중 접속, 특정 주파수 대역에서 여러 명이 동시에 해당 대역을 사용하기 위함 FDMA(1세대), TDMA, CDMA - 복잡성/효율성, Generation, 주요기술, 수용가입자, Handover 으로 구분\n\n유선 통신 또는 무선 통신에서 개선이 필요한 사항\n향후 AI에 통신을 어떻게 적용할 건지"
  },
  {
    "objectID": "ai/ml/ml_17.html",
    "href": "ai/ml/ml_17.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: December. 10, 2025\n\n클러스터링 군집화 계층적 클러스터링에서 응집형과 분리형으로 나뉜다. 완전 연결 기반 상위 클러스터 구성 방법 - 점들 사이의 거리 계산. 허용치 범위 안에 있으면 묶는다.\n밀도 기반 클러스터링 DBSCAN 4개이상이 포함되면 하나의 클러스터로 인정. 코어와 경계 데이터로 나뉜다.\n이 방식은 차원의 저주 문제 발생 가능. 데이터의 차원이 증가 시 해당 공간의 부피가 기하급수적으로 증가하게 되고 모델을 추정을 위해\n노이즈 데이터는 데이터셋이 아닌 클러스터 관점에서의 노이즈인 것이다. 코드: 어떤 결과의 차이가 있는지?\n–\n클러스터링 방법을 이용해서 추천방법을 활용할 수 있다.\n예제: 스포티파이 - 음악 컨텐츠의 풍부. 추천의 정확도가 높다. 카페에서의 매출 영향은 음료 뿐만 아니라 음악도 해당됨.\n고려해야 될 요소 회원가입, 사용자의 인구통계학적 속성 정보 콘텐츠의 속성 정보, 이용정보, 구매정보\n쉬운 - 내용 기반 추천 콘텐츠의 피처를 갖고 벡터변환-유사도 계산 알려진 선호 아이템들의 집합에 대해 내용을 분석 개념은 간단하다. 가장 쉽게 개발할 수 있는 것이다. 콜드 스타트 문제가 없음.\n중간 - 인구통계학적 추천, 인구학적 그룹잉을 통한 방법이며 카이제곱 분석을 한다. 다만 노이즈에 약하다.\n어려운 - 협력적 여과 추천(이용 정보) 여기에서는 콜드스타트 문제가 발생한다. 유사도 계산 - 예측값 계산 아이템에 내용 정보가 없어도 추천 가능 다만 고객 수가 많아질 수록 시간 소요 증가 희소성 문제 발생 가능. 다른 분야를 추천하지 않음. 롱테일 문제\n아마존은 아이템 기반 협력적 추천 기법을 사용하였다. 인구통계학적으로 가로 방향으로 묶지 않고 아이템 간에 세로 방향으로 묶는다. 해당 아이템을 좋아한다면, 다른 아이템도 좋아할 확률 계산.\n넥플리스는 협력 필터링 기반 알고리즘이며, 이때 최초로 나옴 베이지안 분류, 증가하에서 가설일 가능성으로 계산. 동시 발생 상관관계, 연관성 규칙의 측량 방법, 지지도, 향상도 뉴럴 네트워크\n유튜브, 뉴럴렛, 벡터, 협력적 기법 혼합 사용."
  },
  {
    "objectID": "ai/ml/ml_15.html",
    "href": "ai/ml/ml_15.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: November. 19, 2025\n머신러닝에서 최적화(optimization)는 곧 고차원 매개변수 공간(parameter space)에서 최소점을 탐색하는 문제이며, 이 관점에서 학습 알고리즘은 본질적으로 탐색 알고리즘이다.\n인공신경망의 맥락에서는 이를 학습 알고리즘이라 부르며, 가장 대표적인 접근이 확률적 경사하강법(Stochastic Gradient Descent, SGD)이다.\nSGD는 손실 함수(loss function)의 기울기를 소량의 미니배치(mini-batch)로 근사하여 계산하기 때문에 계산 비용이 낮고 대규모 데이터셋에서도 효율적이다. 그러나 이러한 근사적 기울기는 통계적 분산이 크기 때문에 갱신 경로가 불안정하게 진동한다.\n이 문제를 완화하기 위해 도입된 개념이 모멘텀(momentum)이다. 이는 물리학적 관점에서 관성(inertia)과 마찰력(friction)을 도입한 것으로, 갱신 벡터를 단순히 현재 기울기만으로 결정하지 않고 이전 갱신의 방향성을 누적해 속도 벡터를 형성한다.\n결과적으로 최적화 경로는 비평면적(local curvature)의 영향을 덜 받고 안정적으로 수렴하며, 협곡형 지형(ravine)에서 진동을 줄이는 데 효과적이다."
  },
  {
    "objectID": "ai/ml/ml_15.html#적응형-학습률",
    "href": "ai/ml/ml_15.html#적응형-학습률",
    "title": "인공신경망",
    "section": "적응형 학습률",
    "text": "적응형 학습률\nadaptive learning rate AdaGrad는 학습 과정에서 각 파라미터의 변화량에 따라 학습률을 자동 조정한다. 기울기의 제곱 누적합을 기반으로 학습률을 축소하기 때문에 자주 변하는 파라미터는 더 빠르게 학습률이 감소하고, 드물게 변하는 파라미터는 학습률이 상대적으로 유지된다. 이는 데이터의 기하학적 구조에 따라 비등방적(anisotropic) 최적화를 수행하는 효과가 있다.\nAdam(Adaptive Moment Estimation)은 모멘텀과 AdaGrad의 장점을 결합한 비선형 최적화 알고리즘으로, 1차 및 2차 모멘트(기울기의 평균과 분산)를 동시에 추정한다.\nAdaGrad처럼 학습률이 급격히 감소하여 학습이 조기 정지되는 문제를 완화하고, 모멘텀 기반 탐색보다 진동이 적으며 빠른 초기 수렴 속도를 갖는다. 실험적으로 Adam은 다양한 비정상적(non-stationary) 목적 함수에서 강건하며, 딥러닝 모델 전반에서 사실상 표준으로 사용된다.\n최적화와 독립적으로, 모델의 일반화 성능을 향상하기 위해 정규화 기법이 도입되며 대표적인 것이 L2 정규화(weight decay)이다. 이는 가중치를 작게 유지함으로써 오버피팅을 방지하고, 매개변수 공간에서 불필요한 자유도를 억제함으로써 더 안정적인 표현 학습을 유도한다. 이는 통계학적 관점에서는 리지 회귀(ridge regression)의 페널티 항과 동일한 역할을 한다."
  },
  {
    "objectID": "ai/ml/ml_15.html#가중치-초기화",
    "href": "ai/ml/ml_15.html#가중치-초기화",
    "title": "인공신경망",
    "section": "가중치 초기화",
    "text": "가중치 초기화\nweight initialization 학습 안정성을 결정하는 중요한 요소.\nXavier 초기화는 시그모이드 함수와 같은 대칭적 활성화 함수에서, 전방/역방 전파 시 분산이 일정하게 유지되도록 설계된 방식으로 표준편차를 (1/)에 비례하도록 설정한다.\nReLU 계열 함수에서는 활성 뉴런 비율이 달라지기 때문에 분산 유지 조건이 다르게 정의되며, 이에 기반한 He 초기화는 표준편차를 ()로 확장하여 표현력을 높이고 초기 활성화의 비선형 왜곡을 방지한다.\n초기화가 중요한 이유는 깊은 신경망에서 발생하는 기울기 소실(vanishing gradient) 때문이다. 이는 활성화 함수의 포화 구간(saturation region)에서 기울기가 거의 0에 가까워지는 현상으로, 특히 시그모이드 함수는 입력이 조금만 크거나 작아도 기울기가 소멸한다. 이로 인해 초기 레이어는 거의 학습되지 않으며, 네트워크 전체가 비효율적으로 수렴한다. ReLU 함수는 양의 영역에서 기울기가 일정하게 유지되기 때문에 기울기 소실 문제를 근본적으로 완화하고, 깊은 모델의 학습을 가능하게 만든 핵심 요인이다.\n딥러닝 분야에서 비교 실험은 필수적이며, 다양한 최적화 알고리즘과 초기화 전략을 CIFAR-10과 같은 벤치마크 데이터셋에서 체계적으로 검증하는 과정은 모델 성능 평가의 표준 절차다. 이러한 데이터셋은 역사적으로 많은 연구자(예: 마빈 민스키와 관련된 초기 신경망 논쟁 이후의 연구 흐름)에 의해 발전해 왔으며, 현대적 딥러닝 모델의 성능 비교와 구조적 혁신을 검증하는 중요한 실험 환경을 제공한다."
  },
  {
    "objectID": "ai/ml/ml_13.html",
    "href": "ai/ml/ml_13.html",
    "title": "현대 AI 연구",
    "section": "",
    "text": "Reporting Date: November. 5, 2025\n\n엔비디아(NVIDIA)는 인공지능 연산을 위한 GPU 시장의 절대적 선도 기업으로, 최근에는 하드웨어 중심의 성능 향상을 넘어 AI 생태계 전체를 아우르는 플랫폼 전략으로 진화하고 있다. 대표적으로 ‘코스모스(Cosmos) Simulation Model’과 ‘옴니버스(Omniverse)’ 플랫폼은 3D 모델링, 물리 기반 시뮬레이션, 렌더링을 통합하여 현실과 유사한 디지털 트윈(Digital Twin) 환경을 구현하는 기술적 기반을 제공한다. 이러한 시뮬레이션 시스템은 실제 물리 세계의 상호작용을 정밀하게 재현함으로써, AI 에이전트(agent)의 학습, 검증, 최적화 실험을 위한 실증적 환경을 제공한다.\n2025년 기준으로 인공지능 연구의 핵심 개념은 에이전트(Agent)이다. 이는 인간의 인지적 판단과 행동을 모사하거나 대리 수행할 수 있는 자율적 지능 시스템을 의미한다.\n과거의 대화형 AI는 명시적으로 프로그래밍된 규칙에 의존했으나, 현재는 자연어 프롬프트(prompt)를 통해 고차원적 명령을 직접 해석하고 실행할 수 있다. 이러한 고도화된 시스템의 중심에는 대규모 언어모델(LLM, Large Language Model)이 있으며, 이는 언어적 지식뿐 아니라 시각·음성·문서 등 다양한 데이터 모달리티(modality)를 통합한 멀티모달 구조로 발전하고 있다.\nLLM의 기술적 진화는 ‘멀티모달 학습(Multimodal Learning)’ 역량을 중심으로 전개된다. 텍스트, 이미지, 오디오, 비디오 등의 이질적 데이터를 통합적으로 해석하고 상호 연관성을 학습하는 것이 핵심이며, 이는 각 데이터 형식별로 고유한 입출력 구조와 통신 프로토콜을 요구한다.\n현재의 LLM은 외부 데이터를 분석하고 가공할 수 있으나, 인간 수준의 자율적 시각 생성 능력에는 아직 도달하지 못한 상태다.\nAI의 실질적 응용의 종착점은 ’피지컬 AI(Physical AI)’이다. 이는 로봇, 제조, 물류, 스마트 팩토리 등 물리적 환경에서 실시간으로 인지·판단·행동을 수행할 수 있는 지능형 시스템을 의미한다. 미국은 첨단 반도체 설계 능력을 보유하고 있으나, 제조 기반의 약화로 인해 피지컬 AI의 산업적 적용에는 한계가 존재한다. 반면, 중국은 제조 기술과 산업 자동화 인프라 측면에서 강점을 보유하고 있으나, 미·중 기술 패권 경쟁이 양국 간 협력의 제약 요인으로 작용한다.\n이러한 상황 속에서 블랙록(BlackRock), 오픈AI(OpenAI)의 샘 알트먼(Sam Altman), 앤스로픽(Anthropic)의 다리오 아모데이(Dario Amodei), 엔비디아의 젠슨 황(Jensen Huang) 등 AI 생태계를 주도하는 주요 인물들은 한국을 차세대 전략적 거점으로 평가하고 있다. 이는 반도체 제조 분야에서 삼성전자와 SK하이닉스가 가진 기술적 우위와 글로벌 공급망 내 핵심적 위치 때문이다.\n한국의 반도체 산업은 엔비디아와 같은 AI 중심 기업의 연산 가속화를 위한 필수적 기반으로 작용하며, 양측의 기술적 상호의존 관계는 AI 반도체 생산 효율을 높이고, 피지컬 AI 산업화의 촉진을 가능하게 하는 협력 생태계를 형성하고 있다.\n\n인공지능의 이론적 기반은 크게 두 가지로 구분된다.\n첫째, 인과관계 기반의 지식 모델은 인간의 논리적 추론과 유사한 방식으로 동작하며, 둘째, 상관관계 기반의 데이터 모델은 통계적 학습을 통해 패턴을 식별한다.\n즉, 학습 기반 AI의 이해 연결주의(Connectionism) – 데이터에 대한 학습 능력을 이용하여 지능 구현\n후자의 대표적 구현이 머신러닝(ML)과 딥러닝(DL)이며, 인공신경망(ANN) 및 생성형 AI 모델이 이에 포함된다.\n\nANN Artificial Neural Network 인간의 신경세포(뉴런) 구조를 모사한 계산 모델이다.\n뉴런은 수상돌기(다중 입력), 세포핵(통합 중심), 축삭돌기(단일 출력)로 구성되며, 시냅스(synapse)를 통해 연결된다. 인간의 뇌는 약 1000억 개의 뉴런과 100조 개 이상의 시냅스로 이루어져 있으며, 각 뉴런은 평균 1000개 이상의 시냅스를 형성한다.\n입력 신호는 수상돌기를 통해 수집되어 세포핵에서 통합·처리되며, 각 입력의 영향력은 가중치(weight)로 조정된다.\n신호가 일정 임계치를 초과하면 활성화 함수(activation function)에 의해 출력으로 변환된다.\n이때 주로 사용되는 함수는 역치 함수(threshold function)와 시그모이드(sigmoid) 함수이며, 이는 입력 자극의 강도와 출력 반응의 비선형 관계를 수학적으로 모델링한다.\n출력 신호는 축삭돌기를 따라 전도되며, 말단부에서는 아세틸콜린 등의 신경전달물질을 통해 다른 뉴런으로 전파된다. 학습 과정에서 형성되는 시냅스의 가중치 변화가 곧 기억과 학습의 수학적 표현이다.\n시각 정보를 예로 들면, 인간의 눈은 약 700만 개의 원추세포와 1억 2000만 개의 막대세포로 구성되어 색상과 명암을 인식한다. 인공신경망에서는 이러한 시각 입력을 디지털 이미지의 픽셀로 표현하며, 예를 들어 28×28 흑백 필기체 데이터는 784차원 벡터로 변환되어 입력층에 주어진다.\n다층 퍼셉트론(MLP, Multi-Layer Perceptron)은 이 벡터 데이터를 여러 은닉층(hidden layer)을 통해 비선형적으로 변환하며, 각 층의 노드 수와 활성화 방식에 따라 인식 정확도가 달라진다.\n출력층(output layer)에는 가중치 대신 확률 분포를 계산하는 소프트맥스(Softmax) 함수가 사용된다. 이를 통해 예측 결과를 확률적으로 해석할 수 있으며, TOP-1 또는 TOP-3 정확도를 기준으로 분류 성능을 평가한다.\n하지만 모델의 복잡도가 과도하면 과적합(overfitting)이 발생할 수 있으며, 이는 학습 데이터의 특성에 과도하게 종속된 결과를 초래한다. 따라서 모델 구조의 단순화나 규제화(regularization) 기법을 통해 이를 완화한다.\n음성 데이터는 또 다른 모달리티를 구성한다. 파형, 주파수, 진동의 패턴을 학습하여 화자 인식, 음성 인식, 다자 음성 분리 등으로 응용된다. 철도나 엘리베이터의 작동음 분석을 통해 고장 여부를 조기 진단하는 산업적 활용도 이에 해당한다.\n결국 인식(recognition)은 곧 분류(classification)이다.\n신경망이 입력을 해석하고, 그 결과를 특정 클래스에 매핑하는 과정은 본질적으로 확률적 분류 문제이다. 이러한 학습의 핵심은 오차(error)를 줄이는 것이며, 과거에는 복잡한 가중치 구조로 인해 학습이 어려웠으나, 역전파(backpropagation) 알고리즘의 도입으로 이를 해결하였다.\n이 혁신으로 인해 인공신경망은 1980년대 후반의 혹한기를 극복하고, 현대 딥러닝의 토대를 구축하게 되었다."
  },
  {
    "objectID": "ai/ml/ml_11.html",
    "href": "ai/ml/ml_11.html",
    "title": "로지스틱 분류",
    "section": "",
    "text": "[주제] 에 대해 다루고자 한다.\n01 최적화 알고리즘\nAlec Radford’s animations for optimization algorithms\nAlec Radford has created some great animations comparing optimization algorithms SGD , Momentum , NAG , Adagrad , Adadelta , RMSprop (unfo…\nwww.denizyuret.com 기본 SGD 외에 다음과 같은 변형을 자주 사용한다.\nSGD (확률적 경사하강) — 큰 데이터셋에 효율적 Momentum — 관성 도입으로 수렴 가속 및 진동 완화 AdaGrad — 좌표별 학습률 적응 Adam — 1차·2차 모멘트를 사용한 적응형 방법 (실무에서 널리 사용; 성능·안정성 우수)\n\n모델 검증과 일반화 7.1 K-Fold 교차검증 모델 성능 평가를 위해 데이터를 K개의 폴드(fold)로 나누고, 순차적으로 하나의 폴드를 검증셋으로, 나머지를 학습셋으로 사용한다.\n\n데이터를 K등분 (예: 5-fold → 5등분) 각 iteration마다 서로 다른 폴드를 검증셋으로 지정 K번 반복 후 평균 성능 산출 특수 사례: LOOCV(Leave-One-Out CV)\n데이터 포인트 하나를 검증셋으로, 나머지를 학습셋으로 사용 데이터가 적을 때 모델 평가를 정밀하게 할 수 있음 7.2 학습률(Learning Rate)과 데이터 전처리 학습률은 경사하강법의 파라미터 갱신 속도를 조절하며, 과도하면 발산, 너무 작으면 수렴 속도가 느려진다. 데이터 전처리는 모델 안정성과 성능 향상에 중요하며, 일반적으로 정규화(normalization), 스케일링(scaling), 결측치 처리 등을 포함한다. 7.3 오버피팅 방지 모델이 학습 데이터에만 과적합되는 것을 방지하기 위해 다양한 전략을 사용한다.\n데이터 증강(Data Augmentation): 학습 데이터 변형을 통해 다양성을 확보 특징 수 축소(Feature Selection/Reduction): 불필요한 입력 제거 정규화(Regularization): L1/L2 정규화, 드롭아웃(Dropout) 등 이 과정을 통해 모델은 학습 데이터뿐 아니라 미지의 데이터에서도 일반화 성능을 발휘할 수 있다.\n03 퍼셉트론과 다층 신경망 Perceptron &Multi-Layer Neural Network\n1 . 신경망의 구조와 활성화 함수 인공신경망(NN)은 생물학적 뉴런을 모사하여 입력 신호를 처리한다.\n입력 신호(x0,x1,…x_0, x_1, ​,x1​,…): 다른 뉴런(axon)으로부터 전달되는 신호 가중치(w0,w1,…w_0, w_1, ​,w1​,…): 각 신호의 중요도를 나타내며, 시냅스(synapse)와 유사 가중합(dendrite): 각 입력에 가중치를 곱한 후 합산 \\[z = \\sum_i w_i x_i + b\\]\n활성화 함수(Activation Function, f): 뉴런의 세포체(cell body)가 총합 입력을 해석하여 출력 신호 생성 \\[y = f(z) = f\\Big(\\sum_i w_i x_i + b\\Big)\\]\n출력 y는 다음 뉴런(axon)으로 전달되며, 이 신호가 0인지 1인지 또는 확률값인지 결정한다. 이 구조를 MLP이라고 하며, 다층 구성으로 비선형 문제를 해결할 수 있다.\n2 . 역사적 배경 Frank Rosenblatt (1957): 퍼셉트론(Perceptron) 제안 — 단층 신경망 기반의 이진 분류기 Widrow & Hoff (1960): ADALINE(Adaptive Linear Neuron) / MADALINE — 가중치 적응 학습 규칙 제안 Widrow-Hoff Rule (Delta Rule) 가중치 학습 과정은 다음과 같다.\n가중치 초기화Wi(0)을 임의값으로 설정W_i(0) Wi​(0)을 임의값으로 설정 입력 패턴과 목표 출력 제시 출력 계산 (Hard Limiter) \\[y(t) = f_h\\Big(\\sum_{i=0}^{n-1} w_i(t) X_i(t) - \\epsilon\\Big)\\]\n: 임계치 f_h​: 하드리미터 함수 가중치 갱신 \\[W_i(t+1) = W_i(t) + \\alpha \\,[\\,d(t) - y(t)\\,] X_i(t), \\quad 0 \\le i \\le n-1\\]\n: 학습률, 0 &lt; &lt; 1 d(t): 목표 출력값 실제 출력과 목표 출력이 일치하면 가중치는 변하지 않음 반복 수행 출력이 목표에 도달할 때까지 2~4단계 반복\n3 . 퍼셉트론을 통한 논리 연산 퍼셉트론은 기본적인 논리 게이트 연산을 구현할 수 있다.\nAND, OR 연산: 입력값의 가중합이 임계치(θ)를 초과하면 출력 1, 아니면 0 XOR 연산: 단층 퍼셉트론으로는 직선 결정경계로 구분 불가능 — 다층 퍼셉트론 필요 입력 X_0, X_1 ​AND OR XOR 0, 0 0 0 0 0, 1 0 1 1 1, 0 0 1 1 1, 1 1 1 0 AND: 위쪽 위치, 직선 결정 가능 OR: 아래쪽 위치, 직선 결정 가능 XOR: 직선 결정 불가, 곡선 또는 다층 구조 필요\n03 다층 퍼셉트론과 역전파 Backpropagation\n\n초기 비관과 연구의 침체기 1969년, Marvin Minsky 교수와 Seymour Papert는 저서 Perceptrons에서 다음과 같이 지적했다.\n\n“멀티 레이어 퍼셉트론(Multi-Layer Perceptron)으로 구성하면 문제를 해결할 수는 있지만, 실제로 구현하는 사람은 아무도 없을 것이다.”\n이로 인해 퍼셉트론 연구는 첫 번째 머신러닝 침체기(Artificial Intelligence Winter)를 맞이하게 되었다.\n단층 퍼셉트론으로 XOR와 같은 비선형 문제를 해결할 수 없다는 한계 다층 구조의 학습 방법 부재 2. 역전파(Backpropagation)의 등장 1974년과 1982년, Paul Werbos는 다층 신경망의 학습 문제를 해결할 방법을 제안했으며, 1986년, Geoffrey Hinton 등이 이를 체계화하였다.\n핵심 아이디어: 순방향(forward)으로 출력과 실제값의 차이(오차)를 계산 출력층에서 입력층 방향으로 오차를 역전파(backpropagation) 하여 각 가중치를 조정 이로써 단층 퍼셉트론으로 해결할 수 없었던 XOR 등 비선형 문제도 학습 가능하게 되었다.\n\n수학적 원리 다층 퍼셉트론에서 각 뉴런의 출력은 다음과 같이 정의된다.\n\nf=wx+bf = w x + bf=wx+b\n또는 단계적으로,\ng=wx,f=g+bg = w x, f = g + bg=wx,f=g+b\n체인룰(Chain Rule)과 편미분을 이용하여, 출력 오차를 각 층의 가중치에 대해 분배한다. 미분의 기본 정의: ddxf(x)=lim⁡Δx→0f(x+Δx)−f(x)Δx f(x) = _{x } dxd​f(x)=Δx→0lim​Δxf(x+Δx)−f(x)​\n예제: 2차원 입력 x1,x2x_1, x_2x1​,x2​와 가중치 w=[5,−7,−11]w = [5, -7, -11]w=[5,−7,−11]일 경우 출력 [y1,y2]=[1,0,0][y_1, y_2] = [1,0,0][y1​,y2​]=[1,0,0]을 얻을 수 있으며, 각 가중치에 대한 기울기를 계산해 오차를 감소시키는 방향으로 업데이트한다. 4. 의미와 의의 역전파 알고리즘은 신경망 학습의 근간이 되었으며, 다층 퍼셉트론 구조는 XOR와 같은 비선형 문제 해결 가능 현재의 딥러닝 모델(Convolutional Neural Network, Transformer 등)도 이 원리를 확장한 것 즉, 단층 퍼셉트론에서 시작한 연구가 다층 구조와 역전파를 통해 현대 신경망의 기초로 발전하게 된 역사적 과정이다.\n①②③④⑤⑥⑦⑧⑨ ⋅ ⌎\n₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ⁱ ⁿ / ¹ ² ³ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ₋ / α β δ θ ε π μ σ Ω φ ω\n− ± × ∑ ∴ ≥ ≤ ≒ ≓ ⋯ ⋮ / ⇨ ←→↑↓↔︎↕ / ℉ ℃\n[출처]"
  },
  {
    "objectID": "ai/ml/ml_09.html",
    "href": "ai/ml/ml_09.html",
    "title": "경사하강법 (Gradient Descent)",
    "section": "",
    "text": "Reporting Date: October. 01, 2025 선형대수를 통한 기계학습의 수학적 기초와 최적화 원리에 대해 다루고자 한다.\n목차 01 지도학습의 기본원리 02 손실 함수와 최적화의 원리 03 경사하강법 04 손실 함수 최적화\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js 01 지도학습의 기본 원리 지도학습(SL)은 인공지능 학습 방법 중 가장 기초적이면서도 널리 사용되는 형태이다.\n지도학습에서는 입력값(input)과 그에 대응하는 정답값(output)이 모두 주어진 데이터를 바탕으로, 모델이 입력과 출력 간의 규칙을 학습한다.\n1 . 데이터셋의 구성과 의미 지도학습에 사용되는 데이터셋(dataset)은 이미 발생한 사건이나 관측 결과를 정리한 형태로, 각 데이터 샘플은 입력 변수(독립변수)와 이에 대응하는 결과 변수(종속변수)를 포함한다.\n즉, 데이터셋은 “입력과 출력이 모두 알려진 과거의 기록” 으로 AI 모델은 이러한 데이터로부터 입력과 출력 사이의 관계를 학습한다.\n학습 데이터에서 도출된 관계를 일반화하여 새로운 데이터에서도 올바른 예측을 수행할 수 있는 알고리즘을 찾는 것이 핵심 목표이다.\n모델은 학습 과정에서 단순한 암기가 아니라, 데이터에 내재된 규칙과 패턴을 포착해 새로운 상황에도 적용 가능한 일반적 모델을 만든다.\n이를 통해 미지의 입력값에 대해서도 합리적인 예측을 수행할 수 있다.\n2 . 데이터의 품질과 양의 중요성 정확하고 신뢰할 수 있는 모델을 얻기 위해서는 데이터셋의 양적 충족도와 질적 우수성이 필수적이다.\n양적 요인\n데이터 샘플의 수가 많을수록 다양한 패턴을 학습할 수 있으며, 모델이 학습 데이터에만 과도하게 적합되는 과적합 문제를 줄이는 데 도움이 된다.\n질적 요인\n데이터에 포함된 노이즈(noise)가 적고, 대표성 있는 다양한 사례를 포함할수록 실제 현상을 정확하게 반영할 수 있다.\n또한, 데이터 편향(bias)을 최소화하는 것이 중요하며, 이를 통해 모델이 특정 유형의 데이터에만 편향되지 않고 일반화 능력을 유지할 수 있다.\n결과적으로 지도학습에서 얻어지는 모델의 정확성과 신뢰성은 데이터의 양과 질, 다양성에 의해 결정된다.\n따라서 학습 데이터셋의 설계와 전처리 과정은 모델 성능을 좌우하는 핵심 요소라 할 수 있다.\n3 . 회귀 regression\n지도학습의 기본 구조를 가장 단순히 보여주는 예이다.\n회귀 문제에서는 입력값 ( x ) 와 출력값 ( y ) 가 주어질 때, 두 변수 간의 관계를 나타내는 함수적 표현을 학습한다.\n이를 위해 모델은 먼저 임의의 가중치(weight) ( w ) 와 절편(bias) ( b ) 를 설정하고, 가설 함수(hypothesis function)를 다음과 같이 정의한다.\n\\[\\hat{y} = wx + b\\]\n이 함수는 주어진 데이터를 가장 잘 설명하는, 즉 오차가 최소화되는 최적의 직선(best-fit line)을 찾는 것을 목표로 한다.\n4 . 손실 함수와 학습의 핵심 원리 모델이 예측한 값 ()​와 실제 값 (y)의 차이를 오차(error)라고 하며, 모든 데이터의 오차를 합산 또는 평균하여 얻은 값을 손실(loss) 또는 비용 함수(cost function)라고 한다.\n손실 함수는 모델이 얼마나 부정확한지를 수치적으로 평가하는 척도이며, 이 값을 최소화하는 것이 곧 학습의 핵심 목표이다.\n가장 일반적인 예로, 평균제곱오차(MSE)는 다음과 같이 표현된다.\n\\[L = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\n모델은 학습 과정에서 이 손실 함수를 최소화하도록 가중치 ( w ) 와 절편 ( b ) 를 반복적으로 조정한다. 이 과정은 경사하강법과 같은 최적화 알고리즘을 통해 이루어진다.\n02 손실 함수와 최적화의 원리 모델 학습의 본질은 손실(loss)을 최소화하는 방향으로 파라미터(parameter)를 조정하는 과정이다.\n손실 함수는 모델의 예측값과 실제값 간의 차이를 수치적으로 평가하는 지표로, 학습의 목표는 이 차이를 가능한 한 작게 만드는 것이다.\n1 . 손실 함수 Loss Function\n모델의 예측 결과가 실제 데이터와 얼마나 다른지를 측정한다. 이 함수를 계산하는 방식에는 여러 가지가 있으며, 문제의 특성에 따라 적절한 형태가 선택된다.\n① L1 손실 (Mean Absolute Error, MAE)\n각 오차의 절댓값을 더한 뒤 평균한 손실이다.\n\\[L_{1} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n특징: 이상치(outlier)에 덜 민감하며, 오차 분포가 고르게 반영된다. ② L2 손실 (Mean Squared Error, MSE)\n오차를 제곱한 뒤 평균하는 방식이다.\n\\[L_{2} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\n특징: 제곱 연산으로 인해 큰 오차(이상치)에 더 큰 패널티를 부여한다. 응용: 회귀 문제에서 가장 널리 사용되는 손실 함수다.\nL2 손실의 특성과 이상치 민감성 L2 손실은 예측값과 실제값의 오차를 제곱하여 평균한 값으로 계산된다.\n\\[L2 = \\frac{1}{n} \\sum (y_i - \\hat{y_i})^2\\]\n이 제곱 연산으로 인해 오차가 커질수록 손실값이 기하급수적으로 증가한다.\n따라서 데이터에 이상치가 존재할 경우, 그 점의 오차가 전체 손실값에 매우 큰 영향을 미친다.\n이로 인해 L2 손실을 사용하는 모델은 이상치에 민감하게 반응하며, 모델이 이상치 방향으로 왜곡될 가능성이 있다.\n반면 L1 손실은 오차의 절댓값을 사용하므로 오차가 커지더라도 선형적으로 증가한다. 따라서 L1 손실은 이상치의 영향을 상대적으로 덜 받으며 강건(robust) 하다고 평가된다.\n이러한 특성 때문에, L2 손실은 일반적인 데이터에서 더 매끄럽고 안정적인 최적화를 제공하지만, 이상치가 많은 데이터에는 부적합할 수 있다.\n반대로 L1 손실은 이상치가 많은 환경에서 더 신뢰할 수 있는 결과를 제공한다.\n[요약 비교] 구분 L1 손실 (MAE) L2 손실 (MSE) 계산 방식 오차의 절댓값 오차의 제곱 이상치 영향 작음 (강건함) 큼 (민감함) 손실 증가 특성 선형적 제곱적 (기하급수적) 주 용도 이상치가 많은 데이터 일반적인 데이터 대표적 활용 로버스트 회귀 선형회귀 MSE의 제곱근을 취하면 RMSE (Root Mean Squared Error)가 되며, 이는 실제 데이터의 단위와 동일한 척도를 제공하므로 결과 해석이 직관적이다.\n\\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]​\n2 . 손실 함수의 일반화 — Lₚ 노름 손실 함수는 일반적으로 Lₚ 노름(norm)의 형태로 확장할 수 있다. 이는 오차의 절댓값을 (p) 제곱하여 평균한 뒤, 다시 (p) 제곱근을 취하는 방식이다.\n\\[L_p = \\left( \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|^p \\right)^{1/p}\\]\n(p = 1) 일 때 L1 손실, (p = 2) 일 때 L2 손실로 귀결된다. (p) 값이 커질수록 큰 오차에 더 민감해진다.\n3 . 비선형 회귀와 손실 함수의 구분 다항 회귀(Polynomial Regression)는 손실 함수의 형태를 바꾸는 것이 아니다.\n입력 변수 (x) 에 대해 (x^2, x^3, ) 와 같은 고차항(feature)을 추가하여 모델의 가설 함수(hypothesis)를 비선형으로 확장함으로써 데이터의 비선형적 관계를 표현한다.\n즉, 손실 함수는 회귀 문제에서 그대로 사용되며(L1, L2 등), 모델이 복잡해짐에 따라 가설 함수가 비선형 구조를 가지게 되는 것이다.\n4 . 교차 엔트로피 손실 함수 Cross-Entropy Loss\n① 이진(Binary)\n이진 분류에서의 손실 함수는 다음과 같다.\n\\[J(w,b)=-\\frac{1}{m}\\sum_{i=1}^{m}\\big[\\,y^{(i)}\\log H(x^{(i)}) + (1-y^{(i)})\\log(1-H(x^{(i)}))\\,\\big]\\]\n실제값과 예측이 일치하면 손실이 0에 가깝고, 다르면 손실이 급격히 커진다.\n로그 형태이므로 확률이 극단적으로 틀릴수록 패널티가 커져 모델을 강하게 교정한다.\n② 범주형 또는 다중 클래스\nCategorical & Multiclass\n회귀 문제에서 손실이 오차의 크기를 기반으로 계산되는 반면,\n분류(classification) 문제에서는 모델의 출력 확률 분포와 실제 정답 확률 분포 간의 차이를 평가한다.\n이때 주로 사용되는 것이 교차 엔트로피 손실이다.\n\\[L = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\\]\n의미: 모델의 예측 확률이 실제 정답에 가까울수록 손실 값이 작아진다. 특징: 다중 클래스 분류 문제에 특히 유용하다.\n5 . 최적화 Optimization\n손실 함수가 정의되면, 모델의 목표는 이 손실 함수를 최소화하는 파라미터(가중치 (w), 절편 (b))를 찾는 것이다.\n이처럼 손실을 최소로 만드는 방향으로 파라미터를 반복적으로 조정하는 과정을 최적화라 하며, 손실이 가장 작은 지점에서의 파라미터 조합을 최적해(optimal solution)라고 한다.\n손실 함수를 시각화하면, 가로축은 파라미터 w, 세로축은 손실값으로 나타나는 곡선 형태(일반적으로 2차 함수형태)로 표현된다.\n이때 모델이 손실을 줄이기 위해 이동해야 하는 방향을 수학적으로 계산하는 핵심 도구가 기울기(gradient)이다.\n기울기는 특정 지점에서의 손실 함수의 변화율로, 손실이 증가하거나 감소하는 방향성과 속도를 알려준다.\n따라서 최적화 과정은 기울기를 이용해 손실이 감소하는 방향으로 파라미터를 이동시키는 과정이라 할 수 있다.\n가장 널리 사용되는 최적화 방법은 경사하강법(Gradient Descent)이다.\n경사하강법은 손실 함수의 기울기를 계산한 뒤, 그 반대 방향(손실이 줄어드는 방향)으로 파라미터를 반복적으로 갱신한다.\n이를 수식으로 표현하면 다음과 같다.\n\\[w := w - \\eta \\frac{\\partial L}{\\partial w}, \\quad b := b - \\eta \\frac{\\partial L}{\\partial b}\\]\n(:) 학습률(learning rate)로, 한 번의 갱신에서 얼마나 크게 이동할지를 결정한다. 학습률이 너무 크면 최적점을 지나쳐 발산할 수 있고, 너무 작으면 수렴 속도가 느려질 수 있다.\n따라서 적절한 학습률 설정은 최적화의 안정성과 효율성에 매우 중요하다.\n6 . 손실 최소화 & 우도 최대화의 관계 손실 함수는 일반적으로 최소화(minimization)의 관점에서 정의되지만,\n확률적 접근에서는 반대로 우도(likelihood)를 최대화(maximization)하는 방식으로 동일한 목표를 달성할 수 있다.\n예: 최대우도추정(MLE): 주어진 데이터가 특정 파라미터 하에서 관측될 확률(우도)을 가장 크게 만드는 값을 찾는다.\n\\[\\max_{\\theta} \\mathcal{L}(\\theta) \\quad \\Leftrightarrow \\quad \\min_{\\theta} [-\\log \\mathcal{L}(\\theta)]\\]\n즉, 로그 함수의 단조 증가 성질에 따라 우도를 최대화하는 문제는 음의 로그 우도를 최소화하는 문제와 동일하다.\n따라서 실제 학습 과정에서는 (-())를 손실 함수로 정의하고 이를 최소화한다.\n지도학습과 대비되는 예시: 강화학습 지도학습과 달리 강화학습(RL)에서는 모델이 환경과 상호작용하며 얻는 보상(reward)을 최대화하는 방향으로 정책(policy)을 학습한다.\n즉, 손실 함수가 최소화의 대상이라면, 보상 함수는 그 반대 방향인 최대화의 대상이다.\n이처럼 AI의 학습 목표는 ’최소화’와 ’최대화’의 두 관점으로 표현될 수 있으며, 결국 이는 모두 가장 합리적(최적의) 매개변수 조합을 찾는 문제로 귀결된다.\n03 경사 하강법 Gradient Descent, GD\n기계학습에서 손실 함수를 최소화하여 모델의 파라미터 ((w, b))를 최적화하는 가장 기본적이면서 핵심적인 학습 알고리즘(Learning Algorithm)이다.\n1 . 기본 원리 모델 파라미터 (w)를 임의의 초기값으로 설정한다. 현재 파라미터에서 손실 함수 (L(w))를 미분하여 기울기를 계산한다. 손실이 감소하는 기울기의 반대 방향으로 (w)를 반복적으로 갱신(update)함으로써 최적점을 찾아간다.\n① 1차원(고등학교 수준) — 순간변화율(접선 기울기)\n함수 (f(x))에서 한 점 (x)의 순간변화율(도함수)는 다음과 같다.\n\\[f'(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\\]\n이 값은 (x) 근처에서 함수의 선형 근사(접선)를 제공한다.\n\\[f(x+h)\\approx f(x)+f'(x)\\cdot h\\]\n즉, 접선은 ’그 점 근처에서 함수가 어떻게 움직이는지 선형적으로 근사한 직선’이다.\n② 다변수(머신러닝의 손실 함수) — 기울기(gradient)\n머신러닝에서는 (x) 하나가 아니라 여러 파라미터가 벡터 형태로 계산된다. 변수 벡터 (w^n)에 대한 손실 (L(w))의 기울기는 모든 편미분을 모은 모은 벡터로 정의된다.\n\\[\\nabla L(w) = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1}(w)\\\\[2mm] \\frac{\\partial L}{\\partial w_2}(w)\\\\[1mm] \\vdots\\\\ \\frac{\\partial L}{\\partial w_n}(w) \\end{bmatrix}.\\]\n이 벡터은 각 좌표에서의 순간변화율을 모아 놓은 것이며, 함수가 가장 빠르게 증가하는 방향을 가리킨다.\n① 기울기와 이동 방향\n음수 기울기일 때 (L(w) &lt; 0) \\[w \\leftarrow w - (-|\\nabla L(w)|) = w + |\\nabla L(w)|\\]\n→ (w)의 값이 증가하여 기울기가 0이 되는 방향으로 이동한다.\n양수 기울기일 때 (L(w) &gt; 0) \\[w \\leftarrow w - |\\nabla L(w)| = w - |\\nabla L(w)|\\]\n→ (w)의 값이 감소하여 기울기가 0이 되는 방향으로 이동한다.\n즉, 기울기의 부호와 상관없이 항상 최소점으로 이동한다.\n② 학습률 ()의 역할\n학습률은 한 번 이동할 때의 크기를 조절한다.\n\\[w \\leftarrow w - \\eta \\nabla L(w)\\]\n()가 작으면 한 걸음씩 천천히 이동 → 안정적 수렴 ()가 크면 한 번에 크게 이동 → 빠른 수렴 가능하지만, 최적점을 지나치거나 발산 위험 존재\n그래프 상에서 (w)를 점으로 보고, 접선 기울기가 0이 될 때까지 경사를 따라 한 걸음씩 내려가는 것으로 비유할 수 있다.\n\\[w_t = w_{t-1} - \\eta \\nabla L(w_{t-1})\\]\n(w_t:)​ t 번째 반복에서의 파라미터 (L(w_{t-1}):) 손실 함수의 기울기 (:) 학습률(learning rate), 한 번에 이동하는 크기 학습률은 알고리즘 성능에 중요한 영향을 미친다.\n너무 크면: 최적해를 지나치거나 발산할 수 있다. 너무 작으면: 수렴 속도가 느려 학습 시간이 길어진다.\nMastering Gradient Descent: Optimizing Neural Networks with Precision(Omkar Hankare)\n2 . 하이퍼파라미터 hyperparameter\n학습률과 같이 학습 과정에서 사용자가 설정하는 조정 가능한 값들.\n하이퍼파라미터의 적절한 선택은 학습 효율과 모델 성능에 큰 영향을 준다. 이를 최적화하는 과정을 하이퍼파라미터 튜닝(tuning)이라고 한다.\n경사하강법의 효율과 안정성을 높이기 위해 다양한 전략이 사용된다.\n① 학습률 스케줄링(learning rate scheduling) : 학습이 진행됨에 따라 학습률을 점진적으로 조정하여 안정적 수렴 유도\n② 모멘텀(Momentum) : 이전 기울기의 일부를 반영하여 진동을 줄이고 수렴 속도를 향상\n③ 적응적 학습률(Adam, RMSProp 등) : 각 파라미터별로 학습률을 자동 조정\n④ 배치 크기(batch size) 및 반복 횟수(epochs) : 한 번에 처리하는 데이터 양과 학습 반복 횟수를 조절\n이러한 전략들을 적절히 활용하면, 다양한 데이터셋과 모델 구조에서도 손실 함수를 보다 효과적이고 안정적으로 최소화할 수 있다.\nTensorflow — Neural Network Playground\nTinker with a real neural network right here in your browser.\nplayground.tensorflow.org\n3 . 볼록 함수 convex function\n아래로 볼록인 손실 함수에서 특히 안정적이고 효율적이다.\n아래로 볼록한 함수에서는 국소 최소값(local minimum)이 곧 전역 최소값(global minimum)이므로, 경사하강법이 수렴하면 최적해를 확보할 수 있다.\n반면, 비볼록(non-convex) 함수나 위로 볼록한 함수에서는 여러 국소 최소값, 안장점(saddle point)이 존재할 수 있어, 경사하강법이 항상 전역 최적해에 도달한다고 보장할 수 없다.\n따라서 최적화를 설계할 때는 가능한 아래로 볼록한 형태를 사용하는 것이 안정적이다.\n4 . 경사하강법의 직관적 이해 경사하강법은 손실 함수의 순간 변화율(기울기)을 이용하여 파라미터를 작은 단위로 조정하고, 손실이 낮은 방향으로 이동시킨다.\n비유하자면, 물이 자연스럽게 낮은 곳으로 흐르는 과정과 유사하다. 손실이 높은 지역에서 낮은 지역으로 점진적으로 이동하며, 손실 함수가 최소인 지점에 도달하는 원리다.\n04 손실 함수 최적화 Loss Function Optimization\n기계학습의 대부분의 모델은 대규모 데이터를 효율적으로 처리하기 위해 벡터(vector)와 행렬(matrix) 형태로 데이터를 표현한다.\n이러한 수학적 구조화는 계산 효율을 극대화하며, 모델 학습 과정을 선형대수적 관점에서 체계적으로 해석할 수 있게 한다.\n1 . 선형 모델의 행렬 표현 단일 입력값 (x)를 사용하는 경우, 가설 함수는 다음과 같이 정의된다.\n\\[\\hat{y} = w x + b\\]\n(w)는 가중치(weight) (b)는 절편(bias) 그러나 실제 데이터는 다수의 관측치와 다차원 특성(feature)을 포함하므로, 입력 데이터를 행렬 형태로 확장한다.\n\\[\\mathbf{X} \\in \\mathbb{R}^{m \\times n}\\]\n(m:) 데이터 포인트의 개수 (샘플 수) (n:) 각 데이터의 특성(feature) 수 이에 대응하는 가중치 벡터는\n\\[\\mathbf{w} \\in \\mathbb{R}^{n \\times 1}\\]\n로 정의되며, 전체 예측값은 다음과 같이 행렬 연산으로 계산된다.\n\\[\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b}\\]\n이 표현은 모든 데이터 샘플에 대해 한 번의 행렬 연산으로 예측을 수행할 수 있어, 계산 효율성과 모델 학습 속도를 크게 향상시킨다.\n2 . 손실 함수의 수학적 정의 손실 함수는 예측값과 실제값 간의 차이를 수치적으로 표현한다. 선형 회귀에서 가장 일반적으로 사용되는 손실 함수는 평균제곱오차(MSE)이며, 다음과 같이 정의된다.\n\\[L(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\\]\n혹은 벡터 연산 형태로 나타내면,\n\\[L(\\mathbf{w}) = \\frac{1}{m} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top (\\mathbf{X}\\mathbf{w} - \\mathbf{y})\\]\n이 손실 함수를 최소화하는 방향으로 파라미터 와 (b)를 조정하는 것이 모델 학습의 핵심이다.\n3 . 행렬 연산 행렬 기반의 모델 학습에서는 다양한 연산이 사용된다. 대표적인 세 가지 연산은 다음과 같다.\n① 엘리먼트 와이즈 프로덕트 (Element-wise Product)\n각 행렬의 동일 위치 원소끼리 곱하는 연산으로, Hadamard product라 불리며 기호 ()로 나타낸다.\n\\[\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} a_{11}b_{11} & a_{12}b_{12} \\\\ a_{21}b_{21} & a_{22}b_{22} \\end{bmatrix}\\]\n② 내적 (Dot Product)\n두 벡터의 대응 원소를 곱해 합산한 값으로, 스칼라를 생성한다.\n\\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n}\\]\n③ 외적 (Cross Product)\n3차원 벡터 (, ^3)에 대해 두 벡터에 모두 수직인 벡터를 산출하는 연산이다.\n4 . 스케일링과 정규화의 필요성 데이터의 단위나 값의 범위가 서로 다른 경우, 특정 특성이 모델 학습 과정에서 불균형하게 영향을 미칠 수 있다. 이러한 문제를 방지하기 위해 스케일링(scaling) 또는 정규화(normalization)를 적용한다.\n① Min–Max 스케일링\n각 특성 xjx_jxj​을 0과 1 사이의 값으로 선형 변환한다.\n\\[x_j^{(\\text{scaled})} = \\frac{x_j - \\min(x_j)}{\\max(x_j) - \\min(x_j)}\\]\n② Z-점수 정규화(Standardization)\n평균과 표준편차를 이용하여 데이터를 정규분포 형태로 변환한다.\n\\[x_j^{(\\text{norm})} = \\frac{x_j - \\mu_j}{\\sigma_j}\\]\n이러한 변환을 수행하면 손실 함수의 등고선(contour)이 타원형에서 원형으로 변화하여, 모든 특성이 최적화 과정에서 동일한 비중으로 반영된다.\n그 결과, 경사하강법(Gradient Descent) 등 최적화 알고리즘의 수렴 속도와 안정성이 향상된다."
  },
  {
    "objectID": "ai/ml/ml_07.html",
    "href": "ai/ml/ml_07.html",
    "title": "SVM: 비확률적 마진 기반 분류기",
    "section": "",
    "text": "SVM, 즉 비확률적 마진 기반 분류기에 대해 다루고자 한다.\n수준별 이해도 ① 데이터 정제 및 전처리\n대상: 기존 IT 인력 요구 수준: 기본적인 이해로 충분하며, 실무 적용 중심 내용: 결측치 처리, 이상치 제거, 형식 변환 등 데이터의 품질을 높이는 작업 ② 모델링(연계 결합, 파이프라인 구성)\n대상: 석사 및 박사 수준의 연구자 요구 수준: 심층적인 이해와 설계 능력 필요 내용: 모델의 구조 설계, 학습 및 추론 과정의 최적화, 다양한 알고리즘의 조합 및 실험 ③ 알고리즘 설계\n대상: 석사 및 박사 수준의 연구자 요구 수준: 이론적 배경과 수학적 이해가 필수 내용: 새로운 알고리즘의 개발, 기존 알고리즘의 개선, 수학적 모델링 및 분석 이러한 단계별 구분은 일반적으로 데이터 과학 및 AI 분야에서의 역할 분담과 학습 경로를 반영한 것이다.\n실무에서는 데이터 정제 및 전처리가 핵심적인 역할을 하며, 연구 및 개발 단계에서는 모델링과 알고리즘 설계가 중심이 된다.\n우리는 이번 과정에서 모델링(연계 결합 및 파이프라인 구성) 단계에 초점을 맞춰 배워볼 것이다.\n01 서포트 벡터 머신 Support Vector Machine, SVM\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js 기본적으로 선형 분리(linear separator) 찾는 알고리즘.\n데이터가 선형적으로 구분되지 않을 경우, 커널(kernel)을 이용하여 입력 데이터를 더 높은 차원 공간으로 사상(mapping)한 뒤 선형 판별을 수행한다.\n대표적인 예로 다항식 커널(polynomial kernel) 은 다음과 같이 정의된다.\n\\[K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^\\top \\mathbf{y} + c)^d\\]\n이 커널은 입력 벡터의 고차항 조합을 암시적으로 고려한다.\n또한, 명시적으로 모든 고차항을 계산하지 않아도 커널 트릭(kernel trick) 을 통해 고차원 특징 공간에서의 내적을 효율적으로 대체할 수 있다.\n① 특징(feature) / 속성, 변수\n데이터의 각 관측치는 여러 개의 속성으로 구성되며, 이를 (x_1, x_2, , x_n)​으로 표현한다.\n이때 이 (n)개의 속성을 차원(dimension) 이라고 한다.\n② 차수(degree)\n일반적으로 다항식(polynomial)의 최고 지수(exponent)를 의미한다.\n예: 다차원 입력 ((x_1, x_2)) 에 대해 (x_1^2), (x_1 x_2), (x_2^2) 등 고차항(high-order term) 을 추가하면 모델은 비선형(곡선 형태) 관계를 표현할 수 있게 된다.\n02 데이터의 벡터 표현 데이터는 점과 방향성을 갖는 벡터(vector) 로 표현될 수 있다.\n벡터는 스칼라(scalar)와 달리 크기뿐 아니라 방향을 가지며, 이를 표현하기 위해서는 시작 좌표와 끝 좌표가 필요하다.\n한편, 어떤 점 ((x_1, x_2, , x_n))도 원점을 기준으로 하면 원점에서 그 점까지의 벡터로 간주할 수 있다. 즉, 점과 벡터는 서로 밀접한 개념이며, 데이터는 수학적으로 이러한 벡터 형태로 표현된다.\n자연어 처리(NLP)에서는 문장을 단어(토큰) 단위로 나누고, 각 단어를 하나의 임베딩 벡터(embedding vector) 로 나타낸다.\n이를 다음과 같이 표현할 수 있다.\n\\[\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{id}) \\in \\mathbb{R}^d\\]\n(i:) 문장 내 단어의 인덱스 (d:) 임베딩 공간의 차원(dimension) 모든 단어 벡터는 동일한 차원 (d)를 가지며, 유사한 의미를 가진 단어들은 벡터 공간상에서 가까운 위치에 존재한다. 이러한 벡터 표현은 문장 내 의미적 관계를 수치적으로 분석할 수 있게 해 준다.\n03 토큰 수와 표현의 제약\n1 . 언어 모델의 입력 구조 언어 모델은 입력할 수 있는 토큰(token) 의 수에 제한이 있다. 즉, 모델은 정해진 최대 토큰 길이(max token length) 이내의 입력만 처리할 수 있다.\n또한, 각 토큰은 고정된 차원 (d)의 임베딩 벡터(embedding vector) 로 변환되므로, 모델의 입력 공간 역시 차원이 고정되어 있다고 할 수 있다.\n① 고전적 표현 방식\nBag-of-Words 또는 One-hot Encoding 에서는 문장 내 각 단어의 존재 여부를 0과 1로 표시하여 표현하였다.\n예: 특정 단어가 문장에 포함되어 있으면 1, 포함되지 않으면 0으로 나타내는 방식.\n② 현대적 표현 방식\n임베딩 기반 모델(embedding-based model) 에서는 이진값이 아닌 실수(real-valued) 벡터를 사용하며, 단어의 의미 유사성과 문맥 정보를 함께 반영한다.\n이로써 모델은 단순한 단어 포함 여부를 넘어, 단어 간의 의미적 관계와 문맥적 상호작용까지 학습할 수 있다.\n2 . 문장의 벡터 표현과 유사도 계산 문장은 이러한 단어(토큰)들의 벡터로 구성되며, 전체 문장은 다차원 공간 상의 하나의 점(vector) 으로 표현된다.\n문장 간의 유사도는 두 벡터 간의 거리(distance) 나 코사인 유사도(cosine similarity) 를 통해 측정할 수 있으며, 거리가 가까울수록 의미적으로 유사한 문장으로 해석된다.\n그러나 실제 데이터는 차원이 매우 높고, 대부분의 값이 0인 희소(sparse) 형태를 띤다.\n이러한 고차원 희소 표현은 계산 비용은 크고 메모리 효율은 낮아, 이를 더 작은 차원으로 변환하는 차원 축소(dimensionality reduction) 가 필요하다.\n차원 축소는 본래 의미 구조를 최대한 보존 및 데이터 압축 과정이다.\n대표적인 기법: PCA(주성분분석), SVD(특이값 분해), 워드 임베딩(word embedding) 등.\n이 과정을 통해 얻어진 밀집 벡터(dense vector) 는 정보가 효율적으로 압축된 형태로, 계산 효율이 높고 모델이 의미적 관계를 더욱 잘 학습할 수 있다.\n이러한 벡터는 저차원 공간으로 투영(projection) 되어, 이후의 학습, 분류, 또는 유사도 계산 등의 연산에 활용된다.\n04 SVM의 핵심 개념\nSupport Vector Machines (SVM): An Intuitive Explanation\nEverything you always wanted to know about this powerful supervised ML algorithm\nmedium.com\n출처: Tibrewal (2021), Medium – Support Vector Machines (SVM)\n1 . 결정 경계 Decision Boundary\nSVM은 두 클래스를 분리하는 초평면 (^ + b = 0) 을 찾는다.\n쉽게 말해, 데이터를 두 클래스(예: 빨간 점 vs 파란 점)로 나누는 선(2차원) 또는 초평면(3차원 이상) 을 찾는 것이며, 핵심 목표는 두 클래스 사이의 “간격”을 최대화하는 초평면을 찾는 것이다.\n초평면의 수학적 표현과 조건 결정 경계: (^ + b = 0) 마진 경계: (^ + b = +1) 또는 (^ + b = -1) 서포트 벡터 조건: (y_i(^_i + b) = 1)\n2 . 마진 Margin\n결정 경계와 서포트 벡터 사이의 거리. SVM은 이 마진을 최대화하는 초평면을 찾는다.\n마진이 넓을수록 모델의 일반화 능력이 향상되어, 새로운 데이터가 들어와도 결정 경계가 잘못 분류될 가능성이 줄어든다.\n즉, 마진 크기와 분류 오류 확률은 반비례 관계에 있다.\n\\[P_{\\text{error}} \\propto \\frac{1}{\\gamma_2}\\]\nSVM은 “마진 최대화 문제” 를 풀어 초평면을 결정한다. 수학적으로는 다음 문제를 푼다:\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\quad \\text{s.t.} \\; y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1\\]\n(:) 초평면의 법선 벡터(normal vector) ( b ; : ; ) 편향(bias) (y_i:) 클래스 레이블(+1 또는 -1) (_i:) 각 데이터 점 이 문제는 Lagrange 승수법 및 이중 문제(dual problem)를 통해 풀 수 있으며, 서포트 벡터만으로도 최적의 초평면을 결정할 수 있다.\n3 . 서포트 벡터 Support Vectors\n결정 경계 또는 마진 경계에 가장 가까운 데이터 포인트들. 쉽게 말해, “초평면을 밀었을 때 점들이 가장 먼저 닿는 위치”이다.\n이 점들은 초평면의 위치와 방향을 결정하며, 나머지 점들은 보통 결정 경계에 영향을 주지 않는다.\n(단, soft margin SVM에서는 일부 내부 점이 영향을 줄 수도 있다.)\n05 SVM의 최적화 문제 앞서 설명한 마진 최대화 문제에서, 이상치가 존재하거나 데이터가 완전히 선형적으로 분리되지 않는 경우 슬랙 변수 (_i)를 도입한다.\n이는 약간의 분류 오류를 허용하면서 마진 최대화가 가능해, 각 데이터 포인트의 결정 경계에서의 거리를 고려하는 힌지 손실 함수로 최적화 문제를 정의한다.\nSVM의 최적화 문제는 다음과 같이 표현된다:\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i\\]\n단, 제약 조건은 다음과 같다:\n\\[y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\quad \\forall i\\]\n(_i:) 슬랙 변수(허용 오차) (C:) 정규화 파라미터\n1 . 슬랙 변수 & 힌지 손실 Slack Variable & Hinge Loss\n슬랙 변수 (_i)는 각 데이터 포인트가 결정 경계에서 얼마나 벗어났는지를 나타낸다.\n이는 힌지 손실 함수와 동일한 역할을 한다:\n\\[L(y_i, f(\\mathbf{x}_i)) = \\max(0, 1 - y_i f(\\mathbf{x}_i))\\]\n여기서 (f(_i) = ^_i + b)이다. 이 함수는 다음과 같은 경우에 대해 다르게 동작한다:\n(_i)가 정확하게 분류됨, 마진을 만족하는 경우: 손실 0 (_i)가 정확하게 분류됨, 마진 내에 있는 경우: 양의 손실 (_i)가 잘못 분류됨: 큰 손실 이러한 손실은 모델이 마진을 넓히고 오차를 최소화하도록 유도한다.\n2 . 비선형 분류를 위한 커널 방법 비선형 데이터의 경우, SVM은 커널 함수를 사용하여 고차원 특성 공간으로 데이터를 매핑한다. 이는 선형적으로 분리 불가능한 데이터를 선형적으로 분리할 수 있게 된다.\n커널 함수는 다음과 같은 형태를 가진다:\n\\[K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\\]\n(())는 특성 공간으로의 매핑 함수(basis function, kernel)로, 커널 트릭을 사용하면 내적 계산만으로 고차원 공간에서의 계산을 효율적으로 수행할 수 있다.\n현실 데이터에서는 선형 관계보다는 비선형 관계가 더 흔하다.\n예: AND, OR 게이트는 선형적으로 분리 가능하지만 XOR 게이트는 선형 경계로 구분할 수 없으며, 이를 해결하려면 2차원 이상의 고차원 공간으로 매핑해야 직선(초평면)으로 구분 가능하다.\n따라서 XOR 문제와 같이 선형 분리가 어려운 문제를 표현하기 위해 AI 연구자들은 원 공간(original space)에서 고차원 매핑 공간(mapping space)으로 데이터를 변환하는 개념을 도입했다.\n이때 사용되는 함수들을 커널 함수(kernel function)라 부르며, 매핑된 공간(mapping space)에서는 비선형 데이터가 선형적으로 분리 가능해진다.\n대표적인 커널 함수: 선형(linear), Gaussian (RBF), 다항식(polynomial), sigmoid 등.\n한편, 데이터 처리 과정에서 발생하는 노이즈는 측정 오류나 무작위 변동에 의해 생기는 것으로 일반적으로 제거를 고려한다.\n반면 이상치는 데이터 패턴에서 벗어난 극단적인 값으로 유의미한 정보를 포함할 수 있어 분석에 포함되기도 한다.\n노이즈와 이상치를 구분하는 기준은 보통 도메인 전문가의 판단에 따라 결정된다.\n4 . 최적화 문제의 이중 문제 Dual Problem\nSVM의 최적화 문제는 이중 문제로 변환할 수 있으며, Lagrange 승수 (_i)를 도입하여 다음과 같이 표현된다:\n\\[\\max_{\\alpha} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\\]\n단, 제약 조건은 다음과 같다:\n\\[0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0\\]\n이 이중 문제를 풀면 최적의 (_i)를 얻을 수 있으며, 이를 통해 초평면의 계수 ()와 (b)를 계산할 수 있다.\n06 모델 일반화와 조정\n1 . 하이퍼파라미터 상수 (C)는 슬랙 변수에 부여되는 패널티 강도(regularization strength)를 조절한다.\n(C)가 크면: 오차를 최소화하려는 경향이 강해져 과적합 위험 증가 (C)가 작으면: 마진 증가 및 일부 오차를 허용하여 일반화 성능 향상 즉, (C) 값은 결정 경계의 유연성을 결정하는 핵심 요인이다. 이 값을 조정하는 과정을 하이퍼파라미터 튜닝(Hyperparameter Tuning) 또는 파인튜닝(Fine-Tuning)이라 한다.\n2 . 전이학습 Transfer Learning\n모델이 학습 데이터에는 잘 맞지만, 새로운 데이터(테스트 데이터)에서는 성능이 저하되는 경우를 과적합이라 한다.\n이를 방지하려면 다양한 데이터에 대해 잘 작동하는 일반화 능력이 필요하다.\n딥러닝에서는 이러한 일반화를 위해 이미 대규모 데이터(예: 기사, 위키피디아 등)로 학습된 사전 학습 모델(Pre-trained Model)을 기반으로 하는 전이학습(Transfer Learning)이 활용된다.\n전이학습은 기존 지식을 새로운 도메인에 적용하는 과정이며, 특정 분야의 데이터로 추가 학습하여 세부 조정을 수행하는 단계를 파인튜닝이라 한다.\n결국, SVM에서의 (C) 조정과 같이 전이학습 및 파인튜닝 역시 “기존 모델의 일반화 능력을 유지하면서 특정 목적에 맞게 조정하는 과정”이라는 공통 원리를 가진다."
  },
  {
    "objectID": "ai/ml/ml_05.html",
    "href": "ai/ml/ml_05.html",
    "title": "머신러닝: EnjoySport",
    "section": "",
    "text": "의사결정트리 학습 과정에서 활용되는 개념에 대해 다루고자 한다.\n01 함수 근사 Function Approximation\n기계학습(supervised learning, 개념학습) 관점에서 가설(hypothesis)과 목표 함수(target function) 관계를 설명하고자 한다.\nIntroduction to Machine Learning and Design of a Learning System\nLet me go to the google trend, and understand the trend of the keywords — Machine Learning, data science, artificial intelligence,Hadoop…\nmedium.datadriveninvestor.com\n일반화된 수학적 구조\n1 . 문제 설정 입력 공간 ((X))의 각 인스턴스(instance)는 여러 속성(feature) 벡터로 구성된다. 예: 날씨, 온도, 습도 등\n출력 공간 ((Y))은 각 인스턴스에 대응되는 정답(label)이다. 예: EnjoySport = Yes/No\n지도 학습의 핵심은 입력(feature)과 출력(label, target)이 있고, 출력값(정답)을 예측하기 위해 모델을 학습하는 것이다.\n(이러한 표현은 의사결정트리 학습에서도 나오는 방식이다.)\n이는 함수 근사 관점에서, 우리가 직접 알 수 없는 목표 함수 ( f: X Y )를 학습 데이터로부터 추정(hypothesis)하는 과정으로 볼 수 있다.\n(목표 함수는 타겟 함수(target function) 혹은 개념 함수(concept)라고도 부른다.)\n학습 데이터(training set)는 입력-출력 쌍 ( (x^{(i)}, y^{(i)}) )(x^{(i)}, y^{(i)})들로 구성되어 있으며,\n학습자의 목표는 이 제한된 데이터만으로 알려지지 않은 (f)를 가능한 잘 근사하는 가설 함수 (h)를 찾는 것이다. 이 프로세스를 함수 근사라고 부른다. [출처1] [출처2]\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n2 . 예제 설명 아래는 “사람이 스포츠를 즐길까(Yes/No)”를 날씨 등의 조건으로 학습하는 문제이다. 기계학습의 기초 강의 자료인 1강 및 2강 PDF를 참고하였다.\n\n기계 학습의 기초 2-1~2-5 &gt; Machine Learning in Korean (1) | 카이스트 응용인공지능 연구실\n\naai.kaist.ac.kr\nTraining data set 여러 인스턴스가 주어지고, 각 인스턴스는 여러 속성(feature)과 최종 레이블(label)을 포함한다. 예: &lt;Sky, Temp, Humid, Wind, Water, Forest&gt; → EnjoySport Perfect World 예제 조건\n규칙 기반 학습(Rule-Based Learning) 성립을 위한 이상적인(완벽한) 환경을 가정하였다.\n즉, 현실에서는 불가능하지만, 이론적인 학습 개념을 이해하기 위해 아래와 같은 조건을 상정하였다.\n학습 데이터는 오류나 잡음이 없음 (noise-free). 목표 함수는 결정적(deterministic). 목표 함수가 가설 공간(hypothesis space) 내부에 존재한다고 가정 (“가설 공간이 충분히 크다”). 가설 (Hypothesis) 과정\n처음에는 매우 불완전한 가설: &lt;Sunny, ?, ?, ?, ?, ?&gt; 과 같이 일부 속성만 고정 이후 학습 데이터를 반영하면서 점진적으로 구체화 최종적으로 목표 함수 형태: &lt;Sunny, Warm, ?, Strong, ?, ?&gt; 등 일부 속성은 확정 즉, 학습 알고리즘은 데이터로부터 “이 속성은 고정해도 괜찮다”고 판단된 부분을 점점 좁혀가는 방식이다.\n목표 함수 (Target function)\n실제 이상적인 함수 (f)은 &lt;Sunny, Warm, ?, Strong, ?, ?&gt; 와 같이 더 구체적인 형태 학습자는 이 목표 함수에 최대한 근접하는 가설 (h)을 찾는 것이 목적 이 과정은 결국 “제한된 데이터로부터 숨겨진 함수(목표 함수)를 근사하는 것”이다.\n3 . 수학적 관점 & 한계 학습 알고리즘은 손실 함수 (loss function, 예: 제곱 오차, 0-1 손실 등)를 정의하고, 그 손실을 최소화하는 (h)를 선택하는 방식이다. [출처]\n만약 가설 공간이 목표 함수를 포함하지 않거나 너무 제한적이면 표현력 한계(approximation error)가 발생할 수 있다.\n또한 데이터가 잡음을 포함하거나 노이즈가 있으면 완전 근사는 불가능할 수 있다.\n인공신경망은 함수 근사의 강력한 도구로, 충분한 표현력을 가지면 임의의 연속 함수도 근사할 수 있다는 “보편 근사 정리 (Universal Approximation Theorem)”가 존재한다.\n02 노드 불순도 측정 지표 입력-출력 데이터로 학습 트리를 구성할 때, 노드 ((t))에 포함된 데이터의 클래스 분포 ({p(i|t)}_{i=1}^c)을 기준으로 불순도(impurity) 또는 불확실성(uncertainty) 을 측정하는 지표들이 사용된다.\n1 . 지니 불순도 Gini Impurity\n\\[\\mathrm{Gini}(t) = 1 - \\sum_{i=1}^c p(i \\mid t)^2\\]\n또는 (_i p(i t),(1 - p(i t))). 노드 내 임의의 샘플을 랜덤하게 라벨하면 잘못 분류될 확률이다. 값은 0 (노드가 하나의 클래스만 포함할 때) 에서 최대 불순도까지 범위를 가진다.\n2 . 엔트로피 Entropy\n\\[\\mathrm{Entropy}(t) = - \\sum_{i=1}^{c} p(i \\mid t) \\, \\log_2\\Big( p(i \\mid t) \\Big)\\]\n정보 이론적 관점에서, 해당 지표는 노드나 시스템 내 데이터의 무질서 정도를 나타내며, 동시에 특정 입력을 기반으로 결과를 예측할 때 발생하는 불확실성을 측정하는 역할을 한다.\n최대값은 (_2 c.) 이진 분류에서는 최대 1 이다.\n3 . 분류 오류율 Misclassification Error\n\\[\\mathrm{Error}(t) = 1 - \\max_{i} \\big( p(i \\mid t) \\big)\\]\n노드 내 다수 클래스로만 예측할 때의 잘못 분류 비율이다.\n이들 지표를 이용해 노드를 분할할 때는, 분할 전 불순도에서 분할 후 자식 노드들의 가중 평균 불순도를 빼는 값 (즉, 불순도 감소량, 혹은 정보 이득 개념)을 기준으로 한다. [출처]\n4 . 특성과 활용 결정 트리에서 불순도 지표들은 각각 특성이 다르다.\n지니 지수와 엔트로피는 노드 내 클래스 확률의 변화에 연속적으로 반응하므로, 작은 데이터 분포 변화도 분할 평가에 반영할 수 있어 세밀한 트리 성장을 가능하게 한다.\n반면, 분류 오류율은 확률 변화에 대해 불연속적으로 반응하므로 미세한 분할 개선을 평가하기에는 적합하지 않다.\n계산 측면에서는 지니 지수가 제곱 연산 중심이어서 비교적 계산이 간단하며, 엔트로피는 로그 연산을 포함하므로 약간 더 계산 비용이 든다.\n실제 트리 성장 기준으로는 일반적으로 지니 지수나 엔트로피를 분할 기준으로 사용하고, 분류 오류율은 리프 노드 결정 등 종착 조건 평가용 보조 지표로 활용된다.\n해석적 관점에서는 엔트로피가 정보 이론적 해석, 예를 들어 정보 이득이나 상호 정보와 연결되어 있어 속성 선택 기준에 직관성을 제공한다.\n03 정보 이득 Information Gain, IG\n의사결정나무(Decision Tree) 같은 지도학습 알고리즘에서, 어떤 속성(feature)을 기준으로 분할(split)을 했을 때\n불확실성(uncertainty) 또는 엔트로피(entropy)가 얼마나 감소하는지를 측정한 값이다.\n즉 분할 전의 엔트로피에서 분할 후 자식 노드들의 엔트로피를 가중평균한 값을 뺀 것이 정보 이득이다.\n\\[\\mathrm{IG}(S, A) = H(S) - \\sum_{v \\in \\mathrm{Values}(A)} \\frac{|S_v|}{|S|} \\, H(S_v)\\]\n(S): 전체 데이터 집합 (부모 노드) (A): 분할 기준으로 삼는 속성(feature) (S_v)​: 속성 (A)가 값 (v)을 갖는 샘플들의 부분 집합(자식 노드) (H()): 엔트로피 함수 분할 특성(feature)을 선택할 때, “가장 많이 불확실성을 줄이는” 속성을 선택하게 함으로써 트리의 분기가 효과적으로 이루어진다.\n트리의 뿌리(root) 쪽에는 정보 이득이 큰 특성이 온다.\n이는 여러 속성들 중 어느 속성이 타겟 레이블(label)에 대해 더 “정보를 많이 제공하는지” 또는 “불확실성 감소에 기여하는지” 비교할 수 있음.\n정보 이득은 속성(feature)이 갖는 값의 수(value count 또는 카테고리 수) 에 대해 편향(bias)이 있을 수 있음. 값의 종류가 많은 속성이 자칫 더 높은 정보 이득을 갖게 되는 경우 있다.\n이를 보완한 방법으로 정보 이득 비율(IG Ratio) 를 쓰는 경우가 많다."
  },
  {
    "objectID": "ai/ml/ml_03.html",
    "href": "ai/ml/ml_03.html",
    "title": "머신러닝: R2D3",
    "section": "",
    "text": "모델 학습과 R2D3 를 활용한 실무 적용 방법에 대해 다루고자 한다.\n01 신경망 Neural Network, NN\n현대 딥러닝 모델의 대부분은 신경망 구조에서 발전하였으며, 데이터의 특성(공간적·순차적 구조)과 문제 유형에 따라 다양한 형태로 변형되어 사용된다.\n1 . 인공 신경망 Artificial Neural Network\n인공 신경망은 딥러닝의 기본 구조로, 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 구성된다.\n각 뉴런은 입력값에 가중치(weight)와 편향(bias)을 적용한 뒤, 활성화 함수(activation function)를 통해 비선형 출력을 생성한다.\n딥러닝(DL)은 이러한 인공 신경망을 다층화(심층화)하여 복잡한 패턴과 비선형 관계를 학습할 수 있도록 확장한 구조이다.\n2 . 합성곱 신경망 Convolutional Neural Network, CNN\n이미지와 같이 격자형 구조를 가진 데이터의 공간적 패턴(spatial pattern)을 학습하는 데 특화되어 있다.\n합성곱(convolution) 연산을 통해 국소적인 특징(local feature)을 추출하며, 계층이 깊어질수록 점차 추상적인 특징을 학습한다.\n최근에는 Vision Transformer(ViT) 등과 같은 트랜스포머 기반 모델이 이미지 분석에도 적용되며, CNN과 함께 시각 분야의 핵심 구조로 활용되고 있다.\n3 . 순환 신경망 Recurrent Neural Network, RNN\n시간적 또는 순차적 데이터를 처리하기 위해 설계된 구조로, 이전 단계의 은닉 상태(hidden state)를 현재 입력과 함께 사용하여 시점 간 의존성(temporal dependency)을 학습한다.\n다만, 기본 RNN은 긴 시퀀스 처리 시 기울기 소실(vanishing gradient) 문제가 발생할 수 있다.\n이를 개선하기 위해 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit)와 같은 변형 구조가 널리 사용된다.\n4 . 트랜스포머 Transformer\n순환 구조 없이 시퀀스 데이터를 병렬적으로 처리할 수 있도록 고안된 모델로, 자기어텐션(Self-Attention) 메커니즘을 통해 입력 간 상관관계를 학습한다.\n이 구조는 RNN보다 장거리 의존성(Long-range dependency)을 효과적으로 포착하며, 자연어처리(NLP)를 넘어 이미지, 음성, 멀티모달 학습 등 다양한 분야로 확장되고 있다.\n요약하자면,\nCNN은 “공간적 패턴 추출”에, RNN은 “순차적 패턴 학습”에, Transformer는 “병렬적·전역적 패턴 학습”에 강점을 지닌다. 이들은 모두 “신경망”이라는 공통 기반 위에서 발전해 온 핵심적인 딥러닝 구조이다.\n02 사이킷런 이러한 신경망 기반 접근 외에도, 통계적 학습 이론을 바탕으로 한 전통적 머신러닝 기법이 존재한다.\n그 대표적인 라이브러리가 Scikit-learn이며, 파이썬 기반의 대표적인 머신러닝 라이브러리로 회귀, 분류, 클러스터링, 차원 축소 등 다양한 알고리즘을 일관된 인터페이스로 제공한다.\n이 라이브러리는 심층 신경망 구조인 CNN, RNN 등을 기본적으로 포함하지 않으며, 이러한 구조는 TensorFlow, Keras, PyTorch 와 같은 딥러닝 프레임워크에서 주로 구현된다.\n이 알고리즘은 본질적으로 데이터의 구역을 단순 분할이 아닌, 입력 변수(feature), 목표 변수(target) 간의 관계를 함수적 형태로 모델링하는 방식이다.\n일반적으로 데이터는 ” 행(sample) × 열(feature) ” 형태의 구조를 가지며, 각 행은 개별 관측치를, 각 열은 설명 변수를 나타낸다.\n모델은 이러한 특징들을 바탕으로 목표 변수를 예측하는 함수를 학습한다.\n예: 아파트 가격 예측 문제에서 면적, 교통, 교육 환경 등 다양한 요인을 특징으로 사용 시 데이터는 고차원 공간에 분포하게 된다.\n이때 데이터가 3차원에서 도넛 형태와 같이 복잡한 분포를 보일 수 있으며, 단순한 선형 경계로는 구분하기 어려운 경우가 많다.\n이러한 상황에서는 PCA, t-SNE, UMAP 과 같은 차원 축소 기법을 활용하여 고차원 데이터를 저차원 공간으로 투영함으로써 데이터의 패턴과 구조를 보다 명확하게 시각화할 수 있다.\n03 고도와 주택 가격 R2D3의 시각적 기계학습 입문 글에서는 샌프란시스코와 맨해튼의 주택 가격 데이터를 활용하여, 머신러닝 모델이 입력(feature)과 목표(target) 간 관계를 학습하는 과정을 시각적으로 설명한다.\nA visual introduction to machine learning\nWhat is machine learning? See how it works with our animated data visualization.\nwww.r2d3.us\n샌프란시스코는 언덕 지형이 많아 고도(elevation)가 주택 가격에 영향을 미치는 중요한 요인이 된다.\n반면 맨해튼은 대체로 평지 구조이므로 고도 요인의 영향이 미미하거나 다르게 나타난다.\n이처럼 고도는 미국 부동산 시장에서 elevation 이라는 변수로 다뤄지며, 조망(view), 프라이버시, 도시 스카이라인 등과 함께 가격 형성에 작용할 수 있다.\n한국의 경우, 언덕 위 아파트는 접근성 저하, 급경사 도로, 기반 시설 확충 비용 등의 이유로 가격이 낮게 평가되는 경향이 있다.\n반대로 바닷가 조망이 있는 주택은 오히려 낮은 가격을 받는 경우도 있다. 이는 고도 자체보다 조망성, 접근성, 지형 조건 등 복합 요인들이 작용한 결과라 볼 수 있다.\n이러한 차이는 머신러닝에서 하나의 입력 변수(feature)가 지역적·사회적 맥락에 따라 서로 다른 패턴을 학습하게 되는 과정을 잘 보여준다.\n입력 변수(feature)를 2D 평면에 점으로 시각화하고, 이 점들을 기준으로 분류 경계(decision boundary)를 학습하는 과정을 설명한다.\n이와 유사하게, 부동산 시장에서 고도(elevation)를 하나의 특징(feature)로 입력하면, 모델은 고도와 가격 패턴 사이의 관계를 학습하여 경계 또는 함수 형태로 변환할 수 있다.\n예를 들어, 한국에서는 고도 증가가 가격 하락으로 이어지는 함수적 경향이, 미국 일부 지역에서는 고도 증가가 가격 상승과 연결된 함수적 경향이 학습될 수 있다.\n즉, 고도(elevation)는 부동산 가격 모델링에서 하나의 중요한 입력 변수이며, 지역별 맥락과 다른 특징들과의 상호작용을 고려해야 한다.\n이러한 지역별 차이를 반영하여, 머신러닝 모델은 학습 데이터 기반으로 최적의 예측 함수를 찾아내게 된다.\n전체 데이터를 학습용(train), 검증용(validation), 테스트용(test) 으로 분할한다.\n예를 들어, 전체의 70 %를 학습용, 30 %를 테스트용으로 나눈 뒤, 학습용 중 일부(예: 10–20 %)를 검증용으로 재분할한다.\n학습 중에 매 epoch마다 학습 정확도와 검증 정확도를 기록하여 학습 상태를 모니터링한다. 검증 정확도가 점진적으로 상승하면 정상적인 학습 경향이다.\n하지만 검증 정확도가 자주 들쭉날쭉하거나, 학습 정확도만 계속 상승하고 검증 정확도는 정체되면 과적합 또는 학습 불안정성을 의심해야 한다. 이때 하이퍼파라미터 조정, 정규화(regularization) 적용, 또는 데이터 분할 재설계 등을 고려한다.\nR2D3 예제에 따르면,\n학습 정확도(training accuracy): 100% 검증 정확도(validation accuracy): 89.7% 구체적으로,\n뉴욕 주택: 112개 중 100개 정확 분류 샌프란시스코 주택: 130개 중 117개 정확 분류 이는 모델이 학습 데이터에 과적합되어 있으며, 현실적 적용에는 제한적임을 의미한다. 실무 적용 기준으로는 최소 95% 이상의 검증 정확도가 요구되므로, 현 시점에서의 모델 적용은 신중을 요한다.\n04 결정 트리 학습과 과적합 관리\n1 . 단일 변수 기반 분류의 한계 의사결정트리는 각 노드에서 조건문(if-then)을 통해 데이터를 두 그룹으로 분할하며, 경계를 설정하여 분류를 수행한다.\n그러나 단일 변수(예: elevation)만으로는 고도와 주택 가격 간의 비선형적 관계, 지역별 특성 등을 충분히 반영할 수 없어 데이터 간 명확한 구분이 어렵다.\nR2D3 시각화 예시에서도 나타나듯, 고도 하나만으로 주택 가격 범주를 정확히 분류하기 어려우므로, 면적, 교통 접근성, 조망 등 여러 변수(feature)를 고려한 다변량 분할이 필요하다.\n2 . 최적 분할과 분할 기준 트리는 각 노드에서 모든 후보 특성(feature)과 임계값(threshold)을 조합하여 가능한 분할(split candidates)을 탐색한다.\n각 후보 분할은 엔트로피(entropy), 정보 이득(information gain), 지니 불순도(Gini impurity) 등 불순도 지표를 통해 평가되며, 가장 데이터가 균질하게 나뉘는 최적 분할(best split)이 선택된다.\n이 과정을 반복하면, 학습 데이터가 점차 균질한 소그룹으로 세분화되어 목표 변수(target)에 대한 예측 성능이 향상된다.\n3 . 과적합의 발생과 영향 트리 깊이가 증가하면 모델이 학습 데이터에 과도하게 적합(overfit)될 수 있다.\n학습 데이터에서는 높은 정확도를 나타내지만, 테스트 데이터에서는 일반화 성능이 저하되는 과적합 현상이 발생한다.\nR2D3 예시에서는 단일 변수 분할을 반복하여 분류 경계를 지나치게 세분화함으로써 일부 데이터 포인트가 잘못 분류되는 사례가 확인된다.\n이러한 과적합은 모델이 학습 데이터의 불필요한 변동과 노이즈까지 학습하게 만들어, 실무에서의 모델 신뢰도를 떨어뜨리는 부작용을 초래한다.\n4 . 과적합 방지 전략 과적합을 방지하기 위해서는 여러 전략을 병행할 수 있다.\n먼저, 모델 제약을 통해 트리의 최대 깊이(max depth)나 최소 샘플 수(min samples split)와 같은 파라미터를 제한하면, 지나치게 세분화되는 것을 방지할 수 있다.\n또한, 가지치기(pruning)를 통해 불필요하게 세분화된 분할을 사후적으로 제거하며 모델 단순화 및 일반화 성능을 향상할 수 있다.\n마지막으로, 교차검증(cross-validation)을 활용하여 학습과 검증을 반복하면, 모델이 특정 데이터셋에만 과적합되는 것을 방지하고 안정적인 성능을 확보할 수 있다."
  },
  {
    "objectID": "ai/ml/ml_01.html",
    "href": "ai/ml/ml_01.html",
    "title": "AI 산업 전망",
    "section": "",
    "text": "AI 산업 전망에 대해 다루고자 한다.\n01 팬데믹\n1 . 온라인 전환 및 IT 수요 급증 팬데믹으로 인해 오프라인 중심이던 기업들이 온라인으로 빠르게 전환되었다.\nMS의 CEO인 Satya Nadella는 2020년 4월 30일에 발표된 분기 실적 보고서에서 다음과 같이 언급하였다:\n” 우리는 두 달 만에 2년 분량의 디지털 전환을 경험했다. ” [출처] 재택근무와 원격교육이 보편화됨에 따라 화상회의, 실시간 번역, 맞춤형 배경 등 다양한 비대면 기술이 빠르게 발전했으며, 이로 인해 협업 솔루션 시장은 급격히 확대되었다.\n연도 시장 규모 추정치 출처/비고 2019 약 9.878 십억 달러 Allied Market Research 2020 약 15.25 십억 달러 Fortune Business Insights 2021 추정치 계산 불가 COVID-19 효과가 강해, 일관된 비교가 어렵다는 한계 있음. 2022 약 27.4081 십억 달러 Grand View Research 2023 약 21.79 십억 달러 Fortune Business Insights 2024 약 36.1142 십억 달러 Grand View Research 미국의 온라인 쇼핑 매출은 2019년 5,712억 달러에서 2020년 8,154억 달러로 전년 대비 43% 증가했다. [출처]\n유럽에서도 2020년 4월, 전체 소매 판매는 큰 폭으로 감소했지만, 온라인 구매 비중은 2019년 4월 19.1%에서 30.7%로 급증했다. [출처]\n각국의 락다운이 시행되면서 화상회의, 원격근무, 온라인 교육 등의 증가로 인터넷 트래픽이 1주일 이내에 15 ~ 20% 증가했다. [출처]\n2 . 팬데믹 종료 이후 이전 수준으로 회귀 여부 소비 회복의 불균형 가능성이 언급되었다.\n팬데믹으로 대면 서비스 수요는 급감했지만, 팬데믹 이후 일부 행태는 유지될 가능성이 컸다. [출처]\n또한, 많은 오프라인 매장의 폐업, 상업용 부동산 공실 증가 등은 온라인 전환이 영구적인 변화였음을 시사한다.\n1 . 오프라인 회복, 여전히 주요 비중 유지 2019년 디지털 판매 성장률이 53.5%였던 반면, 2022년 오프라인 매출은 전체 성장의 78.1%를 차지하며 오프라인 매장이 여전히 매출 성장의 주요 원천임을 보여준다. [출처]\n실제 매장 개방 시 디지털 매출이 평균 6.9% 증가하지만, 매장을 닫으면 온라인 매출은 11.5% 줄어든다는 연구도 있다. 이는 오프라인 매장의 존재가 디지털 매출에도 긍정적 영향을 준다는 점을 시사한다. [출처]\n2 . 오프라인이 다시 강세를 보이는 추세 팬데믹 중 급성장한 전자상거래는 이후 이전 예상 수준으로 회귀하는 경향이 있는 반면, 오프라인 소매업은 예상보다 더 높은 회복세를 보였다. 오프라인 소매 비중이 온라인보다 더욱 빠르게 반등했다는 분석도 있다. [출처]\n3 . 장기적인 변화는 지속 중 “다시는 이전으로 돌아가지 않을 것”이라는 표현이 있을 만큼, 디지털화된 경제 구조는 팬데믹 이후에도 지속될 가능성이 크다는 분석도 존재한다. 변화된 생산 및 소비 방식이 장기적으로 유지될 것이라는 견해이다.\n02 GPT\n1 . GPT 탄생, 챗봇 혁명 OpenAI는 2022년 11월 30일, GPT-3.5 기반의 ChatGPT를 공개적인 “연구 미리보기(research preview)” 형태로 출범시켰다.\n이 발표 직후 비약적인 성장세를 보였으며, 출시 두 달 만에 1억 명 이상의 사용자를 확보하는 등 빠른 이용자 확산을 기록했다.\n이 모델은 텍스트 기반 응답뿐만 아니라, 코드 생성 및 코드 완성이 가능한 능력을 갖추고 있어 많은 개발자가 실제로 활용 중이다.\n2 . 저렴한 수준의 AI 코딩 서비스 OpenAI가 제공하는 최상위 구독 요금제인 ChatGPT Pro는 월 $200에 무제한 GPT-4o Pro 접근권, 고급 음성 기능 등을 포함하며, 이는 약 50만 원 수준에 해당한다. [출처]\n구글도 마찬가지로 AI Ultra 요금제를 발표했으며, 가격은 $249.99/월로, Deep Think reasoning 모드 등 고급 기능을 탑재한 모델 및 AI 툴 세트를 제공한다. [출처]\n위 요금제는 고급 모델을 사용하려는 개인 혹은 연구자에게 적합하며, 초급 개발자를 비용 대비 대체하는 수준의 AI 기능을 충분히 포함하고 있다.\n3 . 월 10 ~ 20만 원대의 AI 코딩 도구 보다 현실적인 예시로, 다음과 같은 AI 코딩 어시스턴트도 존재한다: [출처]\n구분 비용 및 특징 GitHub Copilot 약 $10/월 , GPT-4 기반 코드 완성 기능 제공 Cursor Pro 약 $20/월, 다양한 모델 액세스 및 IDE 통합 Tabnine Pro 및 유사 도구들 $12–20/월 이들 도구는 초급 개발자 수준의 코딩 지원에 필요한 기능을 충분히 제공하며, 월 약 10 ~ 20만 원의 비용으로 사용 가능하다.\n03 경제 둔화\n1 . 물가 상승 → 금리 인상 중앙은행은 높은 인플레이션을 억제하기 위해 기준금리를 인상한다. 이는 통화정책의 기본 방향이며, 비교적 일반적인 경제 수단이다.\n테일러 룰(Taylor Rule)에 따르면, 인플레이션이 1% 상승할 때마다 명목금리는 그 이상을 인상해야 실제 금리가 상승하여 경제를 안정시키는 효과가 생긴다.\n2 . 금리 인상 → 고용 감소 물가 상승을 막기 위한 금리 인상은 기업의 비용 부담을 높여 고용 축소로 이어질 수 있으며 일부 계층(예: 여성, 소수민족 등)에 더 큰 영향을 줄 수 있다. [출처] 실제로 금리 인상 후 경기 침체(리세션) 즉 고용 시장이 위축되는 경향이 있다.\n3 . 고용 감소 → 소비 위축 고용이 줄면 소득 및 소비 여력이 감소되어, 결과적으로 경제활동 전반에 부정적 영향을 미친다. 이는 중앙은행이 다시 경기 부양을 위한 금리 인하를 고려하게 만드는 요인 중 하나이다. [출처]\n4 . 소득 감소 및 불확실성 → 투자 여력 축소 고용 불안정과 소비 위축은 기업의 투자 여력도 감소시킨다. 이는 재정의 불안정과 기업 활동 위축으로 이어질 수 있다는 일반적 경제 논리와 맞닿아 있다.\n04 고령화\n1 . 기술직 내 세대별 노동력 변화 대형 테크 기업에서 직원 평균 연령이 약 3년 이상 상승했다. 이는 밀레니얼 세대 이상이 IT 업계 내에서 지배적인 위치를 차지하게 되었음을 보여준다. 동시에, 디지털에 익숙한 젊은 세대(Gen Z)의 비중이 감소하고 있는 것으로 해석된다. [출처]\n이러한 경향은 “신입보다 시니어 인력이 주류가 되는 현상”과 연관시켜 볼 수 있다.\n2 . 고령화된 노동층의 도전 및 기회 AI 도입 과정에서 고령 노동자를 배제하지 않고 포함할 수 있도록 하는 전략 즉 ’age-proofing AI’의 필요성을 강조한다.\n이는 인력 다양성과 포용성을 위한 조치이며, 고령자 중심 구조가 불가피한 사회 변화로 다가올 수 있음을 시사한다. [출처]\n55–64세 연령층의 고용률은 학력 수준에 따라 크게 차이가 있으며, 고학력자 중심으로 고령 노동 시장에서의 참여가 증가하고 있다. [출처]\n3 . 시니어 중심 노동 시장에 대한 사회적 우려 45세 이상의 IT 종사자 중 70%가 연령 차별을 경험하거나 목격한 적이 있다고 응답했다.\n이 사실은 고령 인력이 노동 시장에서 고립될 수 있다는 구조적 위험도 내포한다. [출처]\n중국의 “curse of 35” (35세 저주) 사례도 주목할 만한다. 일부 IT 기업들은 30대 중반 이상 직원을 비용 부담과 에너지 부족을 이유로 선호하지 않는 경향이 있으며, 이는 고령 인력의 지위가 위태로울 수 있음을 나타낸다. [출처]\n4 . 기업 내 핵심 지식과 경험의 상실 U.S. 기업들의 경우, 베이비붐 세대가 은퇴하면서 조직의 핵심 지식이 사라지고 경험이 축적되지 않는 상황에 직면하고 있다. 이는 기업의 효율성과 혁신 역량 저하로 이어질 수 있다. [출처]\n고령 직원의 은퇴는 생산성 및 리더십 공백을 초래할 수 있어, 이를 대비한 체계적인 승계 계획과 멘토링 프로그램 수립이 필수적이라고 강조한다. [출처]\nNASA의 아폴로 사업 경험처럼, 핵심 인력 퇴직 후 해당 지식을 대체하지 못한 사례들도 있다. [출처] 2005년 Accenture 조사에서는 많은 기업들이 은퇴자 지식 이전을 위한 계획조차 없었음을 보고했다. [출처]\n5 . 고령 노동 비중 증가 추세 2022년 대비 2032년까지, 65세 이상 노동자의 노동 참여율이 6.6%에서 8.6%로 상승하며, 전체 노동 인구 증가분 중 57%가 고령층에 해당할 것으로 전망된다. 이는 고령층 노동자 비중이 높아지는 구조적 변화를 가리킨다. [출처]\nベイビーブーマー 세대의 대규모 은퇴로, Gen X, 밀레니얼, Z세대가 노동 시장의 주요 역량으로 부상하며, ’Silver Tsunami’로도 불리는 전환기가 도래하고 있다. [출처]\n05 산업 변곡점 1998년, NIPA(한국 IT산업진흥원)이 설립되어 대한민국의 IT 산업 육성을 위한 중추적 역할을 해 왔다.\n1 . 한국 정부의 AI 전략 집중 2025년 8월, 한국 정부는 AI 및 혁신 프로젝트 30개 추진을 골자로 한 AI 중심 경제 전략을 발표했다. 로봇, 자동차, 반도체, 드론 등 산업 전반을 포함하며, 국가 성장 펀드 규모는 100조 원 규모이다. [출처]\n또한 국가 AI 전략위원회를 설립하여 AI 정책 및 전략을 대통령 직속으로 관리하고 있다.\n“한국 정부는 AI를 국가 전략 산업으로 설정했고, 이제 본격적인 출발점에 서 있다.” 결과적으로, 기존 IT는 계속 존재하지만 정책·투자·혁신 관점에서는 AI가 1순위, 전통 IT는 2순위라는 구조적 변화라고 볼 수 있다.\nAI 산업에서의 경쟁 우위 확보는 주요 국정 과제로 꼽힌다. 이를 위해서는 데이터 확보 및 AI 인프라가 중요한 전략으로 강조된다. [출처]\n2 . 플랫폼을 통한 데이터 확보 한국에서는 네이버가 검색 엔진 시장의 절반 이상을 차지하고 있으며, 외국 Big Tech의 점유율은 상대적으로 낮게 유지되고 있다는 보고가 있다. [출처]\n특히, 한국은 지리정보 데이터와 같은 특정 데이터에 대해 엄격한 규제를 두고 있으며, 이로 인해 Google Maps 같은 서비스 기능이 완전히 작동하지 못하는 경우가 있다.\n또, Google은 한국의 앱스토어 수수료 및 자사 앱 우선 노출 등 관련하여 공정거래위원회 또는 규제 대상이 된 사례가 있으며, 이는 국내 플랫폼 보호와 경쟁 환경의 복합적 모습을 보여준다. [출처]\n한국 내 검색·광고 시장 분석에서는 네이버가 국내 사용자 특성, 언어, 콘텐츠 생태계와의 적합성 덕분에 강한 지위를 유지하고 있다는 언급이 있다. [출처]\n네이버와 카카오가 AI·콘텐츠 강화 전략을 내세우며 “자체 생태계 확장”을 추진 중이라는 보도도 있다. [출처]\n3 . 일본의 플랫폼 부재 일본은 전통적으로 내수 중심 플랫폼 기업이 한국·중국 등에 비해 적다는 평가가 있다.\n언어 및 문화, 법률·규제 측면에서 외국 기술 및 플랫폼 기업에 의존하는 경향이 있었고, 이에 따라 일본 정부나 대기업이 데이터 주권, 플랫폼 통합성 확보에 더 민감할 가능성 있다.\n일본과 유럽 간 데이터 공간(data space) 구현 방식을 비교한 연구에서는, 일본이 거버넌스 체계나 인증 프레임워크, 기술 표준 면에서 유럽보다 상호 운용성 확보에 어려움을 겪고 있다는 분석이 나온다. [출처]\n저작권 및 개인정보 보호 규제 측면에서, 일본은 AI 학습용 데이터 활용에 있어 일부 유연한 접근을 허용한 사례도 있으나, 여전히 데이터 접근과 보안 규제의 균형이 과제로 남아 있다. [출처]\n특허 기반 연구에서도 일본 기업들이 기술 복잡성 측면에서는 대응 가능하지만, 신생 플랫폼 기업의 민첩성과 확장성에서는 경쟁국에 비해 약점이 있다는 해석이 나온다. [출처]\n① 생성형 AI는 대규모 데이터와 학습용 데이터의 접근성이 핵심이다.\n일본이 거버넌스 체계, 인증 프레임워크, 기술 표준 면에서 유럽보다 상호 운용성 확보가 어렵다면, 다양한 출처의 데이터를 통합·활용하는 데 제약이 생긴다.\n결과적으로 AI 모델 학습 속도와 성능 향상에 제한이 발생할 수 있으며, 저작권·개인정보 보호 규제의 균형 문제도 데이터 확보와 활용에 직접적 영향을 준다.\n② AI 시스템 개발에는 소프트웨어 설계, 최적화, 확장성이 필수적이다.\n일본의 소프트웨어 산업이 플랫폼 경쟁력 확보에서 취약하다면, 혁신적 AI 서비스 개발이나 글로벌 경쟁에서 불리할 수 있다.\n특허 기반 연구에서는 기술 복잡성 대응은 가능하지만, 신생 플랫폼 기업의 민첩성 및 확장성 부족은 시장 적응과 혁신 속도에 제한을 줄 수 있다.\n따라서 한국도 국내 플랫폼이 강력한 우위를 가진다고 방심할 수 없다. 데이터 확보, 기술 표준화, 소프트웨어 역량 강화, 규제 환경의 유연화 등 다각적인 대비가 필요하며, 지속적인 혁신과 인프라 확충을 통해 경쟁 우위를 유지해야 한다.\n06 AI의 전망\n1 . AI 산업의 급성장 MS는 AI 인프라 공급업체인 Nebius와 최대 200억 달러 규모의 5년 계약을 체결하였다. 이는 AI 모델 학습과 추론을 위한 GPU 리소스 확보를 위한 전략의 일환이다. [출처]\nNVIDIA는 차세대 AI 칩인 ’Rubin CPX’를 2026년 말 출시할 예정이며, 이는 고해상도 비디오 생성 및 AI 기반 SW 개발에 최적화되어 있다. [출처]\n데이터 센터의 전력 수요가 2027년까지 50%, 2030년까지 165% 증가할 것으로 전망하고 있다. [출처]\n2 . 데이터와 GPU의 중요성 IDTechEx는 2025년 GPU 배치량이 기하급수적으로 증가할 것으로 예상하며, 이는 AI 모델 훈련과 추론에 필수적인 요소로 작용하고 있다. [출처]\n데이터 센터 GPU 시장이 2025년 1,199억 7,000만 달러에서 2030년까지 2,280억 4,000만 달러에 이를 것으로 예상하고 있다. [출처]\n3 . AI 관련 인력 수요 Databricks는 AI 제품에 대한 수요 증가로 연간 수익이 40억 달러에 이를 것으로 예상하며, 이는 AI 관련 인력의 수요 증가를 시사한다. [출처]\n4 . AI 산업의 지속 가능성 NVIDIA는 AI 시장이 연평균 성장률(CAGR) 26.6%로 성장할 것으로 전망하며, 이는 AI 산업의 지속 가능성을 뒷받침한다. [출처]\n현재 엔비디아(NVDA)의 주가는 170.76달러로, 전일 대비 2.58달러(1.53%) 상승하였다. 이는 AI 산업의 성장과 엔비디아의 시장 지배력을 반영하는 지표로 볼 수 있다."
  },
  {
    "objectID": "ai/hg_04.html",
    "href": "ai/hg_04.html",
    "title": "다양한 분류 알고리즘",
    "section": "",
    "text": "로지스틱 회귀와 확률적 경사 하강법과 같은 분류 알고리즘을 배우고, 이진 분류와 다중 분류의 차이를 이해하며, 각 클래스에 대한 확률 예측에 대해 다루고자 한다.\n04 - 1 . 로지스틱 회귀\n4-1 로지스틱 회귀.ipynb\nRun, share, and edit Python notebooks\ncolab.research.google.com import pandas as pd # 데이터 준비하기 fish = pd.read_csv(‘https://bit.ly/fish_csv_data’) fish.head()\nSpecies를 타겟으로 하고 나머지를 입력 데이터로 사용한다.\n어떤 종류의 생선이 있는지 확인한다.\nprint(pd.unique(fish[‘Species’]))\n7가지의 생선 종이 있음을 확인.\nSpecies를 제외한 새로운 데이터프레임 만들기.\nfish_input = fish[[‘Weight’,‘Length’,‘Diagonal’,‘Height’,‘Width’]].to_numpy() print(fish_input[:5])\n새로운 데이터 프레임이 만들어 졌음을 확인.\n모델 만들기.\n\n‘Species’ 열의 데이터를 타겟 변수로 설정\nfish_target = fish[‘Species’].to_numpy()\nfrom sklearn.model_selection import train_test_split # 데이터를 훈련 세트와 테스트 세트로 나누기 train_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state = 42) # 결과 재현을 위한 설정\nfrom sklearn.preprocessing import StandardScaler ss = StandardScaler() # StandardScaler 객체 생성 ss.fit(train_input) # 표준화 전처리 작업 train_scaled = ss.transform(train_input) # 훈련 데이터 변환 test_scaled = ss.transform(test_input) # 테스트 데이터 변환\nfrom sklearn.neighbors import KNeighborsClassifier # k-NN 분류기 객체 생성 (k=3) kn = KNeighborsClassifier(n_neighbors=3) # 훈련된 데이터를 사용하여 모델 학습 kn.fit(train_scaled, train_target)\n\n\n훈련 데이터 정확도 출력\nprint(kn.score(train_scaled, train_target)) # 테스트 데이터 정확도 출력 print(kn.score(test_scaled, test_target))\nfish[‘Species’]는 여러 종류의 물고기 종(species)을 대상으로 하므로, 각 샘플이 둘 이상의 클래스 중 하나로 분류된다.\n이처럼 클래스가 3개 이상일 때, 이를 다중 분류라고 한다.\nk-NN 분류기는 다중 분류 문제에서도 잘 동작하며,\n각 데이터 포인트에 대해 가장 가까운 이웃들 중 다수의 클래스를 선택하는 방식으로 클래스를 예측한다.\nscikit-learn의 장점\n분류 알고리즘에서 문자열로 된 타겟값도 자동으로 처리할 수 있도록 설계되어 있다. 내부적으로는 문자열을 고유한 정수 값으로 인코딩하여 사용되며 이때, 사용자는 별도의 인코딩을 하지 않아도 된다.\n문자열 타겟값을 내부적으로 처리할 때에는 알파벳 순서대로 고유한 정수로 인코딩한다.\n예를 들어, 타겟값이 [ ‘Tuna’, ‘Salmon’, ‘Bass’ ] 일 때 다음과 같이 인코딩된다.\n\nBass ⇨ 0\nSalmon ⇨ 1\nTuna ⇨ 2\n\n그러므로, 분류 모델에서 예측된 클래스가 어떤 값인지 해석할 때 이를 염두 해야 한다.\n특히 모델의 성능 평가나 결과 해석 시, 클래스 레이블이 자동으로 정렬된다는 점에 주의해야 한다.\n인코딩 순서를 직접 지정하고 싶다면,\nLabelEncoder 또는 pandas.Categorical을 사용해 명시적으로 타겟값을 숫자로 변환할 수 있다.\n정렬된 순서 확인하기.\nprint(kn.classes_)\n알파벳 순서로 정렬된 것을 확인.\n테스트 세트에 있는 처음 5개의 샘플을 예측해본다.\nprint(kn.predict(test_scaled[:5]))\n5개의 샘플에 대한 예측이 어떤 확률을 갖는지 확인하기.\nimport numpy as np\nproba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) # 소수점 네 번째 자리 표시\n위 숫자는 각각 [ Bream ~ Whitefish ] 순서에 해당된다.\n네 번째 샘플의 최근접 이웃의 클래스 확인하기.\ndistances, indexes = kn.kneighbors(test_scaled[3:4]) print(train_target[indexes])\n1개의 Roach = 0.3333 + 2개의 Perch = 0.6667 클래스 확률을 정확히 예측하였다.\n그러나, k = 3일 경우 이웃이 포함될 확률이 0/3, 1/3, 2/3, 3/3과 같은 제한된 확률만을 가진다.\n이는 확률적으로 다소 불안정한 결과를 초래할 수 있으며, 분류를 좀 더 세밀하게 조정하는 데 한계가 있다.\n그러므로, 더 나은 성능을 얻기 위해 다른 방법이 필요할 수 있다.\n로지스틱 회귀 (Logistic Regression)\n이진, 다중 분류 문제에서 자주 사용되는 통계적 기법.\n선형 회귀와는 달리 예측값이 0 ~ 1 사이의 확률값으로 제한되어 분류 문제에 적합하다.\n이 모델은 본질적으로 선형 모델지만,\n예측값이 (– ∞ ~ + ∞) 범위를 가지므로, 그 값을 그대로 사용하지 않는다.\n이는 분류 문제로 만들기 위한 과정으로 시그모이드 함수를 이용하여 예측값을 0 ~ 1 사이의 확률로 변환할 수 있다.\n피(Φ, φ: 그리스어 알파벳의 21번째 글자)\n–5 ~ +5 사이에 0.1 간격으로 배열 z를 만든 뒤, z 위치마다 시그모이드 함수를 계산한다.\nimport numpy as np import matplotlib.pyplot as plt\nz = np.arange(-5, 5, 0.1) phi = 1 / (1 + np.exp(-z))\nplt.plot(z, phi) plt.xlabel(‘z’) plt.ylabel(‘phi’)\n0 ~ 1의 범위를 갖는 것을 확인.\n\n로지스틱 회귀로 이진 분류 수행하기.\n\n\n\n도미와 빙어의 행만 골라내기.\n\n\n도미&빙어 = True, 그 외 = False\nbream_smelt_indexes = (train_target == ‘Bream’) | (train_target == ‘Smelt’) train_bream_smelt = train_scaled[bream_smelt_indexes] target_bream_smelt = train_target[bream_smelt_indexes]\nfrom sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_bream_smelt, target_bream_smelt)\n처음 5개의 샘플 예측하기.\nprint(lr.predict(train_bream_smelt[:5]))\n두 번째 샘플을 제외한 나머지를 도미로 예측.\n처음 5개의 샘플에 대해 예측 확률 확인하기.\nprint(lr.predict_proba(train_bream_smelt[:5]))\n[ 왼쪽: 음성 클래스(0) / 오른쪽: 양성 클래스(1) ] 샘플마다 2개의 확률이 출력되었다.\n어떤 것이 양성 클래스인지 알기 위해, 알파벳으로 정렬하기.\nprint(lr.classes_)\n오른쪽에 있는 빙어(Smelt)가 양성 클래스인 것을 확인. 따라서, 로지스틱 회귀를 통해 성공적으로 이진 분류를 수행하였다.\n로지스틱 회귀가 학습한 계수 확인하기.\nprint(lr.coef_, lr.intercept_)\n처음 5개의 샘플의 z값을 출력.\ndecisions = lr.decision_function(train_bream_smelt[:5]) print(decisions)\n이 값은 시그모이드 함수를 적용하여 해당 클래스에 속할 확률로 변환하기.\nfrom scipy.special import expit print(expit(decisions))\n두 번째 열의 값이 양성 클래스임을 확인.\n\n로지스틱 회귀로 다중 분류 수행하기.\n\nlr = LogisticRegression(C = 20, max_iter = 1000) lr.fit(train_scaled, train_target)\nprint(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target))\n훈련 세트와 테스트 세트 모두에서 높은 점수를 기록했으며, 과대적합이나 과소적합의 문제가 나타나지 않았다.\n테스트 세트의 처음 5개의 샘플 예측하기.\nprint(lr.predict(test_scaled[:5]))\n테스트 세트의 처음 5개의 샘플에 대한 예측 확률 출력하기.\nproba = lr.predict_proba(test_scaled[:5]) print(np.round(proba, decimals=3))\n5개의 행과 7개의 열을 통해, 다중 분류임을 확인.\n클래스 정보 확인하기.\nprint(lr.classes_)\n위 예측 확률 중 가장 큰 값이 예측된 클래스가 된다.\n다중 분류의 선형 방정식 알아보기.\nprint(lr.coef_.shape, lr.intercept_.shape)\n이 로지스틱 회귀 모델은 7개의 클래스(다중 분류)를 예측하며, 각 클래스에 대해 5개의 특성을 고려한 선형 결정을 내리고 있음을 알 수 있다.\n소프트맥스 함수 (Softmax)\n다중 클래스 분류 문제에서 각 클래스에 대한 확률을 계산하기 위해 사용되는 함수.\n주어진 입력 값(로지스틱 회귀에서의 결정값)을 확률로 변환하여, 각 클래스에 속할 확률을 반환한다.\n결과적으로, 출력된 확률들의 총합이 1이 되도록 정규화한다.\n소프트맥스 함수 사용하여, 확률 변환하기.\ndecision = lr.decision_function(test_scaled[:5]) print(np.round(decision, decimals = 2))\n샘플에 대한 예측 확률 출력하기.\nfrom scipy.special import softmax proba = softmax(decision, axis = 1) print(np.round(proba, # 소수점 세 번째자리 decimals = 3))\n이전에 구한 배열과 결과가 일치한다.\n결과적으로, 7개의 생선 종류에 대한 확률을 예측하는 모델을 성공적으로 훈련하였다.\n04 - 2 . 확률적 경사 하강법\n4-2 확률적 경사 하강법.ipynb\nRun, share, and edit Python notebooks\ncolab.research.google.com\n확률적 경사 하강법 (Stochastic Gradient Descent, SGD)\n기계 학습에서 최적화를 위해 널리 사용되는 알고리즘 중 하나. 경사 하강법의 변형 중 하나로, 특히 큰 데이터셋을 처리할 때 효과적이다.\n이 알고리즘은 매 반복마다 손실 함수의 기울기를 계산한 뒤, 그 방향으로 파라미터를 업데이트한다.\n이를 통해 손실을 줄이도록 학습하는 과정이다.\n에포크 (Epoch)\n전체 훈련 데이터셋을 모델이 한 번 완전히 통과한 주기.\n한 번의 에포크는, 모델이 전체 데이터셋을 사용하여 파라미터를 업데이트하는 과정을 한 번 수행한 것이다.\n미니 배치 경사 하강법 (Mini-Batch Gradient Descent)\n전체 데이터셋을 작은 배치(mini-batch)로 나누어 각각의 배치에 대해 파라미터를 업데이트하는 방법.\n한 번의 업데이트에 하나의 데이터 포인트인 단일 샘플만 사용하는 SGD와 달리\n여러 샘플(미니 배치)로 구성된 소규모 데이터 세트를 사용한다.\n배치 경사 하강법 (Batch Gradient Descent)\n전체 데이터셋을 사용하여 파라미터를 업데이트하는 방법. 모든 샘플을 사용해 손실 함수의 기울기를 계산한다.\n손실 함수 (loss function)\n기계 학습 및 통계에서 모델의 예측 성능 평가용 함수.\n모델이 예측한 값과 실제 값 간의 차이를 수치적으로 나타내며, 이 값을 최소화하는 것이 학습의 목표이다.\n이진 교차 엔트로피 손실 (Binary Cross-Entropy Loss)\n로지스틱 회귀에서의 손실 함수.\n모델의 예측 확률과 실제 클래스 레이블 간의 차이를 측정하여, 모델을 최적화하는 데 사용된다.\n이 손실 함수는 모델의 예측이 실제 클래스와 얼마나 일치하는지를 측정한다.\n예측이 정확할수록 손실 값이 0에 가까워지고, 예측이 틀릴 경우 손실 값은 크게 증가하며, 이론적으로 무한대까지 커질 수 있다.\n특성값의 스케일을 맞춘 두 Numpy 배열 준비하기.\n\n\n이전과 동잉한 코드\nimport pandas as pd fish = pd.read_csv(‘https://bit.ly/fish_csv_data’)\n\n\nSpecies ⇨ 타겟 데이터, 나머지 ⇨ 입력 데이터\nfish_input = fish[[‘Weight’,‘Length’,‘Diagonal’,‘Height’,‘Width’]].to_numpy() fish_target = fish[‘Species’].to_numpy()\n\n\n훈련 세트와 데이터 세트로 나누기\nfrom sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state = 42)\n\n\n각각의 특성 표준화 전처리하기\nfrom sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input)\nSGD를 사용하여 로지스틱 회귀 모델을 학습하기.\nfrom sklearn.linear_model import SGDClassifier sc = SGDClassifier(loss = ‘log_loss’, max_iter = 10, random_state = 42) sc.fit(train_scaled, train_target)\nprint(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))\n두 데이터 세트 모두 점수가 낮아서 개선이 필요하다.\n매 호출 시 1 에포크씩 이어서 진행하는 점진적 학습법을 시도한다.\nsc.partial_fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))\n정확도가 향상된 것을 확인. 이로 인해 에포크를 늘리면 정확도가 향상된다는 것을 알았지만, 계속해서 에포크를 늘리기만 하면 과대적합의 위험이 있다.\n따라서, 과대적합되지 않으면서도 높은 정확도를 유지할 수 있는 최적의 에포크 횟수를 찾아야 한다.\n과대적합되기 전, 조기 종료를 통해 최적의 에포크 횟수를 알아내기.\nimport numpy as np sc = SGDClassifier(loss=‘log_loss’, random_state = 42)\ntrain_score = [] test_score = []\nclasses = np.unique(train_target) for _ in range(0, 300): # 300번의 에포크 동안 훈련을 반복진행 sc.partial_fit(train_scaled, train_target, classes=classes)\ntrain_score.append(sc.score(train_scaled, train_target))\ntest_score.append(sc.score(test_scaled, test_target))\nimport matplotlib.pyplot as plt plt.plot(train_score) plt.plot(test_score) plt.xlabel(‘epoch’) plt.ylabel(‘accuracy’)\n약 100번째 에포크부터 두 데이터 세트의 점수 차이가 증가하기 시작한다.\n위 그래프를 참고하여 반복 횟수를 100으로 맞추어 훈련을 진행한다.\nsc = SGDClassifier(loss = ‘log_loss’, max_iter = 100, tol = None, random_state = 42) sc.fit(train_scaled, train_target)\nprint(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))\n정확도 점수가 매우 높게 나온 것을 확인.\n확률적 경사 하강법을 사용하여 생선 분류 문제를 성공적으로 수행하였다.\n힌지 손실 (Hinge Loss)\nloss의 매개변수의 기본값이다.\n주로 SVM(Support Vector Machine)과 같은 분류 모델에서 사용되는 손실 함수이다.\n힌지 손실을 기반으로 한 추가적인 분류 모델을 학습한다.\nsc = SGDClassifier(loss = ‘hinge’, max_iter = 100, tol = None, random_state = 42) sc.fit(train_scaled, train_target)\nprint(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))\n결과적으로 loss를 log_loss로 설정했을 때, 가장 우수한 성능을 보인다."
  },
  {
    "objectID": "ai/hg_02.html",
    "href": "ai/hg_02.html",
    "title": "데이터 다루기",
    "section": "",
    "text": "머신러닝 알고리즘에 주입할 데이터를 준비하는 방법을 배우고, 데이터 형태가 알고리즘에 미치는 영향에 대해 다루고자 한다.\n\n02 - 1 . 훈련 세트 & 테스트 세트\n2-1. 훈련 데이터와 테스트 데이터\nhttps://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/2-1.ipynb#scrollTo=Z-iCt9zHilaa\n지도 학습 (supervised learning) 입력 데이터(input)와 그에 해당하는 정답(target, label)이 필요하다.\n모델은 주어진 입력과 정답을 사용하여 학습하고, 새로운 입력에 대해 정확한 예측을 할 수 있도록 훈련된다.\n용어 정의\n비지도 학습 (unsupervised learning) 정답이 없이 입력 데이터만 사용하여 패턴을 찾는다. 주로 군집화나 차원 축소 등의 작업에 사용된다.\n훈련 세트를 이용해 모델이 데이터를 학습하는 과정 ≒ 학생이 시험을 준비하는 과정\n훈련 세트와 테스트 세트가 동일할 경우, 모델은 이미 답을 알고 있으므로 100%의 정확도를 달성할 수 있다.\n그러나, 이는 학생이 시험 전에 문제와 답을 미리 알고 있는 것과 같기에 실제 성능을 평가하는 방식으로는 적절하지 않다.\n따라서, 모델의 성능을 올바르게 평가하려면 훈련에 사용한 데이터와는 다른 데이터(테스트 세트)를 사용하여 평가해야 한다.\n이를 통해 모델이 실제로 새로운 데이터에서 얼마나 잘 예측하는지를 측정할 수 있다.\n일반적으로 데이터셋을 준비할 때,\n훈련 데이터(training set)와 테스트 데이터(test set)로 나누는 것이 일반적이며, 이외에도 검증 데이터(validation set)를 추가로 사용하는 경우도 있다.\n이것은 모델이 학습한 패턴을 일반화하는 능력을 평가할 수 있게 되어, 과적합(overfitting)을 방지하고, 실제 환경에서의 성능을 더 정확하게 반영할 수 있다.\n도미와 빙어의 데이터를 합친 하나의 파이썬 리스트\nfish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]\nfish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]\n각 생선의 길이와 무게를 하나의 리스트로 담은 2차원 리스트\n\n\n35(도미) + 14(빙어) = 49개의 샘플\n\n\n사용하는 특성: 길이 & 무게 ⇨ 2개\nfish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1]35 + [0]14 # 훈련 데이터: 35, 테스트 데이터: 14\n모델 객체 만들기\nfrom sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier()\n샘플링 편향 (sampling bias) 데이터가 고르게 섞이지 않아서 발생하는 문제.\n테스트 세트에 빙어만 포함되어 있고, 훈련 세트에 도미만 있을 경우, 모델은 도미만 학습하게 되어, 테스트 세트에서 무조건 도미로 예측하게 되며,\n이는 올바른 평가가 되지 않는다.\n이 문제를 해결하기 위해서는 데이터를 섞어서(train-test split) 훈련 세트와 테스트 세트에 도미와 빙어가 골고루 포함되도록 해야 한다.\nNumpy 나 scikit-learn을 사용하여 데이터를 무작위로 섞을 수 있으며, Numpy 에 경우, 고차원 배열 제작하고, 이를 조작할 수 있는 도구를 제공한다.\n생선 데이터를 2차원 Numpy 배열로 변환하기\nimport numpy as np\ninput_arr = np.array(fish_data) target_arr = np.array(fish_target)\nprint(input_arr)\n이하 생략\n배열의 크기 확인하기\ninput_arr.shape # (샘플 수, 특성 수)\n49개의 샘플과 2개의 특성\n주어진 배열을 무작위로 섞기\nnp.random.seed(42) # 난수 생성기 시드 고정 index = np.arange(49) # 0부터 48까지의 숫자 배열 생성 np.random.shuffle(index) # 배열을 무작위로 섞음 index\n무작위로 섞인 인덱스를 사용해 전체 데이터를 훈련 세트와 테스트 세트로 나누기\n\n\n섞인 인덱스의 처음 35개로 훈련 데이터를 만듦\ntrain_input = input_arr[index[:35]]\n\n\n훈련 데이터에 대응되는 타깃값도 동일하게 선택\ntrain_target = target_arr[index[:35]]\n\n\n섞인 인덱스에서 35번째 이후를 테스트 데이터로 설정\ntest_input = input_arr[index[35:]]\n\n\n타깃 데이터도 동일하게 나눔\ntest_target = target_arr[index[35:]]\nimport matplotlib.pyplot as plt\n\n\n훈련 세트 산점도\nplt.scatter(train_input[:, 0], train_input[:, 1])\n\n\n테스트 세트 산점도\nplt.scatter(test_input[:, 0], test_input[:, 1])\n\n\nx축과 y축에 대한 라벨\nplt.xlabel(‘length’) plt.ylabel(‘weight’)\n\n\n아래 산점도를 통해, 도미와 빙어가\n\n\n각각의 세트에 잘 섞였음을 알 수 있다.\n파란색: 훈련 세트, 주황색: 테스트 세트\nk–최근접 이웃(k–Nearest Neighbors, KNN) 알고리즘을 사용하여 모델을 훈련하고, 테스트 세트에서 그 성능을 평가하기\n\n\n이전에 만든 모델 객체 사용하기\nkn = kn.fit(train_input, train_target) kn.score(test_input, test_target)\n100%의 정확도\n주어진 테스트 데이터에 대해 각 샘플이 어느 클래스에 속하는지 예측한 결과를 반환\nkn.predict(test_input)\n예측 결과를 실제 정답(test_target)과 비교하기\ntest_target\n테스트 세트에 대한 예측 결과가 정답과 일치한다.\n02 - 2 . 훈련 세트 & 테스트 세트\n2-2. 데이터 전처리\nRun, share, and edit Python notebooks\ncolab.research.google.com # 이전과 동일한 데이터 fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]\nfish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]\n두 개 이상의 1차원 배열을 열(column) 단위로 결합하여 2차원 배열 만들기\nimport numpy as np\n\n\n생선의 길이와 무게를 열 단위로 합침\nfish_data = np.column_stack((fish_length, fish_weight)) print(fish_data[:5])\n각 행마다 2개의 열(길이, 무게)\n각 생선이 도미인지 빙어인지 나타내는 라벨(타깃값)을 담고 있다\nfish_target = np.concatenate((\n# 도미: 1, 빙어: 0\nnp.ones(35), np.zeros(14)))\nfish_target\n사이킷런(sklearn)으 훈련 세트와 테스트 세트로 나누기\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = train_test_split( fish_data, fish_target, random_state = 42)\ntrain_input.shape, test_input.shape\n\n\ntrain_test_split:\n\n\n입력된 데이터의 약 25%를 테스트 세트로 분리\n\n\n나머지 75%를 훈련 세트로 사용\n\n\n4개의 배열 반환\n\n\ntrain_input: (36, 2)\n\n\ntest_input: (13, 2)\n\n\ntrain_target: (36, )\n\n\ntest_target: (13, )\n4개의 배열 반환\n훈련 세트와 테스트 세트의 타깃 데이터 크기 확인하기\ntrain_target.shape, test_target.shape\n훈련 세트와 테스트 세트 샘플의 클래스 비율 확인하기\ntest_target\n\n\n도미(1):빙어(0) = 35:14 = 2.5:1\n\n\ntest_target의 비율 = 3.3:1\n\n\n샘플링 편향이 나타나므로,\n\n\n클래스 비율에 맞게 데이터를 재분할 해야 한다.\n클래스 비율에 맞게 데이터 재분할하기\ntrain_input, test_input, train_target, test_target = train_test_split( fish_data, fish_target, stratify = fish_target, random_state = 42)\ntest_target\n\n\nstratify = fish_target:\n\n\n각 클래스의 비율이 훈련 세트와 테스트 세트에\n\n\n동일하게 유지되도록 데이터를 분할한다\n\n\n결과적으로 빙어가 1개 증가하여\n\n\n테스트 세트의 비율이 2.25:1이 되었다\nk–최근접 이웃(k–Nearest Neighbors, KNN) 알고리즘을 사용하여 모델을 훈련하고, 테스트 세트에서 그 성능을 평가하기\nfrom sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target)\n100%의 정확도\n올바르게 분류하는지 확인하기\nkn.predict([[25, 150]]) # 도미(1) 샘플\n올바르게 예측하지 않음\n이 샘플을 다른 데이터와 함께 산점도로 나타내기\nimport matplotlib.pyplot as plt\nplt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker=‘^’) # 삼각형(△) plt.xlabel(‘length’) plt.ylabel(‘weight’)\n샘플은 도미 데이터 주변에 있으나, 모델은 빙어 데이터와 가깝다고 판단\nk–최근접 이웃(KNN) 알고리즘은 새로운 샘플의 클래스를 예측할 때, 해당 샘플 주변의 이웃 샘플들 중 다수인 클래스를 예측값으로 사용한다.\n이 샘플 주변의 이웃들을 알아보려면, KNeighborsClassifier 클래스의 kneighbors() 메서드를 사용할 수 있다.\n이 메서드는 지정한 샘플에서 가장 가까운 이웃들의 거리와 이웃 샘플의 인덱스를 반환한다. 기본적으로, n_neighbors 파라미터의 값은 5로 설정되어 있어, 각 샘플에 대해 5개의 이웃이 반환된다.\n훈련 데이터 중 이웃 샘플을 따로 구분해 그리기\ndistances, indexes = kn.kneighbors([[25, 150]])\nplt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker=‘^’) # 삼각형(△) plt.scatter(train_input[indexes,0], train_input[indexes,1], marker = ‘D’) # 마름모(◇) plt.xlabel(‘length’) plt.ylabel(‘weight’)\n삼각형 샘플과 가까운 5개의 이웃 샘플이 초록 마름모로 표시\n5개의 샘플에 대한 데이터 확인\ntrain_input[indexes], train_target[indexes]\n가장 가까운 생선 4개가 빙어(0)인 것을 확인\n문제의 원인 파악을 위한, distances 배열 출력\ndistances\n각 값은 해당 이웃까지의 거리를 의미\n위에서 출력된 거리를 그래프에 대입하여 비교\n범위 차이가 문제의 요인임을 확인\n따라서, x축의 범위를 y축과 동일하게 조정\nplt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker=‘^’) plt.scatter(train_input[indexes,0], train_input[indexes,1], marker = ‘D’) plt.xlim((0, 1000)) # 범위 조정 plt.xlabel(‘length’) plt.ylabel(‘weight’)\n위와 같이, 두 특성(길이와 무게)의 스케일이 다르면, k–최근접 이웃(KNN)과 같은 거리 기반 알고리즘에서 문제가 발생할 수 있다.\n이 경우, 특성 값의 범위가 큰 변수(무게)가 모델에 더 큰 영향을 미치게 된다. 산점도가 거의 수직으로 나타난 것도 그 이유이다.\n이를 해결하려면 특성 스케일링(Feature Scaling)을 적용하여, 두 특성의 값을 동일한 범위로 맞추는 것이 좋다.\n일반적으로 표준화(standardization) 또는 정규화(normalization)를 사용하며, 이러한 작업은 데이터 전처리(Data Preprocessing)의 일환이다.\n표준화의 대표적인 예시로, 표준점수(Z-score)가 있다.\n각 데이터가 평균으로부터 얼마나 떨어져 있는지를 나타내는 지표로, 데이터의 분포가 평균을 기준으로 얼마나 퍼져 있는지를 평가하는 데 유용하다.\n표준점수 계산을 위한, 평균과 표준편차 계산 및 출력\nmean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) mean, std # 평균, 표준편차\n\n\naxis = 0:\n\n\n행(row) 방향, 세로 방향으로\n\n\n이동하며, 각 열의 평균을 계산\n평균을 빼고, 표준편차로 나누어 표준점수로 변환\ntrain_scaled = (train_input - mean) / std # ↪ 브로드캐스팅(broadcasting): # NumPy에서 배열 간의 연산을 할 때,\n\n\n차원이 맞지 않는 배열을 자동으로\n\n\n조정하여 연산을 가능하게 하는 기능\nplt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(25, 150, marker=‘^’) plt.xlabel(‘length’) plt.ylabel(‘weight’)\n우측 상단에 샘플 하나가 홀로 존재\n동일한 비율을 변환하지 않으면, 계산과정에서 값의 범위가 크게 달라진다. 따라서, 동일한 기준을 적용하여, 다시 산점도를 그린다.\nnew = ([25, 150] - mean) / std # 새로 추가된 샘플의 길이와 무게를 표준화\nplt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker=‘^’) plt.xlabel(‘length’) plt.ylabel(‘weight’)\nx축과 y축의 범위가 바뀜\n위 데이터셋으로 k–최근접 이웃 모델을 훈련한다\nkn.fit(train_scaled, train_target)\n모델 평가\ntest_scaled = (test_input - mean) / std kn.score(test_scaled, test_target)\n100%의 정확도\n모델이 새로 추가된 샘플을 정확히 예측하는지 확인\nkn.predict([new])\n25cm, 150g의 생선을 도미로 예측하는 데 성공\n이 샘플의 k–최근접 이웃을 구하고, 산점도를 그린다\ndistances, indexes = kn.kneighbors([new])\nplt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker = ‘^’) plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker = ‘D’) plt.xlabel(‘length’) plt.ylabel(‘weight’)\n삼각형 샘플과 가까운 5개의 이웃 샘플이 모두 도미인 것을 확인"
  },
  {
    "objectID": "ai/hg_03.html",
    "href": "ai/hg_03.html",
    "title": "회귀 알고리즘 & 모델 규제",
    "section": "",
    "text": "지도 학습의 한 종류인 회귀 문제를 이해하고 다양한 선형 회귀 알고리즘의 장단점에 대해 다루고자 한다.\nhttps://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/3-1.ipynb\n03 - 1 . 최근접 이웃 회귀\n3-1 최근접 이웃 회귀.ipynb\n회귀 (regression) 지도 학습에서 중요한 개념 중 하나로, 주어진 데이터를 바탕으로 연속적인 값을 예측하는 문제를 해결한다.\nk–최근접 이웃(KNN) 회귀는 회귀 알고리즘 중 하나이다.\n이 알고리즘은 예측하려는 값이 주어졌을 때, 가까운 이웃들의 값을 평균 내어 결과를 추정하는 방식으로 작동한다.\nKNN은 단순하면서도 직관적인 알고리즘이기 때문에, 데이터의 패턴을 찾는 데 유용한 경우가 많다.\n예를 들어, 농어의 무게를 예측할 때, 특정 농어와 가까운 다른 농어들의 무게를 이용해 평균값을 예측하는 것이 KNN 회귀 방식이다.\n이 방법은 복잡한 계산을 요구하지 않지만, 데이터의 분포나 k 값 설정에 민감할 수 있다.\n회귀 문제에서는 예측하고자 하는 값이 숫자로 표현되며, 이는 여러 산업 분야에서 다양하게 활용될 수 있다.\n농어의 길이와 무게 데이터\nimport numpy as np perch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )\nperch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] )\n이 데이터의 형태를 보기 위해, 산점도를 그리기\nimport matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel(‘length’) plt.ylabel(‘weight’) 더보기 더보기\n농어의 길이와 무게는 비례한다.\n데이터를 훈련 세트와 테스트 세트로 나누기\nfrom sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state = 42)\ntrain_input.shape, test_input.shape\n두 개의 배열을 2차원 배열로 변환\ntrain_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) train_input.shape, test_input.shape\n\n-1: 배열의 크기를 자동으로 맞추기\n\n\n1: 배열의 두 번째 차원을 1열로 설정\nKNeighborsRegressor 모델을 생성하고 두 개의 데이터를 사용하여 학습시키는 과정\nfrom sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target)\n결정계수 (R², R-squared)\nSSres: 잔차 제곱합, SStot: 총 제곱합\n회귀 분석에서 모델의 예측 성능을 평가하는 지표 중 하나로, 모델이 주어진 데이터를 얼마나 잘 설명하는지를 나타낸다.\n결정계수는 0 ~ 1 사이의 값을 가지며, 1 에 가까울수록 모델이 데이터를 잘 설명하고, 0 에 가까울수록 설명력이 떨어진다는 의미이다.\n테스트 세트의 점수 확인\nknr.score(test_input, test_target)\n이 모델은 99%의 변동성을 설명할 수 있다.\n다만, 결정계수(R²)는 정확도처럼 직관적으로 해석하기는 조금 어려울 수 있다.\n단순히 맞고 틀린 것을 계산하는 지표가 아니기 때문에, “예측이 얼마나 정확한가”를 직관적으로 파악하기는 힘들다.\n즉, 99%의 값이 실제로 좋은 성능인지 여부는 문제의 특성에 따라 다를 수 있다.\n또한, 모든 데이터에서 완벽한 예측을 했는지 여부는 알 수 없고, 오차나 과적합 문제에 대한 구체적인 설명을 해주지 않는다.\n따라서, 모델의 성능을 더 명확히 평가하기 위해서는 추가적인 평가 지표가 필요하다.\n예측이 벗어난 정도를 알아보기 위해, 타겟과 예측한 값 사이의 차이를 계산 및 출력한다\nfrom sklearn.metrics import mean_absolute_error\n\n\n테스트 세트에 대한 예측을 만든다\ntest_prediction = knr.predict(test_input)\n\n\n테스트 세트에 대한 평균 절댓값 오차를 계산한다\nmae = mean_absolute_error(test_target, test_prediction) mae\n평균적으로 약 19g 정도 타겟값과 다르다는 것을 확인\n훈련 세트를 사용해 평가해보기 위해, score() 메서드에 훈련 세트를 전달하여 점수 출력하기\nknr.score(train_input, train_target)\n훈련 세트(96%)가 테스트 세트(99%)보다 낮은 점수임을 확인\n보통 훈련 세트에서의 성능이 (테스트 세트, 검증 세트에서의 성능보다) 더 높게 나오는 경우가 많다.\n그 이유는 모델이 훈련 세트에 직접 노출되어 그 데이터를 바탕으로 학습하기 때문이다. 훈련 세트에 최적화된 상태일 경우, 그 데이터에서의 성능은 당연히 높아질 수밖에 없다.\n위 경우, 훈련 세트의 점수가 낮게 나왔으므로 과소적합된 것이다.\n과대적합(overfitting) & 과소적합(underfitting) 머신러닝에서 모델의 성능을 저하시키는 두 가지 주요 문제.\n\n과대적합 (Overfitting) 과대적합은 모델이 훈련 데이터에 너무 과도하게 적응하여, 새로운 데이터(테스트 데이터)에는 잘 일반화되지 못하는 현상.\n\n즉, 모델이 훈련 데이터의 노이즈나 불필요한 패턴까지 학습하는 경우이다.\n\n훈련 데이터에 대한 정확도는 매우 높지만, 테스트 데이터나 새로운 데이터에 대한 성능이 떨어진다.\n훈련 데이터의 세부 사항까지 모두 학습하므로, 복잡한 모델이나 파라미터 수가 많은 모델에서 자주 발생한다.\n모델이 데이터의 본질적인 패턴보다는 훈련 데이터에 특화된 규칙을 학습하는 문제를 일으킨다.\n과소적합 (Underfitting) 모델이 훈련 데이터를 충분히 학습하지 못해, 훈련 데이터와 테스트 데이터 모두에서 성능이 좋지 않은 상태.\n\n즉, 모델이 너무 단순해서 데이터의 패턴을 제대로 잡지 못하는 경우이다.\n\n훈련 데이터에서조차 예측 성능이 낮다.\n모델이 데이터를 적절히 설명할 수 없을 정도로 너무 단순하거나 제한적인 구조를 가지고 있을 때 발생한다.\n훈련 데이터와 테스트 데이터에서 모두 성능이 낮게 나온다.\n\n모델이 너무 단순하므로, 복잡성을 높이기 위해 이웃의 개수 k를 줄여보기로 한다\n\n\n이웃의 갯수를 3으로 설정\nknr.n_neighbors = 3\n\n\n모델을 다시 훈련\nknr.fit(train_input, train_target) knr.score(train_input, train_target)\n훈련 세트의 점수가 높아진 것을 확인\nknr.score(test_input, test_target)\n테스트 세트의 점수가 낮아진 것을 확인\n위 과정을 통해, 과소적합 문제를 해결했고 두 세트 간에 점수 차이도 크지 않으므로 과대적합 문제도 없다.\n따라서, 성공적으로 회귀 모델을 훈련하였다.\n03 - 2 . 선형 회귀\n3-2 선형 회귀.ipynb\nRun, share, and edit Python notebooks\ncolab.research.google.com # 이전과 동일한 데이터 import numpy as np perch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] ) perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] )\n\n\n훈련 세트와 데이터 세트로 나누고 2차원 배열로 변환하기\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state = 42)\ntrain_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1)\n\n\n최근접 이웃 개수를 3으로 하는 모델 훈련\nfrom sklearn.neighbors import KNeighborsRegressor\nknr = KNeighborsRegressor(n_neighbors = 3) knr.fit(train_input, train_target)\n위 모델을 사용하여 길이 50cm 농어의 무게를 예측한다\nknr.predict([[50]])\n1,033g = 1.033kg으로 예측\n그러나 이 농어의 실제 무게는 더 많으므로, 이 농어를 산점도에 표시하고 최근접 이웃도 함께 시각화하기로 한다.\n\n\n50cm 농어의 이웃을 구한다\ndistances, indexes = knr.kneighbors([[50]])\nplt.scatter(train_input, train_target) plt.scatter(train_input[indexes], train_target[indexes], marker = ‘D’) plt.scatter(50, 1033, marker = ‘^’) plt.xlabel(‘length’) plt.ylabel(‘weight’) 더보기 더보기\n50cm 농어에서 가장 가까운 것은 45cm 근방임을 확인\n위 샘플들의 무게를 평균한다.\nnp.mean(train_target[indexes])\n모델이 예측한 값과 정확히 일치한다.\n따라서, 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측할 수 있다.\nknr.predict([[100]])\n100cm인 농어도 1,033g으로 예측\n위 농어도 산점도에 표시하고 최근접 이웃도 함께 시각화하기로 한다.\n\n\n100cm 농어의 이웃을 구한다\ndistances, indexes = knr.kneighbors([[100]])\nplt.scatter(train_input, train_target) plt.scatter(train_input[indexes], train_target[indexes], marker = ‘D’) plt.scatter(100, 1033, marker = ‘^’) plt.xlabel(‘length’) plt.ylabel(‘weight’) 더보기 더보기\n농어가 아무리 커도 무게는 고정됨을 확인\n이를 해결하려면 가장 큰 농어가 포함되도록 훈련 세트를 다시 구성해야 한다. 그러나 이러한 작업을 매번 반복하는 것은 번거로울 수 있다.\n따라서, 새로운 알고리즘을 적용해 보기로 한다.\n선형 회귀 (Linear Regression)\n직선의 방정식: y = ax + b 기울기는 계수(coefficient) & 가중치(weight)라고도 부른다.\n독립 변수와 종속 변수 간의 선형 관계를 모델링하는 가장 기본적인 회귀 분석 방법이다.\n목표는 주어진 데이터를 기반으로 직선 을 그려서 종속 변수(예측하고자 하는 값)를 예측하는 것이다.\n선형 회귀 모델 훈련\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression() lr.fit(train_input, train_target)\n50cm 농어에 대한 예측\nlr.predict([[50]])\n1,241g = 1.241kg으로 예측\nLinearRegression 클래스가 찾은 a, b를 출력\nlr.coef_, lr.intercept_\n농어의 길이가 15 ~ 50인 범위에 대해 직선을 그려본다\nplt.scatter(train_input, train_target) plt.plot([15, 50], [15lr.coef_+lr.intercept_, 50lr.coef_+lr.intercept_]) plt.scatter(50, 1241.8, marker=‘^’) plt.xlabel(‘length’) plt.ylabel(‘weight’) 더보기 더보기\n선형 회귀 알고리즘이 이 데이터셋에서 찾은 최적의 직선\n훈련 세트와 테스트 세트에 대한 R² 출력\nprint(lr.score(train_input, train_target)) print(lr.score(test_input, test_target))\n전체적으로 과소적합되었음을 확인\n위 직선의 좌측 하단을 보면, 선이 0 이하로 내려가 있다. 하지만 0g 이하의 농어는 존재하지 않으므로, 이를 보완할 필요가 있다.\n다항 회귀 (Polynomial Regression)\n2차 다항 회귀 공식: y = ax² + bx + c\n선형 회귀의 확장으로, 데이터가 비선형적일 때 더 적합한 모델을 만들기 위해 사용된다.\n선형 회귀가 직선을 그리는 것과 달리, 다항 회귀는 곡선 을 그려 비선형 관계를 설명한다.\n새롭게 만든 데이터셋의 크기 확인\ntrain_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) train_poly.shape, test_poly.shape\n원래 특성인 길이를 제곱하여 왼쪽 열에 추가하였고, 그 결과 각각의 데이터 세트에 2개의 특성이 생겼다\ntrain_poly를 사용하여, 선형 회귀 모델을 다시 훈련\nlr = LinearRegression() lr.fit(train_poly, train_target) lr.predict([[50**2, 50]])\n이전에 훈련된 모델보다 더 높은 값을 예측\n이 모델이 훈련한 계수와 절편을 출력\nlr.coef_, lr.intercept_\n농어의 무게 = 1.01 × 농어의 길이² – 21.6 × 농어의 길이 + 116.05\n이전과 동일하게 훈련 세트의 산점도에 그래프로 그려보기\npoint = np.arange(15, 50) plt.scatter(train_input, train_target) plt.plot(point, 1.01*point**2 - 21.6*point + 116.05) plt.scatter([50], [1574], marker=‘^’) plt.xlabel(‘length’) plt.ylabel(‘weight’) 더보기 더보기\n앞선 단순 선형 회귀 모델보다 훨씬 더 나은 예측 곡선이 그려졌다.\n훈련 세트와 테스트 세트에 대한 R² 출력\nprint(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target))\n두 데이터 세트에 대한 점수가 높아졌다.\n다만, 훈련 세트의 점수가 더 낮아 여전히 과소적합 문제가 남아 있다. 또한, 전체 데이터 세트의 점수가 낮기 때문에 이를 개선할 필요가 있다.\n03 - 3 . 특성 공학과 규제\n3-3 특성 공학과 규제.ipynb\nRun, share, and edit Python notebooks\ncolab.research.google.com\n다중 회귀 (Multiple Regression) 두 개 이상의 독립 변수를 사용하여 종속 변수를 예측하는 회귀 분석 기법.\n이 방법은 데이터에서 복잡한 관계를 모델링할 수 있는 유연성을 제공한다.\n그러나 다중 회귀는 다중 공선성(multi-collinearity) 문제가 발생할 수 있어, 독립 변수 간의 상관관계가 높으면 모델의 해석이 어려워질 수 있다.\n즉, 과대적합의 위험이 있으며, 모델이 너무 복잡해질 수 있다.\n또한, 독립 변수가 3개 이상이 되면, 시각적으로 표현하기 어려워진다.\n예를 들어, 4차원 이상의 데이터를 시각화하는 것은 사람의 직관으로는 불가능하다.\n특성 공학 (Feature Engineering) 데이터 전처리 과정에서 기존 데이터를 변형하거나 새로운 특성을 생성하여 모델의 성능을 향상시키는 기법.\n이는 머신러닝 모델이 데이터를 더 잘 이해하도록 돕는 중요한 과정이다.\n농어의 특성이 3개로 늘어나 데이터가 커졌기 때문에, 이를 복사해서 붙여넣는 것은 번거롭다.\n이 경우, 판다스를 사용하여 인터넷에서 농어 데이터를 내려받으면 된다.\nimport pandas as pd df = pd.read_csv(‘https://bit.ly/perch_csv_data’) perch_full = df.to_numpy() perch_full\n이하 생략\n\n\n이전과 동일한 데이터\nimport numpy as np perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] )\n\n\n훈련 세트와 테스트 세트 나누기\nfrom sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split( perch_full, perch_weight, random_state = 42)\n사이킷런의 PolynomialFeatures 클래스를 사용하여 입력 데이터를 다항식 특성으로 변환하는 과정이다.\nPolynomialFeatures 클래스 위와 같은, 데이터 변환 클래스를 변환기(transformer)라고 한다. 이러한 변환 과정은 타겟 데이터의 유무와 관계없이 진행되며\n이를 통해, 모델에 더 적합한 형태로 만들거나, 성능을 향상시킨다.\nfrom sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures() poly.fit([[2, 3]]) poly.transform([[2, 3]])\n2개의 특성: [2, 3] ⇨ 6(=4+2)개의 특성: [1, 2, 3, 4, 6, 9]\n위 모델에 대한 선형 방정식 더보기 더보기\n추가된 4가지 특성: 길이, 높이, 두께, 1\n절편(1)을 위한 항 제거\npoly = PolynomialFeatures(include_bias = False) poly.fit([[2, 3]]) poly.transform([[2, 3]])\n절편(1)이 제거된 것을 확인\n배열의 크기 확인\npoly.fit(train_input) train_poly = poly.transform(train_input) train_poly.shape\n데이터의 샘플 수 42개(훈련 샘플의 개수) / 각 샘플이 9개의 다항식 특성으로 확장\n9개의 특성이 어떤 과정으로 만들어졌는지 확인\npoly.get_feature_names_out()\n변환된 특성을 이용하여 다중 회귀 모델 훈련\ntest_poly = poly.transform(test_input) # 데이터 세트 변환 from sklearn.linear_model import LinearRegression\nlr = LinearRegression() lr.fit(train_poly, train_target) lr.score(train_poly, train_target)\n모델이 훈련 데이터의 변동성을 약 99.03% 설명함을 의미\n테스트 세트의 점수 확인\nlr.score(test_poly, test_target)\n이전과 비교해 점수 상승은 없으나, 농어의 길이만 사용했을 때의 과소적합 문제는 해결되었다\n5제곱까지 특성을 만들어 다시 출력\npoly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) train_poly.shape\n특성이 무려 55개나 증가함을 확인\n선형 회귀 모델 다시 훈련\nlr.fit(train_poly, train_target) lr.score(train_poly, train_target)\n거의 완벽한 점수임을 확인\n테스트 세트의 점수 재확인\nlr.score(test_poly, test_target)\n매우 큰 음수값이 나온 것을 확인\n더 많은 특성을 통해 데이터의 복잡한 패턴과 비선형 관계를 더 잘 모델링할 수 있다는 장점이 있지만\n특성이 너무 많아지면, 모델이 훈련 데이터에 과적합될 위험이 커지며 이는 새로운 데이터에 대한 일반화 성능을 떨어뜨릴 수 있다.\n규제 (Regularization) 머신러닝 모델의 과적합(overfitting)을 방지하기 위해 사용되는 기술. 모델의 복잡성을 줄여 일반화 성능을 향상시키는 데 도움을 준다.\n이를 통해 모델이 훈련 데이터에 지나치게 맞춰지는 것을 방지하고, 새로운 데이터에 대한 예측 성능을 높일 수 있다.\n일반적으로 선형 회귀 모델에 규제를 적용할 때는, 계수 값의 크기가 서로 크게 다르지 않아야 한다.\n그러므로, 규제를 진행하기 전 정규화를 해야만 한다.\n훈련 데이터에 대해 평균과 표준편차를 계산\nfrom sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly)\n\n\n훈련 데이터를 표준화 (평균 0, 표준편차 1)\ntrain_scaled = ss.transform(train_poly)\n\n\n테스트 데이터를 훈련 데이터의 통계에 맞춰 표준화\ntest_scaled = ss.transform(test_poly)\n릿지 회귀 & 라쏘 회귀 둘 다 선형 회귀 모델에 규제를 적용하여 과적합을 방지하는 기법.\n릿지 회귀 (Ridge Regression)\nL2 정규화를 적용, 이는 모든 계수의 제곱합에 비례하는 패널티 추가\n모든 특성을 유지하지만, 계수의 크기를 작게 만들어 모델의 복잡성을 줄인다. 특성 간의 상관관계가 높은 경우, 규제를 통해 더 안정적인 모델을 만든다.\n주로 다중 공선성이 있는 데이터에서 효과적\n라쏘 회귀 (Lasso Regression) L1 정규화를 적용, 이는 모든 계수의 절댓값 합에 비례하는 패널티 추가\n일부 계수를 0으로 만들어 불필요한 특성을 선택적으로 제거하며, 이로 인한 모델의 해석성 향상\n자동으로 특성 선택을 수행하므로, 더 간결한 모델을 생성 가능 특히 고차원 데이터에서 유용하게 사용\n릿지 회귀 모델 훈련\nfrom sklearn.linear_model import Ridge\nridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target))\n이전 모델에 비해 점수가 낮아짐을 확인\n테스트 세트에 대한 점수 확인\nridge.score(test_scaled, test_target)\n이전 음수값의 점수가 정상으로 돌아왔음을 확인\n위 모델들의 규제의 양은 하이퍼파라미터(alpha)를 통해 임의로 조절할 수 있다.\nalpha를 사용하여 L1 및 L2 정규화의 강도를 조절하며, 이 값이 클수록 규제의 강도가 강해지는 비례 관계가 나타난다.\n반대로 계수의 값은 작아지는 반비례 관계를 가지며, 결과적으로 과소적합이 발생할 수 있다.\n적절한 alpha값을 찾기 위해, R² 값을 그리기\nimport matplotlib.pyplot as plt\ntrain_score = [] test_score = []\n\n\n10배씩 늘려가며 훈련하기(0.01 ~ 100)\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target))\n\n\n동일한 간격으로 나타내기 위해 로그 함수로 바꾸어 지수로 표현\nplt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(‘alpha’) plt.ylabel(‘R^2’) 더보기 더보기\n적절한 alpha값으로 최종 모델 훈련\nridge = Ridge(alpha = 0.1) ridge.fit(train_scaled, train_target)\nprint(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target))\n이 모델은 두 데이터 세트의 점수가 모두 높고, 과대적합과 과소적합 사이의 균형을 맞추고 있다\n라쏘 모델 훈련\nfrom sklearn.linear_model import Lasso # 클래스 라쏘로 바꾸기 ↴ lasso = Lasso() # Ridge ⇨ Lasso lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target))\n라쏘 모델 또한 과대적합과 과소적합을 잘 억제하였다\n적절한 alpha값을 찾기 위해, R² 값을 그리기\ntrain_score = [] test_score = []\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha, max_iter = 10000) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target))\nplt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(‘alpha’) plt.ylabel(‘R^2’) 더보기 더보기\n적절한 alpha값으로 최종 모델 훈련\nlasso = Lasso(alpha = 10) lasso.fit(train_scaled, train_target)\nprint(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target))\n이 모델은 두 데이터 세트의 점수가 모두 높고, 과대적합을 잘 억제하였다.\n영향력이 적은 특성의 계수값을 0으로 만든다\nnp.sum(lasso.coef_ == 0)\n라쏘 회귀가 40개의 특성의 계수를 0으로 만들고, 해당 특성들을 모델에서 제거"
  },
  {
    "objectID": "ai/hg_05.html",
    "href": "ai/hg_05.html",
    "title": "트리 알고리즘",
    "section": "",
    "text": "트리 알고리즘, 하이퍼파라미터 튜닝 실습, 그리고 앙상블 모델에 대해 다루고자 한다.\n05 - 1 . 결정 트리\n5-1 결정 트리.ipynb\nhttps://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\n와인 샘플 데이터 불러오기.\nimport pandas as pd wine = pd.read_csv(‘https://bit.ly/wine_csv_data’)\n처음 5개의 샘플 확인.\n전체 와인 데이터에서 화이트 와인을 골라내는 문제.\n각 열 데이터 확인 및 누락된 데이터 확인.\nwine.info()\n열에 대한 간략한 통계량 출력.\nwine.describe()\n훈련 세트와 데스트 세트로 나누기.\ndata = wine[[‘alcohol’, ‘sugar’, ‘pH’]].to_numpy() target = wine[‘class’].to_numpy()\nfrom sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split( data, target, test_size = 0.2, random_state = 42)\nprint(train_input.shape, test_input.shape)\n훈련 세트 전처리.\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler() ss.fit(train_input)\ntrain_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input)\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression() lr.fit(train_scaled, train_target)\nprint(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target))\n점수가 높지 않음을 확인.\n학습한 계수와 절편 출력.\nprint(lr.coef_, lr.intercept_)\n결정 트리 정확도 평가하기.\nfrom sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(train_scaled, train_target)\nprint(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target))\n노트가 어떤 특성으로 이뤄졌는지 확인하기.\nimport matplotlib.pyplot as plt from sklearn.tree import plot_tree\nplt.figure(figsize = (10,7)) plot_tree(dt)\nplt.figure(figsize = (10,7)) plot_tree(dt, max_depth = 1, filled = True, feature_names = [‘alcohol’, ‘sugar’, ‘pH’])\n불순도\n정보이득\n가지치기\n최대 3개의 노드까지 성장히기.\ndt = DecisionTreeClassifier(max_depth = 3, random_state = 42) dt.fit(train_scaled, train_target)\nprint(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target))\n함수로 그리기.\nplt.figure(figsize = (20,15)) plot_tree(dt, filled = True, feature_names = [‘alcohol’, ‘sugar’, ‘pH’])\n다시 훈련하기.\ndt = DecisionTreeClassifier(max_depth = 3, random_state = 42) dt.fit(train_input, train_target)\nprint(dt.score(train_input, train_target)) print(dt.score(test_input, test_target))\n특성값을 표준점수로 바꾸지 않기.\nplt.figure(figsize = (20,15)) plot_tree(dt, filled = True, feature_names = [‘alcohol’, ‘sugar’, ‘pH’])\n특성 중요도 출력하기.\nprint(dt.feature_importances_)\ndt = DecisionTreeClassifier(min_impurity_decrease = 0.0005, random_state = 42) dt.fit(train_input, train_target)\nprint(dt.score(train_input, train_target)) print(dt.score(test_input, test_target))\nplt.figure(figsize = (20,15)) plot_tree(dt, filled = True, feature_names = [‘alcohol’, ‘sugar’, ‘pH’])\n05 - 2 . 교차 검증 & 그리드 서치\n5-2 교차 검증과 그리드 서치.ipynb\nRun, share, and edit Python notebooks\ncolab.research.google.com import pandas as pd wine = pd.read_csv(‘https://bit.ly/wine_csv_data’)\ndata = wine[[‘alcohol’, ‘sugar’, ‘pH’]].to_numpy() target = wine[‘class’].to_numpy()\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)\nsub_input, val_input, sub_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42)\nprint(sub_input.shape, val_input.shape)\n모델 평가하기.\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=42) dt.fit(sub_input, sub_target)\nprint(dt.score(sub_input, sub_target)) print(dt.score(val_input, val_target))\n교차 검증\nfrom sklearn.model_selection import cross_validate import pandas as pd\nscores = cross_validate(dt, train_input, train_target) scores_df = pd.DataFrame(scores) scores_df\n최상의 검증 점수.\nimport numpy as np print(np.mean(scores[‘test_score’]))\n교차 검증.\nfrom sklearn.model_selection import StratifiedKFold scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold()) print(np.mean(scores[‘test_score’]))\n교차 검증.\nsplitter = StratifiedKFold(n_splits = 10, shuffle=True, random_state=42) scores = cross_validate(dt, train_input, train_target, cv=splitter) print(np.mean(scores[‘test_score’]))\n하이퍼파라미터 튜닝 딕셔너리 만들기.\nfrom sklearn.model_selection import GridSearchCV params = {‘min_impurity_decrease’: [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]} gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) gs.fit(train_input, train_target)\ndt = gs.best_estimator_ print(dt.score(train_input, train_target))\nprint(gs.best_params_)\n0.0001이 가장 좋은 값으로 선택.\n5번의 교차 검증.\nprint(gs.cv_results_[‘mean_test_score’])\n최상의 매개변수로 만 검증 점수의 조합.\nbest_index = np.argmax(gs.cv_results_[‘mean_test_score’]) print(gs.cv_results_[‘params’][best_index])\n지정하기.\nparams = {‘min_impurity_decrease’: np.arange(0.0001, 0.001, 0.0001), ‘max_depth’: range(5, 20, 1), ‘min_samples_split’: range(2, 100, 10) }\ngs = GridSearchCV(\nDecisionTreeClassifier(random_state = 42), \nparams, n_jobs = -1)\ngs.fit(train_input, train_target)\n최상의 매개변수 조합확인.\nprint(gs.best_params_)\n최상의 교차 검증 점수 확인.\nprint(np.max(gs.cv_results_[‘mean_test_score’]))\n랜덤서치\nfrom scipy.stats import uniform, randint rgen = randint(0, 10) rgen.rvs(10)\n샘플링 숫자 늘리기.\nnp.unique(rgen.rvs(1000), return_counts=True)\n10개의 실수 추출.\nugen = uniform(0, 1) ugen.rvs(10)\n탐색.\nparams = {‘min_impurity_decrease’: uniform(0.0001, 0.001), ‘max_depth’: randint(20, 50), ‘min_samples_split’: randint(2, 25), ‘min_samples_leaf’: randint(1, 25), }\nfrom sklearn.model_selection import RandomizedSearchCV\ngs = RandomizedSearchCV( DecisionTreeClassifier(random_state = 42), params,n_iter = 100, n_jobs = -1, random_state = 42) gs.fit(train_input, train_target)\n매개변수에 지정.\ngs.best_params_\n최고의 교차 검증 점수 확인.\nprint(np.max(gs.cv_results_[‘mean_test_score’]))\n테스트 세트의 성능 확인.\ndt = gs.best_estimator_ print(dt.score(test_input, test_target))\ngs = RandomizedSearchCV( DecisionTreeClassifier(splitter = ‘random’, random_state = 42), params, n_iter = 100, n_jobs = -1, random_state = 42 ) gs.fit(train_input, train_target)\ngs.best_params_\nprint(np.max(gs.cv_results_[‘mean_test_score’]))\ndt = gs.best_estimator_ print(dt.score(test_input, test_target))\n05 - 3 . 트리의 앙상블\n5-3 트리의 앙상블.ipynb\nRun, share, and edit Python notebooks\ncolab.research.google.com import numpy as np import pandas as pd from sklearn.model_selection import train_test_split\nwine = pd.read_csv(‘https://bit.ly/wine_csv_data’)\ndata = wine[[‘alcohol’, ‘sugar’, ‘pH’]].to_numpy() target = wine[‘class’].to_numpy()\ntrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)\nfrom sklearn.model_selection import cross_validate from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_jobs=-1, random_state=42) scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\nprint(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))\nrf.fit(train_input, train_target) print(rf.feature_importances_)\nrf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\nrf.fit(train_input, train_target) print(rf.oob_score_)\nfrom sklearn.ensemble import ExtraTreesClassifier et = ExtraTreesClassifier(n_jobs=-1, random_state=42) scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\nprint(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))\net.fit(train_input, train_target) print(et.feature_importances_)\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state=42) scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\nprint(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))\ngb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42) scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\nprint(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))\ngb.fit(train_input, train_target) print(gb.feature_importances_)\n\n사이킷런 1.0 버전 아래에서는 다음 라인의 주석을 해제하고 실행하세요.\n\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nhgb = HistGradientBoostingClassifier(random_state=42) scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\nprint(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))\nfrom sklearn.inspection import permutation_importance\nhgb.fit(train_input, train_target) result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1) print(result.importances_mean)\nresult = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1) print(result.importances_mean)\nhgb.score(test_input, test_target)\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(tree_method=‘hist’, random_state=42) scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\nprint(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))\n\n\n!pip install lightgbm\nfrom lightgbm import LGBMClassifier lgb = LGBMClassifier(random_state = 42) scores = cross_validate(lgb, train_input, train_target, return_train_score = True, n_jobs = -1) print(np.mean(scores[‘train_score’]), np.mean(scores[‘test_score’]))"
  },
  {
    "objectID": "ai/ml/ml_02.html",
    "href": "ai/ml/ml_02.html",
    "title": "머신러닝 개론",
    "section": "",
    "text": "머신러닝의 전반적인 개론에 대해 다루고자 한다.\n\n01 머신러닝\nMachine Learning\n데이터로부터 패턴을 학습하여 예측이나 의사결정을 수행하는 인공지능의 한 분야이다.\n머신러닝은 데이터의 라벨(Label) 존재 여부에 따라 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)으로 구분된다. 이를 이해하기 위해 대표적인 예시인 분류와 군집화를 비교하도록 한다.\n1 . 분류(Classification) 지도학습의 대표적인 기법으로, 라벨이 주어진 데이터를 학습하여 새로운 데이터의 정답을 예측한다.\n결과값은 이산형 범주(예: 스팸/일반메일)이며, 대표 알고리즘으로 로지스틱 회귀, 의사결정나무, SVM 등이 있다.\n2 . 군집화(Clustering) 비지도학습에 속하며, 라벨이 없는 데이터를 유사도에 따라 그룹화하여 내재된 구조를 발견한다.\n실제 라벨이 없으므로 손실함수 대신 클러스터 내의 응집도(cohesion)와 클러스터 간 분리도(separation)와 같은 목적함수를 사용하여 데이터의 구조를 평가한다. [출처]\n대표 알고리즘으로는 K-Means, DBSCAN, 계층적 군집화가 있으며, 고객 세분화나 이미지 분석 등에 활용된다. [출처1], [출처2]\n반지도학습(Semi-supervised Learning)\n일부 데이터에만 라벨이 존재하는 경우, 라벨이 없는 데이터를 함께 활용해 학습 성능을 향상시키는 방법이다. 이는 라벨링 비용이 높은 실제 환경에서 자주 사용된다.\n02 손실함수 Loss function\n앞서 살펴본 지도학습은 라벨이 존재하는 데이터를 기반으로 예측 모델을 학습한다. 이러한 학습의 핵심은 예측값과 실제값의 차이를 최소화하는 것으로, 이를 위해 손실함수를 정의한다.\n예측과 정답의 오차(error)를 수치화하며, 학습의 목표는 이 손실을 최소화하는 것이다. [출처]\n이 과정에서 사용되는 가중치(weight) 는 입력 변수의 중요도를 나타내는 모델의 매개변수이며, 학습은 손실함수를 최소화하는 방향으로 가중치를 조정하는 절차이다.\n기울기(gradient)는 손실함수에 대한 가중치의 변화율을 의미하며, 이를 이용해 가중치를 반복적으로 업데이트한다.\n최종적으로 학습이 완료되면 성능 지표(evaluation metric)를 통해 모델의 예측력이 평가된다. [출처]\n강화학습, Reinforcement Learning\n앞서 지도학습이 정답 데이터를 통해 모델을 학습했다면, 강화학습은 보상을 통해 스스로 최적의 행동을 학습한다.\n예를 들어, 로봇이 공의 궤적을 예측하고 어떤 타격 동작을 해야 가장 좋은 보상을 얻는지를 학습하는 방식이다.\n계층적 구조를 적용해, 상위 수준에서는 언제 어떤 자세로 칠지 등의 전략을 결정하고, 하위 수준에서는 실제 관절 제어와 동작 실행을 담당한다.\n결과적으로 로봇이 사람처럼 스핀·속도에 대응하며 탁구를 치는 것이 가능해진다.\n03 차원축소 Dimensionality reduction\n고차원 데이터에서 중요한 정보를 보존한 채 저차원으로 표현하는 기법이다.\n숨겨진 구조나 패턴을 더 명확히 드러내고 데이터의 시각화 및 분석 효율을 높이는 데 활용된다. [출처]\n이들은 차원을 줄이면서도 정보 손실을 최소화하는 것을 목표로 한다.\n이러한 차원 축소 알고리즘들은 학습 방식에 따라 선형과 비선형으로 구분된다.\n1 . 선형 기법 데이터의 분산이나 경계선을 직선(또는 평면) 형태로 설명하는 방식이다.\n공분산 행렬의 고유벡터나 선형 결합 계수를 구하며, 경우에 따라 가중치나 파라미터를 학습하지만 반드시 절편(intercept)이 포함되지는 않는다.\n대표적인 기법으로는 PCA(주성분분석), LDA(선형판별분석)이 있다.\n2 . 비선형 기법 복잡하게 휘어진 데이터 구조를 그대로 보존하려는 방식이다.\n고차원 공간에서의 유사성 확률을 저차원에서도 유지하도록 하며, 전통적인 가중치나 절편 개념을 사용하지 않는다.\n대표적인 기법으로는 t-SNE가 있다.\n3 . 특징의 의미 머신러닝에서 특징(feature)은 관측된 입력 변수로서, 목표값 y를 예측하는 데 사용되는 정보이다. [출처]\n이러한 변수들은 모두 같은 역할을 하지는 않으며, 모델은 학습을 통해 변수의 기여도나 중요도를 반영한다. [출처]\n선형 모델에서는 가중치(coefficient)가 크면 특정 변수의 영향이 크다는 의미가 될 수 있지만, 변수 간 상관관계나 스케일 차이로 단순 비교는 주의가 필요하다. [출처]\n그러나 모든 모델이 가중치를 명확히 제공하는 것은 아니다.\n특히 비선형 구조나 트리 기반 모델에서는 변수의 영향력을 직접 해석하기 어려우므로,\nfeature importance, SHAP, LIME 등의 기법을 이용해 각 변수의 기여도를 시각화한다.\n04 최적화 알고리즘 파라미터 공간(parameter space)을 탐색하여 더 나은 해를 찾는 절차라는 점에서 탐색(search)의 성격을 가진다.\n하지만 전통적인 그래프, 경로 탐색 알고리즘과는 달리, 연속적인 공간에서 목적 함수를 기준으로 한 최적 해를 찾아가는 과정이다.\n비선형(non-convex) 문제나 딥러닝 모델의 경우, 전역 최적해(global optimum)에 도달할 수 있는지,\n아니면 지역 최적해(local optimum)나 안장점(saddle point)에 머무를지에 대한 이론적 증명은 매우 복잡하다.\n특히 Adam, AMSGrad 같은 최적화 기법의 수렴(convergence)을 보장하는 연구에서도, 하이퍼파라미터 설정이나 학습률 조건 등이 중요한 변수가 되며, 수렴 증명 자체가 쉽지 않다는 점이 강조된다.\n또한 최적화 과정은 많은 반복(iteration)과 대규모 데이터, 복잡한 모델 구조가 결합될 경우 계산 비용(computation cost)과 시간적 비용이 매우 크다.\n이러한 이유로 실무에서는 학습 과정이 고비용·고시간 소모적이라는 점이 자주 지적된다.\n05 실무 사용처\n1 . 지자체 분야 지자체는 교통 흐름 예측, 범죄 예측, 환경 모니터링 등 다양한 분야에서 머신러닝 알고리즘을 활용한다.\n예: 교통 혼잡도 예측, 재난 발생 가능성 예측\n2 . 제조 분야 제조업체는 센서 데이터를 분석하여 장비 고장을 예측하고, 이를 통해 유지보수 비용을 절감하는 예지 보수(Predictive Maintenance)를 한다.\n생산 라인에서 발생하는 결함을 실시간으로 감지 및 품질 향상을 위해 머신러닝 알고리즘이 사용된다.\nKAMP, 인공지능제조플랫폼\n스마트 대한민국 구현의 허브!제조 AI 강국으로의 도약 KAMP가 함께합니다.\nwww.kamp-ai.kr\n3 . 금융 분야 금융 기관은 고객의 신용도 평가 및 대출 리스크 관리를 위해 머신러닝 모델을 활용한다.\n거래 패턴을 분석하여 이상 거래를 실시간으로 감지 및 사기 예방에 사용된다.\n06 데이터 분석 파이프라인\n1 . 수집, Collection DB 연계(Repository), 웹 크롤링, 에이전트 설치, 서버 로그, IoT 센서 등 다양한 채널을 통해 데이터를 확보한다.\n이때 수집된 데이터에는 정상치(normal data), 비정상치(outlier 또는 anomaly), 그리고 노이즈(noise) 가 함께 포함될 수 있으며, 이 세 요소의 비율과 품질은 분석 성능에 직접적인 영향을 미친다.\n다만 이러한 비율은 고정된 규칙이 있는 것은 아니며, 데이터의 특성과 도메인(예: 제조, 금융, 의료 등)에 따라 경험적으로 조정·판단되어야 한다.\n핵심은 각 요소를 정확히 구분하고 적절히 처리하는 것이다.\n2 . 정제, Cleaning 수집된 데이터의 품질을 향상시켜 학습 알고리즘이 효율적으로 동작하도록 한다.\n이 단계에서는 먼저 추가·삭제·대체 등의 과정을 통해 누락값(missing values)을 처리하거나 불필요한 데이터를 제거한다.\n또한, 로그 변환이나 스케일 조정 등 변환(Transformation) 작업을 수행하고, 범주형 데이터를 수치형으로 바꾸는 인코딩(Encoding) 과정을 거친다.\n변수 간의 범위를 맞추기 위해 스케일링(Scaling) 또는 정규화(Normalization) 를 적용하며, 이상치(outlier)는 탐지 후 제거하거나 조정하여 데이터의 왜곡을 방지한다.\n한편, 이상치 또는 희소 클래스(sparse class)에 대한 증강(augmentation) 은 전통적 전처리(cleaning) 절차에는 포함되지 않지만,\n불균형(class imbalance) 문제를 완화하거나 특정 도메인(예: 이미지, 텍스트)에서 모델의 일반화 능력 향상을 위해 합성(over-sampling), 생성모델 기반 증강, 혼합 확장(mixup) 등 증강 기법으로 활용된다.\n이러한 방식은 소수 클래스의 표현을 늘려 분포의 대표성을 높이는 전략으로 연구되고 있다.\n예를 들어, SMOTE(Synthetic Minority Over-sampling Technique)는 소수 클래스 데이터 간 보간(interpolation) 방식을 통해 합성 샘플을 생성함으로써 클래스 불균형을 완화하는 대표적인 증강 기법이다.\n또한, GAN(Generative Adversarial Network) 기반 증강 기법(BAGAN 등)은 소수 클래스를 생성하여 불균형 문제를 해결하려는 연구도 있다.\n3 . 저장, Storage 수집 및 정제된 데이터를 효율적으로 보관하고, 분석과 학습에 활용할 수 있도록 관리한다.\n데이터는 세 가지 유형으로 나뉘며, 각 유형에 따라 저장 기술이 다르다.\n① 정형 데이터, Structured Data\n고정된 스키마를 가진 테이블 형식 데이터로, 관계형 DB(RDBMS)에 저장된다.\n예: MES(Manufacturing Execution System)에서 생성되는 생산 이력, IoT 센서 데이터 등\n대표 기술: MySQL, MariaDB, PostgreSQL, SAP HANA(인메모리 DB), 데이터 웨어하우스(Amazon Redshift, Google BigQuery)\n② 비정형 데이터, Unstructured Data\n사전 정의된 스키마가 없으며, 텍스트, 이미지, 영상, 오디오 등 다양한 형식을 포함한다.\n예: CCTV 영상, 소셜 미디어 게시물, 고객 피드백\n대표 기술: MongoDB, Cassandra, HBase(NoSQL DB), Hadoop HDFS, 데이터 레이크(Amazon S3, Azure Data Lake)\n③ 반정형 데이터, Semi-structured Data\n일부 구조적 요소를 포함하지만 완전한 스키마는 없는 데이터\n예: JSON, XML, 로그 파일, IoT 센서 데이터\n대표 기술: MongoDB, Couchbase, 데이터 레이크(Amazon S3, Azure Data Lake)\n참고 사항 Schema-on-write: 데이터를 저장할 때 스키마를 정의하는 방식 (RDBMS)\nSchema-on-read: 데이터를 읽을 때 스키마를 적용하는 방식 (NoSQL, 데이터 레이크)\n4 . 시각화, Visualization 분석 결과를 직관적으로 이해 및 전달하기 위한 단계이다. 특히 대시보드(Dashboard)는 여러 시각화 요소를 통합하여 웹이나 애플리케이션 상에서 상호작용 가능하게 제공한다.\n① 주요 시각화 도구\nPlotly: 웹 기반 인터랙티브 시각화 라이브러리로, 2D·3D 플롯과 대시보드 제작에 적합하다. Dash: Plotly를 기반으로 한 Python 웹 프레임워크로, 코드만으로 대시보드를 구현 가능하다. Matplotlib: 기본 2D 시각화에 강점을 가지며, 세밀한 커스터마이징이 가능하다. Seaborn: Matplotlib 기반으로 통계적 시각화에 특화되어 있으며, 간결한 문법과 미려한 디자인을 제공한다. ② 주요 시각화 유형\n2D/3D 플롯: 데이터 분포 및 변수 간 관계를 표현 바 차트, 파이 차트: 범주형 데이터의 분포를 시각화 히트맵(Heatmap): 데이터 간 상관관계 또는 밀도를 색상으로 표현 맵(Map): 지리적 데이터를 시각화하여 공간적 패턴 분석\n5 . 분석, Analysis 수집·정제·저장된 데이터를 기반으로 의미 있는 인사이트를 도출한다. 빅데이터와 AI 환경에서 머신러닝은 핵심적인 역할을 하며, 네 가지의 주요 기법이 있다.\n① 분류, Classification\n지도학습(Supervised Learning)에 속하며, 입력 데이터에 대해 미리 정의된 레이블을 예측한다.\n활용 예: 스팸 이메일 필터링, 질병 진단 등\n② 군집화, Clustering\n비지도학습(Unsupervised Learning)에 속하며, 라벨 없이 데이터 내 유사한 특성을 가진 그룹을 식별한다.\n활용 예: 고객 세분화, 시장 분석\n③ 예측, Prediction/Regression\n연속형 목표값을 예측하며, 과거 데이터 기반으로 미래 값을 추정한다.\n④ 추천, Recommendation\n사용자 행동이나 선호도를 분석하여 개인화된 추천을 제공한다.\n활용 예: 전자상거래, 콘텐츠 플랫폼"
  },
  {
    "objectID": "ai/ml/ml_04.html",
    "href": "ai/ml/ml_04.html",
    "title": "머신러닝: 실무 사례",
    "section": "",
    "text": "반도체 HBM 공정과 스마트 CCTV를 중심으로 머신러닝의 실제 활용에 대해 다루고자 한다.\n01 제조 공정 혁신\n1 . 과거 제조업 현장 오랜 기간 아날로그 방식의 공정 관리에 의존해 왔으나 글로벌 경쟁 심화와 공정 복잡성 증가로 인해 단순 수작업이나 경험 중심의 운영만으로는 품질과 효율성을 확보하기 어려워졌다.\n이에 따라 디지털 전환(DX) 과 인공지능 전환(AX) 은 제조업 경쟁력 강화를 위한 핵심 과제로 부상하였다.\nDX와 AI 트랜스포메이션의 개념을 정리하고, 실제 사례로서 반도체 HBM 적층 공정의 기술적 차이를 분석한다.\n2 . Digital Transformation (DX) 기존의 아날로그 및 비체계적 공정을 시스템화된 디지털 관리 체계와 데이터베이스 기반 운영으로 전환하는 과정이다.\n공정, 설비, 품질 관리 정보를 실시간으로 수집·저장 운영 데이터의 추적성(traceability) 확보 표준화된 관리 체계 구축을 통해 오류 및 낭비 최소화 제조업에서 DX는 스마트 팩토리로의 진입을 위한 기초 단계라 할 수 있다.\n3 . AI Transformation (AX) DX 위에 구축된 데이터 인프라는 AI 분석을 통한 고도화로 확장된다. 이것은 다음 단계들을 포함한다.\n센서 및 IoT 장치를 통한 빅데이터 수집 실시간 데이터 시각화 및 공정 모니터링 이상치(anomaly) 탐지 및 자동 알림 데이터 기반 원인 분석 및 해결 방안 제시 공정 최적화 및 예측 유지보수 이러한 체계는 설비 수준이 낮은 공장에서도 점진적 도입이 가능하며, 데이터 기반 의사결정을 통해 전사적 품질 및 생산성 개선 효과를 제공한다.\n4 . 사례 분석: HBM 적층 공정 고대역폭 메모리(HBM)는 여러 개의 DRAM 다이를 수직으로 적층하여 초고속 데이터 전송을 가능케 하는 핵심 기술이다. 하지만 적층 단수가 증가할수록 수율(yield) 저하가 주요 문제로 지적된다.\n① SK하이닉스\nMR-MUF 공정을 활용하여 적층 단수 증가에도 수율 저하를 최소화한 것으로 평가된다.\n일부 보고에 따르면 12단 적층에서도 비교적 안정적인 수율 확보가 가능하다.\n② 삼성전자\nTC-NCF 기반 공정을 채택했으며, 접합 안정성 확보 및 void 제어 문제로 인해 수율 측면에서 난제를 겪고 있는 것으로 알려져 있다. 다만, 최근에는 Cu-to-Cu 본딩 등 새로운 기술을 병행 도입하고 있다.\n공개된 산업 보고 및 전문가 분석에 따르면, “삼성은 수율이 70% 미만, SK는 상대적으로 높은 수율 확보”라는 평가가 존재하나, 이는 공식 수치가 아닌 시장 리서치 기반 추정에 가깝다.\n‘HBM’ 골든타임 노리는 엇갈린 시선···삼전 ‘포괄적’ vs SK하닉 ‘트렌드’\n[이뉴스투데이 김진영 기자] AI 반도체 시장이 급속도로 확대되는 가운데 고대역폭메모리(HBM)의 중요성이 커지고 있다. 삼성전자와 SK하이닉스는 모두 AI 생태계 확장을 목표로 하고 있지만 HBM을\nwww.enewstoday.co.kr\nAI 시대의 새로운 심장: 삼성전자와 SK하이닉스, HBM과 차세대 메모리 기술로 펼치는 반도체 패권\nAI 시대의 HBM 기술 혁명! 삼성전자와 SK하이닉스의 패권 경쟁, 차세대 메모리의 미래를 탐구합니다. 한국 반도체의 도약을 확인하세요!\nskywork.ai 따라서 기술적 차이와 경향성은 확인되지만, 구체적인 수치는 신뢰도 높은 자료가 부족하므로 주의가 필요하다.\n5 . 결론 DX 와 AX 는 제조업 혁신의 핵심 경로로, 단순한 디지털화에서 나아가 AI 기반 예측·최적화 체계로의 전환을 의미한다.\n이는 설비 수준과 무관하게 단계적으로 도입이 가능하며, 스마트 팩토리 구현의 기반이 된다.\nHBM 사례는 AX 의 필요성을 잘 보여준다. 고난도의 적층 공정에서는 공정 변수와 결함 데이터를 정밀하게 수집·분석해야 수율을 안정적으로 확보할 수 있다. SK와 삼성의 공정 차이는 이러한 데이터 기반 접근의 중요성을 방증한다.\n따라서 제조업의 경쟁력 확보를 위해서는 DX → AX → 스마트 팩토리로 이어지는 단계적 전략이 필수적이며, 이는 단순한 시스템 도입을 넘어 데이터와 AI 중심의 운영 철학 전환을 요구한다.\n02 서울시 스마트 CCTV 시스템 서울시는 2020년부터 지능형 CCTV를 도입하여 교통 흐름 분석, 범죄 예방, 실시간 교통 신호 제어 등을 시도하였으나, 초기 도입 단계에서 AI 모델이 실제 현장 환경에서 기대 이하의 성능을 보였다.\n이는 모델이 주로 실내 환경이나 정형화된 데이터에만 학습되어, 다양한 날씨, 시간대, 사람의 행동 패턴 등 실제 환경의 변수를 충분히 반영하지 못했기 때문이다.\n서울시는 이러한 문제를 해결하기 위해 산·학·연과 자치구가 참여하는 ‘지능형 CCTV 활성화 계획’ 을 수립하고, 맞춤형 이벤트 설정, 오탐 데이터 학습, 사물·사람 구분 학습 등을 추진하였다.\n지능형CCTV로 만드는 디지털 안전도시 서울\n서울시가 시민 안전 강화 및 범죄 등 예방을 위해 올해 AI기반 지능형 CCTV를 대폭 늘리고, 시민들의 정보접근성을 높여줄 공공와이파이를 확대할 계획입니다. 서울시는 또 유동인구가 많은 곳 등\nscpm.seoul.go.kr\n서울시, ’지능형 CCTV’로 지자체 ICT 우수사례 대통령상 수상\n서울시가 ’제30회 지방자치단체 정보통신 우수사례 발표대회’에서 지능형 CCTV 오탐지 문제 해결로 대통령상인 최우수상을 수상했다. 관제 효율과 시민 안전망 강화가 높이 평가됐다. 서울시는\nwww.etnews.com 그 결과, 지능형 CCTV의 상황 판별 정확도는 36% → 71%로 향상되었고, 관제요원의 이벤트 확인률도 37% → 82%로 높아졌다.\n불필요한 탐지 건수는 월 454만 → 53만 건으로 줄어들며 약 8.8배 감소하였다. 이러한 개선은 사건 처리 건수를 이전보다 6배 이상 증가된 성과를 가져왔다.\n또한, 서울시는 2026년부터 지능형 CCTV에 생성형 AI를 접목하는 시범사업을 추진하여, 기존 CCTV가 단순히 ‘이상 유무’ 만을 판별하는 수준에서 ‘왜 이상한지, 어떤 맥락인지’ 를 설명할 수 있는 단계로 진화할 계획이다.\n이러한 사례는 AI 기반 CCTV 시스템의 효용성을 높이기 위해서는 다양한 상황을 반영한 데이터 수집과 모델 훈련이 필수적임을 보여준다.\n정상 상황뿐 아니라 예외적 상황까지 포함하는 데이터 기반의 모델 훈련이 가장 중요한 요소로 고려되어야 한다."
  },
  {
    "objectID": "ai/ml/ml_06.html",
    "href": "ai/ml/ml_06.html",
    "title": "Credit Approval",
    "section": "",
    "text": "의사결정트리 기반 신용평가 분석에 대해 다루고자 한다\n01 Credit Approval UCI의 Credit Approval 데이터셋의 일부 속성(attribute)들을 보고, 어떤 속성을 학습 가설(feature set)에 포함하는 것이 더 좋은지를 판단하고자 한다.\n참고자료\n\n기계 학습의 기초 2-1~2-5 &gt; Machine Learning in Korean (1) | 카이스트 응용인공지능 연구실\n\naai.kaist.ac.kr 본 장에서는 KAIST 인공지능연구원(AAI)에서 제공한 강의 자료 기계 학습의 기초 3강, 4강을 참고하였다.\nUCI Machine Learning Repository\nA1: b, a. A2: continuous. A3: continuous. A4: u, y, l, t. A5: g, p, gg. A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff. A7: v, h, bb, j, n, z, dd, ff, o. A8: continuous. A9: t, f. A10: t, f. A11: continuous. A12: t, f. A13: g, p, s. A14: continuous. A15:\narchive.ics.uci.edu # pip install ucimlrepo # ucimlrepo 파이썬 패키지를 통해 간편히 불러올 수도 있다.\nfrom ucimlrepo import fetch_ucirepo import pandas as pd\n\n1. 데이터 불러오기 (Credit Approval, ID = 27)\ncredit = fetch_ucirepo(id=27)\n\n\n2. 피처와 타깃 결합\ndf = pd.concat([credit.data.features, credit.data.targets], axis=1) df\n데이터셋 개요\n전체 인스턴스 수: 690개 샘플(instances): 690개 피처(features): 15개 타깃(target, 승인 여부): 1개 일부 피처들이 연속형(continuous), 일부 명목형(categorical).\n결측값(missing values)이 존재함 (UCI 리포지토리에 “Has Missing Values? Yes” 라고 명시됨). [출처]\n결측이 표기된 방식이 ‘?’ 문자열 (특히 categorical 또는 일부 continuous 피처)임. [출처]\n여기서 A1, A9 두 속성이 각각 긍정/부정 클래스를 얼마나 잘 분리할 수 있는지를 비교하고 있고, 어떤 속성이 더 좋은 속성인지 판별하고자 한다.\n\n\n각 컬럼의 고유값이 시리즈 형태로 반환\nunique_values = df[[‘A1’, ‘A9’, ‘A16’]].apply(lambda x: x.unique()) unique_values\nA1에 결측치 확인\nA1, A9 속성 기준 승인/거절 빈도 및 퍼센트 요약표 생성\nimport pandas as pd\n\n\nA1 컬럼의 NaN 값을 문자열 ’?’로 변환 (결측치 표시)\ndf[‘A1_filled’] = df[‘A1’].fillna(‘?’)\n\n\n전체 클래스 분포\nclass_dist = df[‘A16’].value_counts().rename_axis(‘A16’).reset_index(name=‘count’) total_positive = class_dist.loc[class_dist[‘A16’]==‘+’,‘count’].values[0] total_negative = class_dist.loc[class_dist[‘A16’]==‘-’,‘count’].values[0]\n\n\nA1 교차표 (결측치 포함)\ncross_A1 = pd.crosstab(df[‘A1_filled’], df[‘A16’]).reset_index() cross_A1.insert(0, ‘속성’, ‘A1’) cross_A1 = cross_A1.rename(columns={‘A1_filled’: ‘값’, ‘+’: ‘승인(+)’, ‘-’: ‘거절(-)’})\n\n\n퍼센트 컬럼 추가\ncross_A1[‘승인(%)’] = (cross_A1[‘승인(+)’] / total_positive * 100).round(1) cross_A1[‘거절(%)’] = (cross_A1[‘거절(-)’] / total_negative * 100).round(1)\n\n\nA9 교차표 (결측치 없음)\ncross_A9 = pd.crosstab(df[‘A9’], df[‘A16’]).reset_index() cross_A9.insert(0, ‘속성’, ‘A9’) cross_A9 = cross_A9.rename(columns={‘A9’: ‘값’, ‘+’: ‘승인(+)’, ‘-’: ‘거절(-)’}) cross_A9[‘승인(%)’] = (cross_A9[‘승인(+)’] / total_positive * 100).round(1) cross_A9[‘거절(%)’] = (cross_A9[‘거절(-)’] / total_negative * 100).round(1)\n\n\n전체 클래스 분포를 표 형태로 변환\noverall = pd.DataFrame({ ‘속성’: [‘전체’], ‘값’: [‘-’], ‘승인(+)’: [total_positive], ‘거절(-)’: [total_negative], ‘승인(%)’: [100.0], ‘거절(%)’: [100.0] })\n\n\n데이터 병합\ncombined_df = pd.concat([overall, cross_A1, cross_A9], ignore_index=True) combined_df\n1 . 속성 선택 기준 머신러닝 / 개념 학습 이론에서 좋은 속성(feature)을 선택하는 기준은 다음과 같다:\n정보 이득(IG) 혹은 엔트로피 감소 지니 계수 (Gini impurity) 감소 분류 순도 (Purity) 잡음 민감성 — 속성이 얼마나 레이블 노이즈에 영향을 덜 받는가 이 속성 선택 기준들은 결정 트리 알고리즘(C4.5, CART 등)에서 흔히 쓰인다.\n즉, 어떤 속성이 클래스 라벨(+)과 (–)를 더 잘 구분할 수 있을지, 엔트로피(불확실성)를 많이 줄일 수 있을지를 보는 것이다.\n2 . A1 vs A9 비교\n기계 학습의 기초 3강 pdf 자료 A9 속성의 Positive/Negative 비율을 보면, t일 경우 약 79%가 Positive, f일 경우 약 93%가 Negative로 나타나, 클래스 구분력이 매우 높음을 알 수 있다.\n반면 A1 속성은 a와 b 사이의 Positive 비율 차이가 약 3~4% 수준으로 미미하여, 클래스 구분 신호가 약하다.\n따라서 (A9)를 포함하는 학습 가설 모델은 더 높은 성능을 기대할 수 있다.\nSCRIPT https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n3 . 주의할 점 / 한계 제시된 것은 빈도수 기반 단순 비교일 뿐이고, 실제 속성의 복합 상관관계나 다른 속성과의 상호작용은 고려하지 않는다.\n결측치(?) 처리가 중요하다. 제시된 A1의 경우 “?” 값이 일부 존재하므로, 이를 어떻게 처리하는가가 결과에 영향을 준다.\n노이즈(잘못된 레이블, 입력 오류 등)가 존재할 수 있고, 속성 선택이 그 노이즈에 민감할 수 있다.\n속성 하나만으로 모든 분류가 가능하지 않으므로 여러 속성을 조합하는 가설이 필요하다.\n02 암맹처리 UCI의 Credit Approval 데이터셋에서 결측치(missing values, 암맹처리 또는 ‘?’ 표기됨) 처리를 하는 이유에 대해 설명하고자 한다.\n1 . 분류 모델/알고리즘의 단점 대부분의 ML 라이브러리(예: scikit-learn)의 분류/회귀 알고리즘은 입력 피처(feature)의 값이 모두 수치(numeric), 범주형(categorical) 변환이 완료된 상태여야 한다.\n‘?’ 같은 문자열은 수치 연산, 비교, 통계계산, 분기(split) 등이 불가능하거나 오류를 발생시키기 쉬우며, 평균(mean), 엔트로피 계산, 불순도 계산 시 오류가 발생한다.\n따라서 ’?’를 NaN 또는 동일한 결측치 포맷으로 바꾸고, 이를 채우거나 해당 샘플/속성을 제거해야 한다.\n2 . 모델의 성능 및 일반화 결측치를 처리하는 핵심 목적 중 하나가 편향(bias) 제거이다.\n결측치 미처리 시 모델이 학습 중 일부 피처를 무시하거나, 잘못된 피처 분할(split)로 과적합(overfitting) 또는 편향(bias)가 커질 수 있기 때문이다.\n또한 학습 데이터와 테스트 데이터 간의 일관성이 깨질 수 있다.\n학습에는 특정 방식으로 결측치 처리를 했는데, 실제 예측 시에는 결측치 형태가 다르면 예측이 잘 안 되는 경우\n3 . 통계적 분석의 정확성 확보 결측이 있는 상태로 단순히 평균/분포/상관관계(correlation) 계산 등을 하면, 해당 피처가 가진 정보가 왜곡될 수 있다.\n예: 연속형 피처에 결측치가 많으면 그 피처의 평균/표준편차 등의 계산이 왜곡됨 → 전체 엔트로피나 지니, 정보 이득(IG) 계산 등에 영향을 끼침.\n4 . 데이터의 규모와 손실 최소화 전체 데이터 중 결측이 있는 샘플이 많지 않다면(dropna), 이들을 제거하는 것이 하나의 방법임.\n이 데이터셋에서는 약 37개의 행(samples) — 전체의 약 5% 정도가 결측값을 포함함. [출처]\n하지만 결측치가 많은 피처가 있다면 단순 제거는 정보 손실이 큼. 이런 경우 imputation(대체값 채우기) 방법(평균, 최빈값, 예측 모델 등)이 사용된다.\n5 . 실제 적용 예시 여러 프로젝트/분석에서, 이 데이터셋의 결측치는 ’?’로 표시되어 있어 먼저 이를 NaN으로 변환함. [출처]\n연속형 피처에 대해서는 평균(mean) 또는 중앙값(median)으로 채움(imputation). 명목형(categorical) 피처의 경우 최빈값(mode)으로 채우는 사례가 많음. [출처]\n또는 결측이 있는 샘플을 통째로 제거(drop)하는 방법이 사용됨. 예: 37개의 결측 샘플을 제거한 후 분석 진행한 연구 있음. [출처]\n결과적으로, 암맹처리는 단순히 오류를 피하기 위한 것뿐만 아니라 모델의 공정성(fairness)과 일반화 성능을 높이는 중요한 단계이다.\n03 ID3 의사결정나무 분할 전략 ID3(Iterative Dichotomiser 3) 알고리즘은 의사결정나무 모델에서 데이터를 분할할 특성을 선택할 때 정보 이득(IG)을 기준으로 삼는다.\n정보 이득은 특정 속성으로 데이터를 분할했을 때 엔트로피가 얼마나 감소하는지를 측정하는 지표로, 엔트로피는 데이터의 불순도 또는 무질서를 나타낸다.\n즉, 엔트로피 감소가 클수록 해당 속성은 데이터를 더 잘 분리한다고 판단되어 우선적으로 선택된다. [출처].\n엔트로피 계산 예제\n1 . 전체 엔트로피 H(Y) (P(+) = 307/690 ,P(-) = 383/690 ) (H(Y) = - (0.445 _2 0.445 + 0.555 _2 0.555) )\n2 . A1로 분할 후 엔트로피 H(Y|A1) 각 그룹 엔트로피 계산 후 가중 평균:\n\\[H(Y|A1) = \\frac{210}{690} H(a) + \\frac{468}{690} H(b) + \\frac{12}{690} H(?)\\]\n각 그룹 엔트로피:\n(a: (H(a) = - (98/210 _2 (98/210) + 112/210 _2 (112/210)) ) ) (b: (H(b) = - (206/468 _2 (206/468) + 262/468 _2 (262/468)) ) ) (?: (H(?) = - (3/12 _2 (3/12) + 9/12 _2 (9/12)) ) ) 가중 평균:\n\\[H(Y|A1) \\approx \\frac{210}{690}\\cdot0.998 + \\frac{468}{690}\\cdot0.993 + \\frac{12}{690}\\cdot0.811 \\approx 0.994\\]\n3 . A9로 분할 후 엔트로피 H(Y|A9) ( t: (H(t) = - (284/361 _2(284/361) + 77/361 _2(77/361)) )) ( f: (H(f) = - (23/329 _2(23/329) + 306/329 _2(306/329)) ) ) 가중 평균:\n\\[H(Y|A9) \\approx \\frac{361}{690}\\cdot0.722 + \\frac{329}{690}\\cdot0.164 \\approx 0.455\\]\n4 . 정보이득 계산 \\[IG(Y, A1) = H(Y) - H(Y|A1) \\approx 0.99 - 0.994 = -0.004 \\quad (\\text{사실상 0})\\]\n\\[IG(Y, A9) = H(Y) - H(Y|A9) \\approx 0.99 - 0.455 = 0.535\\]\nA9가 정보이득이 훨씬 크므로, ID3에서는 A9를 루트 노드(최초 분할 기준)로 선택한다. A1은 거의 정보이득이 없어 의미 있는 분할이 되지 않는다.\n따라서, A9가 먼저 선택되어 분할 기준으로 사용되며, 이는 모델의 성능 향상에 기여할 수 있다.\n04 의사결정 트리의 한계 의사결정 트리(Decision Tree)는 구조가 직관적이고 해석이 용이하다는 장점이 있으나, 현실의 데이터가 갖는 노이즈와 불일치성에 매우 민감하다는 단점이 있다.\n데이터에 포함된 잡음이나 이상치가 많을 경우, 트리는 그 불완전한 패턴까지 학습하여 과적합(overfitting) 이 발생하거나 작은 데이터 변화에도 트리 구조가 크게 달라지는 불안정성(variance) 을 보인다.\n이러한 이유로, 단일 트리 모델은 실제 업무 환경에서의 예측 신뢰성이 낮을 수 있다.\n이러한 문제를 보완하기 위해 도입된 방법이 앙상블 기법이다.\n1 . 앙상블 기법 Ensemble\n여러 개의 학습기(base learners)를 조합하여, 단일 모델보다 더 안정적이고 일반화 성능이 높은 예측 결과를 도출하는 방법.\n이는 음악의 ’오케스트라’처럼 개별 모델의 예측을 조화롭게 결합하여 전체적인 조화(화음)를 이루는 방식으로 비유할 수 있다.\n앙상블 기법은 일반적으로 두 가지로 구분된다.\n2 . Bagging Bootstrap Aggregating\n데이터로부터 복원추출(bootstrap sampling)된 여러 하위 표본을 생성하여, 각 표본마다 독립된 트리를 학습시킨 후 결과를 평균(또는 투표)하는 방식이다.\n대표적인 알고리즘은 랜덤 포레스트(Random Forest) 로, 각 트리 학습 시 특성(feature)의 일부만 무작위로 선택하여 트리 간의 상관성 및 분산을 효과적으로 감소한다.\n이러한 무작위성과 집계(aggregation) 과정을 통해 단일 트리에 비해 일반화 능력과 예측 안정성이 향상된다.\n3 . Boosting 약한 학습기(weak learner)를 순차적으로 학습시키는 방식으로, 이전 모델이 잘못 예측한 데이터에 더 큰 가중치를 부여하여 점진적으로 오차를 보정한다.\n이 방식은 편향(bias) 을 줄이는 데 효과적이며, 대표적인 알고리즘으로 AdaBoost, Gradient Boosting, XGBoost 등이 있다.\n다만, 데이터에 노이즈가 많을 경우 과적합 위험이 존재하므로, 적절한 규제(regularization)와 학습률 조절이 필요하다.\n4 . 효과와 한계 일반적으로 단일 트리 모델보다 2 ~ 5% 내외의 성능 향상을 보이는 것으로 보고되지만, 향상 폭은 데이터의 특성과 모델 설계에 따라 달라질 수 있다.\n또한, 앙상블은 여러 모델을 결합하므로 해석력이 낮아지고 계산 비용이 증가하는 단점도 존재한다."
  },
  {
    "objectID": "ai/ml/ml_08.html",
    "href": "ai/ml/ml_08.html",
    "title": "의사결정트리 실습",
    "section": "",
    "text": "의사결정트리 실습에 대해 다루고자 한다.\n01 커널 서포트 벡터 머신 Kernel Support Vector Machine\n1 . 제목2 %matplotlib inline import numpy as np import matplotlib.pyplot as plt\n\n랜덤 시드 설정\nnp.random.seed(0)\n\n\nXOR 데이터 생성\nX_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0) y_xor = np.where(y_xor, 1, 0)\n\n\n클래스별 산점도\nplt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c=‘b’, marker=‘o’, label=‘class 1’, s=50) plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c=‘r’, marker=‘s’, label=‘class 0’, s=50)\n\n\n그래프 옵션\nplt.legend() plt.xlabel(“x1”) plt.ylabel(“x2”) plt.title(“XOR Problem”) plt.show()\n%matplotlib inline import numpy as np import matplotlib.pyplot as plt from sklearn.svm import SVC import matplotlib as mpl\n\n\nXOR 데이터 생성\nnp.random.seed(0) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0) y_xor = np.where(y_xor, 1, 0)\n\n\nXOR plot 함수 정의\ndef plot_xor_flipped(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid( np.arange(xmin, xmax, (xmax-xmin)/100), np.arange(ymin, ymax, (ymax-ymin)/100) ) ZZ = np.reshape( model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape ) # 색상 반전 ZZ = 1 - ZZ\nplt.contourf(XX, YY, ZZ, alpha=0.5, cmap=mpl.cm.Paired)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='class 1', s=50)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='class 0', s=50)\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.title(title)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\n\n\n결정 경계 시각화\nsvc = SVC(kernel=‘linear’) svc.fit(X_xor, y_xor) plot_xor_flipped(X_xor, y_xor, svc, “Classification Result by Linear SVC XOR”) plt.show()\nimport numpy as np from sklearn.preprocessing import FunctionTransformer\n\n\n기저 함수 정의\ndef basis(X): return np.vstack([X[:, 0]2, np.sqrt(2)X[:, 0]X[:, 1], X[:, 1]2]).T\n\n\n테스트용 배열 생성\nx = np.arange(8).reshape(4, 2) x\nFunctionTransformer(basis).fit_transform(x)\nimport matplotlib.pyplot as plt from sklearn.preprocessing import FunctionTransformer\n\n\n기저 함수 변환\nX_xor2 = FunctionTransformer(basis).fit_transform(X_xor)\n\n\n변환된 특징으로 산점도\nplt.scatter(X_xor2[y_xor == 1, 0], X_xor2[y_xor == 1, 1], c=“b”, s=50, label=“class 1”) plt.scatter(X_xor2[y_xor == 0, 0], X_xor2[y_xor == 0, 1], c=“r”, s=50, label=“class 0”) plt.legend() plt.show()\n02 ㄷ SVM with various Kernels\nimport matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import FunctionTransformer from sklearn.svm import SVC\n\n\n— 한글 폰트 설정 —\nplt.rc(‘font’, family=‘Malgun Gothic’) # Windows: ‘Malgun Gothic’, Mac: ‘AppleGothic’, Linux: ‘NanumGothic’ plt.rcParams[‘axes.unicode_minus’] = False # 마이너스 기호 깨짐 방지\ndef plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid( np.arange(xmin, xmax, (xmax-xmin)/100), np.arange(ymin, ymax, (ymax-ymin)/100) ) ZZ = np.reshape( model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape )\n# 색상 반전\nZZ = 1 - ZZ  \n\nplt.contourf(XX, YY, ZZ, alpha=0.5, cmap=plt.cm.Paired)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='class 1', s=50)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='class 0', s=50)\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.title(title)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\n\n\n적용\nplot_xor(X_xor, y_xor, basismodel, “기저함수 SVC 모형을 사용한 XOR 분류 결과”) plt.show()\n\n\n시그모이드 커널 SVC 학습\nsigmoidsvc = SVC(kernel=“sigmoid”, gamma=2, coef0=2) sigmoidsvc.fit(X_xor, y_xor)\n\n\nXOR 데이터 결정 경계 시각화\nplot_xor(X_xor, y_xor, sigmoidsvc, “Sigmoid SVC”) plt.show()\nrbfsvc = SVC(kernel=“rbf”).fit(X_xor, y_xor) plot_xor(X_xor, y_xor, rbfsvc, “RBF SVC”) plt.show()\nplot_xor(X_xor, y_xor, SVC(kernel=“rbf”, gamma=2). fit(X_xor, y_xor), “RBF SVC (gamma=2)”) plt.show()\nplot_xor(X_xor, y_xor, SVC(kernel=“rbf”, gamma=50). fit(X_xor, y_xor), “RBF SVC (gamma=50)”) plt.show()"
  },
  {
    "objectID": "ai/ml/ml_10.html",
    "href": "ai/ml/ml_10.html",
    "title": "단계별 머신러닝 학습",
    "section": "",
    "text": "시그모이드, 소프트맥스, 다층 퍼셉트론 등에 대해 다루고자 한다.\n01 다중 회귀 Multiple Regression\n앞서 살펴본 단일 입력(단변량, single variable) 회귀모델은 하나의 독립 변수만을 고려하였다.\n그러나 실제 데이터 분석에서는 여러 입력값을 동시에 반영하는 다중 회귀 모델이 보다 현실적이고 효과적으로 활용된다.\n이 모델은 여러 독립 변수(feature)를 기반으로 종속 변수(target)를 예측하며, 복잡한 현실 세계의 관계를 선형적으로 근사할 수 있다.\n다중 회귀의 기본 구조는 단일 회귀와 동일하다. 입력 변수의 개수만 증가하고, 이에 대응하는 가중치(weight) 역시 벡터 형태로 확장된다.\n\\[\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\\]\nw_i: 각 특성(feature)에 대한 가중치, x_i: 입력 변수, b: 절편(bias)\n1 . 벡터와 행렬 기반 연산 입력 변수가 증가하더라도 손실함수의 계산 원리는 변하지 않는다. 다만, 계산의 복잡성이 높아지므로, 가중치와 입력값을 벡터 또는 행렬 형태로 표현된다.\n\\[\\mathbf{X} \\in \\mathbb{R}^{m \\times n}, \\quad \\mathbf{w} \\in \\mathbb{R}^{n \\times 1}, \\quad \\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b}\\]\nm: 데이터 포인트(샘플)의 수 n: 특성(feature)의 수 이러한 행렬 표현을 통해 모든 샘플에 대한 예측을 단일 연산으로 수행할 수 있어 모델 학습 과정의 계산 효율성(computational efficiency)이 크게 향상된다.\n2 . 핵심 선형대수 연산 핵심 연산은 다음과 같다.\n① 내적(dot product)\n두 벡터의 방향과 크기를 비교하는 연산으로, 회귀에서는 예측값 계산 및 유사도 측정((cos θ))에 주로 활용된다.\n② 외적(cross product)\n두 벡터에 수직인 새로운 벡터를 생성하며, (sin θ)를 통해 방향성을 표현한다. 다만 외적은 회귀 분석이나 유사도 계산보다는 물리적 벡터 계산에서 주로 사용된다.\n고차원 연산과 병렬 처리 다중 입력 연산은 수많은 곱셈과 덧셈을 포함하므로, 입력 차원이 커질수록 연산 부하가 기하급수적으로 증가한다. 특히 이미지, 영상, 음성 등 대규모 데이터셋에서는 이 문제가 더욱 두드러진다.\n이러한 연산 복잡도를 해결하기 위해 병렬 연산(parallel computation)기법이 적용되며, 대표적으로 GPU가 사용된다.\nGPU는 수천 개의 코어를 이용해 행렬 및 벡터 연산을 동시에 수행하므로, 3차원 좌표 변환이나 2차원 이미지 처리 등 고속 연산 환경에서 탁월한 성능을 발휘한다.\n또한 CUDA와 같은 GPU 전용 병렬처리 라이브러리 사용 시 AI 모델 학습 과정에서 연산 속도를 획기적으로 향상시킬 수 있다.\n이와 같이 확장된 회귀모델의 기반에는 선형대수(linear algebra)가 자리하고 있으며, 모든 계산은 행렬 곱, 내적, 외적 등 선형대수적 연산 원리에 의해 수행된다.\n3 . 스케일링과 정규화 scaling & normalization\n다중 회귀 모델의 학습 과정에서는 입력 데이터의 스케일 차이가 학습 안정성과 예측 정확도에 큰 영향을 미칠 수 있다.\n이를 방지하기 위해, 각 특성(feature)을 동일한 기준으로 조정하는 스케일링과 정규화 과정이 필수적이다.\n① 스케일링\n각 변수의 단위나 값의 범위 차이로 인해 특정 특성이 모델 학습 과정에서 과도하게 영향을 미치는 문제를 방지한다.\n예: 각 값에 대해 최대값 또는 범위를 기준으로 나누어 0~1 범위로 조정\n② 정규화\nZ-점수 정규화를 적용하여, 데이터를 평균 0, 표준편차 1의 정규분포로 변환하면 특성 간 스케일 불균형을 더욱 효과적으로 완화할 수 있다.\n예: 데이터가 타원형 분포를 가진 경우 각 값을 최대값으로 나누어 원형 형태로 정규화할 수 있다.\n이러한 변환을 수행하면 손실 함수의 등고선(contour)이 타원형에서 원형으로 바뀌어, 경사하강법의 수렴 속도와 안정성이 향상된다.\n즉, 모든 입력값을 동일한 기준으로 인식하도록 조정하여 모델이 보다 균형 잡힌 방식으로 최적화를 수행하게 만든다.\n참고로, 변동계수(CV)는 데이터의 상대적 분산을 평가하는 보조 지표로 활용될 수 있으나, 본 장에서는 필수 적용 사항은 아니다.\n02 분류와 비선형 함수의 개념 Classification & Nonlinear Functions\n앞서 다중 및 다항 회귀를 통해 연속적인 수치값을 예측하는 모델 구조를 살펴보았다.\n그러나 실제 문제의 상당수는 단순한 수치 예측이 아닌, “이것이냐, 저것이냐”와 같은 범주형(class) 결과를 요구한다.\n이처럼 출력값이 명확히 구분되는 문제에서는 회귀(regression) 대신 분류(classification) 개념이 사용된다.\n그중에서도 두 가지 범주를 구분하는 가장 기본적인 형태가 이진 분류(binary classification) 이다.\n대표적인 활용 사례는 다음과 같다.\n스팸 메일 탐지: 스팸 / 정상 소셜 미디어 콘텐츠 노출 결정: 보이기 / 숨기기 신용카드 부정 거래 탐지: 정상 / 이상 MRI 영상 판독: 정상 / 비정상 이와 같이, 분류 문제는 연속적 수치 예측과 달리 출력값이 불연속적이고 범주형이라는 점에서 학습 방식과 손실 함수, 모델 설계 측면에서 특수성을 가진다.\n1 . 회귀에서 분류로의 전환 선형 회귀는 입력값의 선형 결합을 통해 연속적인 출력값을 생성한다.\n그러나 분류 문제에서는 출력이 특정한 범위 내에 속해야 하며, 즉 결과값이 “0 또는 1”, “긍정 혹은 부정”처럼 해석 가능한 확률 형태로 제한될 필요가 있다.\n예를 들어, 종양의 크기에 따라 악성(1)과 양성(0)을 분류하는 문제를 생각해보자.\n데이터에서 “크기 40 이상일 때 악성” 으로 정의하면, 단순 선형 모델은 30 ~ 40 사이에 결정 경계선(decision boundary)을 그어 두 범주를 구분할 수 있다.\n그러나 새로운 데이터에서 70 이상의 값이 등장하면, 기존 경계선이 더 이상 유효하지 않아 새로운 기준선 재설정 문제가 발생한다.\n이러한 한계를 해결하기 위해, 선형 출력값을 0 ~ 1 사이로 압축하여 확률적으로 해석 가능한 비선형 변환 함수(activation function)가 도입된다.\n그중 가장 대표적인 예가 시그모이드 함수이다.\n2 . 시그모이드 함수 Sigmoid Function\n시그모이드 함수는 실수 입력값 (z)를 받아 이를 0 ~ 1 사이의 실수 값으로 변환하는 비선형 함수이다.\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\n여기서 (z)는 이전 장에서 다룬 선형 결합 (WX)에 해당한다. 즉, 회귀모델의 출력값을 시그모이드 함수에 통과시켜 확률(probability) 형태로 변환하는 과정이다.\n확률적 해석 가능\n입력값이 매우 크면 1에 근접 입력값이 매우 작으면 0에 근접 이 함수는 ” 연속적인 선형 출력을 상·하한이 존재하는 구간으로 압축하는 변환 ” 으로 이해할 수 있다.\n부드러운 결정 경계\nS자 곡선(S-shaped curve)을 형성하며, 입력값이 증가에 따라 악성일 확률이 점진적으로 증가한다. 선형 출력이 비선형적으로 압축되어, 부드러운 결정 경계를 제공한다.\n미분 연속성\n미분이 연속적이므로, 경사하강법과 같은 최적화 알고리즘에서 기울기 계산이 원활하다.\n이러한 특성 덕분에 시그모이드 함수는 암 진단(양성/음성), 이메일 스팸 분류(스팸/정상) 등 이진 분류 문제에서 확률 기반 의사결정을 수행하는 핵심 요소로 활용된다.\n3 . 하이퍼볼릭 탄젠트 함수 Hyperbolic tangent Function\n시그모이드 함수와 유사하게, 하이퍼볼릭 탄젠트(tanh) 함수도 입력값을 비선형적으로 변환하는 함수이다.\n이 함수는 다음과 같이 정의된다.\n\\[\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\]\n출력값은 -1 ~ 1 사이에 위치한다. 중심이 0 에 있어, 시그모이드보다 학습 과정에서 안정성이 높다. 예: 감정 분석에서 긍정(+1)과 부정(-1)을 구분, 좋아요/싫어요 같은 양방향 선택 구조를 다룰 때 유용하다.\n단, 음의 출력 범위를 포함하므로 연산적으로 시그모이드보다 약간 복잡하며, 데이터 특성에 따라 선택적으로 적용된다.\n비교표 함수 출력 범위 중심값 주 용도 장점 시그모이드 0 ~ 1 0.5 확률 기반 이진 분류 확률적 해석 용이, 단순 하이퍼볼릭 탄젠트 -1 ~ +1 0 양극적 분류(긍정/부정) 중심 0, 학습 안정성 높음 시그모이드와 tanh는 모두 선형 회귀 결과를 비선형적으로 변환하여 확률 또는 분류 경계를 생성하는 역할을 한다.\n이러한 활성화 함수의 도입은 회귀모델이 “연속 예측”에서 “범주 판별”로 확장되는 전환점이며, 이후의 심층신경망(Deep Neural Network) 구조에서도 핵심적 역할을 수행한다.\n03 다중 입력 확장\n1 . 가설 Hypothesis\n단일 입력 로지스틱 회귀에서처럼, 여러 입력(feature)을 동시에 고려할 경우에도 원리는 동일하다.\n입력값의 선형 결합 (z = WX + b)를 시그모이드 함수에 통과시키면 출력값이 0 ~ 1 사이의 확률로 변환된다.\n\\[h(X) = \\sigma(WX + b) = \\frac{1}{1 + e^{-(WX+b)}}\\]​\n이 확률값은 주어진 입력이 특정 범주에 속할 가능성을 의미하며, 이를 통해 이진 분류 문제에서 직관적이고 안정적인 의사결정이 가능하다.\n2 . 다중 입력 확장 Multivariate Input\n현실 세계의 데이터는 단일 입력보다는 여러 독립 변수(feature)를 동시에 고려해야 하는 경우가 많다.\n다중 입력의 경우에도 선형 결합의 구조는 동일하며, 단지 입력 벡터와 가중치 벡터를 확장하여 계산한다.\n\\[z = W_1 x_1 + W_2 x_2 + \\dots + W_n x_n + b = WX + b\\]\n이렇게 계산된 다중 입력 결과는 소프트맥스(Softmax) 함수를 통해 정규화될 수 있다.\n\\[\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\]\n소프트맥스는 각 범주의 확률 합이 1이 되도록 조정하며, 출력값이 여러 클래스 중 하나로 분류될 수 있도록 확률 기반 해석 제공\n다중 클래스 분류 문제에서 필수적인 역할을 수행한다.\n3 . 다중 클래스(Softmax + 원-핫) 확장 다중 클래스 문제에서는 각 클래스 k에 대해 선형 점수(score)를 계산하고 소프트맥스(Softmax)로 확률분포를 얻는다.\n\\[s_k = w_k^\\top x + b_k,\\qquad p_k=\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\]\n손실은 다중 클래스 크로스엔트로피(원-핫 레이블 y 기준):\n\\[J=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k} y^{(i)}_k \\log p_k^{(i)}\\]​\n구조적으로 퍼셉트론과 유사하며, 신경망(NN)의 전단계로 이해할 수 있다.\n4 . 다층 페셉트론 Multi-Layer Perceptron, MLP\n단일 로지스틱 회귀와 다중 입력 확장을 기반으로, 구조를 여러 층으로 확장하면 다층 퍼셉트론이 된다.\n각 층에서는 선형 변환과 비선형 활성화가 반복 적용되며, 입력 간의 복잡한 상호작용과 패턴을 학습할 수 있다.\nMLP는 심층 신경망(DNN)의 기본 단위로, 다층 구조를 통해 비선형적 결정 경계와 복잡한 데이터까지 패턴 학습이 가능하다."
  },
  {
    "objectID": "ai/ml/ml_12.html",
    "href": "ai/ml/ml_12.html",
    "title": "현대 AI 연구",
    "section": "",
    "text": "인공지능 발전과 대규모 언어 모델의 CoT 통합 구조에 대해 다루고자 한다.\n01 AI의 학문적 발전 인공지능(AI)의 발전은 소프트웨어적 진화와 하드웨어적 진화라는 두 축을 중심으로 병행되어 왔다.\n이 두 축은 독립적으로 발전한 것이 아니라, 서로 상호 보완적 관계 속에서 영향을 주고받으며 오늘날의 AI 연구 쳬계를 형성하였다.\n1 . 소프트웨어적 발전 ① 인과관계 중심 접근\n심볼릭 어프로치, Symbolic AI\nAI 연구의 초창기에는 인간의 사고 과정을 모사하기 위해 명시적 인과관계(explicit causality)를 기반으로 한 접근이 시도되었다.\n이 시기의 대표적 연구 흐름은 Symbolic AI로, 지식공학(knowledge engineering)을 중심으로 발전하였다.\n대표적 사례로는 전문가 시스템(expert system)이 있으며, MYCIN과 DENDRAL이 대표적 예이다.\n이러한 시스템은 인간 전문가의 지식을 ’규칙(rule)’과 ’추론(inference)’의 형태로 체계화하여, 명시적으로 저장하고, 이를 이용해 논리적 결론을 도출하는 구조를 가진다.\n장점 원인과 결과의 관계가 명확하므로, 시스템의 판단 근거를 추적할 수 있어 설명 가능성(explainability)이 높다.\n한계 모든 지식을 사람이 사전에 정의해야 하므로, 현실 세계의 복잡성과 불확실성을 충분히 반영하지 못한다.\n② 상관관계 중심 접근\n애니매틱 어프로치, Connectionist / Statistical AI\n이후 연구자들은 인과관계를 직접 모델링하기 보다는, 데이터로부터 패턴을 학습하는 방향으로 나아갔다.\n이것이 바로 연결주의적 또는 통계적 접근으로, 오늘날의 머신러닝 및 딥러닝의 기반이 된다.\n이 접근법에서는 대량의 데이터를 수집·정제한 뒤, 심층신경망(deep neural networks) 등의 모델을 학습시켜 결과를 도출한다.\n장점 명시적 인과모형 없이도 높은 예측 정확도를 달성하며, 이미지 인식·자연어 처리·음성 인식 등 다양한 분야에서 혁신적 성과를 보였다.\n한계 모델 내부 구조가 복잡하고 가중치에 의존하므로, 판단 과정이 불투명한 블랙박스(black box) 문제가 발생한다.\n즉, 상관관계를 통해 결과를 도출할 수는 있지만, 그 관계가 ‘왜’ 성립하는가에 대한 설명이 어렵다. 이로 인해 실세계 응용에서 신뢰성과 윤리성의 한계가 제기되었다.\n③ 하이브리드 접근 및 설명 가능한 AI\nExplainable AI, XAI OR Neuro-Symbolic AI\n이러한 한계를 극복하기 위한 새로운 방향으로 설명 가능한 인공지능 연구가 등장하였다.\n이는 심볼릭 AI의 논리적 해석력과 연결주의 AI의 학습 능력을 통합하는 하이브리드 AI, 혹은 뉴로-심볼릭 AI의 형태로 발전하고 있다.\n이 접근의 핵심은 예측성과 해석성의 통합에 있다.\n하이브리드 AI의 운영 구조는 다음과 같다.\n예측 단계: 대규모 데이터를 활용하여 통계적 분석과 패턴 인식을 수행함으로써 예측 결과를 생성한다.\n설명 단계: 규칙 및 온톨로지 기반의 인과 추론 체계를 활용하여, 예측의 근거를 논리적으로 해석하고 검증한다. 적용 분야: 의료, 금융, 법률 등 설명 가능성이 필수적인 영역. 현재 한계: 완전한 인과적 추론 수준에는 아직 도달하지 못하였으며, 인과적 설명의 내재화(causal reasoning integration)는 여전히 AI 연구의 핵심 과제로 남아 있다.\n2 . 하드웨어적 발전 Physical AI\nAI의 발전은 알고리즘적 진보를 넘어, 물리적 지능(physical intelligence)의 단계로 확장되었다.\n이를 피지컬 AI(Physical AI)라고 불리며, 단순한 연산 능력 향상이 아니라 실제 환경과의 실시간 상호작용을 가능하게 하는 새로운 패러다임을 의미한다.\n① 철학적 배경\n피지컬 AI의 이론적 기반은 ” 지능은 환경과의 상호작용을 통해 완성된다 ” 는 체화된 인지(embodied cognition) 개념에 있다.\n이는 지능을 단순한 계산 능력이 아니라, 신체적 경험과 감각적 피드백을 포함한 총체적 능력으로 본다.\n② 실제 구현\n이 개념은 자율주행차, 로봇 조작, 드론 제어, 스마트 IoT 기기 등에서 실현되고 있다.\nAI 알고리즘이 센서 및 엑추에이터와 결합함으로써, 기계는 외부 자극을 인식하고 판단하며 행동하는 실시간 자율 시스템으로 발전한다.\n③ 학문적 의미\n단순 기술 확장이 아니라, AI가 스스로 감지, 학습, 행동하는 실체적 지능을 실현하려는 연구 축으로 이해할 수 있다.\n하드웨어적 발전은 단순한 기술적 진보가 아닌, AI의 실체적 존재화(realization)로 이해된다.\n즉, AI가 데이터로만 존재하던 비물질적 지능에서 벗어나, 감지(sensing)–학습(learning)–행동(acting)의 순환 구조를 스스로 수행하는 단계로 진입한 것이다.\n따라서 하드웨어적 진화는 소프트웨어 중심 AI의 한계를 보완하며, 현실 세계 속에서 자율적 판단과 행동 능력을 구현하는 핵심 축으로 평가된다. 이는 향후 지능의 통합적 구현(integrated intelligence)을 실현하기 위한 필수적 토대로 자리 잡고 있다.\n02 CoT 통합 구조 Chain-of-Thought Integrated Architecture\n대규모 언어 모델(LLM, Large Language Model)은 최근 몇 년간 두 가지 방향으로 발전해왔다.\n① 하나는 언어 생성 능력(Language Generation)에 초점을 둔 학습 중심형 모델(Learning-Oriented Model)\n② 다른 하나는 논리적 사고(Logical Reasoning)를 내재화한 추론 중심형 모델(Reasoning-Oriented Model)이다.\n이 구분은 모델의 구조적 형태보다는, 모델이 최적화하는 목적 함수(Objective Function)와 훈련 패러다임(Training Paradigm)의 차이에 의해 정의된다.\n1 . 학습 중심형 모델 Learning-Oriented Model\nGPT-3에서 GPT-4.5로 이어지는 계열은 지도 학습(SL)과 자기회귀 언어 모델링(Autoregressive Language Modeling)을 기반으로 한 대표적 학습형 구조이다.\n이 모델들은 입력된 문맥(Context)에 대해 다음 단어의 조건부 확률을 최대화하도록 학습된다.\n\\[P(w_t | w_1, w_2, \\ldots, w_{t-1}; \\theta)\\]\n(:) 모델의 파라미터 손실 함수: 일반적으로 음의 로그 가능도(Negative Log-Likelihood)로 정의된다. \\[\\mathcal{L}(\\theta) = -\\sum_{t=1}^{T} \\log P(w_t | w_{&lt;t}; \\theta)\\]\n이 접근은 모델이 방대한 언어 데이터를 통계적으로 모사하며, 언어의 문맥적 패턴을 효율적으로 학습하게 만든다.\nGPT-4.5 이하의 모델들은 주로 텍스트 생성(Text Completion), 요약(Summarization), 번역(Translation) 등 언어적 유창성이 필요한 과제에 최적화되어 있다.\n그러나 이러한 모델들은 논리적 추론(logical reasoning)과 같은 고차적 사고를 명시적으로 수행하지 못한다는 한계를 지닌다.\n2 . 추론 중심형 모델 Reasoning-Oriented Model\n이 한계를 보완하기 위해 OpenAI는 2024년 이후 O 시리즈(O1, O1-mini, O3 등)를 개발하였다.\n이들 모델은 단순한 언어 예측이 아닌, 사고 과정(CoT)을 내재화하여 논리적·수학적 추론을 수행하도록 설계된 구조이다.\n즉, 결과를 곧바로 산출하는 대신, 모델 내부에서 일련의 사고 단계를 거쳐 중간 논리 과정(intermediate reasoning steps)을 생성한 후 최종 응답을 도출한다.\n이를 수식적으로 표현하면 다음과 같다.\n\\[\\text{Answer} = f_\\theta(\\text{Prompt}) = g_\\theta(\\text{Chain of Thought Steps})\\]\n(g_:)​ 모델 내부의 사고 전개 과정.\n즉, 모델은 단순히 단어를 예측하는 확률기계가 아니라, 내재적 추론 구조를 가진 사고 시스템으로 진화한 것이다.\n이러한 추론 능력은 별도의 규칙 기반이 아닌, 사전학습(Pretraining)과 인간 피드백 강화학습(RLHF)을 통해 점진적으로 강화된다.\n결과적으로 O 시리즈는 언어의 표현 능력보다 사고의 정확성과 합리성을 우선시하는 추론 중심형(reasoning-oriented) 모델로 분류된다.\n3 . GPT-5: CoT 기반 통합형 모델 Integrative Model\n2025년 발표된 GPT-5는 기존 GPT 계열의 학습 중심형 구조와 O 시리즈의 추론 중심형 구조를 통합한 CoT 기반 하이브리드 LLM 아키텍처로 정의된다.\nGPT-5는 입력의 복잡도에 따라 자동으로 두 가지 처리 모드 중 하나를 선택한다.\nFast Mode: 단순 질의에 대한 신속 응답 Deliberative Mode: 복합 문제에 대한 단계적 사고(CoT 기반)로 자동 전환하는 이중 처리 구조를 갖는다. 이 과정을 수식으로 나타내면 다음과 같다.\n\\[\\text{Output} = \\begin{cases} f_\\theta(x) & \\text{if } x \\in \\text{simple query} \\\\ f_\\theta(g_\\theta(x)) & \\text{if } x \\in \\text{complex reasoning task} \\end{cases}\\]\n(g_(x):) CoT 기반 내부 사고 전개 과정. 이를 통해 GPT-5는 단순 질의응답(Q&A)에서는 빠른 응답을 제공하고, 복합적인 문제(계획 수립, 코드 분석, 수학적 추론 등)에서는 단계적 사고 절차를 자동으로 활성화한다.\n4 . 이론적 통합 관점 학술적 관점에서 보면 GPT-5의 구조적 진화를 요약하면 다음과 같다.\n\\[\\text{L-O GPT (≤4.5)} + \\text{R-O O-Series} \\Rightarrow \\text{GPT-5 (Integrated CoT Model)}\\]\n즉, GPT-5는 단순히 매개변수 규모가 확장된 모델이 아니라, 학습 중심적 언어 처리와 추론 중심적 사고 구조를 결합하여 인공지능의 인식 능력(perception)과 사고 능력(reasoning)을 동시에 고도화한 모델로 정의된다.\nLLM 발전 모델 비교 구분 모델 계열 중심 개념 주요 기능 학습 중심형 GPT-3 ~ GPT-4.5 언어 패턴 학습 및 문맥 예측 언어 생성, 번역, 요약 등 추론 중심형 O1 ~ O3 논리적 사고 및 CoT 전개 논리·수학적 문제 해결 통합형 GPT-5 언어 생성 + 사고 통합 자동 모드 전환, 고차원 추론 수행"
  },
  {
    "objectID": "ai/ml/ml_14.html",
    "href": "ai/ml/ml_14.html",
    "title": "인공신경망",
    "section": "",
    "text": "Reporting Date: November. 12, 2025"
  },
  {
    "objectID": "ai/ml/ml_14.html#문제점",
    "href": "ai/ml/ml_14.html#문제점",
    "title": "인공신경망",
    "section": "2.1 문제점",
    "text": "2.1 문제점\n하나의 가중치만을 따로 떼어 분석하면 전체 네트워크의 상호의존적 구조를 반영하지 못한다는 점이 역전파의 핵심적 난점 중 하나이다.\n\n신경망은 공동 기여 시스템이다\n\n신경망의 출력은 단일 가중치의 효과가 아니라, 모든 입력 ( x_i )와 가중치 ( w_i ), 그리고 여러 층을 거친 비선형 합성 함수의 결과이다. 즉, 각 가중치는 다른 가중치들과의 조합을 통해서만 의미 있는 출력을 만들어낸다.\n이 때문에 하나의 가중치만 고립적으로 평가하면, 다른 가중치들과의 상호작용(interaction)을 무시하게 되어 실제 영향도를 정확히 알 수 없다.\n\n역전파는 ’부분 기여’를 계산하는 과정이다\n\n역전파 알고리즘의 목적은 각 가중치가 전체 손실(Loss)에 얼마나 기여했는가를 계산하는 것이다. 이때 체인룰을 이용해, 출력층에서 발생한 오차가 어떻게 각 가중치 방향으로 퍼져나가는지를 추적한다.\n즉, \\(\\frac{\\partial L}{\\partial w_i}\\) 는 “모든 다른 파라미터들이 고정되어 있을 때, ( w_i )를 미세하게 변화시켰을 때 손실이 얼마나 변하는가”를 의미한다.\n다시 말해, 하나의 가중치 변화가 전체 결과에 미치는 ‘국소적 기여도(local contribution)’를 구하는 것이며, 이는 전체 맥락에서의 상호작용을 미분의 형태로 부분적으로 포착한 것입니다.\n\n그러나 ’전체적 상호작용’은 여전히 남는다\n\n역전파는 개별 가중치의 기울기를 구하되, 그 계산 과정에 이미 모든 다른 가중치와 뉴런의 값이 포함됩니다. 즉, 수학적으로는 고립된 것이 아니라, 계산 그래프(computational graph) 전체를 거쳐 영향을 받아 나온 결과입니다. 그럼에도 불구하고 다음과 같은 한계가 존재합니다.\n\n비선형성(Nonlinearity) 때문에, 다른 가중치가 조금만 달라져도 각 기울기의 상대적 영향이 크게 바뀐다.\n따라서 한 시점의 기울기만으로는 “전체 네트워크에서의 근본적 관계”를 완전히 파악할 수 없다.\n이로 인해 실제 학습에서는 “한 번의 역전파 결과”보다 “다수의 반복(iteration)을 통한 평균적 수렴”이 중요하다. \n\n결론적으로 하나의 가중치를 따로 본다면 신경망의 다차원 상호작용을 제대로 볼 수 없다. 그러나 역전파는 바로 그 문제를 부분 미분의 누적 형태로 해결합니다.\n즉, 각 가중치의 기울기를 “다른 파라미터가 고정된 상태에서의 국소적 영향”으로 계산하고, 이를 전체 그래프를 따라 합성함으로써, 전체 시스템이 함께 조정되도록 하는 것입니다.\n결국, 하나의 가중치는 단독으로는 의미가 없고, 오직 “네트워크 전체에서의 미분적 상호작용” 속에서만 의미를 가집니다.\n이 점이 신경망이 선형 모델과 본질적으로 다른 이유이며, 딥러닝 학습이 단순한 회귀(regression)가 아닌 비선형적 협조 최적화(non-linear cooperative optimization)라는 점을 보여준다."
  },
  {
    "objectID": "ai/ml/ml_14.html#해결-방안",
    "href": "ai/ml/ml_14.html#해결-방안",
    "title": "인공신경망",
    "section": "2.2 해결 방안",
    "text": "2.2 해결 방안\n역전파 + 경사하강법 작동원리\n\n역전파의 순서적 기울기 계산\n\n출력층에서 손실이 계산된 후, 그 오차를 뒤로 거슬러 올라가며(backward) 각 층의 가중치가 손실에 어떤 영향을 미치는지를 따져봅니다.\n즉,\n\n마지막 층부터 오차의 영향을 계산하고,\n그 결과를 이전 층의 가중치로 전달하며,\n각 가중치의 변화 방향(∂L/∂w)을 구합니다.\n\n이게 바로 chain rule을 층별로 적용하는 과정입니다.\n\n경사하강법(Gradient Descent)\n\n각 가중치의 기울기(∂L/∂w)는 “이 방향으로 가면 손실이 증가한다”를 의미합니다. 따라서 반대 방향(−∂L/∂w)으로 이동하면 손실이 감소합니다. 이를 수식으로 표현하면: [ w_{new} = w_{old} - ] 여기서 ()는 학습률(learning rate)로, 이동 속도를 조절합니다.\n즉, 각 가중치는 “에러가 줄어드는 방향으로” 조금씩 움직이며, 이 과정이 전체 네트워크에서 동시에 일어납니다.\n\n순차적 파라미터 갱신\n\n하나의 가중치 ( w_i )를 업데이트할 때는, 그 시점의 다른 파라미터를 임시로 고정한 상태로 취급합니다. 즉, 한 번의 역전파에서는 모든 가중치를 동시에 업데이트하되, 각각은 “현재 다른 값들이 고정되어 있다”는 전제 하에서 계산된 기울기에 따라 움직입니다.\n이 점에서 동시적이면서도 독립적인 최적화 단위가 형성됩니다. 그 후, 전체 네트워크의 파라미터가 한 번에 갱신되며 다음 반복(iteration)으로 넘어갑니다."
  },
  {
    "objectID": "ai/ml/ml_14.html#예제-시그모이드의-연산-흐름-구조",
    "href": "ai/ml/ml_14.html#예제-시그모이드의-연산-흐름-구조",
    "title": "인공신경망",
    "section": "2.3. 예제: 시그모이드의 연산 흐름 구조",
    "text": "2.3. 예제: 시그모이드의 연산 흐름 구조\n시그모이드 함수 \\[\ng(z) = \\frac{1}{1 + e^{-z}}\n\\]\n를 계산 그래프로 풀어쓰면, 다음과 같은 일련의 연산 노드로 표현됩니다.\n\\[\nz → ( * -1 ) → exp → ( +1 ) → ( 1/x ) → g\n\\]\n즉,\n\n입력값 ( z )에\n음수를 곱하고( * -1 ),\n지수함수를 취하고( exp ),\n1을 더하고( +1 ),\n역수를 취하면( 1/x ) 결과적으로 시그모이드 출력 ( g )가 나옵니다.\n\n이처럼 단순한 하나의 수식도 여러 개의 연산 노드로 분해되어, 각 노드에서 미분(기울기)이 계산되고 역전파될 수 있도록 구성됩니다.\n\n복잡한 신경망 = 수식의 확장적 노드화\n\n신경망이 복잡해진다는 것은 결국 “이러한 단순한 연산 노드들이 수천, 수만 개로 연결되어 합성된 형태”를 의미합니다.\n즉,\n\n층이 깊을수록 합성 함수의 깊이가 늘어나고,\n뉴런 수가 많을수록 병렬적인 연산 노드가 많아지며,\n전체 네트워크는 거대한 계산 그래프(computational graph)로 확장된다.\n\n이 그래프 상에서 역전파는 체인룰을 적용하여, 출력 노드에서 입력 노드로 기울기를 노드 단위로 전파(backpropagate) 한다.\n\n직관적으로 보면 신경망의 “복잡성”은 사실 수학적 표현의 압축 정도입니다.\n\n즉,\n\n수식으로는 간단히 적힌 ( g(f(h(x))) ) 같은 표현이,\n실제 계산 단계에서는 수십 개의 노드로 세분화되어 구현됩니다.\n\n따라서, 복잡해 보이는 신경망도 결국 “단순한 기본 연산들의 반복적 조합”이며, 그 조합을 노드 그래프 형태로 펼쳐놓은 것이 바로 신경망 구조입니다.\n\n예 — hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) 같은 표현을 TensorFlow 코드·계산 그래프·TensorBoard 이미지로 웹에서 쉽게 찾을 수 있습니다.\n권장 이미지 유형 코드 스니펫과 간단 다이어그램(활성화 함수 흐름). TensorBoard로 그린 연산 그래프(MatMul → Add(b) → Sigmoid). 계산 그래프의 노드(곱셈·덧셈·비선형 연산) 시각화 예시."
  },
  {
    "objectID": "ai/ml/ml_14.html#해결-방법2",
    "href": "ai/ml/ml_14.html#해결-방법2",
    "title": "인공신경망",
    "section": "2.4 해결 방법2",
    "text": "2.4 해결 방법2\n\n순전파(Forward Propagation)\n\n먼저 신경망은 입력 데이터를 받아 층을 따라 순방향으로 연산을 수행합니다.\n즉, \\[\nx \\rightarrow (W_1, b_1) \\rightarrow L_1 \\rightarrow (W_2, b_2) \\rightarrow L_2 \\rightarrow \\cdots \\rightarrow \\hat{y}\n\\]\n\n\\(\\hat{y}\\): 예측값(hypothesis)\n\n이 단계에서는 단지 “현재 가중치로 계산된 결과”를 내는 것뿐입니다.\n\n역전파(Backpropagation)\n\n출력값이 실제 정답 (y)와 다르면 손실 함수 (L(y, ))가 커집니다. 이때 오차를 역방향으로 전파하여 각 가중치 (w)가 오차에 미친 영향을 계산하고, 그에 따라 가중치를 오차가 줄어드는 방향으로 미세하게 수정합니다.\n즉, \\[\nw := w - \\eta \\frac{\\partial L}{\\partial w}\n\\]\n이 과정을 데이터셋의 각 샘플(혹은 배치)에 대해 반복하면서 학습이 진행됩니다.\n\n대규모 데이터가 필요한 이유\n\n오류를 줄이기 위해서는 다양한 입력 상황을 학습해야 합니다. 데이터가 많을수록 네트워크는\n\n특정 패턴에 과적합되지 않고,\n전체 분포의 일반적 경향을 학습할 수 있습니다.\n\n즉, 대규모 데이터는 가중치가 균형 있게 조정되도록 도와주며, 이는 결국 일반화 능력(generalization)을 향상시킵니다.\n\n가중치 변화 폭의 수렴\n\n학습이 진행될수록\n\n손실이 줄어들고,\n기울기(gradient)의 크기도 점점 작아집니다.\n\n이로 인해 가중치의 변동 폭은 점점 감소하며, 결국 오차가 거의 변하지 않는 수렴 상태에 도달합니다.\n이때 각 가중치의 분포는 무작위 초기화 상태에서 점차 안정된 정규분포 형태로 가까워집니다. 이는 확률론적으로 “많은 데이터 샘플에 의해 평균적으로 조정된 결과”이기 때문입니다.\n\n테스트 데이터로 검증\n\n훈련(Training) 과정에서 사용되지 않은 테스트 데이터(Test set)를 이용해 학습된 모델이 새로운 데이터에서도 잘 작동하는지 검증합니다.\n이 과정을 통해 정확도(accuracy), 손실(loss), 과적합 여부(overfitting) 등을 평가할 수 있습니다."
  },
  {
    "objectID": "ai/ml/ml_16.html",
    "href": "ai/ml/ml_16.html",
    "title": "인공신경망",
    "section": "",
    "text": "딥러닝 모델의 학습 과정은 고차원 매개변수 공간에서의 비선형 최적화 문제로 구성되며, 네트워크는 일반적으로 ’Affine → Batch Normalization → ReLU’의 층 구조를 반복적으로 적용하고 마지막 단계에서 Affine 변환 후 Softmax를 통해 확률적 출력 분포를 생성한다. 이 구조는 표현 학습 과정에서 분산 안정화, 비선형성 확보, 기울기 흐름 유지라는 세 가지 관점에서 수학적으로 정당화된다.\n학습 과정의 이론적 핵심 중 하나는 바이어스-분산 트레이드오프이다. 바이어스는 모델의 구조적 단순화로 인해 발생하는 체계적 오차이며, 분산은 데이터의 작은 변화에 모델이 과도하게 반응하며 발생하는 민감도이다. 이 두 요소는 역비례적 관계에 있어, 바이어스를 지나치게 줄이기 위해 모델 복잡성을 높이면 분산이 급격히 증가하여 일반화 성능이 악화된다(오버피팅). 반대로 분산을 줄이기 위해 모델을 지나치게 단순화하면 고정적 편향이 커져 언더피팅이 발생한다. 적절한 최적점은 검증 데이터의 경험적 리스크 최소 지점을 찾는 방식으로 설정된다.\n오버피팅이 발생하는 상황에서는 모델이 훈련 데이터의 세부적 노이즈 패턴까지 과도하게 학습하는데, 이는 파라미터 수가 데이터에 비해 과도하게 많거나, 데이터 분포의 다양성이 충분치 않을 때 더욱 심화된다. 이를 완화하기 위한 실질적 접근으로는 상위 다중 정답(top-k) 평가 지표 사용, 훈련 데이터의 증강, 범주별 데이터 균형화, 정규화(regularization) 등이 있다.\n정규화 중에서도 L2 정규화(가중치 감소)는 기하학적 관점에서 파라미터 공간을 L2 볼 안에 구속하는 효과를 갖는다. 손실 함수에 라그랑주 항 형태로 λ‖W‖²가 추가되며, 이는 고차원 공간에서 가중치 벡터의 크기를 제어해 최적점이 과도하게 한 방향으로 치우치지 않도록 한다. 수식적으로는 단순히 항을 더하는 것으로 보일 수 있으나, 실제로는 매개변수 공간의 형태를 바꾸어(implicit geometry alteration) 최적화 경로와 수렴 특성을 근본적으로 변화시키는 역할을 한다.\n언더피팅을 해결하기 위해서는 특징(feature) 확장, 비선형성이 높은 모델(KNN, SVM, Decision Tree 등)의 활용, 그리고 모델 용량 증가가 효과적이다. 본질적으로 언더피팅은 함수 공간 자체의 표현력이 낮을 때 발생한다.\n데이터셋의 규모 또한 모델 성능에 강한 영향을 준다. 데이터가 적으면 분산이 높아지고 신뢰도 낮은 추정이 발생한다. 이를 보완하기 위해 전통적 데이터 증강뿐 아니라, 최근에는 생성형 모델을 통한 고차원적 데이터 증강이 활용된다. 이는 단순 변환 기반 증강보다 데이터 다양성을 보다 풍부하게 확보할 수 있다는 점에서 중요한 의미를 가진다.\n드롭아웃(dropout)은 완전 연결 계층(fully connected layer)에서 특정 뉴런을 확률적으로 비활성화하여 특정 패턴에 대한 파라미터 공동 의존성을 줄이는 방식이다. 이는 모델이 특정 경로에 과도하게 적응하는 것을 방지해 일반화 오차를 개선하며, 수학적으로는 앙상블 평균 효과를 근사하는 것으로 해석된다. 단, 드롭아웃은 학습 시에만 적용되고, 추론 단계에서는 비활성화된 뉴런을 포함한 전체 네트워크를 사용한다.\n또한, 교차 검증(cross validation)은 모델의 일반화 오류를 안정적으로 추정하기 위해 데이터셋을 K개의 폴드(fold)로 분할하여 반복적으로 평가하는 방법이다. 이는 단일 분할로 인한 편향을 제거하고 모델의 평균적 성능을 측정하는 데 필수적이다.\n종합적으로, 이러한 최적화 기법·정규화 전략·데이터 관리 기법들은 딥러닝에서 안정적인 학습과 일반화 성능 향상의 핵심 요소이며, 실험 환경에서 모델의 성능을 체계적으로 평가하기 위한 기본적 도구들이다.\n\n모델링의 목적과 목표 변수의 특성은 평가 지표 선택의 이론적·실무적 정당성을 결정하는 핵심 요소이다. 각 지표는 통계적 특성, 손실 함수의 기하학적 구조, 모델의 오차 감도 등을 반영하고 있으므로, 단순 비교가 아니라 ’데이터 생성 과정(DSG: Data-Generating Process)’과 ’목표 함수의 성질’을 고려해 해석해야 한다.\n\n예측·회귀 모델의 성능 평가 회귀 문제에서 사용되는 지표들은 대부분 오차의 분포적 성질과 민감도를 달리한다.\n\nMSE(Mean Squared Error): 잔차의 제곱을 평균한 값으로, 큰 오차에 대해 제곱 패널티를 부여함으로써 이상치(outlier)에 매우 민감하다. 이는 L2-리스크 최소화와 연결되며, 확률적 관점에서 정규분포 오차 가정에 대한 최대우도추정(MLE)과 동등하다.\nRMSE(Root MSE): MSE의 제곱근으로 단위를 원래 스케일로 복구한다. 기하학적으로는 L2-거리 기반 손실이며, 곡률이 크기 때문에 경사 기반 최적화에서 강한 페널티를 제공한다.\nMAPE(Mean Absolute Percentage Error): 상대적 오차 비율을 측정하므로 스케일이 다른 시계열·수요 예측 등에서 많이 사용된다. 다만 목표값이 0에 근접하면 불안정해진다는 결함이 있다.\nWeighted Quantile Loss(평균 가중 분위수 손실): 분위수 회귀(quantile regression)의 손실로, 비대칭 오차 구조(과소·과대 예측 비용이 다름)를 모델링할 때 적합하다. 수치 예측에서 리스크 기반 의사결정(예: 보험·수요 예측)에 널리 사용된다.\nWASE(Weighted Absolute Scaled Error): 시계열 예측 평가에서 사용되며, 단순 차분 기반 성능 대비 상대적 향상을 평가할 수 있다.\n\n분류 모델의 성능 평가 분류 지표는 클래스 불균형, 임계값(threshold), 비용 민감도에 따라 해석이 크게 달라진다.\n\nAccuracy(정확도): 전체 예측 중 맞춘 비율이지만, 클래스 불균형이 존재할 경우 의미가 급격히 저하된다.\nPrecision(정밀도 = TP/(TP+FP)): 양성 예측 중 실제로 양성인 비율로, FP에 대한 비용이 큰 문제에서 핵심 지표가 된다.\nRecall(재현율 = TP/(TP+FN)): 실제 양성 중 모델이 탐지한 비율로, FN 비용이 큰 의료·보안 도메인에서 매우 중요하다.\nConfusion Matrix(오차 행렬): 클래스 간 오차 구조를 직접 파악할 수 있으며, 이후 ROC·PR 커브 분석의 기초가 된다.\nF1 Score: 정밀도와 재현율의 조화평균으로, 두 지표 간 균형이 중요할 때 사용된다. 특히 불균형 데이터에서 모델 선택에 핵심 지표가 될 수 있다. 분류 문제에서는 단순 지표 나열보다, “어떤 오류가 비용 측면에서 가장 치명적인가?”를 우선적으로 판단해야 한다.\n\n객체 탐지 모델의 성능 평가 객체 탐지 문제는 공간적 위치(Localization)와 클래스 분류(Classification)가 동시에 존재하는 복합 구조다.\n\nIoU(Intersection over Union): 예측 박스와 실제 박스의 겹침 비율을 나타내는 핵심 지표로, 공간적 정합도(spatial alignment)를 정량화한다. 특정 IoU 임계값(예: 0.5, 0.75 등)에 따라 AP(Average Precision)을 계산하며, 이는 COCO·Pascal VOC 등 주요 벤치마크의 기본 평가 기준이다.\n\n\n종합하면, 성능 평가는 단순히 지표를 선택하는 과정이 아니라 문제의 목적 함수, 오차 비용 구조, 데이터 통계적 특성을 기반으로 이루어져야 하며, 동일 모델이라도 지표 선택에 따라 결론이 달라질 수 있음을 항상 고려해야 한다.\n\n클러스터링은 분류의 한 종류이다. 이것은 비지도 학습이다.\n이것의 예시는 회원관리서비스에서 주로 쓰인다. 고객을 관리하고 분류해서 등급을 나누고 그에 걸맞는 서비스를 주는 것이다.\n이를 전문용어로 CRM이라 한다.\n분류 &gt; 세그멘테이션 &gt; 클러스터링으로 좁아지는 형태\n그런 그룹을 많이 만들어서 디테일한 서비스를 제공한다면  이는 좋을 수도 있지만 비용적인 측면이 높을 수 있으니 경영적인 측면에선 덜 디테일한 것이 오히려 실무진 입장에선 좋을 수 있음.\n이때 그룹잉의 기준 중 하나인 거리기반 그룹잉이 있다.\n하이-인트라-클레스 유사도\n로우-인트라-클레스 유사도\n둘 중 하나로 데이터 세트를 클래스 구성한다. 이를 통해 클래스 레이블과 클래스 개수를 찾아낸다.\n클러스터링 알고리즘 수행 시 선호되는 속성들\n\n시간과 공간적 관점에서의 규모\n서로 다른 타입(유형)의 데이터를 다룰 수 있는 능력\n입력 파라미터들을 결정하기 위한 최소한의 도메인 지식\n잡음과 이상치를 다룰 수 있는지 여부, 또한 판별도 해야 함.\n입력 레코드의 순서에 민감하지 않아야 함, 최근 과거 상관없이 섞어서 골고루.\n사용자 관점의 제약 조건 반영 여부, 코로나 시기 등.\n결과에 대한 해석능력과 사용성, \n\n거리를 계산할 때, 가장 근접 계산 방법과 가장 멀리 떨어진 계산 방법의 차이는 전향적 계산과 보수적 계산으로 나뉘는 것이다. 이것의 중간인 평균 거리 계산도 있다. 워드 링크잉은 합병괸 클러스터들의 분산을 최소화하려고 한다는 것.\n거리 측정 방법의 종류\n\n유클리드 거리(가장 많이 쓰는 방법, 대각선 즉, 직선 거리) 이것은 항공 네비에서 쓰일 수 있다. 사이킷런에서 파라미터로 p=1로 줄 수 있다.\n맨하튼 거리, 바로 대각선으로 가로지르지 않고 가로 세로 몇 블록인지 가는 방법. 이는 보통 자동차 네비게이션에서 쓰인다. 사잇클런에서 파라미터로 p=2로 줄 수 있다.\n최대 놈, 점들의 분포를 고려한 거리 공분산을 고려한 것. 중심이 아니라 데이터의 분포가 곧 가중치가 된다.\n마할라노비스 거리\n코사인 거리, 문서간 유사도 자카드 계수(희박한 데이터에 유용), 편집거리(검색창 등)도 있다. 피어슨 상관계수. “문서”든 “영화”든 결국 벡터로 표현할 수 있으며, 유사도는 벡터 간 거리로 계산되므로 문서 유사도 방식 그대로 영화 등 다양한 비정형 데이터 추천·검색·분류에 활용된다.\n편집거리의 연산들: 헤밍, 레벤슈타인 거리 등\n\n알고리즘릭하게 하거나 학습을 시켜서 하거나 둘 중 하나일 것이다.\n클러스터링 유형\n\n중심 기반(가장 자주 사용하는 방법, 쉬운 개산, 개수 정하기 힘듦)\n중간점 기반\n밀도 기반\n계층적 기반\n\n\n\n03 벡터 변환\n영화의 장르·감독·가격처럼 서로 다른 속성(이질적 특징)이 문서 간 유사도에 사용될 때는, 모든 속성을 같은 공간에서 비교 가능하도록 변환한 뒤 벡터 형태로 통합한다.\n핵심은 표현 통일 → 벡터화 → 가중 결합 → 유사도 계산이다.\n\n서로 다른 개념을 “벡터 공간”으로 통일\n\n문서는 텍스트가 아니어도 모든 특징을 숫자 벡터(feature vector)로 바꿀 수 있다.\n범주형(장르, 감독)\n\nOne-hot encoding, Multi-hot encoding, Embedding을 사용\n예: 장르 = {액션, 드라마, SF}\n\n한 영화가 액션·SF라면 → [1, 0, 1]\n\n감독도 동일한 방식 → 감독 수가 많으면 embedding 사용\n\n연속형(가격, 평점, 상영시간)\n\n그대로 사용하되 정규화(normalization) 적용 예: 가격을 0~1 사이로 스케일링\n\n\n모든 특징을 하나의 대형 벡터로 결합\n\n예시:\n\n\n\n특징\n표현\n\n\n\n\n장르\n[1, 0, 1, 0, 0]\n\n\n감독\n[0, 0, 1, 0, 0, …]\n\n\n가격\n0.42\n\n\n평점\n0.88\n\n\n\n→ 최종 문서 벡터: [1, 0, 1, 0, 0, | 0, 0, 1, 0, 0, … | 0.42 | 0.88]\n이렇게 하면 “완전히 다른 개념”도 하나의 벡터 안에서 함께 존재할 수 있음.\n\n\n3. 속성별 영향력(중요도)을 조절\n속성이 본질적으로 다르기 때문에 가중치(weight)를 준다.\n예:\n\n줄거리 텍스트: 50%\n장르: 30%\n감독: 10%\n가격: 10%\n\n이를 반영하면,\n\n감독이 다르다고 해서 문서 유사도가 0이 되는 것을 방지\n가격 같은 숫자형 특징이 과도하게 영향 주는 것도 방지\n\n\n결합된 벡터에 코사인 유사도를 적용\n\n최종적으로는 두 영화 벡터의 코사인 유사도를 계산:\n[ (A, B) = ]\n이렇게 하면\n\n장르가 비슷하면 해당 부분 벡터가 기여\n가격·감독이 비슷하면 그 부분이 기여\n전체적으로 종합된 유사도가 나옴\n\n\n핵심 정리\n\n\n서로 다른 개념이어도 → 모두 동일한 벡터 공간의 차원으로 변환\n각 속성을 수치화한 뒤 하나의 벡터로 결합\n필요하면 가중치를 부여\n마지막에는 코사인 유사도로 비교\n\n\n핵심은 “서로 다른 정보들을 모두 숫자 벡터로 만들어 하나의 ’영화 프로필’로 통합하고, 그 프로필끼리 유사도를 비교한다”는 점입니다. 이게 가능한 이유는 모든 종류의 데이터(장르·감독·가격·텍스트 등)는 결국 수치화할 수 있기 때문입니다.\n\n핵심 1: 모든 속성을 한 줄짜리 숫자 벡터로 만든다\n\n영화마다 정리된 요약 정보(프로필)를 하나의 벡터로 만든다고 생각하면 됩니다.\n예:\n\n“액션·SF” → [1, 0, 1]\n“감독 A” → [0, 1, 0, 0]\n“가격 9,000원” → 0.45\n“평점 8.3” → 0.83\n\n→ 결국 하나의 벡터: [1, 0, 1, 0, 1, 0, 0, 0.45, 0.83]\n\n핵심 2: 두 영화 벡터의 거리를 비교하면 ’비슷한 영화’가 된다\n\n벡터 A와 벡터 B가 비슷하면 → 영화도 비슷하다고 판단. 다르면 → 유사도가 낮다. 즉, 문서(영화) 간 유사도 = 벡터 간 유사도 이 원리 하나로 모든 속성이 비교 가능해진다.\n\n핵심 3: 활용되는 곳 (실제 사례)\n\n추천 시스템\n\n사용자가 본 영화와 비슷한 벡터를 가진 영화를 찾아 추천\n장르·감독·가격·평점 모두 반영된 추천 가능\n\n콘텐츠 검색\n\n“액션이면서 SF 느낌 나는 영화” → 장르 벡터와 유사한 영화 자동 추출\n\n마케팅 타겟팅\n\n특정 가격대 + 특정 감독 선호 + 특정 장르 조합을 가진 영화군 자동 분류\n\n클러스터링(군집)\n\n영화 전체를 벡터화 → 비슷한 영화끼리 자동 그룹화\n가격/감독/장르 차이까지 포함해 군집 생성 가능\n\n\n문서 유사도와 “영화 유사도(메타데이터 기반)”가 완전히 같은 원리로 작동한다는 점이 핵심입니다. 즉, 문서를 비교하든 영화를 비교하든 결국 ’특징 벡터 간 거리 비교’라는 한 원리를 공유합니다.\n\n문서 유사도 계산의 본질\n\n문서 유사도는 일반적으로 다음 절차를 따릅니다.\n\n문서를 특징 벡터(feature vector)로 변환\n\n단어를 TF-IDF, BOW, Embedding 등으로 숫자 벡터화\n\n문서 간 코사인 유사도를 계산\n유사한 문서를 찾음\n\n즉, 문서 → 벡터, 문서 간 유사도 = 벡터 간 유사도 이 구조입니다.\n\n영화도 “문서 취급”해서 동일한 구조로 비교\n\n영화는 텍스트가 아니지만, 다음 속성들을 모두 숫자 벡터로 변환할 수 있습니다.\n\n장르\n감독\n배우\n상영시간\n가격\n평점\n줄거리 텍스트(TF-IDF, embedding)\n\n이걸 하나의 벡터로 합치면, 영화 = 문서와 동일한 ’특징 벡터’가 됨 따라서 문서 유사도와 완전히 같은 방식으로 비교할 수 있다.\n\n문서 유사도 = 특징 벡터 유사도\n\n영화도 문서도 결국, “특징 벡터끼리의 거리/유사도 계산”으로 정의됩니다.\n\n\n\n\n\n\n\n\n\n비교 대상\n벡터 만드는 방식\n유사도 계산\n결과\n\n\n\n\n문서\n단어 벡터(TF-IDF 등)\n코사인 유사도\n유사 문서\n\n\n영화\n장르/감독/가격/텍스트 등 통합 벡터\n코사인 유사도\n유사 영화\n\n\n\n둘이 100% 동일한 구조입니다.\n\n활용 방식: 문서 유사도와 완전히 동일\n\n문서 유사도가 다음을 가능하게 하듯:\n\n비슷한 문서 검색\n토픽 클러스터링\n문서 분류\n유사 문서 추천\n\n영화도 같은 방식으로 활용 가능합니다.\n예)\n\n사용자가 본 영화 A → 벡터(A)\n전체 영화 벡터 중 벡터(A)와 가장 가까운 영화들 검색 → 영화 추천 시스템 구현\n\n즉,\n\n문서 추천에서 문서를 “영화”로 바꾸기만 하면 그대로 적용된다."
  },
  {
    "objectID": "cs/bn/bn_01.html",
    "href": "cs/bn/bn_01.html",
    "title": "네트워크 개론",
    "section": "",
    "text": "네트워크 개론에 대해 다루고자 한다.\n01 통신 두 개 이상의 주체 간 정보의 송수신과 이해를 포함하는 상호작용 과정으로 정의된다.\n통신 주체는 인간 또는 기계, IoT 시스템과 같은 장치가 될 수 있으며, 전달되는 정보는 언어, 데이터, 음성, SNS 메시지 등 다양한 형태를 갖는다.\n효율적 통신을 위해서는 일정한 규약과 전송 시점, 데이터 인코딩 방식 등 명확한 방법이 요구되며, 이는 프로토콜의 형태로 구현된다.\n통신 과정은 단순한 정보 전달에 그치지 않고, 수신자가 해당 정보를 흡수하고 의미를 해석하는 과정을 포함한다. 아울러 제한된 자원을 어떻게 효율적으로 활용할 것인가는 통신 설계와 운영에서 핵심적 고려 사항이다.\n1 . 기본요소 통신은 정보를 주고받는 과정에서 다섯 가지 기본 요소로 구성된다. 먼저 송신기는 정보를 생성하고 전송하는 주체로, 사람, 컴퓨터, IoT 장치 등이 될 수 있다.\n메시지는 송신기가 전달하고자 하는 실제 정보로, 언어, 데이터, 음성, 영상 등 다양한 형태를 가진다.\n전송 매체는 메시지가 송신기에서 수신기로 전달되는 물리적 또는 논리적 경로를 의미하며, 유선, 무선, 광섬유 등 다양한 방식이 존재한다.\n수신기는 전달된 메시지를 받아 해석하고 이해하는 주체로, 송신기와 마찬가지로 사람이나 기계가 될 수 있다.\n마지막으로 프로토콜은 송신기와 수신기 간의 통신을 원활하게 수행하기 위해 적용되는 규약이나 약속으로, 메시지 형식, 전송 순서, 오류 처리 방법 등을 정의한다.\n이 다섯 요소는 상호 유기적으로 작용하여 신뢰성 있고 효율적인 통신을 가능하게 한다.\n2 . 유튜브 영상 재생 과정 사용자가 폰에서 영상을 재생하면, 폰은 HTTP나 HTTPS와 같은 프로토콜을 통해 유튜브 서버에 요청 패킷을 전송한다.\n이 과정에서 IP 주소를 기반으로 목적지를 지정하고, TCP 를 통해 전송 신뢰성을 확보하며 데이터가 순서대로 전달되도록 제어한다.\n유튜브 서버는 요청을 수신한 후, 해당 영상 데이터를 폰으로 전송한다. 전송된 영상 데이터는 일반적으로 RTP 와 같은 스트리밍 프로토콜을 통해 재생되며, 폰은 이를 받아 디코딩하고 화면에 출력한다.\n이러한 일련의 과정은 사용자가 요청한 영상이 안정적이고 실시간으로 재생되도록 설계되어 있다.\n3 . 통신의 종류 음성 통신은 사람 간 실시간 대화를 목적으로 하는 통신으로, 전통적인 전화선과 이동통신망을 통해 이루어진다.\n대표적으로 GSM, VoLTE 와 같은 이동통신 기술이 있으며, SIP 과 같은 신호 프로토콜을 통해 통화 연결과 종료, 세션 관리를 수행한다.\n데이터 통신은 문자, 파일, 인터넷 서비스 등 디지털 데이터를 전달하는 통신을 의미한다. Wi-Fi, LTE, 5G 와 같은 네트워크를 통해 TCP/IP 기반의 신뢰성 있는 전송이 이루어지며, IoT 환경에서는 MQTT와 같은 경량 메시지 프로토콜이 사용되기도 한다.\n방송 통신은 라디오, TV, 위성, 케이블과 같이 단방향 전송을 특징으로 하며, 다수의 수신자가 동시에 정보를 받을 수 있도록 설계되어 있다. 사용자는 송신자의 신호를 수신만 할 수 있으며, 피드백은 제한적이다.\n사물 인터넷(IoT) 통신은 다양한 장치와 센서가 네트워크에 연결되어 데이터를 주고받는 통신을 의미한다. Wi-Fi, Zigbee, LoRa와 같은 무선 기술을 사용하며, CoAP, MQTT 등 경량화된 프로토콜을 통해 제한된 자원에서도 효율적인 데이터 전송과 관리가 가능하다.\n4 . 통신의 역사와 발전 과정 고대·중세 초기 벽화, 연기 신호, 문자를 통한 기본적인 정보 전달. 15 ~ 18세기 인쇄술의 발명으로 정보의 대량 복제와 확산 가능. 세마포어 신호기(망루에서 깃발, 팔 신호로 메시지 전달). 19세기 전신기(모스 부호) 발명으로 장거리 신속 통신 실현. 전화기의 등장으로 음성 전달 가능. 무선 통신 기술 개발 시작. 20세기 라디오 방송 보급 → 대중 매체로 자리잡음. 인공위성(스푸트니크, 통신 위성)으로 전 지구적 통신 가능. 현대 인터넷 시대 ARPANET에서 시작 → TCP/IP 프로토콜 확립. WWW(월드 와이드 웹) 등장으로 정보 공유 혁신. 모바일 인터넷 확산으로 휴대 기기 중심 통신 발전. 최신 세대 5G: 초고속, 초저지연, 초연결을 특징으로 하여 IoT, 자율주행, 메타버스 등 새로운 서비스 기반 제공.\n02 인터넷\n1 . 작동 원리 인터넷은 패킷 교환 방식을 기반으로 동작한다.\n전송하고자 하는 데이터는 일정한 크기의 패킷 단위로 분할되며, 각 패킷에는 출발지와 목적지의 주소 정보가 포함된다.\n이 패킷들은 네트워크 상에서 최적의 경로를 따라 독립적으로 전송되고, 목적지에 도착한 후 원래의 데이터로 재조립된다.\n이 과정에서 핵심 역할을 하는 것이 TCP/IP 프로토콜이다.\nTCP 는 데이터의 신뢰성 있는 전송을 보장하고, 패킷이 순서대로 도착하도록 제어한다. IP 는 각 패킷이 정확한 목적지로 전달되도록 주소 지정과 경로 설정을 담당한다.\n또한, 인터넷에서는 사람이 기억하기 쉬운 도메인 이름을 사용한다. 이를 실제 통신에 필요한 IP 주소로 변환하는 과정이 DNS 이며, 사용자가 도메인 이름을 입력하면 DNS 서버가 해당 이름에 대응하는 IP 주소를 찾아 반환한다.\n이 과정을 통해 최종적으로 사용자의 요청이 해당 서버로 전달되고, 서버는 응답 데이터를 다시 패킷 형태로 전송한다.\n03 네트워크 기반 서비스 구조\n1 . 서버-클라이언트 구조 (중앙집중형) 클라이언트(PC, 브라우저, 앱)가 인터넷을 통해 중앙 서버에 요청을 보내고, 서버는 웹 애플리케이션 및 DB를 통해 응답을 제공. 예: AWS, Azure, GCP\n장점:\n데이터와 서비스를 통합적으로 관리 가능 중앙 통제 용이 스케일 업(서버 성능 확장) 또는 스케일 아웃(서버 추가)으로 확장성 확보 단점:\n서버 장애 시 전체 서비스가 중단될 위험 서버 의존도가 높아 운영 비용 및 리스크 집중\n2 . P2P 구조 (분산형) 모든 노드가 동등한 지위를 가지며, 직접 연결을 통해 데이터를 공유하고 처리. 예: IPFS, 토렌트, 블록체인\n장점:\n분산 구조로 인한 고가용성 확보 뛰어난 확장성 중앙 서버 비용 절감 단점:\n보안 관리가 상대적으로 어려움 운영·유지 관리 복잡성 데이터 동기화 지연 발생 가능\n3 . 하이브리드 구조 서버-클라이언트 구조와 P2P 구조를 혼합한 형태.\n특징:\n핵심 데이터나 보안이 중요한 부분은 중앙 서버에서 관리 대용량 파일 전송, 분산 저장, 연산 등은 P2P 네트워크 활용 중앙 집중 관리와 분산 구조의 장점을 모두 살리면서 단점을 보완 가능\n04 네트워크 기본 개념\n1 . 기본 구성 요소 노드(Node): 네트워크에 연결된 모든 장치(PC, 서버, 라우터, 스마트폰 등). 링크(Link): 노드 간 데이터를 전달하는 경로(유선 케이블, 무선 전파 등). 대역폭(Bandwidth): 네트워크가 단위 시간당 전송할 수 있는 데이터의 최대량, 즉 네트워크의 ‘용량’.\n2 . 네트워크 주소 체계 IP 주소: 네트워크에서 장치를 구분하는 논리적 주소 (IPv4, IPv6). MAC 주소: 네트워크 인터페이스 카드(NIC)에 부여된 물리적 주소, 전 세계적으로 유일. DNS: 사람이 이해하기 쉬운 도메인 이름을 IP 주소로 변환해주는 시스템.\n3 . 통신 방향 분류 (비용 증가 순) 단방향(Simplex): 한쪽 방향으로만 데이터 전송 가능 (예: 라디오, TV). 반이중(Half Duplex): 양방향 통신 가능하지만 동시에 불가능 (예: 무전기). 전이중(Full Duplex): 양방향 동시 통신 가능 (예: 전화, 현대 네트워크).\n4 . 거리 기준 네트워크 분류 PAN: 개인 영역 네트워크 (블루투스, 개인 기기 간 연결). LAN: 근거리 네트워크 (가정, 사무실). MAN: 도시 규모 네트워크. WAN: 광역 네트워크, 인터넷 포함.\n5 . 계층화 개념 네트워크는 복잡성을 줄이고 유지보수를 용이하게 하기 위해 계층적으로 설계됨. 각 계층은 독립적으로 동작하며, 모듈화된 구조로 상호 의존성을 최소화.\n비유: 비행기 서비스 과정\n매표소 계층 → 티켓 발권 수화물 계층 → 짐 위탁 게이트 계층 → 탑승 절차 활주로/비행기 계층 → 실제 이동 수행\nOSI 7계층 모델은 네트워크 통신을 7개의 층으로 나누어 각각의 역할과 기능을 정의한 개념 모델이다.\n1 . 물리층 Physical Layer\n역할: 실제 데이터 전송 매체를 통해 0과 1의 비트 신호를 전기적·광학적 신호로 변환하여 전송. 주요 기능: 케이블, 허브, 리피터 등 하드웨어 장치와 비트 전송. 예시: UTP 케이블, 광섬유, RJ-45 커넥터, 전송 속도(100Mbps, 1Gbps 등).\n2 . 데이터 링크층 Data Link Layer\n역할: 물리적 주소(MAC 주소)를 기반으로 신뢰성 있는 프레임 전송. 주요 기능: 에러 검출(CRC), 흐름 제어, 프레임화, 스위치 동작. 예시: Ethernet, Wi-Fi, 스위치, 브리지, MAC 주소.\n3 . 네트워크층 Network Layer\n역할: 서로 다른 네트워크 간의 데이터 전달 및 경로 선택(Routing). 주요 기능: 논리 주소(IP), 라우팅, 패킷 분할/조립. 예시: IP, ICMP, 라우터, 서브넷.\n4 . 전송층 Transport Layer\n역할: 종단 간(end-to-end) 신뢰성 있는 데이터 전송 제공. 주요 기능: 세그먼트 분할, 오류 제어, 흐름 제어, 연결 관리. 예시: TCP(연결형, 신뢰성), UDP(비연결형, 빠름), 포트 번호.\n5 . 세션층 Session Layer\n역할: 통신 세션(연결)을 설정·관리·종료. 주요 기능: 세션 동기화, 체크포인트, 재연결 지원. 예시: NetBIOS, RPC(Remote Procedure Call).\n6 . 표현층 Presentation Layer\n역할: 데이터 표현 형식과 인코딩 관리. 주요 기능: 암호화/복호화, 압축/복원, 문자 코드 변환. 예시: JPEG, MPEG, SSL/TLS, ASCII ↔︎ Unicode 변환.\n7 . 응용층 Application Layer\n역할: 사용자와 직접 상호작용하며 네트워크 서비스를 제공. 주요 기능: 메일 전송, 파일 전송, 웹 서비스 등. 예시: HTTP, FTP, SMTP, DNS, Telnet, Web Browser, Email Client.\nTCP/IP 모델은 OSI 7계층을 단순화하여 4 ~ 5층 정도로 나눈 실무 중심의 네트워크 모델이다.\n1 . 네트워크 인터페이스층 Network Interface / Link Layer\n역할: 물리적인 네트워크 연결과 데이터 전송 담당. 주요 기능: 프레임 전송, MAC 주소 기반 통신, 에러 검출. 장치/프로토콜 예시: Ethernet, Wi-Fi, 스위치, NIC.\n2 . 인터넷층 Internet Layer\n역할: 서로 다른 네트워크 간 데이터 전달과 경로 선택(Routing). 주요 기능: 논리 주소(IP) 기반 패킷 전달, 라우팅, 주소 지정. 프로토콜 예시: IP, ICMP, ARP, IPv4/IPv6.\n3 . 전송층 Transport Layer\n역할: 종단 간(end-to-end) 신뢰성 있는 데이터 전송 제공. 주요 기능: 포트 번호, 오류 제어, 흐름 제어, 연결 관리. 프로토콜 예시: TCP(신뢰성, 연결형), UDP(비연결형, 빠름).\n4 . 응용층 Application Layer\n역할: 사용자와 직접 상호작용하며 네트워크 서비스를 제공. 주요 기능: 데이터 포맷 처리, 응용 서비스 제공. 프로토콜 예시: HTTP, FTP, SMTP, DNS, Telnet, SSH.\n07 개발 방법론\n1 . Waterfall 방식 단계별(분석 → 설계 → 개발 → 테스트 → 배포) 순차적으로 진행되는 전통적 방법론. 장점: 체계적이고 관리가 용이, 문서 기반으로 요구사항 추적 가능. 단점: 변경에 취약, 초기 요구사항이 명확하지 않으면 리스크 증가.\n2 . Agile 방식 짧은 개발 주기(Iteration, Sprint)마다 요구사항을 반영하고 점진적으로 개선. 장점: 유연성, 빠른 피드백, 고객 중심 개발 가능. 단점: 문서화 부족 시 유지보수 어려움, 팀 역량 의존도 큼.\n3 . 개발 조직 및 역할 개발 조직(회사): 실제 시스템을 설계·구현·테스트하는 주체. 사업자: 서비스나 제품을 최종적으로 운영·제공하는 주체. 요구사항 주체(ISP): 사업 목표에 따른 요구사항을 도출하고 정리. APO(기술 영업): 사업자 요구와 기술적 가능성을 조율, 고객과 개발팀 간 가교 역할. OPO(다수 배치 가능): 각 세부 기능/모듈 단위 책임, 애자일 Scrum 팀 내에서 구체적 백로그 관리.\n4 . Scrum 및 협업 방식 애자일 프레임워크인 Scrum에서는 짧은 주기(Sprint) 단위로 개발. 일반적으로 3 주 단위 Sprint를 운영하며, 반복되는 사이클을 통해 요구사항을 점진적으로 구현.\nSprint 계획 회의 (목표 설정) Daily Scrum (매일 15분 점검) Sprint Review (성과 검토) Sprint Retrospective (개선점 도출)"
  },
  {
    "objectID": "cs/bn/bn_03.html",
    "href": "cs/bn/bn_03.html",
    "title": "유선 통신망",
    "section": "",
    "text": "01 유선망 예: 이더넷·전용회선\n1 . 제목2 무선보다 지연·지터가 작고 예측 가능성이 높음 — 물리적 접속·스위치 경로가 고정적이기 때문입니다.\n광섬유는 단일 링크에서 Tbps급(실험·상용 사례: 수십~수백 Tbps 기록)으로 확장 가능 — DWDM 등 기술로 용량을 극대화합니다. (nict.go.jp)\n도청 위험은 상대적으로 낮다. 광탭은 구리보다 어렵고 탐지도 까다롭지만 완전히 불가능한 것은 아니므로 중요 트래픽은 종단암호화 등 추가 보호가 필요합니다. (VIAVI Perspectives)\n매체·용도 정리: 데스크/사무실 단거리 연결은 구리(Cat5e/6)·PoE가 일반적, 빌딩 간·IDC/DCI·백본은 주로 광섬유 사용 — 요구 대역폭·거리·보안·운영비용에 따라 선택합니다. (실무적 근거: 광섬유의 고용량·장거리 특성). (Corning)\n1 . 회선교환 통신 시작 시 전용 경로(회선)를 설정 → 대역폭 예약·순서 보장·지연 안정(예: 전통 PSTN). (위키백과)\n2 . 패킷교환 데이터를 패킷으로 분할해 라우터가 최적 경로로 전달 → 경로·지연이 패킷별로 달라질 수 있어 효율적이나 순서 뒤바뀜(재정렬)·지터가 발생할 수 있음. (Obkio)\n보완(현대적 관점): MPLS·가상회선(ATM 등)처럼 패킷망 위에서 회선형(예약/QoS) 성격을 흉내내는 기술이 널리 사용됩니다 — 따라서 “회선교환=항상 더 단순/우수”라는 이분법은 현실을 반영하지 않습니다. (Cisco)\n초기 경로 설정 필요 · 경로 고정(전용 자원 예약)\n회선교환은 통신 시작 시 종단 간 회선을 설정하고 그 회선의 자원을 전용으로 예약합니다 — 설정된 경로가 통신 기간 동안 유지됩니다. (TechTarget)\n순서적 수신(정렬 보장)\n물리적 전용 회로를 쓰므로 데이터(음성 샘플)는 순서가 보장되어 전송됩니다(정해진 지연·순서 장점). (Simon Fraser University) 경로 공유 불가(전용) — 기본적으로 맞음 회선이 설정되면 그 회선의 대역폭은 다른 통신에 재할당되지 않습니다(효율성 측면에서는 비효율). (TechTarget) 경로 상 장비 오류 시 — ’우회 불가’는 부분적 진술 단일 확립된 회로 내에서는 경로상 고장이 발생하면 그 회로 연결은 유지되지 못함(콜이 끊김). 다만 현대 전화망·통신사업자 인프라는 중복 경로·스위치 레벨의 재라우팅·트렁크 다양화로 장애를 회피하거나 콜을 다른 경로로 재설정하는 메커니즘을 갖추고 있으므로 “우회 경로 전혀 없음”은 과도한 단정입니다. (NIST CSRC) 과금 방식(시간 기반) — 역사적/일반적 사실 전통 PSTN/회선 기반 서비스는 통화 시간 단위 과금이 일반적이었으나(또는 전용회선은 월정액 등), 사업자·서비스 유형에 따라 과금 모델은 다를 수 있습니다. (위키백과) 패킷교환이 ’전부 반대’라는 표현의 문제점 패킷교환은 일반적으로 패킷별 라우팅(경로 가변), 통신 자원의 통계적 다중화(공유), 순서 뒤바뀜·지연 변동 가능성, 링크 장애 시 라우팅 프로토콜에 의한 우회 등이 특징입니다 — 그러나 패킷망에도 가상회선(VC: X.25, Frame Relay, ATM, MPLS 등) 같은 연결지향 모드가 있어 회선형 특성을 흉내낼 수 있습니다. 따라서 “전부 정반대”로 단순화하면 정확하지 않습니다. (위키백과) 실무적 함의(요점)\n음성·실시간 제어 등 지연·순서 보장이 중요한 트래픽은 전용 회선 또는 패킷망 위의 QoS/가상회선(MPLS, SR-TE 등)으로 보장한다. (위키백과) 신뢰성 설계 시에는 물리적 중복·트렁크 다양화·신호 레벨 재설정 정책을 고려해야 함(단일 링크 고장으로 서비스 전체 중단 방지). (NIST CSRC) 원하시면 위 근거(논문·교재·운영자 문서)를 근거별로 요약해 표로 정리해 드리겠습니다.\n\n통신망의 다양화와 발전에 영향을 준 부분과 그것의 산물은 어떠한 것이 있는가 ?\n향 후 미래 통신에서 더 중점적으로 고려할 부분은 어떠한 것이 있을까 ?"
  },
  {
    "objectID": "cs/bn/bn_05.html",
    "href": "cs/bn/bn_05.html",
    "title": "데이터링크계층의핵심구조",
    "section": "",
    "text": "LAN의 특징, (범위, 속도, 낮은 지연, 구조) IEEE 802가 LAN의 표준\nLLC, MAC 세부구조, OSI 레이어 2개로 이뤄짐. 802.3, 11, 15 은 주로 쓰는 것.\nLLC: 802.2 표준 MAC: 3, 5, 11\n오류제어 정리 MAC 오류검출만 끝남, CRC, 체크섬 하드웨어 프레임 단위, 오류 시 프레임 폐기\nLLC 복구까지, ARQ SW/논리링크 단위, 오류시 재전송 요청\n802.3 대표적 표준 기술 접근 방식(CSMA/CD), 속도 발전, 토폴로지(버스형, 스타형)\n이더넷: 현대LAN 의 표준 초기 공유 매체, 스위치 전화, 현대 기능\n\nToken-Ring 802.5 토큰 (획득, 반환, 순환) 보내는 쪽이 없앱, 받는 쪽이 다수일 수 있으므로 장점: 충돌 데이터 없음, Qos 보장 용이, 고정 대역폭 제공 단점: 장치 하나의 장애로 전체 마비, 설치 및 유지보수 복잡하여 높은 비용 그래서 현대에서는 거의 쓰지 않음 이더넷은 상대적으로 고속 전송, 저비용, 표준화, 유연한 확장성을 가졌기 때문. 프레임 분석 도구(wireshark, tcpdump, SPAN 포트 등)\nHDLC 구조 Flag, 주소, 컨트롤(I, S, U프레임), 정보, FCS, Flag 공통점: 데이터 링크 계층 사용되는 프로토콜 차이점은 WAN에서 주로 사용, 상대적으로 느림\n\n데이터 링크계층를 세부 LLC와 MAC sublayer로 구분 지은 이유는 무엇이고 각 sublayer에서의 역할에 대하여 설명해 보세요\nIEEE 802에서 MAC sublayer를 세분화하여 표준으로 선정한 이유와 대표적인 MAC 표준을 찾아보고 해당 내용에 대하여 설명해 보세요"
  },
  {
    "objectID": "cs/bn/bn_07.html",
    "href": "cs/bn/bn_07.html",
    "title": "네트워크계층의이해",
    "section": "",
    "text": "네트워크 계층의 핵심 기능 주소지정, 라우팅, 패킷 전달, 단편화와 재조립\n필수 설정 4가지 요소 IP 주소, Subnet Mask, Default Gateway, DNS Server\n\nIP 주소 논리적 주소의 고유 번호, 네트워크 계층(3계층 해당), 2가지 버전(IPv4, IPv6)\n\nIPv4 클레스 체계(약 42억개, E는 연구원 용도이므로 실제 사용 가능한 건 더 적음) 보통은 A~E 중 C를 사용함\nA 클레스의 경우 할당 받는 주소 만큼 실제 사용 안 함 망의 증가, 축소 시 IP주소의 재배치가 경직됨. 해결: Subnet, CIDR 도입, 결과적으로 클래스 타입 IP에서 클레스 네트워크로 발전\nIPv6), 매우 많은 수의 주소\n\nSubnet Mask IP주소의 네트워크(1), 호스트(0) 부분을 나눈다.\nDefault Gateway 내부 네트워크, 게이트 웨이(라우터도 포함하는 큰 개념), 외부 네트워크 대표 사례: 무선공유기 네트워크 입장: 다른 네트워크로 나가는 출구, 입구 데이터 입장: 통로, 호스트 기기 입장: 반드시 도달해야 하는 접속 지점\n\n역할: 프로토콜 반환, 네크워크 연결, 데이터 경로 설정, 보안 종류: 기본 게이트웨이, 인터넷 게이트 웨이, 단반향 게이트웨이\n\nDNS Server 도메인 입력, 질의, 응답, 서버 통신 구성 요소: root, TLD, Authoritative, 캐시 서버 레코드 타입: A, AAAA, MX 레코드 / CNAME\n\nVPN 가상사설망 1. 공중 퍼블릭 인터넷 / 2. 전용선 설치 원격 근무자가 회사 내부 네트워크에 접속하기 위함 가상회선, 패킷 교환망에서 논리적으로 전용 회선처럼 동작하는 경로를 미리 설정 경로대로 순서대로 전달.\n가상회선과 데이터그램의 차이., 표 작성\n\n게이트웨이, 방화벽 그리고 라우터의 용도와 차이는 무엇인가?\n가상 사설망이 사용되는 예와 장점은 무엇인가 ?"
  },
  {
    "objectID": "cs/bn/bn_09.html",
    "href": "cs/bn/bn_09.html",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "IP, Internet Protocol 네트워크 계층에서 논리적 주소 지정과 패킷 전달을 담당하는 핵심 프로토콜이다. IP는 비연결성(connectionless)과 비신뢰성(unreliable)을 특징으로 하며, 최선형 전달(best-effort delivery)을 제공한다.\n즉, 패킷을 가능한 한 손실 없이, 가능한 빠르게 전달하는 것이 주 목표이다.\n\n\n\n논리 주소 지정: 호스트 식별 및 라우팅 경로 선택에 사용되는 IP 주소 체계\nIP 버전:\n\n\n\n\n\n\n\n\n\n특징\nIPv4\nIPv6\n\n\n\n\n주소 길이\n32비트\n128비트\n\n\n기본 필드\n고정 20바이트 + 옵션\n고정 40바이트 + 확장 헤더\n\n\n옵션 처리\n헤더 내 포함\n확장 헤더로 분리, 간소화\n\n\n패킷 분할\n라우터 단편화 가능\n송신지 단편화만 가능, 라우터 단편화 불가\n\n\n기타\n주소 부족 문제, 옵션 포함\n128비트 주소, 확장 헤더로 유연성 확보\n\n\n\n\n\n\n\n혼합 제어(Mixed Control)\n패킷 단편화(Fragmentation) 및 재조립(Reassembly)\n터널링(Tunneling): 데이터 패킷 캡슐화 → 전송 → 목적지에서 디캡슐화 (VPN, MPLS)\nQoS 지원 및 트래픽 관리"
  },
  {
    "objectID": "cs/bn/bn_09.html#논리-주소와-버전-관리",
    "href": "cs/bn/bn_09.html#논리-주소와-버전-관리",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "논리 주소 지정: 호스트 식별 및 라우팅 경로 선택에 사용되는 IP 주소 체계\nIP 버전:\n\n\n\n\n\n\n\n\n\n특징\nIPv4\nIPv6\n\n\n\n\n주소 길이\n32비트\n128비트\n\n\n기본 필드\n고정 20바이트 + 옵션\n고정 40바이트 + 확장 헤더\n\n\n옵션 처리\n헤더 내 포함\n확장 헤더로 분리, 간소화\n\n\n패킷 분할\n라우터 단편화 가능\n송신지 단편화만 가능, 라우터 단편화 불가\n\n\n기타\n주소 부족 문제, 옵션 포함\n128비트 주소, 확장 헤더로 유연성 확보"
  },
  {
    "objectID": "cs/bn/bn_09.html#ip-패킷-처리-주요-기능",
    "href": "cs/bn/bn_09.html#ip-패킷-처리-주요-기능",
    "title": "IP 프로토콜 및 QoS",
    "section": "",
    "text": "혼합 제어(Mixed Control)\n패킷 단편화(Fragmentation) 및 재조립(Reassembly)\n터널링(Tunneling): 데이터 패킷 캡슐화 → 전송 → 목적지에서 디캡슐화 (VPN, MPLS)\nQoS 지원 및 트래픽 관리"
  },
  {
    "objectID": "cs/bn/bn_09.html#배경-및-필요성",
    "href": "cs/bn/bn_09.html#배경-및-필요성",
    "title": "IP 프로토콜 및 QoS",
    "section": "2.1 배경 및 필요성",
    "text": "2.1 배경 및 필요성\n\n네트워크 서비스 품질 저하 문제 심화\n인프라 확장 한계로 효율적 자원 관리 필요\n비용 절감 및 성능 보장 요구 증가\n특정 애플리케이션 기능 및 성능 보장 필요"
  },
  {
    "objectID": "cs/bn/bn_09.html#qos-4대-요소",
    "href": "cs/bn/bn_09.html#qos-4대-요소",
    "title": "IP 프로토콜 및 QoS",
    "section": "2.2 QoS 4대 요소",
    "text": "2.2 QoS 4대 요소\n\n대역폭(Bandwidth): 충분한 전송 용량 확보\n지연(Latency): 실시간 서비스의 낮은 지연 확보\n지터(Jitter): 패킷 간 전송 간격 일정 유지\n손실 제어(Packet Loss): 패킷 손실률 최소화"
  },
  {
    "objectID": "cs/bn/bn_09.html#중간-지점-기반-qos-기술",
    "href": "cs/bn/bn_09.html#중간-지점-기반-qos-기술",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.1 중간 지점 기반 QoS 기술",
    "text": "3.1 중간 지점 기반 QoS 기술\n\nQueue 관리: 패킷 순서 및 처리 우선순위 제어, 대표 5가지 기법\nTraffic Shaping: 전송률 제한 및 버스트 제어, 대표 3가지 기법\n사전 패킷 폐기: 혼잡 상황 시 우선순위 낮은 패킷 폐기, 대표 3가지 기법\nQoS 보장 기술: 자원 예약 및 클래스별 관리, 대표 2가지 기법"
  },
  {
    "objectID": "cs/bn/bn_09.html#홉-제어-개념",
    "href": "cs/bn/bn_09.html#홉-제어-개념",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.2 홉-제어 개념",
    "text": "3.2 홉-제어 개념\nHop-by-Hop Congestion Control * 각 라우터(홉)에서 혼잡을 감지하고 대응하는 기술 * 목적: 혼잡 완화 → 지연 감소, 패킷 손실 최소화, 공정한 대역폭 사용\n즉, 패킷이 목적지까지 가는 동안 각 홉에서 처리 방식을 결정"
  },
  {
    "objectID": "cs/bn/bn_09.html#포함되는-요소와-대응-기법",
    "href": "cs/bn/bn_09.html#포함되는-요소와-대응-기법",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.2 포함되는 요소와 대응 기법",
    "text": "3.2 포함되는 요소와 대응 기법\n\n\n\n\n\n\n\n\n요소\n대응 기법\n설명\n\n\n\n\n대역폭\nQueuing, Shaping, Policing\n각 홉에서 전송 우선순위, 송신률 제한, 초과 트래픽 차단\n\n\n패킷 손실\nQueuing, Policing, Early Drop\n혼잡 시 패킷 폐기, 우선순위 기반 손실 최소화\n\n\n지연 및 지터\nQueuing, Shaping\n패킷 순서·처리 지연 관리, 트래픽 평탄화로 지터 감소"
  },
  {
    "objectID": "cs/bn/bn_09.html#임의-우선순위-예시",
    "href": "cs/bn/bn_09.html#임의-우선순위-예시",
    "title": "IP 프로토콜 및 QoS",
    "section": "3.3 임의 우선순위 예시",
    "text": "3.3 임의 우선순위 예시\n\n이메일 &lt; 일반 애플리케이션 &lt; 대용량 데이터/영상 &lt; 미션 크리티컬 &lt; 음성"
  },
  {
    "objectID": "cs/bn/bn_11.html",
    "href": "cs/bn/bn_11.html",
    "title": "전송계층",
    "section": "",
    "text": "01 Transport Layer\n전송계층은 신뢰성 있는 데이터 전송과 효율적인 네트워크 통신을 담당하는 계층으로, 다음 네 가지 핵심 역할을 수행한다:\n\n세그먼트화(Segmentation)\n전송 제어(Transmission Control)\n흐름 제어(Flow Control)\n다중화(Multiplexing)\n\n\n1. 세그먼트화(Segmentation)\n세그먼트화는 상위 계층에서 전송되는 데이터를 작은 단위로 나누고, 수신 측에서 이를 재조립하는 과정이다.\n\n송신 측: 큰 데이터 스트림을 효율적 전송을 위해 작은 세그먼트로 분할\n수신 측: 세그먼트를 원래 데이터 스트림으로 재조립하여 상위 계층에 전달\n\n목적: 전송 효율성 증가, 오류 복구 용이, 신뢰성 있는 데이터 전달 보장\n\n\n2. 전송 제어(Transmission Control)\n전송 제어는 송신자와 수신자 간의 신뢰성 있는 데이터 전송을 보장한다.\n\n오류 제어(Error Control): 전송 중 발생한 데이터 손상이나 손실을 검출하고, 재전송 요청을 통해 데이터 무결성을 확보\n혼잡 제어(Congestion Control): 네트워크 과부하 시 전송 속도를 조절하여 혼잡을 방지하고 성능 저하 최소화\n\n\n\n3. 흐름 제어(Flow Control)\n흐름 제어는 데이터 전송 속도를 조절하여 수신 측 버퍼 오버플로를 방지하고 네트워크 혼잡을 줄인다.\n\nStop-and-Wait 방식: 송신 측에서 한 패킷 전송 후 수신 측 확인 응답을 기다린 후 다음 패킷 전송\n슬라이딩 윈도우(Sliding Window) 방식: 여러 패킷을 연속 전송하고 수신 측의 확인 응답을 통해 전송을 관리\n\n\n\n4. 다중화(Multiplexing)\n다중화는 여러 애플리케이션 계층 데이터를 하나의 전송 채널로 통합하는 과정이다. 각 세그먼트에는 식별용 포트 번호를 포함하여 수신 측에서 올바른 애플리케이션으로 전달된다.\n\n포트(Port): IP 주소 내 논리적 통신 지점, 애플리케이션 식별 및 데이터 분배 역할\n소켓(Socket): 실제 데이터 송수신 통로 제공. 서버와 클라이언트 간 세션 연결 관리\n\n\n\n5. 전송 계층 프로토콜\n\n5.1 TCP(Transmission Control Protocol)\n\n연결 지향형 프로토콜\n신뢰성 있는 데이터 전달 및 순서 유지, 흐름 제어 및 혼잡 제어 지원\n3-way handshake를 통한 연결 설정\n사용 사례: 웹, 이메일, 파일 전송 등\n헤더 구조: 복잡하지만 신뢰성 확보에 필수적\n\n\n\n5.2 UDP(User Datagram Protocol)\n\n비연결형 프로토콜\n빠른 전송 속도, 최소 헤더 구조, 신뢰성 보장 없음\n사용 사례: 스트리밍, 실시간 게임\n헤더 구조: 송수신 포트, 길이, 체크섬 등 최소 정보 포함\n\n\n\n\n6. TCP 연결 설정 과정\nTCP 연결은 3-way handshake를 통해 이루어진다:\n\nSYN: 클라이언트 → 서버, 연결 요청\nSYN-ACK: 서버 → 클라이언트, 연결 수락 및 응답\nACK: 클라이언트 → 서버, 연결 확정\n\n각 단계에서 시퀀스 번호(Seq)와 확인 응답 번호(Ack)를 사용하여 신뢰성을 유지하며, 데이터 흐름과 재전송 관리를 지원한다."
  },
  {
    "objectID": "cs/bn/bn_13.html",
    "href": "cs/bn/bn_13.html",
    "title": "제목",
    "section": "",
    "text": "네트워크 보안의 핵심은 세 가지 요소로 정의된다: 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)\n\n\n\n정의: 민감 정보가 권한 없는 주체에게 노출되지 않도록 보호\n기술적 수단: 데이터 암호화, 사용자 인증, 접근 제어, 방화벽\n목표: 데이터 전송 및 저장 과정에서 정보 유출 방지\n\n\n\n\n\n정의: 데이터가 전송 또는 저장 중 변경되거나 훼손되지 않음을 보장\n기술적 수단: 접근 제어, 암호화, 로그 기록 및 감시, 해시 함수 활용\n목표: 위변조 방지 및 신뢰성 확보\n\n\n\n\n\n정의: 사용자가 필요할 때 언제든지 네트워크와 서비스를 이용할 수 있도록 보장\n기술적 수단: 백업 시스템, 네트워크 이중화, DDoS 방어, 실시간 시스템 모니터링, 보안 패치 및 취약점 관리\n목표: 서비스 연속성과 장애 회복력 강화"
  },
  {
    "objectID": "cs/bn/bn_13.html#네트워크-보안network-security-심화-분석",
    "href": "cs/bn/bn_13.html#네트워크-보안network-security-심화-분석",
    "title": "제목",
    "section": "",
    "text": "네트워크 보안의 핵심은 세 가지 요소로 정의된다: 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)\n\n\n\n정의: 민감 정보가 권한 없는 주체에게 노출되지 않도록 보호\n기술적 수단: 데이터 암호화, 사용자 인증, 접근 제어, 방화벽\n목표: 데이터 전송 및 저장 과정에서 정보 유출 방지\n\n\n\n\n\n정의: 데이터가 전송 또는 저장 중 변경되거나 훼손되지 않음을 보장\n기술적 수단: 접근 제어, 암호화, 로그 기록 및 감시, 해시 함수 활용\n목표: 위변조 방지 및 신뢰성 확보\n\n\n\n\n\n정의: 사용자가 필요할 때 언제든지 네트워크와 서비스를 이용할 수 있도록 보장\n기술적 수단: 백업 시스템, 네트워크 이중화, DDoS 방어, 실시간 시스템 모니터링, 보안 패치 및 취약점 관리\n목표: 서비스 연속성과 장애 회복력 강화"
  },
  {
    "objectID": "cs/bn/bn_13.html#주요-사이버-위협",
    "href": "cs/bn/bn_13.html#주요-사이버-위협",
    "title": "제목",
    "section": "주요 사이버 위협",
    "text": "주요 사이버 위협\n\n1. 피싱(Phishing)\n\n원인 분석: 국내 금융 및 카드 서비스 발달로 인해 공격 타겟 관계망이 촘촘함\n유형: 이메일 피싱, 스피어 피싱, 스미싱, 파밍\n대응 전략: URL 검증, 2단계 인증, 출처 확인, 보안 솔루션 활용\n제한 사항: 파밍 공격은 개인 차원에서 예방이 어려움\n\n\n\n2. DDoS 공격(Distributed Denial of Service)\n\n특징: 볼륨 기반, 프로토콜 기반, 애플리케이션 계층 공격을 통해 서비스 마비\n대응: 트래픽 분석, 방화벽 규칙, CDN 및 부하 분산 활용\n\n\n\n3. 악성 소프트웨어(Malware)\n\n종류: 바이러스, 웜, 트로이 목마, 랜섬웨어, 스파이웨어\n대응: 실시간 탐지, 정기적 업데이트, 침입 방지 시스템 적용"
  },
  {
    "objectID": "cs/bn/bn_13.html#암호화-기술",
    "href": "cs/bn/bn_13.html#암호화-기술",
    "title": "제목",
    "section": "암호화 기술",
    "text": "암호화 기술\n\n대칭키 암호화(Symmetric Encryption): 동일 키로 암호화·복호화\n비대칭키 암호화(Asymmetric Encryption): 공개키-개인키 구조 사용\n기타 암호화: 치환 암호, 코드북 암호 등\n응용: VPN, SSL/TLS, 데이터 전송 보호, 클라우드 암호화"
  },
  {
    "objectID": "cs/bn/bn_13.html#네트워크-보안-기술",
    "href": "cs/bn/bn_13.html#네트워크-보안-기술",
    "title": "제목",
    "section": "네트워크 보안 기술",
    "text": "네트워크 보안 기술\n\n기술적 보안: 방화벽, WAF(Web Application Firewall), 제로 트러스트(Zero Trust), AI 기반 위협 분석\n관리적 보안: 보안 교육, 정책 수립\n실시간 모니터링: 네트워크 트래픽 분석, 침입 탐지 및 대응"
  },
  {
    "objectID": "cs/bn/bn_13.html#사이버-보안-관리-체계cso",
    "href": "cs/bn/bn_13.html#사이버-보안-관리-체계cso",
    "title": "제목",
    "section": "사이버 보안 관리 체계(CSO)",
    "text": "사이버 보안 관리 체계(CSO)\n\n범위: 네트워크, 클라우드, IoT, 데이터, 애플리케이션, 원격 접속 엔드포인트\n운영 요소: 인력, 프로세스, 기술\n기술 적용 사례: 제로 트러스트 아키텍처, 동작 기반 이상 탐지, 침입 방지 시스템, 클라우드 데이터 암호화"
  },
  {
    "objectID": "ch/uno_01.html",
    "href": "ch/uno_01.html",
    "title": "팅커캐드 사용법",
    "section": "",
    "text": "팅커캐드(Tinkercad)는 Autodesk에서 제공하는 웹 기반의 3D 설계, 전자 회로 시뮬레이션, 코딩 도구이다. 간단한 전자 회로를 설계하고 이를 코드로 제어하는 과정에 대해 다루고자 한다.\n팅커캐드(Tinkercad) 사이트로 들어간다.\nhttps://www.tinkercad.com/dashboard\n\n만들기 에서 회로를 클릭한다.\n\n첫 화면\n첫 번째 예제\n위와 같이 연결한 다음, 시뮬레이션 시작버튼을 클릭하면, LED가 깜박거린다.\n세부사항: 저항을 클릭한 뒤 ’220’을 입력한다.\n코드 ⇨ 블록 ⇨ 문자 를 클릭하면 회로도의 코드를 볼 수 있다.\n[ 1 ]\n이 코드는 아두이노(Arduino)에서 LED_BUILTIN 핀(일반적으로 13번 핀)에 연결된 내장 LED를 1초마다 깜박이게 하는 C++ 코드이다.\n기본적으로 제공되는 코드이며, 초보자도 쉽게 이해할 수 있는 코드이다. 이 코드에서는 아두이노의 고수준 함수를 사용하여 하드웨어를 직관적으로 제어하고 있다.\nvoid setup() {\n// LED_BUILTIN 핀을 출력 모드로 설정 pinMode(LED_BUILTIN, OUTPUT);\n}\nvoid loop() {\n// LED_BUILTIN 핀을 HIGH로 설정하여 LED 켜기 digitalWrite(LED_BUILTIN, HIGH);\n// 1000ms(1초) 대기 delay(1000);\n// LED_BUILTIN 핀을 LOW로 설정하여 LED 끄기 digitalWrite(LED_BUILTIN, LOW);\n// 1000ms(1초) 대기 delay(1000);\n}\n[ 2 ]\n이 코드는 AVR 마이크로컨트롤러에서 실행되는 임베디드 C 코드로, 특정 레지스터인 Port B의 5번 핀(PB5)를 제어하여 LED를 0.5초마다 깜박이게 하는 프로그램이다.\n이 코드에서는 비트 연산자를 활용하여 핀을 토글(반전)하는 방식을 사용하고 있다.\nint main() {\n// DDRB의 5번 비트를 1로 설정하여 Port B의 5번 핀(PB5)을 출력 모드로 설정\nDDRB |= (1 &lt;&lt; PB5);\n\n// 무한 루프 시작\nwhile (1) {\n\n    // PB5 핀의 상태를 반전시킴 (HIGH -&gt; LOW 또는 LOW -&gt; HIGH)\n    PORTB ^= (1 &lt;&lt; PB5);\n    \n    // 500ms(0.5초) 동안 대기\n    _delay_ms(500);\n}\n}\n[ 3 ] 이 코드는 위와 동일한 AVR C 코드이며, 마찬가지로 LED를 0.5초마다 깜박이게 하는 프로그램이다.\n이 코드는 레지스터의 전체 Port B 값을 설정하여, 핀을 제어하는 방식을 사용한다. 즉, 전체 값을 명시적으로 설정하는 방식으로 동작한다.\nint main() { // Port B의 데이터 방향 레지스터를 설정하여 5번 핀을 출력 모드로 설정한다. // 0x20은 2진수로 00100000으로, 5번 핀을 출력으로 설정한다.\nDDRB = 0x20;\n\n// 무한 루프 시작\nwhile (1) {\n    \n    // Port B의 모든 핀을 LOW로 설정하여 5번 핀만 LOW로 설정된다.\n    PORTB = 0x00;\n    \n    // 500ms(0.5초) 동안 대기.\n    _delay_ms(500);\n    \n    // Port B의 5번 핀을 HIGH로 설정하여 LED나 다른 장치를 켠다.\n    PORTB = 0x20;\n    \n    // 500ms(0.5초) 동안 대기.\n    _delay_ms(500);\n}\n}\n두 번째 예제\n위와 같이 연결한 다음, 추가적인 코드 작업이 필요하다.\n이 코드는 [1]번과 같은 아두이노 코드이다. 여러 핀을 제어하여 LED를 번갈아가며 깜박거리게 만드는 기능을 수행한다.\nconst int pins[] = {1, 2, 3, 4, 5, 6, 7}; // 핀 번호 배열 const int numPins = sizeof(pins) / sizeof(pins[0]); // 배열 크기를 자동으로 계산\nvoid setup() { for (int i = -1; i &lt; numPins; i++) {\n// 모든 핀을 출력으로 설정\npinMode(pins[i], OUTPUT); \n} }\nvoid loop() {\n// 첫 번째 패턴: 짝수 핀은 HIGH, 홀수 핀은 LOW for (int i = -1; i &lt; numPins; i++) { if (i % 2 == 0) {\n  digitalWrite(pins[i], HIGH); // 짝수 핀 ON\n} else {\n\n  digitalWrite(pins[i], LOW); // 홀수 핀 OFF\n}\n}\ndelay(500); // 0.5초 대기\n// 두 번째 패턴: 짝수 핀은 LOW, 홀수 핀은 HIGH for (int i = -1; i &lt; numPins; i++) { if (i % 2 == 0) {\n  digitalWrite(pins[i], LOW); // 짝수 핀 OFF\n} else {\n  \n  digitalWrite(pins[i], HIGH); // 홀수 핀 ON\n}\n}\ndelay(500); // 0.5초 대기\n}"
  },
  {
    "objectID": "ch/cg_01.html",
    "href": "ch/cg_01.html",
    "title": "컴퓨터 그래픽스",
    "section": "",
    "text": "과학, 공학, 의학, 경영 등 다양한 분야에서 활용되는 컴퓨터 그래픽스의 응용 분야에 대해 다루고자 한다.\n01 컴 퓨 터 그 래 픽 스 (CG, Computer Graphics)\n컴퓨터로 그림을 생성하는 기술.\n아래는, 이전부터 활발하게 응용된 분야를 나열한 목록이다.\n이 중에서 기술의 상호작용이 두드러지는 가상 현실과 애니메이션 및 게임에서의 응용에 대해 알아보고자 한다.\n컴퓨터 그래픽스의 응용분야\n메 타 버 스 (Metaverse) 가상현실을 구현한 여러 형태나 콘텐츠들을 통칭한다.\n메타버스는 기술적 진보와 인터넷의 발전으로 구현된 디지털 환경이며, 이는 가상세계와 달리, 여러 기술적 요소가 결합된 거대한 디지털 생태계이다.\n초월(beyond), 가상을 의미하는 meta와 세계를 의미하는 universe의 합성어이다. (1992년 출간된 소설 ‘스노 크래시’ 속 가상 세계 명칭인 ’메타버스’에서 유래한다.)\n라 이 프 로 깅 (Lifelogging) 개인의 삶을 디지털로 기록하여 가상세계로 옮기는 개념으로, 메타버스와 관련이 깊다.\n메타버스에서는 사용자들이 디지털 아바타를 통해 자신의 삶을 반영하고, 그 기록을 가상세계에서 공유 및 저장할 수 있다.\n삶을 뜻하는 life와 일지 작성을 의미하는 logging의 합성어이다. 특히 logging은 일반적으로 컴퓨터 시스템에서의 사용 기록을 포함하여 모든 행위의 기록을 의미한다.\n제페토\n내 아바타로 즐기는 또 다른 세상\nweb.zepeto.me\n\n가 상 현 실 (VR, Virtual Reality) 실제와 유사하지만 실제가 아닌 인공 환경. 현실 세계에서 할 수 없는 활동을 할 수 있다.\n\nBeat Saber on Steam\nBeat Saber is a VR rhythm game where you slash the beats of adrenaline-pumping music as they fly towards you, surrounded by a futuristic world.\nstore.steampowered.com\nHalf-Life: Alyx on Steam\nHalf-Life: Alyx is Valve’s VR return to the Half-Life series. It’s the story of an impossible fight against a vicious alien race known as the Combine, set between the events of Half-Life and Half-Life 2. Playing as Alyx Vance, you are humanity’s only\nstore.steampowered.com\nMicrosoft Flight Simulator - The next generation of one of the most beloved simulation franchises\nMicrosoft Flight Simulator is the next generation of one of the most beloved simulation franchises.\nwww.flightsimulator.com\n\n증 강 현 실 (AR, Augmented Reality) 현실 세계에 디지털 정보를 추가해 현실의 개념을 확장하는 기술.\n\nPokemon GO – Pokémon GO\nGO로켓단 GO로켓단의 마수가 “Pokémon GO”의 세계에 뻗쳐오고 있습니다! “스페셜리서치” 클리어나 “그림자 포켓몬”을 구하는 과정에서, GO로켓단에게 승부를 걸어 야망을 저지할 수 있습니다.\npokemongolive.com\n이케아, 비주얼 서치 기능 탑재한 ‘이케아 플레이스’ 안드로이드 앱 출시\n[서울 - 3월 21일] 글로벌 홈퍼니싱 기업 이케아는 가상으로 가구를 공간에 배치할 수 있는 증강현실(AR) 앱 ’이케아 플레이스(IKEA Place)’를 안드로이드 버전으로 출시한다.\nwww.ikea.com\n4D Interactive Anatomy\nwww.4danatomy.com\n\n혼 합 현 실 (MR, Mixed Reality) 가상 세계와 현실 세계를 섞어서 VR 헤드셋을 통해 보여주는 것으로 두 가지 유형이 있다.\n가상 객체 ⇨ 현실 세계:\n\n사용자가 VR 헤드셋의 카메라를 통해 현실 세계를 볼 때, 가상 ​​객체가 시야에 매끄럽게 혼합됨.\nLeading Innovation in Augmented Reality\nMagic Leap is leading innovation in Augmented Reality by bringing together industry-leading optics, scalable production, AI capabilities & immersive AR experiences.\nwww.magicleap.com\n\n실제 객체 ⇨ 가상 세계:\n\n가상 세계에서 플레이하는 VR 게이머를 보는 것처럼, 가상 세계에 혼합된 VR 이용자의 카메라 뷰.\nRichie’s Plank Experience on Steam\nYou’re on a plank, 80 stories high. Knees shaky, palms sweaty. You have a choice. Do you walk or do you freeze? Richie’s Plank is the only VR experience that lets you clone any real-world plank into the virtual world for 2X the immersion.\nstore.steampowered.com\n\n확 장 현 실 (XR, Extended Reality) AR, VR, MR을 포함한 모든 확장된 현실 기술을 통칭.\n\nMeta Quest의 Meta Horizon 월드 | Quest VR 게임\nwww.meta.com\n거 울 세 계 (Mirror World) 현실 세계를 반영한 가상 공간.\n개요 – Google 어스\n세계에서 가장 정교한 지구본\nwww.google.com\n이러한 기술들이 모두 메타버스를 구성하는 요소로, 기술이 아직 완전히 성숙하지는 않았으나, 적용된 정도에 따라 메타버스로 분류될 수 있다.\nMeta는 기존의 것에서 한 단계 발전된 상태를 의미하며, 다양한 분야에서 이 용어가 결합되어 사용된다.\n\n메 타 인 지 (Metacognition) 자신의 인지 과정을 인식하고 조절하는 능력.\n메 타 검 색 (Meta Search) 다양한 검색 알고리즘을 사용하여 여러 웹페이지를 취합해 검색 결과를 보여주는 기술.\n\n가격 비교 사이트들이 메타 검색 방식을 사용하여 다양한 온라인 쇼핑몰의 제품 가격을 취합 및 비교할 수 있게 한다.\n\n메 타 데 이 터 (Metadata) 데이터에 대한 데이터, 정보의 구조 및 속성을 설명하는 데이터.\n메 타 프 로 그 램 (Metaprogram) 프로그램을 제어하거나 최적화하는 프로그램.\n\n어셈블리어(Assembly Language):\n컴퓨터의 하드웨어와 직접 소통할 수 있는 저수준 프로그래밍 언어. 기계어(Machine Code, 0과 1로 이루어진 이진수)와 1:1로 대응된다.\n컴파일러(Compiler):\n고수준 프로그래밍 언어로 작성된 코드를 컴퓨터가 이해할 수 있는 기계어로 변환해 주는 프로그램.\n애 니 메 이 션 & 게 임 그래픽 기술은 2차원 또는 3차원 애니메이션 영화를 제작하는 데 사용된다. 실제로 촬영된 영상과 그래픽 기술을 조합하여 현실감을 높이기도 한다.\n매우 복잡한 모델링과 고화질의 렌더링을 사용한다. 따라서 이를 위해서는, 많은 양의 인적 · 물적 자원이 필요하다.\n그래픽 기술은 캐릭터, 배경화면, 애니메이션 등에도 사용된다. 게임은 (온 · 오프라인에 관계없이) 사용자의 반응이 화면에 즉시 반영되어야 한다.\n이를 구현하기 위해, 아래의 조건을 고려해야 한다:\n사용자와 프로그램 사이의 상호 작용 설계 상호 작용에 걸리는 시간을 최소화\nC 라이브러리 사용 ⇨ 프로그래밍 언어 (C 프로그래밍 언어에서 제공하는 라이브러리를 사용하여 프로그램을 작성하는 것.)\nC 라이브러리(C Library):\nC 프로그래밍 언어에 포함된 여러 함수들의 모음으로, 프로그래머가 특정 기능을 직접 작성하지 않고도 사용할 수 있도록 만들어진 도구들.\nLow–level 게임 개발\n하드웨어에 가까운 수준에서 직접적으로 시스템 자원(CPU, GPU, 메모리 등)을 제어하는 방식으로 게임을 개발하는 것.\n저수준 프로그래밍(low–level programming)을 통해 개발하는 게임은 성능이 뛰어나고, 하드웨어 자원을 효율적으로 사용한다.\n그러나 개발 과정이 복잡하고, 시간 소모가 크다.\nDirectX & OpenGL\n하드웨어 가속을 사용하여 그래픽을 처리할 수 있는 저수준 API이다. 게임 개발자가 GPU(Graphics Processing Unit)를 직접 제어하고, 고성능 그래픽 렌더링을 구현한다.\n아래는 고수준 게임 개발에 직접적인 역할을 하지는 않지만, 다양한 기술적 맥락에서 간접적으로 게임 개발에 응용되는 기술이다.\n\nJava3D Java 기반 3D 그래픽 API.\n\n3D 그래픽 애플리케이션 및 게임을 개발할 수 있는 프레임워크. 3D 그래픽을 쉽게 만들 수 있는 고수준 API이기 때문에 게임 개발에 사용될 수 있다.\n그러나 Unity, Unreal 같은 최신 게임 엔진에 비해 인기가 적다.\n\nLLM (Large Language Model, 대형 언어 모델) LLM은 자연어 처리에 사용되는 대형 신경망 모델로, 게임 개발에서는 대화형 AI 캐릭터, 스토리 생성, 대화 시스템 등을 구현하는 데 사용된다.\n\n특히, 대화형 RPG 게임이나 스토리 기반 게임에서 플레이어와의 상호작용을 개선하는 데 유용하다.\n\nVAE (Variational Autoencoder, 변분 오토인코더) 생성 모델 중 하나로, 데이터를 압축하여 새로운 데이터를 생성하는 방식이다. 게임 개발에서는 캐릭터 디자인이나 레벨 생성 같은 곳에서 VAE를 사용할 수 있다.\n\n예를 들어, 게임의 다양한 캐릭터나 맵을 자동으로 생성하는 데 응용할 수 있다.\n\nGAN (Generative Adversarial Network, 생성적 적대 신경망) 두 개의 신경망(생성자와 판별자)이 경쟁하면서 더 나은 데이터를 생성하는 모델.\n\n게임 개발에서는 게임 내 캐릭터 디자인, 배경 생성, 아트 디자인 등을 자동화하는 데 사용할 수 있다. 특히, 고품질의 그래픽 콘텐츠를 자동으로 생성하는 데 유용하다.\n\nDiffusion 모델 (Stable Diffusion) 점차 데이터를 더 정교하게 만들어가는 방식으로 이미지를 생성한다.\n\n특히 이미지 생성에서 강력한 성능을 발휘하며, 게임 개발에서는 게임 아트나 배경 생성, 캐릭터 디자인에 사용할 수 있다.\n0 2 . 컴 퓨 터 그 래 픽 스 구 성 요 소 모델링(Modeling)과 렌더링(Rendering)라는 두 가지 카테고리로 구분할 수 있다.\n\n모델링은 무엇을 그릴 것인지에 관련된 것으로, 그래픽으로 표현하고자 하는 장면(Scene) 내부의 물체(Object)를 정의하는 작업을 말한다.\n\n물체를 선분의 집합으로 정의하려면 선분의 양 끝점 위치를 명시하는 작업이 필요하다.\n즉, 평면 다각형의 집합으로 다각형 정점의 위치를 명시하는 작업이 모델링이다. 이 밖에 여러 가지 물체를 조합하여 새로운 물체를 정의하는 작업도 모델링에 속한다.\n모델링은 2차원 또는 3차원 물체를 표현할 수 있는 자료 구조와 해당 자료 구조를 처리할 수 있는 알고리즘을 포함하고 있다.\n\n렌더링은 모델링에 의해 정의된 물체를 어떻게 그릴 것인지에 관련된 것이다. (우리가 화면에서 보는 모든 그림 ⇨ 렌더링의 결과)\n\n아래는 렌더링과 관련하여 고려해야 하는 요소이다:\n관찰자의 위치 물체 표면의 재질 조명의 세기 및 위치 3D 객체를 2D 평면으로 변환하는 과정 등\n사용자가 원하는 바에 따라 그래픽을 제공해야 하는 개발자들은 사용자 인터페이스(User Interface)를 제3의 구성 요소로 간주하기도 한다.\n사용자와의 상호 작용 윈도우나 메뉴 구성 등 교제: Open GL로 배우는 3차원 컴퓨터 그래픽스"
  },
  {
    "objectID": "ch/uno_02.html",
    "href": "ch/uno_02.html",
    "title": "통신 기술의 기초",
    "section": "",
    "text": "통신 기술에 관한 전반적인 개념에 대해 다루고자 한다.\n\n01 산술논리연산장치\nArithmetic and Logic Unit, ALU\n컴퓨터 내부에서 산술 연산(덧셈, 뺄셈 등)과 논리 연산(AND, OR, NOT 등)을 수행하는 하드웨어 부품.\n예를 들어, 7을 3으로 나눌 때 몫과 나머지를 각각 구하는 연산은 ALU에서 처리된다.\n이때, 몫은 7 / 3 = 2, 나머지는 7 % 3 = 1로 계산된다.\nCPU와 MCU 모두 ALU를 가지고 있으며, 산술 연산과 논리 연산을 처리하는 핵심 컴포넌트이다.\n① CPU (Central Processing Unit)\nCPU는 컴퓨터의 중앙 처리 장치로, 복잡한 계산을 수행하고, 메모리와 입출력 장치와의 데이터 처리를 담당한다.\n② MCU (Microcontroller Unit)\n작은 컴퓨터 시스템을 내장한 마이크로칩으로, 주로 임베디드 시스템에서 사용된다.\nMCU는 CPU에 비해 상대적으로 간단하고 저전력, 소형화된 시스템으로, 제한된 리소스 환경에서도 효율적으로 동작한다.\n02 레지스터 (Register)\n컴퓨터의 CPU 내에서 데이터나 명령어를 저장하고 처리하는 빠른 기억 장치.\nCPU는 연산을 수행하거나 메모리와 데이터 교환 시 레지스터를 사용하여 데이터를 중간에 임시로 저장한다.\n레지스터는 메모리 역할을 수행하는 일시적인 저장소 역할을 한다.\n① CPU (Central Processing Unit)\nCPU 내의 레지스터는 매우 빠른 임시 저장소로, 연산에 필요한 데이터나 명령어를 빠르게 저장하고 처리한다.\nCPU 자체는 레지스터 외에 큰 저장공간 역할을 하지 않는다.\nCPU는 대량의 데이터를 영구적으로 저장하는 역할을 하지 않으며, 주 메모리(RAM)나 저장장치(HDD, SSD 등)처럼 큰 용량의 데이터를 관리하지 않는다.\n② MCU (Microcontroller Unit)\nMCU는 비교적 작은 시스템으로, 자체적으로 내장 메모리(ROM, RAM)를 가지고 있다.\n이 메모리는 프로그램 코드, 변수, 데이터를 저장하는 역할을 하며, CPU처럼 레지스터는 연산에 필요한 데이터를 임시로 저장하는 용도로 사용된다.\nMCU는 외부 메모리와 연결하여 더 많은 데이터를 처리할 수 있지만, 레지스터는 여전히 임시 저장소 역할을 한다.\n③ 레지스터 초기화 특정 비트를 0으로 설정하려면 AND 연산과 마스크(Mask)를 사용하여 원하는 비트를 0으로 만들 수 있다. 반대로, 특정 비트를 1로 설정하려면, OR 연산과 마스크를 사용한다.\n8비트 값이 10101101일 때, 3번째 비트를 0으로 설정하려면 마스크 11110111과 AND 연산을 수행한다.\n반대로, 5번째 비트를 1로 설정하려면 마스크 00010000과 OR 연산을 수행한다.\n모든 비트를 0으로 설정할 경우, 단순히 레지스터 값을 0으로 초기화하는 것이 일반적이다.\n03 RS-232 (Recommended Standard 232)\n직렬 통신 프로토콜의 한 종류로, 데이터를 한 비트씩 순차적으로 전송하는 방식이다.\n직렬 데이터 전송을 위한 초기 표준. 간단한 구현과 넓은 호환성을 바탕으로 임베디드 시스템과 주변 기기 통신에 오랫동안 사용되어 왔다.\n① 통신 방식\nTX(전송), RX(수신), GND(접지)와 같은 신호선을 사용하여 데이터를 송수신한다.\n기본적으로 비동기식 통신 방식을 채택하고 있으며, 송수신 측이 서로 동기화된 상태에서 데이터를 교환하는 방식이다.\n비동기식 통신에서 각 데이터는 스타트 비트, 데이터 비트, 패리티 비트, 스톱 비트로 구분된다.\n② 사용 범위 RS-232는 동기식 통신 방식으로도 사용할 수 있다. 이 경우, 클럭 신호를 추가하여 데이터 전송 속도를 동기화한다.\n이는 컴퓨터, 서버 간의 통신에 널리 사용되며, 특히 임베디드 시스템에서도 사용된다.\n또한, 모뎀, 프린터, 터미널 장치 등과의 연결에도 사용된다.\n04 기본 논리 연산 디지털 회로와 프로그래밍에서 사용하는 가장 기본적인 연산으로, 논리 상태(0과 1)를 처리하는 데 사용된다.\n아래는 기본 논리 연산의 주요 종류이다.\n① AND 두 입력이 모두 참(1)일 때만 결과가 참(1)이 되는 연산.\nAND 연산의 진리표\n② OR 입력 중 하나라도 참(1)이면 결과가 참(1)이 되는 연산.\nOR 연산의 진리표\n③ NOT 단일 입력에 대해, 입력이 참이면 결과가 거짓(0), 입력이 거짓이면 결과가 참(1)이 되는 연산.\nNOT 연산의 진리표\n④ XOR 두 입력이 서로 다를 때만 참(1)이 되는 연산. 즉, 둘 중 하나만 참이어야 참이 된다.\nXOR 연산의 진리표\n⑤ NAND AND 연산 후에 결과를 부정(NOT)한 연산. AND의 반대 결과를 생성한다.\nNAND 연산의 진리표\n⑥ NOR OR 연산 후에 결과를 부정(NOT)한 연산. OR의 반대 결과를 생성한다.\nNOR 연산의 진리표\n이러한 논리 연산은 전자 회로와 디지털 컴퓨터에서 기본 연산으로 사용된다.\n간단한 결정이나 조건을 설정하는 데 유용하며, 복잡한 연산도 이러한 기본 연산을 조합하여 처리할 수 있다.\n05 통신 기술 Wi-Fi, Bluetooth, 5G 같은 무선 통신 기술에서 데이터 송수신 과정에서 모뎀 역할을 하는 칩셋이나 모듈이 사용되며, 동기/비동기 통신 방식과 대역폭이 전송 속도와 품질에 중요한 영향을 미친다.\n서버 기반의 제어 시스템을 조작할 때는 C++과 같은 언어를 활용하여 GUI를 개발하거나 데이터를 효율적으로 처리할 수 있으며,\n특히 실시간 데이터 처리와 고성능 연산이 요구되는 환경에서 유용하다.\n① 제어 흐름 관리 for 문과 if-else 문을 사용하며, 이를 통해 데이터를 수집하거나 명령을 실행하는 프로세스를 제어하는 코드를 작성한다.\n이러한 방식은 시리얼 통신, 네트워크 통신 등 다양한 통신 방식에 맞춰 데이터 패킷을 처리하거나, 사용자 입력을 관리하여 프로그램의 동작을 유연하게 제어하는 데 중요한 역할을 한다.\n② C++의 활용 C++는 성능과 메모리 관리를 직접 제어할 수 있는 언어로, 이를 활용하면 실시간 통신 시스템에서 응답성을 크게 향상시킬 수 있다.\n특히, 서버 제어 시스템에서 통신 장비와 상호 작용하는 프로그램을 개발할 때, C++의 효율성과 네트워크 라이브러리를 활용하면 안정적이고 빠른 성능을 달성할 수 있다.\n③ 디지털 (Digital) 0과 1과 같은 이진 값을 사용하여 정보를 표현한다.\n디지털 신호는 일정한 시간 간격으로 구분된 이진 상태를 나타내며, 최소 단위는 비트이다.\n이진 상태가 “0”과 “1”로만 표현되므로, 값은 이산적이다.\n④ 아날로그 (Analog) 시간에 따라 연속적으로 변화하며, 0과 1 사이의 모든 값을 가질 수 있다.\n06 데이터 단위\n① 비트 (Bit) 0과 1로 이루어진 가장 작은 데이터 단위. 디지털 정보의 기본 단위로, 두 가지 상태를 표현할 수 있다.\n② 니블 (Nibble): 4비트 1니블은 4개의 비트로 구성된다. 보통 1바이트를 2니블로 나누는 데 사용된다.\n③ 바이트 (Byte): 8비트 1바이트는 8비트로 이루어지며, 컴퓨터에서 문자나 데이터를 표현하는 기본 단위로 많이 사용된다.\n④ 워드 (Word) 프로세서가 한 번에 처리할 수 있는 데이터의 크기로, 시스템 아키텍처에 따라 달라진다.\n워드 크기는 아키텍처에 따라 대체로 위와 같이 설정된다.\n과거에는 16비트 였던 워드 크기가 현재는 32비트 혹은 64비트로 변화되어 사용된다.\n07 진법 컴퓨터와 디지털 시스템에서는 데이터를 다양한 진법으로 표현한다.\n각 진법은 사용되는 숫자의 개수와 자리 값을 기반으로 데이터를 표현하며, 특정 진법은 특정 상황에서 유용하다.\n① 2진수(Binary)\n숫자 2를 기반으로 하는 진법으로, 0과 1의 두 가지 숫자만 사용된다. 디지털 회로와 컴퓨터 내부 연산에서 가장 기본적인 진법이다.\n2의 거듭제곱으로 자리값이 결정된다.\n​\n② 8진수(Octal) 숫자 8을 기반으로 하는 진법으로, 0 ~ 7의 8가지 숫자를 사용한다. 3개의 이진수 그룹을 한 자리의 8진수로 표현할 수 있어, 컴퓨터 시스템에서 간단한 데이터 표현에 사용된다.\n8의 거듭제곱으로 자리값이 결정된다.\n​\n③ 10진수 (Decimal) 우리가 일상생활에서 사용하는 기본 진법.\n숫자 10을 기반으로 하는 진법으로, 0 ~ 9의 숫자를 사용한다.\n10의 거듭제곱으로 자리값이 결정된다.\n​\n④ 16진수 (Hexadecimal)\n숫자 16을 기반으로 하는 진법으로, 숫자 0 ~ 9와 문자 A ~ F를 사용한다.\n문자 A ~ F 는 10 ~ 15의 값을 나타낸다.\n4개의 이진수 그룹을 한 자리의 16진수로 표현할 수 있어, 메모리 주소나 색상 표현 등에 자주 사용된다.\n16의 거듭제곱으로 자리값이 결정된다.\n08 8비트의 정의 8자리의 2진수로 표현되며, 각 자리의 가중치는 다음과 같다:\n왼쪽에서 오른쪽으로 자리의 가중치가 점점 작아진다.\n① 값의 범위\n8비트의 최소값과 최대값\n총 가능한 값의 개수는 2⁸ = 256 가지이다. 11111111₂은 각 자리의 가중치를 모두 더하는 방식으로 계산된다.\n② 응용 8비트의 범위는 0 ~ 255로, 컴퓨터가 데이터를 저장하거나 표현할 때 자주 사용된다.\n색상 표현에서 RGB 값은 각각 0 ~ 255로 표현된다. 문자 저장에 사용되는 ASCII 코드도 보통 8비트를 사용한다."
  },
  {
    "objectID": "pd/od/od_06.html",
    "href": "pd/od/od_06.html",
    "title": "개발 일지4",
    "section": "",
    "text": "재실 감지 시스템을 개발하기 위한 기초 개념에 대해 다루고자 한다.\n01 기본 개념\n1 . 비동기 프로그래밍 (async/await) FastAPI는 비동기 처리를 지원하므로 기본적인 개념을 이해해야 한다.\n2 . 데이터베이스 개념 SQL 기본 문법 (SELECT, INSERT, UPDATE, DELETE)\n관계형 데이터베이스 (MySQL, PostgreSQL, SQLite 등)\nhttps://www.w3schools.com/sql/\n02 FastAPI 기본 사용법 FastAPI 설치 및 프로젝트 구조\n기본적인 API 엔드포인트 만들기 (@app.get(), @app.post())\nPydantic을 이용한 데이터 검증 (BaseModel)\nFastAPI에서 비동기(async def) 사용법\nfrom fastapi import FastAPI\napp = FastAPI()\n@app.get(“/”) async def home(): return {“message”: “Hello, FastAPI!”}\nFastAPI\nFastAPI framework, high performance, easy to learn, fast to code, ready for production\nfastapi.tiangolo.com\nTutorial - User Guide - FastAPI\nFastAPI framework, high performance, easy to learn, fast to code, ready for production\nfastapi.tiangolo.com\n03 SQLModel을 활용한 데이터베이스 연동 SQLModel 설치 및 기본 사용법\nORM 개념 이해하기 (객체를 데이터베이스 테이블과 매핑)\n데이터베이스 모델 정의 및 CRUD(Create, Read, Update, Delete) 구현\nSQLite로 간단한 DB 실습 후, MySQL/PostgreSQL 연동\nSQLModel\nSQLModel, SQL databases in Python, designed for simplicity, compatibility, and robustness.\nsqlmodel.tiangolo.com\n04 재실 감지 시스템 API 설계 및 구현 RESTful API 설계 방법\nAPI 요청 및 응답 데이터 형식(Pydantic)\nCRUD API 구현\nAPI 테스트(Postman, Curl 활용)\nfrom fastapi import FastAPI, Depends from sqlmodel import Session, select\napp = FastAPI()\n@app.post(“/rooms/{room_id}/status”) async def update_room_status(room_id: int, is_occupied: bool, session: Session = Depends(get_session)): room = session.exec(select(RoomStatus).where(RoomStatus.id == room_id)).first() if room: room.is_occupied = is_occupied session.commit() return {“message”: “Room status updated”} return {“error”: “Room not found”}\n05 실시간 감지 시스템과 연동 (추가 학습) WebSocket을 이용한 실시간 데이터 전송\n센서 데이터 연동 (IoT 장치와 연결)\n백엔드에서 프론트엔드와 통신 (웹 애플리케이션 또는 모바일 앱과 연동)\nWebSockets - FastAPI\nFastAPI framework, high performance, easy to learn, fast to code, ready for production\nfastapi.tiangolo.com\nMQTT - The Standard for IoT Messaging\nWhy MQTT? Lightweight and Efficient MQTT clients are very small, require minimal resources so can be used on small microcontrollers. MQTT message headers are small to optimize network bandwidth. Bi-directional Communications MQTT allows for messaging betwe\nmqtt.org 추가 학습 방향 ✅ Docker와 배포 방법\nDocker로 FastAPI + SQLModel 앱 컨테이너화\n서버에 배포 (AWS, GCP, Heroku 등)\n✅ 보안 및 인증\nJWT 인증 방식 (OAuth2)\nAPI 보안 (CORS, CSRF 방어)\n\nhttps://pypi.org/project/paho-mqtt/"
  },
  {
    "objectID": "prog-dev.html",
    "href": "prog-dev.html",
    "title": "프로그래밍·개발",
    "section": "",
    "text": "개발 일지4\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n개발 일지3\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n개발 일지2\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n개발 일지\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFastAPI\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGit 사용법\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n함수형 프로그래밍\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(1)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(2)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(3)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(4)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(5)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(6)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact를 이용한 Blog 만들기(6)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\n비동기 자바스크립트\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2. JavaScript forReact\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 5, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\n개론\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\n\n\n\n\n\n\n\nReact 설치\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nLearning React: Modern Patterns for Developing React Apps\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pd/re/re_10.html",
    "href": "pd/re/re_10.html",
    "title": "React를 이용한 Blog 만들기(6)",
    "section": "",
    "text": "MongoDB를사용하여, 데이터베이스에서 단일 데이터를 삽입하는 작업인 Insert One 실습에 대해 다루고자 한다.\n1 . 관계형 데이터베이스 (Relational Database) 데이터를 표(테이블) 형태로 구성하며, 각 테이블은 열(컬럼)과 행(레코드)으로 이루어져 있다. 열은 데이터의 속성을, 행은 데이터의 값을 나타낸다.\n위에 데이터베이스가 있고 아래에 표가 있음 SNS 발달 이전은 모드 관계형 데이터베이스였음 일종의 파일로 관리를 했는데 이는 기업에서 문제가 있었음 바로, 갱신이 제대로 안 되는 문제가 있었기에 이를 보안하고자 관계형 데이터베이스라는 것을 만듦.\nSQL (Structured Query Language) 데이터를 삽입(INSERT), 조회(SELECT), 수정(UPDATE), 삭제(DELETE)하는 데 사용된다. 데이터베이스를 생성하고 테이블 구조를 정의하는 데에도 사용된다.\n데이터베이스용 문법을 사용해야 한다.\n중복 데이터 비허용 데이터 무결성(Data Integrity)을 유지하기 위해 중복 데이터를 방지해야 한다. 기본 키(Primary Key) 또는 유니크 키(Unique Key)를 설정하여 테이블 내에서 중복을 방지한다.\n정규화 (Normalization) 데이터를 논리적으로 나누어 중복을 제거하고, 데이터의 무결성과 효율성을 높이는 과정이다. 정규화 단계를 거치면 데이터의 불필요한 중복이 제거되고 유지보수가 쉬워진다.\n정확도가 중요한 이유 관계형 데이터베이스는 금융, 의료, 제조 등 데이터의 정확성과 신뢰성이 중요한 분야에서 많이 사용된다. ACID(Atomicity, Consistency, Isolation, Durability) 속성을 지원하여 데이터의 신뢰성을 보장한다.\n보통 MySQL(무료, 소규모 웹사이트), PostgreSQL, Oracle(주로 사용, 비쌈, 대규모 프로젝트), Microsoft SQL Server , 아마존Sql같은 시스템이 관계형 데이터베이스로 많이 쓰인다.\n\n비관계형 데이터베이스 (NoSQL) 데이터를 유연한 구조로 저장할 수 있는 데이터베이스.\n\n관계형 데이터베이스처럼 고정된 스키마가 없으므로, 데이터의 구조를 미리 정의하지 않아도 되며, 다양한 데이터 형식을 지원한다.\n키-값, 문서(Document), 컬럼(Column), 그래프(Graph) 등.\nRedis 데이터 저장 방식: Key-Value 구조. 예: { “username”: “john_doe”, “score”: 100 }\n속도가 빠르고, 캐싱(Cache), 실시간 데이터 처리에 적합하다. 메모리 기반 데이터 저장소로, 데이터를 휘발성(메모리 저장) 또는 영구적(디스크 저장)으로 저장 가능.\nMongoDB Document 기반의 데이터 저장 방식을 사용하며, 이때 데이터는 JSON 형식으로 저장한다.\n{ “name”: “Alice”, “age”: 30, “hobbies”: [“reading”, “cycling”] }\n데이터 구조가 유연하며, 관계형 데이터베이스처럼 테이블 간 조인을 강제하지 않는다. 대량의 데이터를 처리하기 용이하며, 비정형 데이터(예: 로그, 소셜 미디어 데이터)에 적합하다.\n고정된 테이블 스키마 필요 스키마 자유로움 데이터 정확성 우선 유연성과 확장성 우선 SQL 사용 각 DB마다 다른 쿼리 방식 트랜잭션 처리 강력 빠른 읽기/쓰기 성능 제공\n사용 사례\nRedis: 세션 관리, 실시간 채팅, 게임 점수판, 캐시.\nMongoDB: 비정형 데이터 저장, 콘텐츠 관리 시스템(CMS), 로그 데이터 분석.\n이런 비관계형 데이터베이스는 특히 빅데이터나 실시간 응용 프로그램에서 강점을 발휘한다.\n왜 비관계형 데이터베이스가 적합한가?\n데이터 크기\nSNS 플랫폼에서는 매일 수십억 건의 텍스트, 이미지, 동영상 데이터가 생성됩니다. 관계형 데이터베이스는 대규모 데이터를 처리하기 위해 확장이 어렵고, 속도가 느려질 수 있습니다. 비관계형 데이터베이스는 수평적 확장(Scale-out)이 가능하여 서버를 추가함으로써 용량과 성능을 늘릴 수 있습니다.\n데이터 구조의 다양성\nSNS 데이터는 텍스트, 이미지, 동영상, 댓글, 해시태그 등 다양한 형태로 존재합니다. 비정형 데이터(동영상, 이미지 메타데이터 등)는 관계형 데이터베이스의 정형화된 스키마에 맞추기 어렵습니다. 비관계형 데이터베이스는 스키마가 없거나 유연하여 다양한 데이터를 쉽게 저장할 수 있습니다.\n속도와 실시간 처리\nSNS는 사용자가 게시물 업로드, 좋아요, 댓글 등을 실시간으로 처리해야 합니다. 비관계형 데이터베이스는 높은 쓰기/읽기 성능을 제공하여 실시간 데이터 처리가 가능합니다.\n확장성\n관계형 데이터베이스는 주로 수직적 확장(Scale-up)으로 성능을 높여야 하지만, 이는 비용이 많이 듭니다. 비관계형 데이터베이스는 수평적 확장(Scale-out)을 통해 저비용으로도 서버를 추가해 성능을 높일 수 있습니다.\nSNS에서 자주 사용되는 비관계형 데이터베이스\nCassandra\n유튜브: 대규모 동영상 메타데이터 관리에 사용. 분산 시스템으로 높은 가용성과 확장성을 제공.\nMongoDB\n인스타그램: 유저 프로필, 사진 메타데이터 저장. JSON 기반 데이터 처리에 최적화.\nHBase\n페이스북: 메시징 시스템과 같은 대량 데이터 저장에 사용. Hadoop 기반으로 동작하며, 빅데이터 처리에 강점.\nRedis\n캐싱 및 실시간 데이터 분석에 활용. 예: 실시간 좋아요 수 카운팅.\n데이터 규모 이해 1페타바이트(1PB)는 1,024 테라바이트(TB)로, 약 50억 개의 고화질(HD) 사진을 저장할 수 있는 용량이다. 이처럼 초대형 데이터가 매일 쌓이는 환경에서는 유연하고 확장성 있는 비관계형 데이터베이스가 필수적이다.\nSNS와 같은 플랫폼은 비관계형 데이터베이스를 통해 데이터를 효율적으로 관리하며, 사용자의 경험을 실시간으로 최적화하고 있다!\n비관계형 데이터베이스의 장점 1. 빠른 데이터 입출력\n데이터를 저장할 때 정규화(Normalization) 과정을 거치지 않으므로, 데이터를 바로 저장할 수 있다. 관계형 데이터베이스에서 테이블을 나누고 조합(join)하는 과정을 피할 수 있어 속도가 매우 빠르다.\n\n단순한 데이터 구조\n\n데이터 구조를 복잡하게 설계하지 않아도 된다. 개발 초기 단계에서 빠르게 시스템을 구축하고 테스트할 수 있다.\n단점 1. 데이터 정확성 문제\n중복 데이터가 저장될 가능성이 높다. 예를 들어, 사용자 이름을 여러 곳에 저장해 놓은 경우, 한 곳에서 이름을 수정하면 모든 관련 데이터를 직접 수정해야 한다.\n데이터 정합성(Consistency)을 유지하기 어렵다.\n\n유지 보수 어려움\n\n데이터 구조를 정규화하지 않은 상태에서 시스템이 커지면, 수정 작업이 복잡해지고 시간이 많이 소요될 수 있다.\n중복된 데이터를 처리하다가 오류가 발생할 가능성이 높아진다.\n비관계형 데이터베이스가 적합한 경우\n빠른 입출력 속도가 중요한 애플리케이션. 예: 게시판, SNS, 실시간 채팅.\n데이터 정확성이 상대적으로 덜 중요한 경우. 예: 로그 데이터, 캐시, 임시 저장소.\n관계형 데이터베이스가 적합한 경우\n데이터 정합성과 정확도가 중요한 경우. 예: 금융 시스템, 재고 관리, 전자 상거래.\n데이터 구조가 명확히 정의되어 있고, 테이블 간 관계가 중요한 경우.\n\nMongo-DB 이전에는 mysql로 웹사이트를 만들었음 위 db를 사용한다면 SQL 배울 필요가 없으며, 테이블스키마를 생성 안 해도 된다.\n\n이는 Document 데이터베이스라고도 불린다.\n\nDocument 기반 데이터베이스\n\n데이터를 JSON과 유사한 형식(BSON, Binary JSON)으로 저장한다. 관계형 데이터베이스처럼 행(Row)과 열(Column)이 아니라, 데이터가 Document(문서) 단위로 관리된다.\n데이터 구조가 유연하므로, 다양한 형태의 데이터를 저장할 수 있다.\n\n테이블 스키마 필요 없음\n\n사전에 테이블 구조를 정의할 필요가 없다. 필드와 데이터 타입은 문서마다 달라도 된다.\n{ “name”: “Alice”, “age”: 25, “email”: “alice@example.com” } { “name”: “Bob”, “hobbies”: [“cycling”, “painting”] } 이런 유연성 덕분에 빠르게 프로토타입을 만들거나 동적 데이터 처리에 유리합니다.\n사용 사례\n게임 데이터, IoT 센서 데이터, 비정형 데이터 로그, 사용자 프로필 정보 등.\nMongoDB 데이터 저장 방식 1. Database(데이터베이스)\n데이터를 저장하는 최상위 단위입니다. 예: myDatabase\n\nCollection(컬렉션)\n\n데이터의 그룹. 관계형 데이터베이스의 테이블과 유사하다. 예: users라는 컬렉션에 사용자 정보를 저장한다.\n\nDocument(문서)\n\n개별 데이터 단위. JSON 형태로 저장.\n{ “name”: “John Doe”, “age”: 30 }\n초보자와 숙련자 사용법 1. 로컬 설치 (숙련자)\nMongoDB를 직접 컴퓨터에 설치하여 커스터마이징 가능하다. 복잡한 설정을 다룰 수 있는 개발자에게 적합하다.\n\n클라우드 서비스 추천 (초보자)\n\nMongoDB Atlas를 사용하면 클라우드에서 바로 시작할 수 있다. 무료로 소규모 프로젝트를 호스팅할 수 있으며, 필요 시 비용을 지불해 업그레이드 가능하다.\n백업, 보안 설정, 검색 기능 등 추가 서비스를 자동으로 제공한다.\n주요 기능 1. Replica Set\n데이터가 3개의 노드로 자동 복제되므로, 하나의 노드가 고장 나더라도 데이터 손실이 없다. 이를 통해 고가용성(High Availability)을 제공한다.\n\nFull-Text Search\n\n문서 내 텍스트 데이터를 빠르게 검색할 수 있는 인덱스 생성 가능하다. 클라우드 호스팅(MongoDB Atlas)을 통해 더 쉽게 구현 가능하다.\n결론적으로, MongoDB는 관계형 데이터베이스와는 다른 자유로운 설계와 확장성 덕분에 스타트업, 빅데이터 처리 및 애자일 개발 환경에 특히 유리하다.\n\nMongo-DB 사용하기 나는 초보자이므로, 클라우드에서 호스팅 받아 쓰는 것을 진행해 보겠다.\n\nMongoDB: 개발자 데이터 플랫폼\n업계를 선도하는 모던 데이터베이스를 토대로 구축된 애플리케이션 데이터 플랫폼을 사용해 아이디어를 더욱 빠르게 실현하세요. MongoDB는 데이터를 손쉽게 처리할 수 있도록 지원합니다.\nwww.mongodb.com\nMongoDB를 직접 컴퓨터에 설치하지 않을 것이므로, 다운로드가 아닌 왼쪽에 위치한 등록 버튼을 클릭한다.\n본인이 원하는 방법으로 가입한다.\n약관 허용 여부를 선택하고 제출한다.\n그러면 다음 화면이 잠시 보일 것이다.\n본인의 목적에 따라 다음 설문을 진행하면 된다.\n나는 다음과 같이 작성했다.\n첫 프로젝트이므로, 무료 버전을 사용하겠다.\n본인이 원하는 이름을 쓰고, Provider을 선택하면 된다. 단, Region에 경우, Seoul로 해야 속도가 빠르므로 이 설정은 바꾸지 않는 것을 권장한다.\n준비가 되었다면, Create Deployment를 클릭한다.\nUsername에는 본인의 계정 아이디가 적혀 있고, Password에는 자동으로 비밀번호가 생성되어 있을 것이다.\n이 부분은 본인이 자유롭게 바꾸면 되는데, 비밀번호에 경우, 웹서버 연동에 써야 하므로 꼭 기억해 놓기를 바란다.\n보안이 중요하지 않다면 비밀번호는 단순하게 설정해도 무방하다.\nCreate Database User - Choose a connection method 순으로 클릭한다.\n첫 화면\nNetwork Access - +ADD IP ADDRESS를 클릭한다.\n전체 포트 번호를 지정해주며, 아래 경고는 모든 곳에서 접속할 수 있으니 주의하라는 메세지이다.\nConfirm을 클릭한다.\nClusters - Browse Collections - + Create Database 순으로 클릭한다.\n자유롭게 지으면 된다.\nCreate를 클릭한다.\n위 과정을 끝내면, 다음과 같은 화면이 보일 것이다.\n지금까지 데이터베이스를 설정하는 과정이었다. 이제 데이터베이스를 연결하여 구동해볼 것이다.\nDB연동하기 post로 나옴, 웹 서버를 통해 데이터 입력하는 과정을 진행하겠다. 먼저, VS코드 프롬프트에 mongodb 버전 5를 설치한다.\n이 버전은 최신 버전은 아니지만, 이미 검증되어 안정적이므로, 구버전을 설치하도록한다.\nnpm install mongodb@5\nmongodb 설치 완료.\n이전에 VS코드 파일인 Server.js에 아래의 코드를 추가한다.\n// 서버 포트 설정(이전에 작성된 코드) app.listen(8080, function () { console.log(‘listening on 8080’); });\n// 이번에 작성할 코드 const { MongoClient } = require(‘mongodb’);\nlet db; const url = ‘DB접속 URL’; // 여기에 실제 접속 URL을 넣는다. new MongoClient(url).connect().then((client) =&gt; { console.log(‘DB연결 성공’); db = client.db(‘todoapp’); // ‘todoapp’ 데이터베이스를 선택한다. }).catch((err) =&gt; { console.log(‘DB 연결 실패:’, err); }); 어떠한 용어를 쓰던 매우 흡사함.\nClusters - Connect 순으로 클릭하기.\n데이터를 안전하게 주고받기 위해서는 코드로 감시자를 설정하여 보안을 강화해야 하며, 이곳에 접속을 시도하려면 서버 주소를 알고 있어야 한다.\n클라우드 호스팅 서비스를 사용하면, 무료로 서버를 빌릴 수 있다. 이 경우에도 접속하려면 호스팅된 서버의 주소를 알아야 한다.\n127.0.0.1은 로컬 주소로, 내 컴퓨터를 의미한다. 원격 서버에 접속하려면, 해당 서버의 IP 주소 또는 도메인 이름을 반드시 알아야 한다.\n하단에 본인의 웹 서버 주소가 있을 것이다.\nadmin은 지정된 이름이며, ‘’ 자리에 이전에 본인이 설정한 비밀번호를 기입한다.\n몽고디비 호스팅 서버가 이걸로 접속하세요 하고 알려주는 것이므로 사람마다 다 다르다.\n위 웹 주소를 복사하여 VS코드에 ‘DB 접속 URL’ 부분에 붙여 넣는다. Ctrl + S를 눌러 저장한 뒤, nodemon server.js를 입력한다.\n연결이 될 경우, ‘DB 연결 완료’ 메세지가 나온다.\n이제 가장 아래 부분에 다음 코드를 작성한다.\napp.get(‘/news’, function (req, res) { db.collection(‘post’).insertOne({title:‘프론트웹개발’}) res.send(‘DB에 데이터 삽입’) });\n저장을 한 뒤, 크롬 URL에 http://localhost:8080/news라고 입력하면 다음과 같은 화면이 보일 것이다.\n그리고 다음과 같이 DB에 데이터가 생성된 것을 확인할 수 있다.\n마지막으로 데이터를 데이터베이스에서 직접 생성한 뒤, 이를 출력하는 과정을 진행해보겠다.\n본인의 화면 우측에 Insert Document 버튼을 클릭한다.\n아래와 같이 입력해본다.\nInsert 버튼을 클릭한다.\n그러면, 다음과 같이 데이터가 추가된 것을 볼 수 있다.\n이제 VS코드로 돌아가서 아래의 코드를 최하단에 입력한다.\napp.get(‘/post’, async(req, res) =&gt; { // 익명 함수 let result = await db.collection(‘post’).find().toArray(); console.log(result[1].title) res.send(result[1].title) });\n저장하면, 프롬프트에는 다음과 같이 나온다.\n그리고 크롬을 새로고침 하면 다음과 같이 나온다.\n이것으로 Insert One 실습을 마치도록 한다."
  },
  {
    "objectID": "pd/re/re_08.html",
    "href": "pd/re/re_08.html",
    "title": "React를 이용한 Blog 만들기(4)",
    "section": "",
    "text": "1 . 동적인 UI 만들기 사용자가 발행한 글 다루기 생성하는 기능과 삭제하는 기능\nfunction App() { let [글제목, 글제목변경] = useState([“소고기 맛집-토방”, “불고기 맛집-마포숯불갈비”, “한식 맛집-백만석”]); let [따봉, 따봉변경] = useState(0); let [선택된제목, 선택된제목변경] = useState(글제목[0]); // 초기값 설정 let [modal, setModal] = useState(false);\nif 또는 for와 같은 기본 문법을 사용할 수 없음,\n  { \n    // 이 영역은 html 코드로 작성하는 곳으로 기본문법 if를 쓸 수 없다.\n    (modal == true) ? &lt;Modal/&gt; : null\n  }\n삼항 연산자만 사용가능 ? 또는 : 파이썬에는 없고, 자바와 C언어에서 쓰인다.\n(조건식) ? (참) : (거짓)으로 구성되어 있다.\n앱을 만들기 어려운 이유가 여러 개의 문법을 동시에 사용해야 되기 때문이다. DB 연동 및 쿼리 이용하는 것까지가 기본적으로 되어 있어야 한다.\n프로파게이션: 전파하다, 즉, 전파를 막다, 이벤트 버블링\n코드가 많으면 관리하기 어려움, 이를 줄여주는 map함수를 통해 자동으로 코드를 줄일 수 있다.\n글제목이름 이라는 변수이름으로 만듦 글제목이라는 변수로 상태이름을 저장한 것임, 괄호 안에 작성, 맵이 처리가 될 때 처리\nimport React, { useState } from ‘react’; import ‘./App.css’;\n// Modal 컴포넌트 정의 function Modal() { return (\n\n  &lt;h2&gt;&lt;/h2&gt;\n  &lt;p&gt;&lt;/p&gt;\n&lt;/div&gt;\n); }\nfunction App() { let [글제목, 글제목변경] = useState([“소고기 맛집-토방”, “불고기 맛집-마포숯불갈비”, “한식 맛집-백만석”]); let [따봉, 따봉변경] = useState(0); let [선택된제목, 선택된제목변경] = useState(글제목[0]); // 초기값 설정 let [modal, setModal] = useState(false);\nreturn (\n\n  &lt;div className=\"black-nav\"&gt;\n    &lt;div&gt;블로그&lt;/div&gt;  \n  &lt;/div&gt;\n\n  {\n    /*&lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; setModal(turn)}&gt;\n        {글제목[0]} &lt;span onClick={(e) =&gt; {e.stopPropagation(); 따봉변경(따봉 + 1)}}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 4일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; setModal(turn)}&gt;{글제목[1]}\n        {글제목[1]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 5일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; setModal(turn)}&gt;{글제목[1]}\n        {글제목[2]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 6일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;*/\n  }\n\n  {글제목.map(function (a, i) {\n    return (\n      &lt;div className='list' key={i}&gt;\n        &lt;h4 onClick={() =&gt; { setModal(true); }}&gt;\n          {a} \n          &lt;span onClick={(e) =&gt; { e.stopPropagation(); 따봉변경(따봉 + 1); }}&gt;👍&lt;/span&gt;\n          {따봉}\n        &lt;/h4&gt;\n        &lt;p&gt;11월 {i + 4}일 발행&lt;/p&gt;\n        &lt;hr /&gt;\n      &lt;/div&gt;\n    );\n  })}\n\n  { \n    // 이 영역은 html 코드로 작성하는 곳으로 기본문법 if를 쓸 수 없다.\n    (modal == true) ? &lt;Modal/&gt; : null\n  }    \n\n  &lt;/div&gt;\n); }\nexport default App;\n앱이 부모 모달이 자식\n• Type –text :텍스트 입력상자 –range : 범위 입력상자 –checkbox :선택 입력상자 –date :날짜 입력상자\n•입력상자의 내용이 변경되면 –onchange={() - &gt; { } }\n•입력된 내용을 가져오고 싶을땐 –onchange={(e) - &gt; {console.log(e.target.value) } }\nfunction App() { let [글제목, 글제목변경] = useState([“소고기 맛집-토방”, “불고기 맛집-마포숯불갈비”, “한식 맛집-백만석”]); let [따봉, 따봉변경] = useState(0); let [선택된제목, 선택된제목변경] = useState(글제목[0]); // 초기값 설정 let [modal, setModal] = useState(false); let [입력값, 입력값변경] = useState(’’)\n입력값 state에 저장하기 - 입력값을 처리하기 위한 state생성 버튼에 이벤트 등록 이벤트 등록 코드 작성하기 –원본 state를 그대로 사용하면 안됨!! –사본 state를 만들기 –사본의 첫부분에 입력된 내용을 추가하기(unshift()) –state변경함수로 상태 변경하기\n응용 – 발행된 글 아래에 삭제 버튼을 추가하기. 또한, 발행된 글은 발행된 날짜로 나오도록 추가.\nimport React, { useState } from ‘react’; import ‘./App.css’;\nfunction App() { let [글제목, 글제목변경] = useState([ “소고기 맛집-토방”, “불고기 맛집-마포숯불갈비”, “한식 맛집-백만석” ]); let [따봉, 따봉변경] = useState([0, 0, 0]); // 각 글에 대한 따봉을 배열로 관리 let [선택된제목, 선택된제목변경] = useState(글제목[0]); // 초기값 설정 let [modal, setModal] = useState(false); let [입력값, 입력값변경] = useState(’’);\n// 제목 추가하는 함수 const handleAddTitle = () =&gt; { let copy = […글제목]; copy.unshift(입력값); // 입력값을 맨 앞에 추가 글제목변경(copy); // 상태 업데이트 따봉변경([0, …따봉]); // 새로 추가된 글에 대한 따봉 초기화 };\nreturn (\n\n  &lt;div className=\"black-nav\"&gt;\n    &lt;div&gt;블로그&lt;/div&gt;  \n  &lt;/div&gt;\n\n  {/* 제목 추가 */}\n  &lt;input \n    type=\"text\" \n    value={입력값} \n    onChange={(e) =&gt; 입력값변경(e.target.value)} \n  /&gt;\n  &lt;button onClick={handleAddTitle}&gt;제목 추가&lt;/button&gt;\n\n  {\n    /* &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; setModal(turn)}&gt;\n        {글제목[0]} &lt;span onClick={(e) =&gt; {e.stopPropagation(); 따봉변경(따봉 + 1)}}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 4일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; setModal(turn)}&gt;{글제목[1]}\n        {글제목[1]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 5일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; setModal(turn)}&gt;{글제목[1]}\n        {글제목[2]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 6일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt; */\n  }\n\n  {글제목.map((a, i) =&gt; (\n    &lt;div className='list' key={i}&gt;\n      &lt;h4 onClick={() =&gt; { setModal(true); 선택된제목변경(a); }}&gt;\n        {a} \n        &lt;span onClick={(e) =&gt; { e.stopPropagation(); 따봉변경(prev =&gt; { \n          const newThumbs = [...prev]; \n          newThumbs[i] = newThumbs[i] + 1;\n          return newThumbs;\n        }); }}&gt;👍&lt;/span&gt;\n        {따봉[i]}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 {i + 4}일 발행&lt;/p&gt;\n      &lt;hr /&gt;\n    &lt;/div&gt;\n  ))}\n\n  { \n    // 이 영역은 html 코드로 작성하는 곳으로 기본문법 if를 쓸 수 없다.\n    (modal == true) ? &lt;Modal title={선택된제목} /&gt; : null\n  }    \n&lt;/div&gt;\n); }\nfunction Modal({ title }) { return (\n\n  &lt;h2&gt;{title}&lt;/h2&gt;\n\n  &lt;p&gt;발행 날짜: \n    {title === '소고기 맛집-토방' ? '11월 4일' : \n    title === '불고기 맛집-마포숯불갈비' ? '11월 5일' : \n    title === '한식 맛집-백만석' ? '11월 6일' : ''}\n  &lt;/p&gt;\n\n  &lt;p&gt;이곳은 {title}에 대한 상세내용입니다.&lt;/p&gt;\n&lt;/div&gt;\n); }\nexport default App;\n/* eslint-disable */ import React, { useState } from ‘react’; //import Hello from ’ ./component/Hello’; import logo from ‘./logo.svg’; import ‘./App.css’;\nfunction App() {\nlet [글제목, 글제목변경] = useState([‘소고기 맛집 - 토방’,‘불고기 맛집 - 마포숯불갈비’,‘한식맛집 - 백만석’]); let [따봉, 따봉변경] = useState(0); let [선택된제목, 선택된제목변경] = useState(null); // 선택된 제목을 관리하는 상태 추가 let [modal,setModal] = useState(false); // Modal의 상태를 관리하기 위한 상태 let [입력값, 입력값변경] = useState(’’); //입력값을 저장하기 위한 상태 return (\n\n     &lt;div className=\"black-nav\"&gt;\n        &lt;div&gt;대진대 맛집 소개&lt;/div&gt;    \n     &lt;/div&gt;\n    { /*&lt;div className=\"list\"&gt;\n        &lt;h4 onClick={() =&gt; setModal(true)}&gt;\n           {글제목[0]} &lt;span onClick={(e) =&gt; { e.stopPropagation(); 따봉변경(따봉 + 1); }}&gt;&lt;/span&gt;{따봉}\n        &lt;/h4&gt;\n        &lt;p&gt;11월 4일 발행&lt;/p&gt;\n        &lt;hr /&gt;\n     &lt;/div&gt;\n     &lt;div className=\"list\"&gt;\n        &lt;h4 onClick={() =&gt;{setModal(true)}}&gt;{글제목[1]}&lt;/h4&gt;\n        &lt;p&gt;11월 5일 발행&lt;/p&gt;\n        &lt;hr /&gt;\n     &lt;/div&gt;\n     &lt;div className=\"list\"&gt;\n        &lt;h4 onClick={() =&gt; {setModal(true)}}&gt;{글제목[2]}&lt;/h4&gt;\n        &lt;p&gt;11월 6일 발행&lt;/p&gt;\n        &lt;hr /&gt;\n     &lt;/div &gt;*/}\n\n     {\n        글제목.map(function(a,i){\n           return (\n           &lt;div className=\"list\"&gt;\n           &lt;h4 onClick={() =&gt;{setModal(true); 선택된제목변경(글제목[i])}}&gt;{글제목[i]}&lt;span onClick={(e) =&gt; { e.stopPropagation(); 따봉변경(따봉 + 1); }}&gt;&lt;/span&gt;{따봉}&lt;/h4&gt;\n           &lt;p&gt;11월 5일 발행&lt;/p&gt;\n           &lt;hr /&gt;\n           &lt;/div&gt;\n           )\n        })\n     }\n        &lt;input onChange={(e) =&gt; {입력값변경(e.target.value)}}/&gt;\n        &lt;button onClick={() =&gt; {\n           let copy = [...글제목];\n           copy.unshift(입력값);\n           글제목변경(copy);\n        }}&gt;버튼&lt;/button&gt;\n     {\n        //이 영역은 html코드를 작성하는 곳이라 기본문법 if를 쓸수없다.\n        (modal == true) ? &lt;Modal title = {선택된제목} /&gt; : null\n     }\n  &lt;/div&gt;\n); }\n// 함수형 컴포넌트 function Modal(props) { return (\n\n     &lt;h2&gt;{props.title}&lt;/h2&gt;\n     &lt;p&gt;날짜&lt;/p&gt;\n     &lt;p&gt;상세 내용&lt;/p&gt;\n  &lt;/div&gt;\n); }\nexport default App;"
  },
  {
    "objectID": "pd/re/re_06.html",
    "href": "pd/re/re_06.html",
    "title": "React를 이용한 Blog 만들기(2)",
    "section": "",
    "text": "대표적인 명령어 리눅스에서는 명령어와 옵션을 활용해 다양한 작업을 수행한다.\ncd // (change directory, 디렉토리 이동) md // (make directory, 디렉토리 생성) ..cd // 상위 디렉토리로 이동 지난 시간에는 함수와 이미지가 데이터 바인딩을 통해 연결될 수 있다는 내용까지 다루었다.\nJSX React에서 JavaScript와 HTML을 결합한 문법으로, 컴포넌트 안에서 HTML처럼 보이는 코드를 작성할 수 있게 해준다.\nJSX를 사용하면 JavaScript의 표현력과 HTML의 직관성을 동시에 활용할 수 있어, UI 구성 요소를 작성하는 데 편리하다.\n변환 과정 JSX는 브라우저에서 직접 실행되지 않으므로, 실행 전에 Babel과 같은 도구가 JSX 코드를 JavaScript로 변환한다.\n이를 통해 브라우저는 일반 JavaScript로 처리된 코드를 실행하게 된다.\nXML/HTML과의 차이점 HTML에서는 class 속성을 사용하지만, JSX에서는 className을 사용해야 한다. 이처럼 JSX는 React가 브라우저와 상호작용할 수 있도록 설계된 JavaScript 확장 문법으로, 순수 HTML과는 다른 방식으로 동작한다.\nJSX는 코드 가독성을 높이고, 컴포넌트 기반 UI를 쉽게 구성할 수 있도록 도와준다. 또한 JavaScript의 로직과 HTML 형태의 UI를 결합해 하나의 컴포넌트 안에서 관리할 수 있게 한다.\nReact 요소 React 애플리케이션의 기본 구성 요소. 컴포넌트나 HTML처럼 화면에 표시될 수 있는 단위로, React 요소는 JSX 문법으로 작성된다.\nJavaScript 삽입 JSX 내부에서 JavaScript 표현식을 사용하려면 중괄호 {}를 사용한다.\n예를 들어, &lt;h1&gt;{user.name}&lt;/h1&gt;에서 {user.name}은 JavaScript 표현식이며, 이를 통해 동적으로 데이터를 렌더링할 수 있다.\n자바스크립트 이름 작성 방식 낙타체 (Camel Case) 여러 단어로 구성된 변수나 함수 이름에서 첫 단어는 소문자로 시작하고, 두 번째 단어부터는 첫 글자가 대문자여야 하는 규칙이다.\n예를 들어, myDogName, userAge 등이 낙타체로 작성된 이름이다.\n언더스코어 방식 때로는 언더스코어(_)를 변수명 앞이나 중간에 넣어 작성하는 경우도 있지만, JavaScript에서는 주로 낙타체를 사용하는 것이 관례이다.\n오류 주의 JSX에서는 HTML 속성을 camelCase로 작성해야 한다. 예를 들어, HTML에서는 class를 사용하지만, JSX에서는 className을 사용해야 한다.\nclass를 사용하면 오류가 발생한다.\n\n자체 종료 태그 예를 들어, &lt;img /&gt;와 같이 내용이 없는 태그는 자체 종료 태그로 사용할 수 있다. 하지만 내용이 있을 경우에는 &lt;img&gt;&lt;/img&gt;와 같이 여닫는 태그를 사용해야 한다.\n배열 매핑: 리스트 렌더링 배열을 렌더링하려면 JavaScript의 map() 함수를 사용한다. map() 함수는 배열의 각 항목을 처리하여 새로운 배열을 반환하는 함수이다.\n\nconst items = [‘One’, ‘Two’, ‘Three’]; const list = items.map(item =&gt;\n\n{item}\n\n); 이 코드는 items 배열의 각 항목을 &lt;li&gt; 요소로 매핑하고, 그 결과를 list에 저장한다.\n매개변수 item은 배열의 각 항목을 의미한다. li는 list item의 약자로, &lt;li&gt; 태그를 사용하여 각 항목을 리스트 아이템으로 렌더링한다.\nuseState() React에서 자주 변경되는 데이터를 다룰 때는 useState를 사용하여 상태를 관리하는 것이 일반적인 방법이다.\n반면, 자주 변경되지 않는 데이터는 일반 변수를 사용하여 저장한다.\n사용 방법 useState는 상태(state)를 관리하기 위해 React에서 제공하는 내장 함수를 사용한다.\n이 함수를 사용하면 컴포넌트 내에서 데이터를 상태로 저장하고, 해당 상태가 변경될 때마다 컴포넌트가 재렌더링된다.\n예를 들어, const [count, setCount] = useState(0);는 count라는 상태 변수를 만들고, 이 상태를 변경할 수 있는 setCount 함수를 생성한다.\n이 함수는 호출의 결과를 [변수, 함수]형태의 배열을 반환해준다.\nuseState(0)에서 0은 상태의 초기값을 설정하는 값이다.\nsetCount는 count 값을 변경할 때 사용된다. 버튼 클릭 시 setCount(count + 1)을 호출하여 count를 1씩 증가시킨다.\n실습 이 코드에서는 React의 기능을 사용하여 간단한 블로그 스타일의 컴포넌트를 만들고 있으며, 버튼을 클릭할 때마다 카운트가 증가하는 인터랙티브한 기능이 포함되어 있다.\nimport React, { useState } from ‘react’; import logo from ‘./logo.svg’; import ‘./App.css’;\nfunction App() { let [글제목, 글제목변경] = useState(“남자 코드 추천”); let posts = “대진대 고기 맛집”; const [count, setCount] = useState(0);\nreturn (\n\n  &lt;div className=\"black-nav\"&gt;\n    &lt;div&gt;블로그 BLOG&lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h3&gt;{글제목}&lt;/h3&gt;\n      &lt;p&gt;11월 4일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div&gt;\n    &lt;p&gt;현재 카운트: {count}&lt;/p&gt;\n    &lt;button onClick={() =&gt; setCount(count + 1)}&gt;카운트 증가&lt;/button&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n); }\nexport default App;\n\nuseState Hook 함수형 컴포넌트에서 상태를 관리하기 위한 React의 Hook 아래 코드에서는 두 개의 상태 변수가 선언되어 있다.\n\nlet [글제목, 글제목변경] = useState(“남자 코드 추천”); const [count, setCount] = useState(0);\n\n글제목\n\n문자열 “남자 코드 추천”을 기본값으로 가지는 상태 변수.\n\n글제목변경\n\n글제목의 값을 업데이트할 때 사용하는 함수.\n\ncount\n\n숫자 0을 기본값으로 가지는 상태 변수이며, 카운트 값이 저장된다.\n\nsetCount\n\ncount의 값을 업데이트하는 함수.\n이 함수는 5번에서 익명함수로 호출하여 사용된다.\n함수 사용의 이유는 재사용성 때문이며, 이때 화살표 함수나 익명 함수가 사용된다. 익명 함수나 고차 함수는 고급 JavaScript의 중요한 개념으로, 코드의 간결함과 재사용성을 높여준다.\n비슷한 요구사항이나 구조, 패턴을 해결하는 방식은 디자인 패턴으로 불리며, 이를 통해 코드의 유지보수성과 확장성을 향상시킬 수 있다.\n반응형 웹은 사용자의 상호작용에 따라 동적으로 UI가 변하는 웹을 의미하며, 이는 현대 웹 개발의 추세입니다. React는 이러한 동적인 UI 업데이트를 손쉽게 구현할 수 있도록 도와주는 라이브러리입니다. React를 사용하면, 상태 변화에 따라 UI가 실시간으로 반영되어 반응형 웹 개발을 간단하게 할 수 있다.\n\nJSX React가 화면에 표시할 HTML 요소의 구조와 내용을 정의하는 것으로, 코드에서 return 문 내부에 작성된 내용이 컴포넌트의 UI를 렌더링하는 부분이다.\ndiv 요소 및 클래스명\n\n\n\n\n블로그 BLOG\n\n\n…\n\n최상위 div에 “App” 클래스가 적용되어 App.css 파일에 정의된 스타일이 적용된다. “black-nav”라는 클래스를 가지는 div가 블로그의 상단 내비게이션 역할을 한다.\n\n데이터 바인딩 및 JSX 사용\n\n\n{글제목}\n\n\n11월 4일 발행\n\n\nh3 태그에서 {글제목}을 사용하여 데이터 바인딩을 한다. 이로 인해 글제목의 값 “남자 코드 추천”이 화면에 표시된다.\n\n데이터 바인딩을 위해 글제목 변수를 중괄호 { }로 감싸 JSX 내에서 사용한다.\n\n상태 업데이트 및 이벤트 핸들링\n\n\n\n현재 카운트: {count}\n\n&lt;button onClick={() =&gt; setCount(count + 1)}&gt;카운트 증가\n\n\n요소의 onClick 이벤트 핸들러에서 setCount를 사용하여 count 값을 1씩 증가시킨다.\n버튼이 클릭될 때마다 setCount(count + 1)을 호출하여 count 상태를 업데이트하고, React는 count의 변경을 감지해 p 태그 안의 {count}를 자동으로 업데이트한다.’\n전체 실습 코드 import React, { useState } from ‘react’; import logo from ‘./logo.svg’; import ‘./App.css’;\nfunction App() { // useState의 초기값을 배열로 설정 let [글제목, 글제목변경] = useState([“소고기 맛집-토방”, “불고기 맛집-마포숯불갈비”, “한식 맛집-백만석”]); let posts = “대진대 고기 맛집”; const [count, setCount] = useState(0);\nreturn (\n\n  &lt;div className=\"black-nav\"&gt;\n    &lt;div&gt;대진대 맛집 소개&lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n    &lt;h3&gt;{글제목[0]}&lt;span onClick={() =&gt; {setCount(count + 1)}}&gt;👍&lt;/span&gt;{count}&lt;/h3&gt;\n      &lt;p&gt;11월 4일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h3&gt;{글제목[1]}&lt;span onClick={() =&gt; {setCount(count + 1)}}&gt;👍&lt;/span&gt;{count}&lt;/h3&gt;\n      &lt;p&gt;11월 5일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h3&gt;{글제목[2]}&lt;span onClick={() =&gt; {setCount(count + 1)}}&gt;👍&lt;/span&gt;{count}&lt;/h3&gt;\n      &lt;p&gt;11월 6일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n&lt;/div&gt;\n); }\nexport default App;"
  },
  {
    "objectID": "pd/re/re_04.html",
    "href": "pd/re/re_04.html",
    "title": "함수형 프로그래밍",
    "section": "",
    "text": "(P.68)\n기본적으로 함수들을 통해 프로그램을 구성하는 스타일에 대해 다루고자 한다.\nChapter 3 Functional programming with JavaScript\n패러다임 (Paradigm) 한 시대의 사고 방식이나 가치관, 믿음 체계를 의미한다. 이는 특정 시대나 분야에서 표준으로 여겨지는 틀을 말하며,\n학문, 기술, 예술 등 다양한 분야에서 어떤 접근이나 방법론이 주류로 자리잡고 그것을 중심으로 사고하게 되는 것을 가리킨다.\n프로그래밍 패러다임 (Programming Paradigm) 프로그래머가 코드를 바라보는 관점을 제공하고, 어떻게 문제를 해결하고 코드를 작성할지에 대한 방향을 제시하는 역할을 한다.\n패러다임마다 특정 방식과 철학을 가지고 있어, 각 패러다임이 지향하는 목표에 따라 문제를 다루고 해결하는 방식이 달라진다.\n주요 프로그래밍 패러다임에는 다음과 같은 것들이 있다.\n명령형 프로그래밍 (Imperative programming) 무엇(What)을 할 것인지 나타내기보다 어떻게(How) 할 건지를 설명하는 방식\n\n절차지향 프로그래밍 (Procedural Programming) C 언어, C++ 같은 명령형 프로그래밍 언어는 수행되어야 할 순차적인 처리 과정을 포함하는 방식으로 프로그램을 작성한다.\n\n특히, C 언어는 1970년대에 등장하면서 시스템 프로그래밍의 기반이 되었다.\n그러나 1만 줄 이상의 코드를 작성할 경우 오류를 수정하기 어렵고,\n한 부분을 수정하기 위해 그것과 연관된 다른 줄을 모두 찾아서 수정해야 하는 상황이 발생할 수 있다.\n소프트웨어 공학에서는 소프트웨어 개발 주기가 존재하며, 이 주기에서 가장 비용이 큰 부분은 유지 보수이다.\n유지 보수 비용은 전체 개발 비용의 약 70%를 차지할 정도로 매우 높아서, 효율적인 유지 보수 방안이 중요한 요소로 여겨진다.\n따라서 명령형 프로그래밍 언어는 유지보수를 하기에 적합한 언어가 아니라고 할 수 있다.\n\n객체지향 프로그래밍 (Object-Oriented Programming, OOP) 객체들의 집합으로 프로그램의 상호작용을 표현하는 방식.\n\nC++, Java, C#과 같은 언어들이 대표적이며, 이들 언어는 1980년대에 개발된 이후 현실 세계의 모든 것을\n객체로 바라보는 접근 방식을 채택하고 있다.\n데이터와 그 데이터를 처리하는 메소드를 하나의 단위인 객체로 묶어서 관리함으로써, 코드의 재사용성과 유지보수성을 높이는 데 기여한다.\n특히, 2000년대 이후에는 Java 개발자에 대한 수요가 급증하였다.\n이는 Java가 플랫폼 독립성과 강력한 생태계, 다양한 프레임워크를 제공하므로, 많은 기업들이 Java를 기반으로 한 시스템 개발에 집중하고 있음을 나타낸다.\n이는 소프트웨어 개발의 효율성을 높이고, 복잡한 시스템을 보다 쉽게 관리할 수 있도록 도와준다.\n선언형 프로그래밍 (Declarative Programming) 어떻게 할 것인지를 나타내기보다 무엇을 할 것인지를 설명하는 방식\n함수형 프로그래밍 (Functional Programming) 순수 함수를 조합하여 소프트웨어를 만드는 방식.\n대표적인 언어로는 클로저(Closure), 하스켈(Haskell), 리스프(Lisp)가 있으며, 이 패러다임은 1930년대에 제안된 람다 함수법에 기반한다.\n그러나 그 당시에는 이를 실제로 구현할 수 있는 환경이 마련되지 않았다. 2015년에는 함수형 프로그래밍의 개념이 실제로 구현될 수 있는 기술적 발전이 이루어졌다.\n주요 특징은 거의 모든 것을 순수 함수(Pure function)로 나누어 문제를 해결하는 기법이며, 이는 다음과 같은 장점을 가진다.\n\n복잡한 문제를 작은 문제로 나누어 각각을 해결하기 위한 함수를 작성함으로써, 코드의 구조가 명확해진다.\n각 함수가 명확한 기능을 가지므로, 코드의 가독성이 높아져 유지보수가 용이해진다.\n상태 변화가 없거나 최소화되어 부작용이 줄어드므로, 코드의 신뢰성과 예측 가능성이 향상된다.\n\n이러한 특성은 복잡한 소프트웨어 시스템을 더 쉽게 이해하고 관리할 수 있는 방법으로 자리 잡혀 있다.\n클린 코드 저자 Rober C.Martin의 함수형 프로그래밍에 대한 의견\n깨끗하게 작성하는 방법이란, 할당하는(대입연산자) 문장이 없는 프로그램이다.\n코드만 보더라도 이것이 어떤 기능을 하는지 알 수 있도록 만든다.\n함수형 프로그래밍은 대입문을 사용하지 않는 프로그래밍 방식이며, 작은 문제를 해결하기 위한 함수를 작성하는 데 중점을 둔다.\n예를 들어, 함수는 다음과 같은 두 개의 인자를 가질 수 있다.\n첫 번째 인자: 몇까지 반복(iteration)을 돌 것인지 받는 매개변수 두 번째 인자: 전달받은 값을 출력하는 함수\n이러한 방식은 함수형 프로그래밍이 무엇을(What) 중시하는지를 보여준다. 즉, 함수형 프로그래밍에서는 ’출력을 하는 함수’를 파라미터로 넘길 수 있는 유연함을 가지고 있다.\n이것은 함수형 프로그래밍의 기본 원리 중 하나인 함수를\n1급 시민(First-Class Citizen) 또는 1급 객체(First-Class Object)로 관리하는 특징 때문이다.\n1급 시민이라는 개념은 다음과 같은 특성을 의미한다.\n\n함수를 변수에 할당할 수 있다.\n함수를 다른 함수의 인자로 전달할 수 있다.\n함수를 다른 함수의 결과로 반환할 수 있다.\n\n이러한 특성 덕분에 함수형 프로그래밍에서는 코드의 재사용성과 모듈화가 용이하며, 프로그램의 가독성을 높이는 데 기여한다.\nJava에서는 멀티스레딩이 기본적으로 지원되며, 여러 스레드를 활용해 동시에 여러 작업을 처리할 수 있다.\n스레드는 하나의 작업 단위로, 자바의 Thread 클래스나 Runnable 인터페이스를 통해 쉽게 만들 수 있다.\n스레드 안전성이 보장되는 함수는 여러 스레드가 동시에 접근하더라도 데이터 충돌 없이 동작할 수 있어 동기화(synchronization) 없이도 병렬 처리가 가능합니다.\n이로 인해 오버헤드가 줄어들고, 성능이 향상되는 장점이 있다. 이렇게 스레드 안전성이 보장된 코드는 동기화 없이도 각 스레드가 독립적으로 작업을 수행할 수 있어 효율적입니다.\nJava에서 synchronized 키워드를 사용해 공통 데이터를 다룰 때 동기화를 적용하여, 여러 스레드가 동시에 접근하지 못하도록 제한할 수 있다.\n특히, 동영상 재생처럼 화면, 소리, 자막 등 여러 요소가 동시에 다루어지는 환경에서는 스레드가 공통 데이터를 정확히 처리하지 않으면 원치 않는 값이 기록될 수 있고, 시각적이나 청각적인 어긋남이 발생할 수 있습니다.\nsynchronized 한 스레드가 잠금(lock)을 걸어 해당 코드 블록을 독점적으로 실행하고, 작업이 완료되면 다른 스레드가 접근할 수 있는 권한을 부여하게 된다.\n\n데이터 일관성이 유지된다.\n동기화된 처리가 보장되어 여러 스레드가 공통 데이터에 접근할 때 문제가 발생하지 않는다.\n경쟁 상태(race condition)와 같은 문제를 예방할 수 있다.\n\n하지만, 오버헤드가 발생할 수 있어 성능 저하가 있을 수 있으므로, 필요한 곳에만 신중하게 사용하는 것이 좋습니다.\n1급 객체 (First-Class Object) 프로그래밍 언어에서 특정 요소를 변수나 데이터 구조에 담을 수 있고, 함수의 파라미터로 전달하거나 반환값으로 사용할 수 있는 객체\nJavaScript와 같은 언어에서 함수는 1급 객체로 취급되며, 이로 인해 함수 자체를 객체처럼 다룰 수 있다.\n특징 1. 함수를 변수에 저장하거나 배열에 넣어 사용할 수 있다.\n\n함수는 다른 함수의 파라미터로 전달될 수 있다.\n함수는 다른 함수의 반환값으로 사용할 수 있다.\n함수는 할당된 이름에 관계없이 고유하게 식별될 수 있다.\n\n이러한 특성 덕분에 고차 함수를 쉽게 만들 수 있다.\nWhat It Means to Be Functional\n함수는 변수로 사용될 수 있다.\nvar log = function(message) { console.log(message); }; log(“In JavaScript, functions are variables”);\n화살표 함수(arrow function)\nconst log = message =&gt; { console.log(message); };\n함수가 객체의 속성으로 추가될 수 있다.\nconst obj = { message: “They can be added to objects like variables”, log(message) { console.log(message); } }; obj.log(obj.message);\n함수는 배열의 요소로도 추가될 수 있다.\nconst messages = [ “They can be inserted into arrays”, message =&gt; console.log(message), “like variables”, message =&gt; console.log(message)]; messages1; messages3;\n함수가 다른 함수에 인수로 전달될 수 있다.\nconst insideFn = logger =&gt; { logger(“They can be sent to other functions as arguments”); }; insideFn(message =&gt; console.log(message));\n함수를 반환하는 함수를 작성하고, 반환된 함수를 호출하는 방식.\nconst createScream = function(logger) { return function(message) { logger(message.toUpperCase() + “!!!”); }; }; const scream = createScream(message =&gt; console.log(message)); scream(“functions can be returned from other functions”); scream(“createScream returns a function”); scream(“scream invokes that returned function”);\n화살표 함수를 사용하여 함수를 반환하는 함수를 간결하게 정의하였다.\nconst createScream = logger =&gt; message =&gt; { logger(message.toUpperCase() + “!!!”); };\nImperative Versus Declarative\n명령형(Imperative) 프로그래밍과 선언형(Declarative) 프로그래밍의 차이를 이해하기 위해, 두 가지 접근 방식을 비교해보겠습니다. 선언형 프로그래밍은 “무엇을 해야 할지”를 설명하는 방식이고, 명령형 프로그래밍은 “어떻게 해야 할지”를 명시적으로 정의하는 방식입니다.\n먼저, 명령형 프로그래밍 접근을 살펴봅시다. 여기서는 주어진 문제를 해결하기 위해 순차적인 단계를 명시적으로 정의합니다. 예를 들어, 문자열에서 공백을 하이픈(-)으로 바꾸는 작업을 수행할 때, 명령형 접근은 문자열을 한 문자씩 확인하고, 공백이 있을 경우 하이픈으로 바꾸는 방식입니다. 이 과정은 매우 구체적이고, 각 단계를 명확하게 기술해야 합니다.\n공백을 하이픈으로 변환하여 URL 친화적인 문자열을 만드는 코드.\nconst string = “Restaurants in Hanalei”; let urlFriendly = ““;\nfor (var i = 0; i &lt; string.length; i++) { if (string[i] === ” “) { urlFriendly +=”-“; } else { urlFriendly += string[i]; } }\nconsole.log(urlFriendly);\n자바스크립트의 String 객체에 내장된 replace 메서드를 사용하여 문자열 내의 공백을 하이픈(-)으로 변환하는 방법.\nconst string = “Restaurants in Hanalei”; const urlFriendly = string.replace(/ /g, “-”); console.log(urlFriendly);\n함수형 프로그래밍 패턴을 사용하여 여러 함수를 조합하고, 비동기적으로 데이터를 로드하여 가공하는 과정.\nconst loadAndMapMembers = compose( combineWith(sessionStorage, “members”), save(sessionStorage, “members”), scopeMembers(window), logMemberInfoToConsole, logFieldsToConsole(“name.first”), countMembersBy(“location.state”), prepStatesForMapping, save(sessionStorage, “map”), renderUSMap ); getFakeMembers(100).then(loadAndMapMembers);\nHTML 문서에 새로운 요소를 동적으로 추가하는 예시.\nconst target = document.getElementById(“target”); const wrapper = document.createElement(“div”); const headline = document.createElement(“h1”);\nwrapper.id = “welcome”; headline.innerText = “Hello World”;\nwrapper.appendChild(headline); target.appendChild(wrapper);\nReact를 사용하여 Hello World 메시지를 화면에 렌더링하는 예제\nconst { render } = ReactDOM; const Welcome = () =&gt; (\n\n    &lt;h1&gt;Hello World&lt;/h1&gt;\n&lt;/div&gt;\n); render(, document.getElementById(“target”));\nFunctional Concepts\n함수형 프로그래밍의 핵심 개념에는 불변성(immutability), 순수성(purity), 데이터 변환(data transformation), 고차 함수(higher-order functions), 재귀(recursion)\n불변성(Immutability) 불변성은 데이터를 변경할 수 없는 특성을 의미합니다.\n변경하지 않음: 함수형 프로그래밍에서는 데이터가 절대 변경되지 않습니다. 한 번 생성된 데이터는 절대 바뀌지 않으며, 변화를 주고 싶을 때는 새로운 데이터 복사본을 생성하는 방식으로 작업을 수행합니다. 이점: 불변성을 유지하면 프로그램이 예측 가능하고 오류가 발생할 가능성이 줄어듭니다. 왜냐하면, 데이터가 어디서든 변하지 않기 때문에 한 부분에서 다른 부분으로 데이터가 변경되면서 발생하는 사이드 이펙트를 방지할 수 있기 때문입니다.\nJavaScript에서 객체를 생성한 예시. color_lawn라는 객체는 잔디의 색상을 나타낸다.\nlet color_lawn = { title: “lawn”, color: “#00FF00”, rating: 0 };\nfunction rateColor(color, rating) { color.rating = rating; return color; } console.log(rateColor(color_lawn, 5).rating); console.log(color_lawn.rating);\n이 코드에는 불변성이 지켜지지 않고 있습니다. rateColor 함수가 color_lawn 객체를 직접 수정하면서 원래 객체가 변경되는 부작용(side effect)이 발생합니다.\n불변성을 지키기 위해 원본 데이터를 수정하지 않고 복사본을 생성하여 수정하는 방법.\n기존 rateColor 함수는 원본 객체를 직접 수정했지만, 이를 수정하여 원본 객체를 건드리지 않고 새로운 객체를 반환하도록 개선한 방법을 보여준다.\nrateColor라는 함수를 사용하여 객체의 속성 값을 변경하는 예제.\nlet color_lawn = { title: “lawn”, color: “#00FF00”, rating: 0 };\nfunction rateColor(color, rating) { return { …color, rating: rating }; }\nconsole.log(rateColor(color_lawn, 5).rating); console.log(color_lawn.rating);\n이 함수는 기존의 color 객체를 직접 수정하지 않고, …color를 사용하여 원본 객체의 복사본을 생성한 다음 rating 속성만 새로운 값으로 설정합니다.\nlet color_lawn = { title: “lawn”, color: “#00FF00”, rating: 0 };\nconst rateColor = (color, rating) =&gt; ({ …color, rating });\nconsole.log(rateColor(color_lawn, 5).rating); console.log(color_lawn.rating);\n다음과 같은 배열이 있다고 가정하자. addColor 함수는 배열에 항목을 추가하는 예시로, Array.push 메서드를 사용한다.\nlet list = [{ title: “Rad Red” }, { title: “Lawn” }, { title: “PartyPink” }];\nconst addColor = function(title, colors) { colors.push({ title: title }); return colors; }; console.log(addColor(“Glam Green”, list).length); console.log(list.length);\n이 함수는 colors.push()를 통해 원본 배열인 colors를 직접 수정한다. list 배열이 원본 데이터라면 addColor 함수 호출 후 list의 길이가 변경된다.\n위 코드는 불변성을 지키지 않으므로, 원본 배열이 변경됩니다.\nArray.concat 메서드는 배열을 합치는 역할을 하지만, 원본 배열을 수정하지 않고 새로운 배열을 반환하므로 불변성을 유지할 수 있다.\nlet list = [{ title: “Rad Red” }, { title: “Lawn” }, { title: “PartyPink” }]; const addColor = (title, array) =&gt; array.concat({ title });\nconsole.log(addColor(“Glam Green”, list).length); console.log(list.length);\narray.concat({ title })는 array 배열의 복사본을 만들고 { title } 객체를 그 뒤에 추가합니다. 결과적으로 원본 배열은 그대로 유지되고, 새로운 배열이 반환됩니다.\n이제 list 배열은 수정되지 않고, 새로운 배열만이 길이가 4인 상태로 반환됩니다.\nArray.concat 대신 스프레드 연산자(…)를 사용해서도 동일한 결과를 얻을 수 있습니다. 스프레드 연산자를 사용하면 코드가 더욱 간결해집니다.\nlet list = [{ title: “Rad Red” }, { title: “Lawn” }, { title: “PartyPink” }]; const addColor = (title, list) =&gt; […list, { title }];\nconsole.log(addColor(“Glam Green”, list).length); console.log(list.length);\nPure Functions\n순수 함수 부수 효과들을 제거한 함수.\nMemory OR I/O의 관점에서 Side Effect가 없는 함수로, 함수의 실행이 외부에 영향을 끼치지 않는다.\n장점 함수 자체가 독립적이며, Side-Effect가 없으므로, 스레드(Thread)에 안정성을 보장받을 수 있다.\n스레드 안정성을 확보하면 멀티스레드 환경에서 동시 실행 시 발생할 수 있는 데이터 경쟁(race condition), 교착 상태(deadlock) 등의 문제가 방지된다.\n\n여러 스레드가 동일한 데이터를 동시에 수정하려 할 때 발생할 수 있는 예기치 않은 오류가 방지된다.\n\n스레드 안전성이 있으면 각 스레드가 독립적으로 동작해 코드의 신뢰성이 높아진다.\n\n스레드 안전한 코드에서는 병렬 처리가 가능해 프로그램 성능이 개선된다. 각 스레드가 서로 간섭 없이 데이터를 처리할 수 있어 작업을 더 빠르게 완료할 수 있다.\n스레드 관련 오류는 발생 시 찾기 어려운 경우가 많지만, 스레드 안전성을 확보하면 이런 오류 가능성이 줄어들어 디버깅과 유지 보수가 더 쉬워진다.\n스레드 안전한 코드는 여러 프로세서나 코어에서 동시 실행이 가능해 고성능 환경에서도 효과적으로 확장할 수 있다.\n\n이 함수는 인자를 받지 않으며, 내부적으로 외부 변수 frederick의 속성을 변경한다. selfEducate 함수 호출 후에는 frederick 객체의 canRead와 canWrite 속성이 true로 변경된다.\n이 함수는 외부 상태를 변경했으므로, 부작용이 발생하며 순수 함수가 아니다.\nconst frederick = { name: “Frederick Douglass”, canRead: false, canWrite: false };\nfunction selfEducate() { frederick.canRead = true; frederick.canWrite = true; return frederick; }\nselfEducate(); console.log(frederick);\nselfEducate 함수가 person이라는 인자를 받아 그 인자 객체의 속성을 변경했다.\nselfEducate가 받은 인자를 변경하므로, 그 인자로 무엇이 전달되느냐에 따라 함수의 동작이 달라질 수 있다.\nconst frederick = { name: “Frederick Douglass”, canRead: false, canWrite: false };\nconst selfEducate = person =&gt; { person.canRead = true; person.canWrite = true; return person; };\nconsole.log(selfEducate(frederick)); console.log(frederick);\nselfEducate 함수는 이제 person이라는 객체를 인자로 받아, 그 객체의 복사본에 canRead와 canWrite 속성만 변경하여 새로운 객체를 반환한다.\n이 함수는 외부의 frederick 객체를 수정하지 않고, 새로운 객체를 생성하여 반환하므로, 부작용이 없다. 같은 person 객체가 주어지면 항상 동일한 결과를 반환하므로, 이 함수는 순수 함수가 됩니다.\nconst frederick = { name: “Frederick Douglass”, canRead: false, canWrite: false };\nconst selfEducate = person =&gt; ({ …person, canRead: true, canWrite: true });\nconsole.log(selfEducate(frederick)); console.log(frederick);\nselfEducate 함수는 frederick 객체를 변경하지 않고 새로운 객체를 반환했으므로, frederick은 여전히 변경되지 않은 상태이다.\nHeader 함수는 document.createElement와 document.body.appendChild를 사용하여 DOM을 변경한다.\nfunction Header(text) { let h1 = document.createElement(“h1”); h1.innerText = text; document.body.appendChild(h1); } Header(“Header() caused side effects”); 이렇게 DOM을 변경하는 행위는 부수효과(side effect)를 일으킨다. 부수효과가 발생하면 이 함수는 순수 함수가 아니다.\n즉, 함수 실행 후 상태가 변경되거나 외부 시스템에 영향을 미친다.\n함수형 프로그램의 특징 부수 효과가 없는 순수 함수를 1급 객체로 간주하여 파라미터나 반환값으로 사용할 수 있으며, 참조 투명성을 지킬 수 있다.\n\n부수 효과 (Side Effect)\n\n다음과 같은 변화 또는 변화가 발생하는 작업을 의미한다.\n\n변수의 값이 변경된다.\n자료 구조를 제자리에서 수정한다.\n\n불변성 (배열, 리스트, 튜플 ⇨ 여러 개의 데이터를 저장)\n데이터 처리에서 원본을 그대로 두고, 사본을 만들어서 수정해야 한다는 점은 매우 중요합니다.\n특히 실시간 데이터와 같은 유일한 데이터를 다룰 때 원본을 손상하면 해당 시간대의 의미가 상실될 수 있다.\n따라서 정보화를 위해 가공할 때는 반드시 원본을 건들지 않고 사본으로 작업하는 것이 바람직하다.\n\n객체의 필드값을 설정한다. (즉, 할당문 금지)\n예외나 오류가 발생하며 실행이 중단된다.\n콘솔 또는 파일 I/O가 발생한다.\n\nReact에서는 UI를 표현할 때 순수 함수를 사용한다. 순수 함수는 부수효과를 일으키지 않고 입력값에 따라 결과값을 반환한다.\nReact 컴포넌트는 내부에서 DOM을 직접 변경하지 않는다. 대신 JSX나 React의 상태(state) 등을 활용하여 UI를 설명한다.\n이 예제에서는 Header 컴포넌트가 순수 함수로 작성되었다.\nconst Header = props =&gt;\n\n{props.title}\n\n;\nData Transformations\nJavaScript는 데이터를 변형하는 데 필요한 도구를 이미 내장하고 있으며, 그 중 두 가지 중요한 함수는 Array.map과 Array.reduce입니다.\n이 함수들을 잘 활용하면 데이터 변환 작업을 효율적으로 처리할 수 있다.\n다음은 학교 목록을 가진 배열이다. 이 배열을 쉼표로 구분된 문자열로 변환하려면 Array.join 함수를 사용할 수 있다.\nconst schools = [“Yorktown”, “Washington & Liberty”, “Wakefield”]; console.log(schools.join(“,”));\n불변성을 유지하면서 데이터를 필터링할 수 있다. 학교 이름이 “W”로 시작하는 학교만 선택하는 예제를 보겠다.\nconst schools = [“Yorktown”, “Washington & Liberty”, “Wakefield”]; const wSchools = schools.filter(school =&gt; school[0] === “W”); console.log(wSchools);\n불변성을 지키면서 원본 배열을 수정하지 않으며,\nschool !== cut 조건을 통해 지정한 학교 이름을 제외한 배열을 생성한다. 이어서 join 메서드를 사용하여 새로운 배열을 문자열로 결합하였다.\nconst schools = [“Yorktown”, “Washington & Liberty”, “Wakefield”]; const cutSchool = (cut, list) =&gt; list.filter(school =&gt; school !== cut); console.log(cutSchool(“Washington & Liberty”, schools).join(“,”));\nArray.map 메서드는 배열의 각 항목에 대해 변환을 적용하여 새로운 배열을 만든다.\n이 메서드는 배열의 각 항목을 변경한 새로운 배열을 생성하며, 원본 배열을 변형하지 않는다. 위의 예제에서는 각 학교 이름에 “High School”을 덧붙여 새로운 배열을 생성한다.\nconst schools = [“Yorktown”, “Washington & Liberty”, “Wakefield”]; const highSchools = schools.map(school =&gt; ${school} High School); console.log(highSchools.join(“”));\n여기서, schools 배열은 문자열 배열로 되어 있다.\nmap을 사용하여 각 학교 이름을 name 속성으로 갖는 객체로 변환한 새로운 배열을 생성했다.\n결과는 각 학교 이름을 객체로 감싼 배열이 된다.\nconst schools = [“Yorktown”, “Washington & Liberty”, “Wakefield”]; const highSchools = schools.map(school =&gt; ({ name: school })); console.log(highSchools);\n배열 내의 특정 객체를 변경할 때도 map을 사용할 수 있다.\n예를 들어, “Stratford”라는 학교 이름을 “HB Woodlawn”으로 변경하고 싶을 때, map을 사용하여 원본 배열을 변경하지 않고 새로운 배열을 만들 수 있다.\nconst editName = (oldName, newName, list) =&gt; { return list.map(school =&gt; school.name === oldName ? { name: newName } : school ); };\nlet schools = [ { name: “Yorktown” }, { name: “Stratford” }, { name: “Washington & Liberty” }, { name: “Wakefield” } ];\nlet updatedSchools = editName(“Stratford”, “HB Woodlawn”, schools); console.log(updatedSchools[1]); console.log(schools[1]);\n{ …item, name } 구문은 의도한 대로 name 속성만 업데이트하려고 하는데, 이 코드에서는 새로 정의된 name 속성이 이전 name 값을 덮어쓰게 된다.\n실제로 name을 수정하려면 name을 덮어쓰는 게 아니라, name: newValue 형태로 작성해야 한다.\nconst editName = (oldName, name, arr) =&gt; arr.map(item =&gt; { if (item.name === oldName) { return { …item, name }; } else { return item; } });\n이 코드는 editName이라는 함수를 한 줄로 작성한 버전이다.\n이 함수의 역할은 특정 배열에서 name 속성이 oldName과 일치하는 객체의 name을 변경한 새로운 배열을 반환하는 것이다.\n이 코드에서는 삼항 연산자를 사용하여 조건에 따라 배열을 변환하고 있다.\nconst editName = (oldName, name, arr) =&gt; arr.map(item =&gt; (item.name === oldName ? { …item, name } : item));\n이 코드는 schools라는 객체를 schoolArray라는 배열로 변환하는 예제. 객체의 키와 값을 활용하여 새로운 배열을 만들고, 각 배열 항목은 객체 형태로 만들어진다.\nconst schools = { Yorktown: 10, “Washington & Liberty”: 2, Wakefield: 5 };\nconst schoolArray = Object.keys(schools).map(key =&gt; ({ name: key, wins: schools[key] }));\nconsole.log(schoolArray);\n이 예제에서는 reduce 함수를 사용하여 배열을 단일 값으로 변환하는 방법을 설명한다. reduce 함수는 배열을 순회하면서 각 요소를 누적하여 최종적인 값을 도출할 수 있는 매우 유용한 도구이다.\nconst ages = [21, 18, 42, 40, 64, 63, 34];\nconst maxAge = ages.reduce((max, age) =&gt; { console.log(${age} &gt; ${max} = ${age &gt; max}); if (age &gt; max) { return age; } else { return max; } }, 0);\nconsole.log(“maxAge”, maxAge);\n이 코드는 reduce 함수를 사용하여 배열에서 가장 큰 값을 계산하는 간결한 방법.\nshorthand if/else 문법을 사용하여 조건문을 간결하게 작성하고, console.log 없이 바로 최대값을 계산한다.\nconst ages = [-5, -18, -42, -40, -64, -63, -34]; const max = ages.reduce((max, value) =&gt; (value &gt; max ? value : max)); console.log(max);\nreduce 메서드를 사용하여 배열을 객체로 변환하는 예제.\n이 예제에서는 colors 배열을 key-value 쌍을 가진 객체로 바꾸고 있다. 각 색상 객체의 id 값을 객체의 key로 사용하고, title과 rating을 포함하는 객체를 그에 대응하는 value로 사용한다.\nconst colors = [ { id: “xekare”, title: “rad red”, rating: 3 }, { id: “jbwsof”, title: “big blue”, rating: 2 }, { id: “prigbj”, title: “grizzly grey”, rating: 5 }, { id: “ryhbhsl”, title: “banana”, rating: 1 }];\nconst hashColors = colors.reduce((hash, { id, title, rating }) =&gt; { hash[id] = { title, rating }; return hash; }, {});\nconsole.log(hashColors);\nreduce 메서드를 사용하여 배열을 다른 형태로 변환하는 방법을 보여준다. 특히, 배열에서 중복되는 값을 제거하고 고유한 값만 남기는 방법에 대해 설명하고 있다.\nconst colors = [“red”, “red”, “green”, “blue”, “green”];\nconst uniqueColors = colors.reduce( (unique, color) =&gt; unique.indexOf(color) !== -1 ? unique : […unique, color], [] );\nconsole.log(uniqueColors);\nHigher-Order Functions\n고차 함수 다른 함수를 인자로 받거나 함수의 결과로 반환하는 함수.\nJavaScript의 Array.prototype.map()이나 filter() 같은 메서드는 모두 고차 함수이다.\n리액트에서의 활용 함수형 컴포넌트와 useEffect 또는 useCallback 등의\n훅(hook)에서 함수를 인자로 전달하거나, 특정 이벤트 발생 시 전달된 함수를 실행하는 방식으로 자주 사용된다.\ninvokeIf는 조건에 따라 다른 함수를 호출하는 고차 함수입니다.\nconst invokeIf = (condition, fnTrue, fnFalse) =&gt; condition ? fnTrue() : fnFalse();\nconst showWelcome = () =&gt; console.log(“Welcome!!!”); const showUnauthorized = () =&gt; console.log(“Unauthorized!!!”);\ninvokeIf(true, showWelcome, showUnauthorized); invokeIf(false, showWelcome, showUnauthorized);\n이 코드는 고차 함수와 클로저를 사용한 예제입니다.\nconst getFakeMembers = num =&gt; { return new Promise((resolve, reject) =&gt; { const members = Array(num).fill(“Member”); resolve(members); }); };\nconst userLogs = userName =&gt; message =&gt; console.log(${userName} -&gt; ${message});\nconst log = userLogs(“grandpa23”);\nlog(“attempted to load 20 fake members”);\ngetFakeMembers(20).then( members =&gt; log(successfully loaded ${members.length} members), error =&gt; log(“encountered an error loading members”) );\nRecursion\n재귀(recursion) 를 사용하여 숫자를 10부터 0까지 카운트다운하는 함수.\nconst countdown = (value, fn) =&gt; { fn(value); return value &gt; 0 ? countdown(value - 1, fn) : value; }; countdown(10, value =&gt; console.log(value));\n재귀 함수 countdown을 사용하여 숫자를 카운트다운하지만, 지연 시간(delay) 을 두고 실행된다. 이전에 설명한 카운트다운과 비슷하지만, setTimeout을 사용해 호출 간격을 조절한다.\n즉, 지정한 시간이 지난 후에야 다음 숫자가 출력된다.\nconst countdown = (value, fn, delay = 1000) =&gt; { fn(value); return value &gt; 0 ? setTimeout(() =&gt; countdown(value - 1, fn, delay), delay) : value; };\nconst log = value =&gt; console.log(value); countdown(10, log);\ndeepPick이라는 함수를 사용하여, 객체 내의 중첩된 속성을 접근하고 가져오는 기능을 구현하고 있다. 이 함수는 객체에서 깊숙이 중첩된 경로(type, data.info.fullname.first 등)를 문자열 형태로 받아 그 값을 반환한다.\nconst deepPick = (path, obj) =&gt; { return path.split(‘.’).reduce((acc, key) =&gt; acc && acc[key], obj); };\nconst dan = { type: “person”, data: { gender: “male”, info: { id: 22, fullname: { first: “Dan”, last: “Deacon” } } } };\nconsole.log(deepPick(“type”, dan)); console.log(deepPick(“data.info.fullname.first”, dan)); console.log(deepPick(“data.info.id”, dan)); console.log(deepPick(“data.gender”, dan)); console.log(deepPick(“data.info.fullname.last”, dan));\ndeepPick 함수를 재귀적으로 구현하여 객체의 중첩된 속성에 접근하는 방법.\n주어진 경로(fields)를 문자열 형태로 입력받아, ‘.’ 으로 구분된 경로를 따라가면서 해당하는 속성 값을 가져온다.\nconst dan = { type: “person”, data: { gender: “male”, info: { id: 22, fullname: { first: “Dan”, last: “Deacon” } } } };\nconst deepPick = (fields, object = {}) =&gt; { const [first, …remaining] = fields.split(“.”);\nif (!object || !(first in object)) {\n    return undefined;\n}\n\nreturn remaining.length\n    ? deepPick(remaining.join(\".\"), object[first])\n    : object[first];\n};\nconsole.log(deepPick(“data.info.fullname.first”, dan));\n“data”에 접근 → “info”에 접근 → “fullname”에 접근 → “first”에 접근 → 최종적으로 “Dan”이 반환된다. 따라서 deepPick 함수는 재귀를 사용하여 객체의 중첩된 속성에 안전하게 접근하도록 구현된 함수입니다.\nComposition\n함수형 프로그래밍에서 작은 순수 함수들을 조합하여 더 큰 기능을 수행하는 방식을 합성이라 한다.\n함수형 프로그래밍은 각 함수가 특정 작업에 집중하도록 설계하는데, 이러한 작은 함수들을 서로 연결하여 더 복잡한 로직을 만들 수 있다.\nJavaScript에서 체이닝(Chaining)은 함수 합성의 한 예로, 메서드를 연속으로 호출하여 이전 함수의 반환 값을 다음 함수의 입력 값으로 사용하는 방식이다.\n문자열의 replace 메서드는 주어진 문자열을 특정 패턴으로 교체하고, 그 결과 문자열을 반환한다. 이 반환 값도 여전히 문자열이므로, replace 메서드를 다시 호출하여 연속적으로 변환할 수 있다.\nconst template = “hh:mm:ss tt”; const clockTime = template .replace(“hh”, “03”) .replace(“mm”, “33”) .replace(“ss”, “33”) .replace(“tt”, “PM”); console.log(clockTime);\n이 코드는 함수 합성(Composition)의 예. 두 개의 함수 civilianHours와 appendAMPM을 합성하여 새로운 함수를 만들고 있다.\nconst civilianHours = date =&gt; date % 12;\nconst appendAMPM = time =&gt; ${time} AM;\nconst both = date =&gt; appendAMPM(civilianHours(date));\nconsole.log(both(13));\n함수가 많아지면 이런 방식은 확장성과 유지보수성에서 어려움이 있다. 예를 들어, 함수가 20개라면 코드가 매우 복잡해지므로, 가독성과 관리가 힘들어진다.\n보다 깔끔하고 확장성 있는 방법은 고차 함수(Higher-order function)인 compose를 사용하는 것이다. compose 함수는 여러 개의 함수를 결합하여 하나의 함수로 만들어 주는 함수이다.\n이 방법은 더 많은 함수가 필요할 때 쉽게 추가할 수 있고, 함수의 순서도 변경하기 쉽습니다.\ncompose를 사용하여 두 개의 함수를 결합하고, 그 결과를 새로운 함수로 만들어 사용하고 있다.\ncompose는 일반적으로 함수형 프로그래밍에서 여러 개의 함수를 합성하여 새로운 함수를 생성하는 방법이다.\nconst both = compose( civilianHours, appendAMPM ); both(new Date());\n이 함수는 reduce를 사용하여 함수들을 하나씩 실행하면서 그 결과를 이어가게 만듭니다.\nconst compose = (…fns) =&gt; arg =&gt; fns.reduce((composed, f) =&gt; f(composed), arg);\nPutting It All Together\n이 코드는 JavaScript로 간단한 시계를 만들기 위한 명령형(Imperative) 해결 방법을 설명하고 있다.\n이 시계는 시간, 분, 초를 표시하고, AM/PM 형식으로 표시된 시간을 군용 시간(civilian time) 형식으로 변환하여 출력한다.\n각 시간 단위는 항상 두 자릿수로 표시되며, 1 또는 2와 같은 한 자릿수 값에는 선행 0을 추가해야 한다.\n또한, 이 시계는 매초마다 업데이트되어 화면에 새로운 시간이 표시된다.\nsetInterval(logClockTime, 1000);\nfunction logClockTime() { let time = getClockTime(); console.clear(); console.log(time); }\nfunction getClockTime() { let date = new Date(); let time = { hours: date.getHours(), minutes: date.getMinutes(), seconds: date.getSeconds(), ampm: “AM” };\nif (time.hours == 12) { time.ampm = “PM”; } else if (time.hours &gt; 12) { time.ampm = “PM”; time.hours -= 12; }\nif (time.hours &lt; 10) { time.hours = “0” + time.hours; }\nif (time.minutes &lt; 10) { time.minutes = “0” + time.minutes; }\nif (time.seconds &lt; 10) { time.seconds = “0” + time.seconds; }\nreturn time.hours + “:” + time.minutes + “:” + time.seconds + ” ” + time.ampm; }\n시계가 작동됨을 확인.\n이 해결책은 잘 동작하고, 주석 덕분에 무엇이 일어나고 있는지 이해할 수 있습니다. 하지만 각 함수가 크고 복잡하며, 각 함수가 여러 일을 하므로 이해하기 어렵고, 주석을 많이 필요로 하며 유지보수가 힘들다.\n이제 더 확장 가능하고 유지보수하기 쉬운 함수형 접근 방식을 사용하여 애플리케이션을 개선해보겠습니다.\n목표 우리는 애플리케이션의 논리를 더 작은 부분인 함수로 나누고자 합니다. 각 함수는 하나의 작업에만 집중하며, 이를 조합해서 큰 함수로 만들어 시계를 구현할 수 있습니다.\n함수형 접근 방식 각 기능을 작은 함수로 나누고, 각 함수는 하나의 일만 하도록 합니다. 이렇게 하면 각 함수가 명확하고, 테스트하기 쉬우며, 재사용 가능하게 됩니다.\n함수형 프로그래밍에서는 값보다는 함수를 사용하는 것이 중요합니다. 값이 필요할 때마다 함수를 호출하여 값을 얻습니다.\nconst compose = (…fns) =&gt; arg =&gt; fns.reduce((composed, f) =&gt; f(composed), arg);\nconst oneSecond = () =&gt; 1000; const getCurrentTime = () =&gt; new Date(); const clear = () =&gt; console.clear(); const log = message =&gt; console.log(message);\n이제 시계를 구현하기 위해 필요한 데이터 변환 함수를 작성해 보겠습니다. 이 세 가지 함수는 Date 객체를 시계를 위한 객체로 변환하는 데 사용됩니다.\n변환 함수들 serializeClockTime: Date 객체를 받아 시계 시간을 담고 있는 객체를 반환합니다. 반환되는 객체는 시, 분, 초를 포함합니다. civilianHours: 시계 시간 객체를 받아 시간(hour)을 12시간제로 변환합니다. 예를 들어, 1300은 1:00으로 변환됩니다. appendAMPM: 시계 시간 객체를 받아 오전(AM) 또는 오후(PM)를 붙여주는 함수입니다. const serializeClockTime = date =&gt; ({ hours: date.getHours(), minutes: date.getMinutes(), seconds: date.getSeconds() });\nconst civilianHours = clockTime =&gt; ({ …clockTime, hours: clockTime.hours &gt; 12 ? clockTime.hours - 12 : clockTime.hours });\nconst appendAMPM = clockTime =&gt; ({ …clockTime, ampm: clockTime.hours &gt;= 12 ? “PM” : “AM” });\n이제 데이터를 변환하는 데 사용할 고차 함수들을 작성하겠습니다. 이 함수들은 데이터를 변경하지 않고 불변성을 유지하면서 시간을 표시하거나 포맷을 변환하는 데 사용됩니다.\n고차 함수들 display: target 함수(예: console.log)를 받아서, 시간을 그 함수로 전달하는 함수를 반환합니다. formatClock: 주어진 템플릿 문자열을 사용하여, hours, minutes, seconds, ampm 등의 값으로 시간을 포맷합니다. prependZero: 시계 시간 객체의 특정 key 값을 받아서, 그 값이 10 미만이면 앞에 0을 추가하여 반환합니다. const display = target =&gt; time =&gt; target(time);\nconst formatClock = format =&gt; time =&gt; format .replace(“hh”, time.hours) .replace(“mm”, time.minutes) .replace(“ss”, time.seconds) .replace(“tt”, time.ampm);\nconst prependZero = key =&gt; clockTime =&gt; ({ …clockTime, [key]: clockTime[key] &lt; 10 ? “0” + clockTime[key] : clockTime[key] });\n이번에는 우리가 만든 모든 함수를 조합하여 틱킹 시계를 만드는 방법을 설명합니다. 이 시계는 1초마다 시간을 업데이트하며, 컴포지션을 사용하여 여러 함수를 결합합니다. 여기서는 compose 함수를 이용해 각 기능을 조합하고, 각 함수들이 하나의 흐름으로 이어지게 만듭니다.\n주요 함수들 convertToCivilianTime: clockTime을 인자로 받아서, 시계를 민간 시간(AM/PM 형식)으로 변환하는 함수입니다. civilianHours와 appendAMPM을 차례로 적용하여 시간 객체를 변환합니다. doubleDigits: 민간 시간을 받아서, hours, minutes, seconds 값에 대해 두 자리를 보장하도록 만듭니다. 10보다 작은 값에는 앞에 0을 추가하는 방식입니다. prependZero 함수를 사용하여 이 작업을 수행합니다. startTicking: setInterval을 사용하여 1초마다 반복 실행되는 콜백을 설정합니다. 콜백 함수는 시계가 계속해서 시간을 가져오고, 변환하고, 포맷을 변경한 후 출력하는 역할을 합니다. 이 콜백은 앞서 정의한 함수들을 컴포지션을 통해 결합하여 하나의 흐름으로 만듭니다. const convertToCivilianTime = clockTime =&gt; compose( appendAMPM, civilianHours )(clockTime);\nconst doubleDigits = civilianTime =&gt; compose( prependZero(“hours”), prependZero(“minutes”), prependZero(“seconds”) )(civilianTime);\nconst startTicking = () =&gt; setInterval( compose( clear, getCurrentTime, serializeClockTime, convertToCivilianTime, doubleDigits, formatClock(“hh:mm:ss tt”), display(log) ), oneSecond() );\nstartTicking();"
  },
  {
    "objectID": "pd/re/re_02.html",
    "href": "pd/re/re_02.html",
    "title": "Chapter 2. JavaScript forReact",
    "section": "",
    "text": "44 페이지부터 시작 자바스크립트에는 여러가지 이것을 알고 리엑트를 써먹어야 한다.\n01 Code Runner 설치하기\n위와 같이 이동하거나 단축키 [Ctrl + Shift + X]를 누르면 Extensions으로 이동한다.\ncode까지만 입력해도, 좌측 최상단으로 Code Runner 나온다.\ninstall 버튼을 클릭해서 Code Runner를 설치한다.\nUninstall로 바뀌면 설치가 완료된 것이다.\n객체 리터럴 구문 (Object Literal Syntax)\n자바스크립트에서 객체를 생성하는 간단한 방법으로, 이 구문을 사용하면 중괄호 {}를 이용하여 객체를 정의하고, 그 안에 속성과 값을 직접 설정할 수 있다.\n아래 코드의 문제는 객체 리터럴을 반환하려는 의도와 JavaScript 구문 오류 때문이다. 코드를 살펴보면, JavaScript는 중괄호 {}를 함수 본문의 시작과 끝으로 해석하므로 객체를 반환하려는 시도가 제대로 작동하지 않는다.\nconst person = (firstName, lastName) =&gt; { first: firstName, last: lastName } console.log(person(“Brad”, “Janson”));\n위 코드를 실행하면 ” SyntaxError: Unexpected token ’ : ’ ” 오류가 발생한다. 이는 함수 내에서 올바른 객체 리터럴 구문 사용하지 않아 발생하는 문제이다.\n이 문제를 해결하려면, 반환하고자 하는 객체를 소괄호()로 감싸면 된다.\n위 코드가 어떤 과정을 거쳐 오류가 출력되는지 자세히 알아보고자 한다.\n속성부터 시작해서 객체가 반환되는 과정을 담은 그림. firstName 매개변수로 전달된 값이 객체의 first 속성에 설정된다. person 함수는 { first: firstName, last: lastName } 이러한\n객체 리터럴을 생성하고, 그 객체를 반환한다.\n매개변수 firstName과 lastName는 함수가 호출되면, 전달된 값(예: “Brad”, “Janson”)을 담는다. 이 값들은 객체의 속성으로 사용된다.\n객체는 메모리 상에 저장되며, 함수는 그 객체의 참조(메모리 주소)를 반환한다. 이 참조를 통해 메모리 상에 객체에 접근할 수 있다.\n결과적으로, { first: ‘Brad’ , last: ‘Janson’ }와 같은 객체가 반환되어야 하지만, 위 코드는 반환되지 않고 오류가 출력되는 것이다.\n그러면, 어떤 이유로 오류가 출력되는 것인가?\n자바스크립트에서는 중괄호{}만 있을 경우,\nfirst: firstName은 객체 리터럴의 속성 정의가 아닌, labelled statement로 해석이 가능해진다.\n자바스크립트 문법에서 labelled statement 다음에 유효한 표현식이 필요하며, 이것이 코드에 제대로 반영되어 있지 않아서\n컴파일러가 스스로 이 코드 구조에 오류가 있다고 판단한 것이다.\n따라서, 객체 리터럴 속성 정의로 단정할 표현식이 필요하므로, 소괄호로 한 번 더 감싸는 과정을 거치는 것이다.\nconst person = (firstName, lastName) =&gt; ({ first: firstName, last: lastName }); console.log(person(“Flad”, “Hanson”));\n클래스와 객체의 차이 객체를 위한 설계도를 클래스라고 한다. 클래스는 만든다고 해서 메모리에 올라가지 않음\n클래스는 객체를 만들기 위한 설계도.\n클래스 자체는 그저 정의일 뿐, 이것을 만들었다고 해서 바로 메모리에 올라가거나 실행되는 것은 아닙니다. 클래스는 객체가 가져야 할 속성(데이터)과 동작(메서드)을 정의할 뿐입니다.\n클래스가 메모리에 올라가는 시점은 객체를 생성할 때입니다.\n객체는 클래스의 인스턴스로, 클래스에서 정의한 속성들과 메서드를 실제로 사용하는 실체입니다. 객체가 생성되면 그때 클래스의 구조에 따라 메모리에 적재됩니다.\n일반 함수는 this를 새롭게 바인딩한다.\n바인딩(Binding)이란?\n바인딩은 프로그래밍에서 특정 값이나 객체를 특정 변수 또는 키워드에 연결하는 과정을 의미한다. 쉽게 말해, 어떤 변수나 키워드가 실제로 가리키는 대상(값 또는 객체)을 결정하는 과정이다.\n왜 바인딩이 중요한가?\n과거에는 프로그래밍에서 바인딩 개념이 명확하지 않았을 수 있지만, 점점 더 복잡한 프로그램을 작성하다 보면 바인딩이 왜 중요한지 느끼게 됩니다.\n특히 객체지향 프로그래밍에서, 객체들이 상호작용하는 방식과 객체 내의 메서드가 특정 컨텍스트에서 어떻게 동작하는지 이해하는 데 있어서 바인딩은 중요한 개념이다.\n예를 들어, 객체지향 프로그래밍에서는 클래스의 인스턴스를 생성하고, 그 인스턴스의 메서드가 특정 객체와 결합되어 동작하게 된다.\n이 과정에서 변수나 메서드가 어떤 객체에 연결되는지(바인딩되는지)를 알아야 합니다.\n자바스크립트에서 this 바인딩 자바스크립트에서는 바인딩은 중요한 개념이다. 자바스크립트의 this 키워드는 함수가 호출될 때 그 함수의 문맥에 따라 달라진다.\n그 이유는 함수가 호출되는 방식에 따라 this가 가리키는 객체가 결정되기 때문이며, 이것은 5가지 방식으로 나타낼 수 있다.\nthis 바인딩의 5가지 방식\n\n전역 바인딩 (Default Binding)함수가 그냥 호출되면 this는 전역 객체 (브라우저에서는 window, Node.js에서는 global)를 가리킨다.\n암시적 바인딩 (Implicit Binding)함수가 객체의 메서드로 호출되면 this는 그 객체를 가리킨다.\n명시적 바인딩 (Explicit Binding)call, apply, bind를 사용해서 명시적으로 this를 설정할 수 있다.\nnew 바인딩\n\nnew 키워드를 사용해 생성자 함수를 호출하면, this는 새로 생성된 객체를 가리킨다.\n\n화살표 함수 바인딩 (Lexical Binding)화살표 함수는 this를 자신의 외부 컨텍스트에서 상속받는다. 즉, 화살표 함수의 this는 정의된 위치의 문맥에 의해 결정된다.\n\n호출시점이 동적으로 결정된다.\nconst tahoe = { mountains: [“Freel”, “Rose”, “Tallac”, “Rubicon”, “Silver”], print: function(delay = 1000) { setTimeout(function() { console.log(this.mountains.join(“,”)); }, delay); } }; tahoe 객체를 가리키지 않아 에러가 난다. function()함수를 화살표 함수로 바꿔야만 오류가 해결된다.\nconst tahoe = { mountains: [“Freel”, “Rose”, “Tallac”, “Rubicon”, “Silver”], print: function(delay = 1000) { setTimeout(() =&gt; { console.log(this.mountains.join(“,”)); }, delay); } }; tahoe.print(); // Freel, Rose, Tallac, Rubicon, Silver Compiling JavaScript JavaScript는 전통적으로 인터프리터 언어로, 컴파일 단계 없이 실행 시에 코드가 해석된다.\n그러나 최신 JavaScript와 다양한 브라우저 환경을 지원하기 위해 컴파일처럼 작동하는 몇 가지 도구가 사용되기도 한다.\n이를 컴파일링이나 트랜스파일링이라고 부르며, 대표적인 도구로는 Babel(바벨)이 있다.\nJavaScript의 컴파일 개념 JavaScript는 보통 직접 기계어로 변환되는 것이 아니라, 바이트코드로 변환되어 실행된다.\n예를 들어, V8 엔진(Chrome, Node.js에서 사용)은 JavaScript 코드를 바이트코드로 변환하여 최적화된 방식으로 실행한다.\n여기서 바이트코드는 Java와 유사하게 가상 머신(Virtual Machine)에서 실행된다.\nBabel(바벨)\n최신 ECMAScript(ES6, ES7 등) 표준을 옛 브라우저나 구형 환경에서도 사용할 수 있도록 트랜스파일링을 수행하는 도구입니다.\n트랜스파일링은 최신 문법을 구형 문법으로 변환해 주는 작업으로, 예를 들어 const, let, arrow function 등을 구버전 브라우저에서 지원할 수 있게 변환합니다.\nBabel을 사용하면, 새로운 기능이나 문법을 적용한 코드를 구형 환경에서도 동작할 수 있게 해 주므로, 개발자는 최신 기능을 활용할 수 있습니다.\n엄격 모드(Strict Mode)\nuse strict;를 통해 JavaScript의 엄격 모드를 활성화하면, 더 엄격한 오류 검사가 이루어집니다.\n선언되지 않은 변수를 사용하는 것이 금지되고, JavaScript의 몇 가지 불안정한 기능이 비활성화됩니다.\n바벨로 변환할 때 이 엄격 모드 역시 코드의 일관성을 유지하고 잠재적인 오류를 방지하는데 도움을 줍니다.\n이 코드는 ES6(ECMAScript 2015)의 문법을 사용하는 예이다.\n// p.46 const add = (x = 5, y = 10) =&gt; console.log(x + y); add() // 함수 호출\n\n기본 매개변수(Default Parameters) 함수의 매개변수 x와 y는 각각 5와 10이라는 기본값을 가지고 있다. 만약 함수 호출 시 x나 y 값을 전달하지 않으면 기본값이 사용된다.\n\n예: add()를 호출하면 x는 5, y는 10으로 계산되어 15가 출력된다.\n\n화살표 함수(Arrow Function) 화살표 함수 문법을 사용하여 함수를 정의한 것이다.\n\n화살표 함수는 더 짧고 간결하게 함수를 작성할 수 있게 해 주며, 기존의 function 키워드 대신 사용할 수 있다.\n\nconsole.log() 주어진 인자를 콘솔에 출력하는 함수. 이 코드에서는 x, y를 더한 결과를 콘솔에 출력한다.\n\n이전 코드에 비해 확실히 길어진 것을 알 수 있다. 이 코드는 use strict를 사용하여 JavaScript의 엄격 모드에서 동작하도록 설정한 후, add라는 함수를 정의한 것이다.\n// p.47 “use strict”; var add = function add() { var x = arguments.length &lt;= 0 || arguments[0] === undefined ? 5 : arguments[0]; var y = arguments.length &lt;= 1 || arguments[1] === undefined ? 10 : arguments[1]; return console.log(x + y); }; add() // 함수 호출\n이 코드는 Babel과 같은 트랜스파일러를 사용했을 때 최신 JavaScript 문법(ES6)에서 구문이 변환된 형태처럼 보입니다.\n함수는 두 개의 기본 매개변수 x, y를 사용하며, 각각 기본값으로 5와 10을 가지도록 설정되었다.\n코드를 실행하면 x와 y의 합을 콘솔에 출력합니다.\n\nuse strict (엄격 모드) 엄격 모드를 활성화하면 JavaScript의 더 엄격한 오류 검사를 수행한다. 선언되지 않은 변수를 사용하는 등의 실수를 방지해준다.\n함수 정의 함수는 add라는 이름을 가지고 있으며 기본 매개변수 x = 5, y = 10을 사용한다. ES5로 변환된 코드에서 arguments 객체를 사용하여 함수에 전달된 인수를 확인한다.\n\n만약 x와 y가 전달되지 않으면 각각 기본값이 할당된다.\n\n변환된 코드 설명 이 코드는 최신 화살표 함수 문법을 전통적인 함수로 변환한 것과 유사합니다. 다음과 같이 변환된 이유는 최신 ES6 문법을 지원하지 않는 환경에서도 동작하도록 하기 위함이다.\n\n첫번째 인자가 정의되어 있지 않으면, 뭐뭐해라 매우 엄격하게 정의해서 실행하게끔 이러한 식으로 변환이 된다.\n객체와 배열\nconst sandwich = { bread: “dutch crunch”, meat: “tuna”, cheese: “swiss”, toppings: [“lettuce”, “tomato”, “mustard”] }; const { bread, meat } = sandwich; console.log(bread, meat); // dutch crunch tuna 매칭을 시켜서 재구화되어서 출력이 된다. 객체의 필드를 띄어내서 매칭을 시키는 유연한 문법이다.\n다른 언어에서는 볼 수 없는 문법이다.\n//이전과 동일한 코드 const sandwich = { bread: “dutch crunch”, meat: “tuna”, cheese: “swiss”, toppings: [“lettuce”, “tomato”, “mustard”] };\nlet { bread, meat } = sandwich; bread = “garlic”; meat = “turkey”; console.log(bread); // garlic console.log(meat); // turkey console.log(sandwich.bread, sandwich.meat); // dutch crunch tuna\n재구조화\nconst lordify = regularPerson =&gt; { console.log(${regularPerson.firstname} of Canterbury); }; const regularPerson = { firstname: “Bill”, lastname: “Wilson” }; lordify(regularPerson); // Bill of Canterbury 전달되는 사람의 이름을 출력하는 것 이 객체를 함수에 전달, 함수는 잘라서 전달 즉, 스크립트 엔진이 알아서 해준다는 것.\n퍼스트 네임은 맴버이다.\n도트를 안 써도 도출이 된다. 리플레트 효과라고 한다.\nconst lordify = ({ firstname }) =&gt; { console.log(${firstname} of Canterbury); }; const regularPerson = { firstname: “Bill”, lastname: “Wilson” }; lordify(regularPerson); // Bill of Canterbury\n빅데이터에서 자주 사용되는 기\nconst regularPerson = { firstname: “Bill”, lastname: “Wilson”, spouse: { firstname: “Phil”, lastname: “Wilson” } };\nlist matching\nconst [firstAnimal] = [“Horse”, “Mouse”, “Cat”]; console.log(firstAnimal); // Horse 이것을 전달하니 첫번째 데이터가 나온다. 즉, 객체처럼 작동한다.\n배열의 첫번째 주소를 넘기기 때문이며 절대 전체 주소를 넘길 수 없다.\nC언어를 배웠다면, 알고 있을 지극히 당연한 문법 성질이다.\nconst [, , thirdAnimal] = [“Horse”, “Mouse”, “Cat”]; console.log(thirdAnimal); // Cat\n리터럴, 고정된 값\nconst name = “Tallac”; const elevation = 9738; const funHike = { name, elevation }; console.log(funHike); // {name: “Tallac”, elevation: 9738} 약 2968m\nconst name = “Tallac”; const elevation = 9738; const print = function() { console.log(Mt. ${this.name} is ${this.elevation} feet tall); }; const funHike = { name, elevation, print }; funHike.print(); // Mt. Tallac is 9738 feet tall 기능을 만들어 놓음 이것은 객체를 만들 다음\n구식(old) 방식과 신식(new) 방식의 자바스크립트 객체 생성 방식의 차이 // Old var skier = { name: name, sound: sound, powderYell: function() { var yell = this.sound.toUpperCase(); console.log(${yell} ${yell} ${yell}!!!); }, speed: function(mph) { this.speed = mph; console.log(“speed:”, mph); } }; // New const skier = { name, sound, powderYell() { let yell = this.sound.toUpperCase(); console.log(${yell} ${yell} ${yell}!!!); }, speed(mph) { this.speed = mph; console.log(“speed:”, mph); } }; 타이핑을 덜하도록 만들었다.\nvar는 함수 스코프를 가지며 재선언이 가능하지만, const는 블록 스코프를 가지며 재할당이 불가능합니다.\n신식 방식에서는 메서드를 정의할 때 function 키워드를 생략할 수 있어 코드가 더 간결해집니다.\n속성의 이름과 값이 같을 경우 축약 표현을 사용할 수 있어 코드의 가독성이 높아집니다.\n스프레드 연산자\n가변인자\nconst peaks = [“Tallac”, “Ralston”, “Rose”]; const canyons = [“Ward”, “Blackwood”]; const tahoe = […peaks, …canyons]; console.log(tahoe.join(“,”)); // Tallac, Ralston, Rose, Ward, Blackwood\n역순 출력\nconst peaks = [“Tallac”, “Ralston”, “Rose”]; const [last] = peaks.reverse(); console.log(last); // Rose console.log(peaks.join(“,”)); // Rose, Ralston, Tallac 여기서 원본 배열은 절대 건드리면 안된다.\n현재는 데이터가 매우 많다 어떠한 작업을 할 때 원본 데이터가 회손이 되면 원본 데이터를 볼 수가 없게 되는 것이다.\n이것은 문법적인 오류가 아니므로 이를 원본이 이미 회손된 이후에는 늦으므로 이를 인지하는 것이 매우 중요하다\n그러므로, 자바에세는 클론이라는 것이 있음 얕은 복사, 깊은 복사를 통해 원본 데이터를 유지하는 것이다.\n사본 만들기\nconst peaks = [“Tallac”, “Ralston”, “Rose”]; const [last] = […peaks].reverse(); console.log(last); // Rose console.log(peaks.join(“,”)); // Tallac, Ralston, Rose 모든 데이터에 대한 작업은 사본을 먼저 만든 다음에 해야 된다.\n자동 분할\nconst lakes = [“Donner”, “Marlette”, “Fallen Leaf”, “Cascade”]; const [first, …others] = lakes; console.log(others.join(“,”)); // Marlette, Fallen Leaf, Cascade\nfunction directions(…args) { let [start, …remaining] = args; let [finish, …stops] = remaining.reverse(); console.log(drive through ${args.length} towns); console.log(start in ${start}); console.log(the destination is ${finish}); console.log(stopping ${stops.length} times in between) ;} directions(“Truckee”, “Tahoe City”, “Sunnyside”, “Homewood”, “Tahoma”); 출력 결과 예측하기\nconst morning = { breakfast: “oatmeal”, lunch: “peanut butter and jelly” }; const dinner = “mac and cheese”; const backpackingMeals = { …morning, dinner }; console.log(backpackingMeals); // { // breakfast: “oatmeal”, // lunch: “peanut butter and jelly”, // dinner: “mac and cheese” // } 스프레드 시크 모닝\n비동기\n쿠팡 쇼핑몰과 같은 사이트를 만드는 개발자에게 최적화된 자바스크립트 노드에서 만들면 앱에서도 빌려서 쓰는 것이 가능하기 때문"
  },
  {
    "objectID": "pd/re/re_00.html",
    "href": "pd/re/re_00.html",
    "title": "개론",
    "section": "",
    "text": "1. 서론\n프로그래밍 언어(특히 고급언어)는 인간이 이해하기 쉬운 형태로 작성되지만, 컴퓨터는 기계어(이진수 형태)만을 이해할 수 있다.\n그래서 소스 코드를 작성한 후, 컴파일러라는 프로그램을 사용하여 소스 코드를 기계어로 변환하는 과정이 필요하다.\n이 과정을 컴파일이라고 부르며, 기계가 실행할 수 있는 바이너리 코드 또는 실행 파일을 생성하게 된다.\n하지만 스크립트 언어는 보통 컴파일 과정 없이 소스 코드를 바로 실행할 수 있다.\n스크립트 언어는 주로 인터프리터라는 프로그램이 코드를 한 줄씩 읽고 바로 실행한다. 대표적인 스크립트 언어로는 Python, JavaScript 등이 있다.\n컴퓨터 기술의 초기 개발은 군사 목적으로 진행된 경우가 많았다.\n특히 제2차 세계대전 동안 미사일 발사와 암호 해독과 같은 군사 목적을 위해 컴퓨터가 발명되었다.\n앨런 튜링은 컴퓨터 과학의 아버지로 불리며, 제2차 세계대전 당시 독일의 에니그마 암호를 해독하는 데 중요한 역할을 했다.\n이를 통해 현대 컴퓨터 과학의 기초를 세웠다.\n초기 컴퓨터들은 크기가 매우 컸다.\n예를 들어, 애니악(ENIAC)은 운동장만한 크기의 첫 상업용 컴퓨터 중 하나로, 많은 진공관을 사용해 작동했으며, 이 진공관의 전원을 끄고 켜는 방식으로 0과 1의 이진수(기계어)를 표현했다.\n이는 컴퓨터가 논리적 연산을 수행하는 기본 방식이다.\nCPU는 컴퓨터의 중앙 처리 장치로, 저급언어(기계어)로 작동한다. 이 저급언어는 0과 1로 구성되어 있으며, CPU가 직접 이해하고 실행할 수 있다.\n고급언어는 인간이 이해하기 쉽게 작성된 언어로, CPU가 실행할 수 있도록 기계어로 변환하는 컴파일 과정이 필요하다.\n예를 들어 C 언어는 고급언어이며, 유닉스는 C 언어로 개발된 대표적인 운영체제이다.\n변수는 데이터를 저장하는 임시 공간이다. 프로그래밍에서 변수는 값을 저장하고 나중에 재사용할 수 있게 해준다.\nvar는 자바스크립트에서 오래된 방식으로 변수를 선언하는 키워드. var로 선언된 변수는 함수 스코프를 가진다.\nlet는 ES6(ECMAScript 2015)에서 추가된 키워드로, 블록 스코프를 가진다. 즉, 변수가 선언된 블록 내에서만 유효하다.\nconst도 ES6에서 추가된 키워드로, 상수를 선언할 때 사용한다. const로 선언한 변수는 재할당이 불가능하며, 블록 스코프를 가진다.\n즉, 한번 값을 할당하면 변경할 수 없고 const 키워드로 선언한 변수는 재할당이 불가능하다.\nconst a = 10; a = 20; // 오류 발생\n하지만 const로 선언된 객체의 속성은 변경할 수 있다.\nconst obj = { name: “Alice” }; obj.name = “Bob”; // 가능\nvar topic = “JavaScript”; // 전역 변수 topic 선언 및 초기화 if (topic) { // topic이 truthy한 값이므로 이 블록이 실행됨 var topic = “React”; // var로 재선언, 전역 스코프에 영향을 미침 console.log(“block”, topic); // block React } console.log(“global”, topic); // global React\nlet 키워드를 사용하면 var의 전역 스코프 문제를 해결할 수 있다. let은 블록 스코프를 따르므로, 변수가 선언된 블록 내부에서만 유효하게 동작한다.\n이를 통해 의도치 않은 값의 덮어쓰기를 방지할 수 있다.\nnode클릭\nvar name = “name”; console.log(name);\n스칼라를 동적으로 사용하려면, 코딩이 제대로 되어 있는지 인간이 직접 분석하고 판단해야 한다.\n세부적인 부분은 반드시 개발자가 직접 처리해야 한다.\nC언어는 기초 언어 p.32\nECMA 컴퓨터 제조 위원회는 JavaScript 표준을 만들고 이를 승인하는 역할을 한다.\nES6까지는 var 키워드를 사용해 변수를 선언했으나, 이 방식에는 여러 문제가 있었다. 예를 들어, 상수를 선언할 방법이 없었으며, var로 선언된 변수는 재할당이 가능했다.\nES2015(ES6)부터는 const 키워드를 도입해 상수 선언이 가능해졌다. 현재 JavaScript 표준은 ES2024로 발전했다.\nconst pizza = true; pizza = false;\n함수는 특정 기능을 수행하는 코드의 집합이다. 블록은 중괄호 {}로 감싸진 코드 영역을 의미하며, 일종의 상자로 비유할 수 있다.\n블록 안에서 선언된 변수는 지역 변수로, 해당 블록 내에서만 접근이 가능하다.\nlet & const ES6 (ECMAScript 2015)에서 도입된 새로운 변수 선언 키워드. var 로 인한 몇 가지 문제점을 해결하기 위해 만들어졌다.\n1 . 스코프(scope) 문제 var는 함수 스코프를 가지며, 블록(if, for, while 등) 안에서 선언된 변수도 블록을 벗어나면 여전히 접근이 가능하다.\n이는 개발자가 의도하지 않은 변수 접근이나 값의 변경을 일으킬 수 있다.\nlet과 const는 블록 스코프를 따른다.\n중괄호 {} 안에서 선언된 변수는 해당 블록 내부에서만 유효하고, 블록을 벗어나면 소멸된다. 이로 인해 변수가 의도하지 않은 곳에서 사용되는 문제를 방지할 수 있다.\n2 . 변수 호이스팅(hoisting) 문제 var로 선언된 변수는 호이스팅이 되어, 코드 상에서 변수를 선언하기 전에 접근할 수 있다.\n하지만 변수는 선언된 시점 이전에 undefined로 초기화된다. 이로 인해 코드의 가독성이 떨어지고, 오류를 유발할 수 있다.\nlet과 const는 변수 호이스팅이 발생하지만, 일시적 사각지대(TDZ) 때문에 변수를 선언하기 전에는 접근할 수 없다.\n이는 더 안전한 변수 사용을 가능하게 한다.\n3 . 중복 선언 문제 var은 같은 스코프 내에서 동일한 이름으로 변수를 중복 선언할 수 있다.\nlet은 같은 스코프 내에서 동일한 이름으로 변수를 중복 선언할 수 없으며, 중복 선언 시 오류가 발생한다.\nvar topic = “JavaScript”; if (topic) { var topic = “React”; console.log(“block”, topic); // block React } console.log(“global”, topic); // global React\n아래와 같이 작성하여 지역변수와 전역변수를 동작한다. 자바에서는 이것이 당연하게 일어나지 않아 이 문제점을 해결하였다.\nvar topic = “JavaScript”; if (topic) { let topic = “React”; console.log(“block”, topic); // React } console.log(“global”, topic); // JavaScript\n호이스팅(Hoisting)이란 컴퓨터가 코드를 실행하기 전에 전체를 미리 살펴보고, 변수와 함수의 선언을 메모리에 먼저 올려두는 과정을 의미한다.\n이로 인해 변수나 함수가 나중에 초기화되더라도 에러 없이 실행이 가능하다. 다만, 변수는 초기화 전에 접근할 경우 기본값인 undefined를 출력한다.\n한편, JavaScript는 초기 웹 개발자들의 편의를 위해 변수와 함수 사용에 대한 규제를 느슨하게 설계했다.\n이는 코드가 운영체제(기계)에 직접적인 영향을 미친다는 점에서 유연성과 엄격함 사이의 균형을 고려한 결정이었다.\nconsoloe.log(a) var a = 1; consoloe.log(a)\n에러가 발생하지 않고 그대로 출력되는 것을 알 수 있다.\nfor(var i = 0; i &lt; 5; i++){ console.log(i); } console.log(i);\n지역변수와 전역변수의 역할이 나누어져 에러가 나타나는 것을 알 수 있다.\nfor(let i = 0; i &lt; 5; i++){ console.log(i); } console.log(i);\nDOM 개념 본래 각 박스를 클릭할 때, 1 ~ 5 까지 순서대로 출력되어야 하나 어떤 박스를 클릭해도 5가 나오는 에러가 발생한다.\nvar div, container = document.getElementById(“container”);\nfor (var i = 0; i &lt; 5; i++) { div = document.createElement(“div”); div.onclick = function() { alert(“This is box #” + i); }; container.appendChild(div); }\n따라서, 변수 선언 시에는 항상 let 키워드를 사용하는 것이 좋으며, 특히 React와 같은 환경에서는 let을 더욱 권장한다.\nJavaScript는 변수의 자료형을 명시적으로 지정할 필요 없이 자동으로 결정한다. 이 과정을 타입 추론이라고 하며, 이는 var 키워드가 제공하는 특징 중 하나다.\n참고:\n파이썬의 철학은 최소한의 타이핑으로 간결하게 코드를 작성하자는 데 있다. 이를 구현하기 위해 엔진과 컴파일러가 이러한 원칙에 맞춰 설계되었다.\n반면, 자바는 상대적으로 모든 것을 상세히 작성해야 하는 특징이 있다.\n\n는 값을 더하는 역할뿐만 아니라 문자열을 연결하는 기능도 수행한다.\n\n과거에는 +를 사용해야 할 상황에서 이를 실수로 생략하는 경우가 종종 있었으며, 이로 인해 여러 번 오류가 발생한 경험이 있다.\nconsole.log(lastName + “,” + firstName + ” ” + middleName);\n데이터 바인딩 서버에서 데이터를 넘겨줘서 클라이언트에 보여주는 것. 데이터를 담고 보여줘야 할 때, 이때 이 개념을 사용한다.\nconsole.log(${lastName}, ${firstName} ${middleName});\n사람마다 다르다.\nconst email = Hello ${firstName}, Thanks for ordering ${qty} tickets to ${event}. Order Details ${firstName} ${middleName} ${lastName} ${qty} x $${price} = $${qty*price} to ${event} You can pick your tickets up 30 minutes before the show. Thanks, ${ticketAgent} 이것은 실습을 알 수 없음, 데이터가 있어야 가능하다.\ndocument.body.innerHTML = &lt;section&gt; &lt;header&gt; &lt;h1&gt;The React Blog&lt;/h1&gt; &lt;/header&gt; &lt;article&gt; &lt;h2&gt;${article.title}&lt;/h2&gt; ${article.body} &lt;/article&gt; &lt;footer&gt; &lt;p&gt;copyright ${new Date().getYear()} | The React Blog&lt;/p&gt; &lt;/footer&gt; &lt;/section&gt;; 이러한 것을 템플릿이라고 한다.\n모든 것을 객체로 보고, 객체 간에 상호작용을 보는 것\n함수 만들기\n리텃 유형, ()가 붙은 건 함수, {}는 블럭이다. 이 함수가 호출되면 블럭이 실행되도록 한다.\n// 함수 선언 function logCompliment() {\nconsole.log(“You’re doing great!”); }\nlogCompliment(); 이것은 사용자 정의함수, 파이썬의 print()에 경우 개발자가 미리 만들어 놓았다.\nconst logCompliment = function() { console.log(“You’re doing great!”); };\nlogCompliment();\n외부 클래스와 상호작용하기 싫다면 함수의 인자로 함수 전달 간단하게 함수를 만들어 쓰기\n// Invoking the function before it’s declared hey(); // Function Declaration function hey() { alert(“hey!”); }\n그러나 아래의 코드는 에러가 발생한\n// Invoking the function before it’s declared hey(); // Function Expression const hey = function() { alert(“hey!”); };\nconst logCompliment = function(firstName) { console.log(You're doing great, ${firstName}); };\nlogCompliment(“Molly”);\n2가지 선언\nconst logCompliment = function(firstName, message) { console.log(${firstName}: ${message}); };\nlogCompliment(“Molly”, “You’re so cool”);\nconst lordify = firstName =&gt; ${firstName} of Canterbury\nDefault Parameters function logActivity(name = “Shane McConkey”, activity = “skiing”) { console.log(${name} loves ${activity}); }\n유연하게 대체하도록\nfunction logActivity(name = “Shane McConkey”, activity = “skiing”) { console.log(${name} loves ${activity}); }\n화살표\nconst lordify = firstName =&gt; ${firstName} of Canterbury;\n// Typical function const lordify = function(firstName, land) { return ${firstName} of ${land}; };\n// Arrow Function const lordify = (firstName, land) =&gt; ${firstName} of ${land};\nconsole.log(lordify(“Don”, “Piscataway”)); // Don of Piscataway console.log(lordify(“Todd”, “Schenectady”)); // Todd of Schenectady\nconst lordify = (firstName, land) =&gt; { if (!firstName) { throw new Error(“A firstName is required to lordify”); } if (!land) { throw new Error(“A lord must have a land”); } return ${firstName} of ${land}; }; console.log(lordify(“Kelly”, “Sonoma”)); // Kelly of Sonoma console.log(lordify(“Dave”)); // ! JAVASCRIPT ERROR\n릭터닝 객체, 44페이\nconst person = (firstName, lastName) =&gt; { first: firstName, last: lastName } console.log(person(“Brad”, “Janson”)); 람다 1930년, 기본철학은 자바, 파이썬이 쉬워진다.\n컴퓨터가 많은 방은 차가워야 맞음, 70, 80도 항상 24도 맞춰 놓음 추석 떄 전자출석이 더워서 서버가 장애를 일으킴\n리엑트!!!"
  },
  {
    "objectID": "pd/od/od_04.html",
    "href": "pd/od/od_04.html",
    "title": "개발 일지2",
    "section": "",
    "text": "01 VS코드 실행 관리자 권한으로 실행하기.\n1 . uv 설치 pip install uv\n2 . Python 3.12로 가상 환경 생성 uv venv -p 3.12 fastapi-env\n3 . 프로제트 클론하기 djailab/backend를 클론 후 파일 생성 및 폴더에 저장.\ngit clone https://github.com/djailab/backend.git\n4 . 프로젝트 설정 동기화 uv sync\n5 . 파일 실행 명령어 uv run hello.py"
  },
  {
    "objectID": "pd/od/od_02.html",
    "href": "pd/od/od_02.html",
    "title": "FastAPI",
    "section": "",
    "text": "CHAPTER 3에 대해 다루고자 한다.\n01 소개 https://fastapi.tiangolo.com/#requirements\n02 FastAPI 애플리케이션\n3 - 1 . hello 파일 생성하기 from fastapi import FastAPI #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI() @app.get(“/hi”) def greet(): return “Hello? World?”\n3 - 2 . cmd 실행 uvicorn hello:app –reload\n3 - 3 . 파일에 추가 코드 if name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 4 . 검색창 입력 http://localhost:8000/hi\n3 - 5 . Requests로 /hi 테스트 import requests r = requests.get(“http://localhost:8000/hi”) r.json()\n3 - 6 . Requests와 거의 동일한 HTTPX로 /hi 테스트 # pip install httpx import httpx r = httpx.get(“http://localhost:8000/hi”) r.json()\nHTTPie 설치가 안되어 있을 경우 pip install httpie # 설치 명령어 where http # 설치된 경로 출력 명령어\n3 - 7 . HTTPie로 /hi 테스트 httpie localhost:8000/hi\n3 - 8 . HTTPie로 /hi 를 테스트해 응답 본문만 출력 http -b localhost:8000/hi\n3 - 9 . HTTPie로 /hi를 테스트하고 모든 정보 출력 http -v localhost:8000/hi\n03 HTTP 요청\n3 - 11 . 인사말 경로 변환 from fastapi import FastAPI\napp = FastAPI()\n@app.get(“/hi/{who}”) def greet(who): return f”Hello? {who}”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 12 . 브라우저에서 hi/Mom 테스트 http://localhost:8000/hi/Mom\n3 - 13 . HTTPie로 hi/Mom 테스트 http localhost:8000/hi/Mom\n3 - 14 . Requests로 /hi/Mom 테스트 import requests r = requests.get(“http://localhost:8000/hi/Mom”) r.json()\n3 - 15 . 인사말 쿼리 매개변수 변환 from fastapi import FastAPI #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI()\n@app.get(“/hi”) def greet(who): return f”Hello? {who}”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 16 . 브라우저에서 테스트 http://localhost:8000/hi?who=Mom\n3 - 17 . HTTPie 테스트 http -b localhost:8000/hi?who=Mom\n3 - 18 . HTTPie 및 매개변수 사용 테스트 http -b localhost:8000/hi who==Mom\n3 - 19 . Requests로 테스트 import requests r = requests.get(“http://localhost:8000/hi?who=Mom”) r.json()\n3 - 20 . Requests 및 매개변수 테스트 import requests params = {“who”: “Mom”} r = requests.get(“http://localhost:8000/hi”, params=params) r.json()\n3 - 21 . 인사말 본문 반환 from fastapi import FastAPI, Body #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI()\n@app.post(“/hi”) def greet(who : str = Body(embed=True)): return f”Hello? {who}?”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 22 . HTTPie로 테스트 http -v localhost:8000/hi who=Mom\n3 - 23 . Requests로 테스트 import requests r = requests.post(“http://localhost:8000/hi”, json={“who”: “Mom”}) r.json()\n3 - 24 . Requests로 테스트 from fastapi import FastAPI, Header #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI()\n@app.get(“/hi”) def greet(who : str = Header()): return f”Hello? {who}?”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 25 . Requests로 테스트 http -v localhost:8000/hi who:Mom\n3 - 26 . 헤더 반환 from fastapi import FastAPI, Header\napp = FastAPI()\n@app.get(“/agent”) def get_agent(user_agent : str = Header()): return user_agent\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 27 . HTTPie로 User-Agent 헤더 테스트 http -v localhost:8000/agent\n04 HTTP 요청\n3 - 28 . HTTP 상태 코드 지정 from fastapi import FastAPI, Header\napp = FastAPI()\n@app.get(“/happy”) def happy(status_code=200): return “:)”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 29 . HTTP 상태 코드 지정 http localhost:8000/happy\n3 - 30 . HTTP 상태 코드 지정 from fastapi import FastAPI, Response\napp = FastAPI()\n@app.get(“/header/{name}/{value}”) def header(name: str, value: str, response: Response): response.headers[name] = value return “normal body”\n3 - 31 . 응답 HTTP 헤더 테스트 http localhost:8000/header/marco/polo\n파일 저장\n위 코드를 test_json.py 파일로 저장한다.\n3 - 32 . JSON 폭발 방지 # pip install pytest import datetime import pytest from fastapi.encoders import jsonable_encoder import json\n@pytest.fixture def data(): return datetime.datetime.now()\ndef test_json_dump(data): with pytest.raises(TypeError): _ = json.dumps(data)\ndef test_encoder(data): out = jsonable_encoder(data) assert out json_out = json.dumps(out) assert json_out\npytest 실행 pytest test_json.py\n3 - 33 . 모델 변형 from datetime import datetime from pydantic import BaseModel\n\nPydantic 모델 정의\nclass TagIn(BaseModel): tag: str\nclass Tag(BaseModel): tag: str created: datetime secret: str\nclass TagOut(BaseModel): tag: str created: datetime\n\n\n입력 데이터 생성\ninput_data = {“tag”: “fastapi”}\n\n\nTagIn 모델 사용 (데이터 검증)\ntag_in = TagIn(**input_data) print(tag_in)\n\n\nTag 모델 사용\ntag_data = { “tag”: “fastapi”, “created”: datetime.now(), “secret”: “my_secret” } tag = Tag(**tag_data) print(tag)\n\n\nTagOut 모델 사용 (Tag에서 특정 필드 제외)\ntag_out = TagOut(**tag.dict()) # secret 필드는 자동으로 제외됨 print(tag_out)\npytest 실행 python tag.py\n3 - 34 . 모델 변형 from datetime import datetime from model.tag import Tag\ndef create(tag: Tag) -&gt; Tag: “““태그를 생성한다.”“” return tag\ndef get(tag_str: str) -&gt; Tag: “““태그를 반환한다.”“” return Tag(tag=tag_str, created=datetime.utcnow(), secret=““)\n3 - 35 . 모델 변형 from datetime import datetime from model.tag import TagIn, Tag, TagOut import service.tag as service from fastapi import FastAPI\napp = FastAPI()\n@app.post(‘/’) def create(tag_in: TagIn) -&gt; TagIn: tag: Tag = Tag(tag=tag_in.tag, created=datetime.utcnow(), secret=“shhhh”) service.create(tag) return tag_in\n@app.get(‘/{tag_str}’, response_model=TagOut) def get_one(tag_str) -&gt; TagOut: tag: Tag = service.get(tag_str) return tag\n3 - 36 . 실행 uvicorn web.tag:app\n3 - 37 . Tag 가져오기 요청 http -b localhost:8000/GoodTag\n04 자동 문서화\n3 - 1 . 접속하기 http://localhost:8000/docs\n3 - 2 . ㅇ\n3 - 3 . ㅇ\n3 - 4 . ㅇ"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "나노 소재\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n보석들\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n나노 소재\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n광섬유와 보석\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n첨단 세라믹스 (Fine Ceramics)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n세라믹(Ceramic)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n전통 세라믹스 (Classical Ceramics)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n고무\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n섬유\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n철과 알류미늄\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n금(Ag)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n구리(Cu)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n청동\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n철기\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n건축\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n중금속 (Heavy Metal)\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n플라스틱\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n범용 플라스틱\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n신소재\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "misc/mse_05_0.html",
    "href": "misc/mse_05_0.html",
    "title": "나노 소재",
    "section": "",
    "text": "나노 소재의 원리와 이를 활용한 3가지 기술에 대해 다루고자 한다.\n01 거리의 단위\n① 밀리 (Milli) (1 밀리미터: 1mm)\n1m의 1/1000 = 0.001m\n② 마이크로 (Micro) (1 마이크로미터: 1μm)\n1m의 1/1,000,000 = 0.000001m\n③ 나노 (Nano) (1 나노미터: 1nm)\n1m의 1/1,000,000,000 = 0.000000001m\n02 파괴의 정의 (破壞, Failure)\n모든 면이 동일한 정육면체 물체를 힘으로 눌러서 파괴하면, 구조적 무결성이 깨져 이전에는 존재하지 않았던 두 개 이상의 새로운 표면이 생성된다.\n이러한 새로운 표면은 재료 내부의 원자가 분리되어 형성된 균열면(파단면)으로, 파괴의 직접적인 증거이다.\n재료공학에서 파괴 재료가 기계적 하중(압축, 인장 등)에 의해 원래 구조를 유지할 수 없는 상태로 변화할 때 파괴로 간주한다.\n파괴는 연성 또는 취성에 따라 다르게 나타날 수 있다.\n① 연성 재료는 변형 후 파괴되면서 상대적으로 거친 표면을 남긴다. ② 취성 재료는 변형 없이 깨져 비교적 매끄러운 파단면이 나타난다.\n03 나노 소재의 원리 (Nanomaterials)\n\n결합 에너지 (Bond Energy, BE)\n\n재료 내부의 원자들은 주위에 있는 다른 원자들과 균형 잡힌 결합을 이루고 있어 에너지가 낮고 안정된 상태에 있다.\n반면, 표면에 있는 원자들은 일부 결합이 끊어진 상태로 노출되어 불안정하고 높은 에너지 상태를 유지한다.\n\n표면 에너지 (Surface Energy)\n\n표면 원자들이 불완전한 결합 상태를 가지기 때문에 이 상태를 유지하려면 더 많은 에너지가 필요하다.\n이 높은 에너지가 바로 표면 에너지이며, 표면을 최소화하려는 자연스러운 경향으로 이어진다.\n\n표면 에너지 증가 정육면체가 파괴되면서 새로운 표면이 형성될 때 표면 에너지는 증가한다. 이는 내부 원자들이 안정적으로 결합했던 상태에서 일부 원자들이 표면으로 노출되어 결합이 끊기기 때문이다.\n\n나노 크기에서는 표면 에너지가 상대적으로 커지고, 물질의 물리적, 화학적 특성이 이 표면 에너지에 의해 크게 영향을 받는다.\n\n새로운 특성 발현 기존 재료와 다른 성질이 나노 크기에서 나타날 수 있다. 이러한 나노입자는 전자, 의약품, 에너지 저장, 촉매, 코팅 기술 등 다양한 분야에서 사용된다.\n표면 대 부피 비 증가 (Surface-to-Volume Ratio)\n\n물체가 나노미터 크기로 축소되면, 전체 원자 중 표면에 위치한 원자들의 비율이 급격히 증가한다.\n나노 크기로 분할될수록 표면적이 빠르게 증가하여 원래 덩어리 상태보다 수억 ~ 수십억 배에 달할 수 있다.\n\n기하학적 효과 정육면체를 예로 들면, 각 변을 절반으로 자를 때마다 표면적은 증가하고 부피는 감소한다. 이 과정이 반복되면 작은 입자로 나뉘며 새로운 표면이 계속 생성되어 총 표면적이 급격히 증가한다.\n\n가로, 세로, 높이가 각 1m인 정육면체의 표면적과 부피는 다음과 같다:\n표면적과 부피 각 조각이 1nm 크기인 정육면체가 된다고 가정하면, 각 조각의 면적은 다음과 같다:\n각 조각의 면적\n나노 입자수와 총 표면적 따라서 물체를 나노 단위로 잘랐을 때 표면적이 약 60억 배(6 x 10⁹) 커질 수 있다.\n\n표면적과 용해 속도 관계 가루 설탕은 미세한 입자로 구성되어 있어 단위 질량당 표면적이 훨씬 더 크다. 표면적이 크다는 것은 물 분자와 접촉할 수 있는 면적이 많다는 의미이며, 이는 설탕 분자가 더 빨리 물에 퍼져 용해 속도가 증가함을 의미한다.\n입자 크기의 차이 덩어리 설탕은 표면적이 상대적으로 작아 물 분자와의 접촉이 제한적이다. 결과적으로 설탕 분자가 서서히 녹아 더 긴 시간이 걸린다.\n\n이 원리는 설탕뿐만 아니라 소금, 약물 등 다양한 고체 물질의 용해에도 적용된다. 제약 산업에서도 약물이 미세화될수록 체내 흡수 속도가 증가하는 원리를 활용한다.\n04 나노 금의 성질 금은 벌크 금(Bulk Gold)과 나노 금(Nano Gold)으로 나뉘며, 이는 일반적인 금과 나노 금을 구별하기 위한 용어이다.\n\n색상 변화 나노 크기에서 금은 표면 플라스몬 공명 효과로 색상이 변화한다.\n\n(Surface Plasmon Resonance, SPR)\n100nm 이상: 황금색 75nm: 갈색 75-20nm: 연두 20nm 이하: 빨간색 이는 빛과 나노 입자 표면 전자의 상호작용이 달라지기 때문이다.\n\n화학 반응성 증가 귀금속에 속하는 금은 화학 반응성이 매우 적은 금속이며, 부식에 강하고 산화되지 않는 성질을 가지고 있다.\n\n금은 원자 구조 상 안정된 전자 배치을 가지며, 외부 환경에서 쉽게 전자를 잃거나 받지 않는다.\n그러나 나노 크기의 금은 연료 전지의 촉매로 사용될 만큼 화학 반응성이 증가한다.\n\n전자기적 성질 변화 금은 일반적으로 전기 전도도가 높은 금속이다.\n\n그러나 나노 크기의 금은 반도체와 유사한 특성을 나타내어, 금속과 달리 특정 조건에서만 전기 전도성이 증가하는 특성을 보일 수 있다.\n\n자성 변화 일반적으로 상온에서 강자성을 띠지 않는 비자성 금속이다. 그러나 나노 크기에서는 전자의 양자 효과가 두드러지며, 이로 인해 자성 특성도 달라질 수 있다.\n\n금의 나노입자가 강자성을 띠는 현상은 특별한 조건에서 일어나며, 금이 다른 물질들과 합금되거나 특정한 결정 구조를 가질 때 더 자주 나타날 수 있다.\n예를 들어, 금 나노입자에 다른 자성 금속(예: 철, 코발트)을 도핑하거나, 금이 고유의 전자 구조를 통해 자성 모멘트를 유도할 수 있는 경우가 있을 수 있다.\n\n녹는점 변화 일반적인 금의 녹는점은 약 1,064°C (1,947°F)이다.\n\n이는 금속 중에서 비교적 높은 편에 속하는 녹는점으로, 금은 고온에서 변하지 않고 안정된 특성을 유지한다.\n나노 금의 경우, 표면 원자들이 더 활성화되어 낮은 온도에서 융합할 수 있다. 나노 금 입자의 크기가 작아지면 표면 에너지가 증가하여 녹는점이 100°C까지 떨어질 수 있다.\n05 크랜베리 글라스 (Cranberry Glass) 빅토리아 시대 (Victorian Era, 1837–1901)\n빅토리아 시대에 유행한 독특한 붉은 색 유리로, 금(Au) 또는 은(Ag) 나노 입자가 포함된 유리이다.\n당시 사람들은 나노 기술 개념을 몰랐지만, 전통적인 제작 기술을 통해 오늘날 우리가 이해하는 나노 입자 효과를 무의식적으로 활용한 결과물이었다.\n\n제작 원리 유리 제조 과정에서 염화금 용액을 첨가하여 금 나노 입자를 포함시킨다.\n\n(Chloroauric Acid, AuCl₃)\n열처리 중 금 이온이 나노 크기의 금 입자로 환원되어 유리 내부에 분포하게 된다.\n\n색상 변화의 원리 금 나노 입자는 빛과 상호작용하여 특정 파장의 빛을 흡수하고 다른 파장의 빛을 반사한다. 표면 플라스몬 공명 효과에 의해, 크랜베리 글라스 특유의 붉은 색상을 띠게 한다.\n\n입자 크기에 따라 색상이 달라질 수 있으며, 금 입자의 크기가 작을수록 더 짙은 붉은색이 나타난다.\n\n나노 기술적 관점 비록 나노 기술이 명확히 이해되지 않았던 시절이었지만, 크랜베리 글라스는 나노 크기의 금 입자가 물질의 광학적 특성을 변화시키는 대표적인 예시이다. 오늘날에는 이러한 원리가 다양한 광학 센서, 약물 전달 시스템, 나노 전자 소자 등에서 사용된다.\n\n06 양자점 기술 (QD, Quantum Dot)\n나노미터 크기의 반도체 입자로, 빛의 흡수 및 방출 특성이 조절 가능한 고급 소재.\n\n양자점의 핵심 소재 셀레늄화 카드뮴(CdSe)는 빛을 흡수하는 핵심 소재로 사용되고, 황화 아연(ZnS)는 이를 감싸는 외부 층이다.\n\n양자점의 안정성을 높이고 빛을 방출하는 성질을 향상시킨다.\n이 기술은 빛에 대한 반사나 방출 특성이 나노 크기에서 달라지므로, 물질의 특성을 매우 정밀하게 조정할 수 있다.\n예를 들어, 양자점의 크기를 조절하여 파란색 빛을 흡수하고 빨간색 또는 초록색을 방출하는 특성을 가질 수 있다.\n\n디스플레이 및 광학 소자 양자점은 크기에 따라 빛의 파장을 조절할 수 있어, 더 넓은 색상 범위와 높은 밝기를 제공하는 디스플레이 기술에 적용될 수 있다.\n\n이미 QLED TV와 같은 상용 제품들이 등장했으며, 양자점의 특성을 활용하여 색 재현율을 높이는 기술이 발전하고 있다.\n이는 기존의 LED 기술보다 더 풍부하고 정밀한 색감을 제공하는 데 중요한 역할을 한다.\n\n위조 방지 양자점은 특정 파장의 빛을 흡수하고 방출하는 특성을 가지고 있기에, 자외선(UV)을 이용한 위조 지폐 판별에도 활용될 수 있다.\n\n자외선이 비추어졌을 때 양자점이 특정한 방식으로 빛을 방출하는 성질을 이용하면, 진품과 위조지폐를 구분할 수 있다.\n형광 잉크는 위조 지폐에 적용하기 비교적 쉬워, 자체적으로 쉽게 모방되거나 위조될 수 있다.\n반면, 양자점과 같은 기술은 원자 단위로 소재를 정밀하게 제어해야 하므로 기술적 난이도가 높다.\n따라서 양자점을 활용한 위조 지폐 방지 기술이 상용화되었을 때, 위조 방지 효과는 매우 클 것으로 기대된다.\n\n에너지 효율성 향상 태양광 발전 시스템에 양자점을 활용하면 에너지 효율성을 향상시킬 수 있다.\n\n양자점은 광흡수와 발광 효율이 높아, 저비용 고효율의 에너지 생성이 가능해질 수 있다.\n또한, 광합성 장치와 같은 에너지 생산 기술에서도 중요한 역할을 할 수 있다.\n광합성 효율을 향상시키는 양자점이 연구되고 있으며, 차세대 에너지 생산 기술에 큰 영향을 미칠 것으로 예상된다.\n\n의료 및 환경 모니터링 의료 분야에서 이미징 및 진단 기술에 사용될 수 있다.\n\n양자점은 고해상도 이미징에 매우 유용하며, 약물 전달 또는 세포 추적 등에 적용될 수 있다.\n또한, 환경 모니터링에서도 양자점은 오염 물질 감지나 대기 질 분석에 활용될 수 있다."
  },
  {
    "objectID": "misc/mse_04_3.html",
    "href": "misc/mse_04_3.html",
    "title": "광섬유와 보석",
    "section": "",
    "text": "첨단 세라믹스의 포함되는 광섬유와 인공 다이아몬드에 대해 다루고자 한다.\n\n01 광섬유\nOptical Fiber 빛을 이용하여 신호를 전송하는 전송 매체로, 주로 유리로 제작되며, 매우 얇고 유연한 특성을 지닌다. 이러한 특징 덕분에 정보 전송 분야에서 중요한 역할을 한다.\n구조 1. 코어 (Core)\n광섬유의 중심 부분으로 빛이 직접 전파되는 경로. 고굴절율 유리로 만들어져 있으며, 정보 전송의 핵심 역할을 한다.\n\n클래딩 (Cladding)\n\n코어를 감싸고 있는 외부 유리 층. 저굴절율 유리로 이루어져 있어, 빛이 코어 내부에 머물도록 전반사를 유도한다.\n\n코팅 (Coating)\n\n클래딩 바깥을 감싸는 보호층으로 주로 플라스틱 재질. 광섬유를 물리적 손상이나 환경적 요인(습기, 열)으로부터 보호하는 역할을 한다.\n이 두 유리층이 내부에서 빛을 가두는 원리로 작동하여, 빛이 고속으로 이동할 수 있다.\n광섬유를 통해 디지털 신호를 전송할 때, 빛을 켜고 끄는 방식으로 0과 1의 신호를 전송한다. 빛이 통과하면 ‘1’, 빛이 없으면 ’0’으로 해석된다.\n특징 빛의 속도는 매우 빠르기에, 신호 전송도 매우 빠르다. 다만, 스마트 기기나 네트워크 장비의 처리 속도에 따라 신호의 전송 속도가 영향을 받을 수 있다.\n광섬유는 신호 손실이 적어, 수백 또는 수천 킬로미터까지 신호를 전달할 수 있다. 이를 이용해 해저 광섬유 케이블이 대양을 가로질러 전 세계를 연결한다.\n해저 광섬유 케이블 설치 방법 ① 광섬유 케이블을 선박에 말아 놓는다.\n이 선박은 일반적으로 케이블 레이어(Cable Layer) 또는 케이블 설치선(Cable Laying Ship)이라고 하며, 대형 케이블 레이어에 경우, 대형 화물선이나 유조선에 가까운 크기이다.\n② 설치 지점에 도착한 뒤, 케이블을 조금씩 풀어가며 해저에 놓는다.\n각 케이블 마디마다 풍선이나 부력 장치가 부착되어 있어 케이블이 천천히 바다 속으로 내려가도록 돕는다.\n이 풍선들은 케이블이 너무 빠르게 가라앉아 손상되는 것을 방지한다.\n또한, 광섬유 케이블 외부는 폴리에틸렌(PE)로 감싸져 있어, 물고기나 해저 바닥에서 오는 충격으로부터 광섬유를 보호하는 기능을 한다.\n③ 선박에 설치된 장비는 케이블을 해저 바닥에 맞춰서 정확히 놓을 수 있도록 조정된다.\n필요시 트렌처(Trencher) 또는 ROV(무인 잠수정)와 같은 추가 장비가 사용되어, 케이블을 해저에 묻거나 고정할 수 있도록 돕는다.\n④ 케이블을 설치하는 동안 실시간 모니터링 시스템이 작동하여 케이블의 상태와 설치 위치를 추적한다.\n이 시스템은 케이블이 올바르게 설치되었는지 확인하고, 문제가 발생하면 즉시 조치를 취할 수 있도록 한다.\n02 보석 (寶石, Gemstone) 자연에서 발견되며 높은 압력과 온도에서 형성된 순수한 단결정 물질.\n이러한 조건은 지구 내부의 깊은 지층에서 발생하며, 이곳에서 원자들은 가장 안정적인 배열을 이루는 구조로 재배치된다.\n주로 화산 활동이나 지각 변동을 통해 지표면 가까이로 이동하여 발견된다.\n특징 희소성이 뛰어나 특정 지역에서만 형성되고 발견되기 때문에 매우 드물어 높은 가치를 지닌다.\n또한 내구성이 강해 높은 경도와 화학적 안정성 덕분에 쉽게 손상되지 않으며, 독특한 색상과 투명도, 그리고 뛰어난 빛 반사 특성 덕분에 귀중하게 여겨진다.\n\n주요 보석\n\n① 루비 및 사파이어\n(Ruby / Sapphire, Al₂O₃ 산화 알루미늄)\n루비는 산화 알루미늄에 크롬(Cr)이 미량 포함되어 붉은색을 띠며, 사파이어는 티타늄(Ti)과 철(Fe) 등의 불순물이 색상을 변형시켜 파란색을 비롯한 다양한 색상을 나타낸다.\n② 다이아몬드\n(Diamond, C 순수탄소)\n탄소 원자들이 3차원 구조로 결합한 매우 견고한 결정체로, 지구에서 가장 단단한 천연 물질로 알려져 있다.\n③ 수정\n(Quartz, SiO₂ 이산화 규소)\n다양한 색상으로 존재하며, 맑고 투명한 형태가 보석으로 사용된다. 자수정, 황수정 등 다양한 변종이 있다.\n④ 에메랄드\n(Emerald, Be₃Al₂Si₆O₁₈ 베릴알루미늄실리케이트)\n녹색의 색상을 띠며, 이는 결정 구조 내의 크롬(Cr) 또는 바나듐(V) 원소 때문이다. 매우 희귀하며 결함이 많은 경우가 흔해 보석 품질을 유지하기 어렵다.\n\n형성 과정\n\n보석은 지구 깊은 곳에서 형성되며, 이 과정은 주로 고온과 고압 상태에서 이루어진다.\n지구의 내부는 여러 층으로 구성되어 있으며, 보석이 형성되는 주요 환경은 맨틀이나 지구 깊은 곳이다.\n약 1,000 ~ 2,000°C의 고온과 수백 기압 이상의 고압 환경이므로, 원소들이 결합하거나 결정화가 일어날 수 있는 상태가 된다.\n즉, 이 환경에서 원소들이 결합하여 보석이 될 수 있는 단결정 구조를 만든다.\n보석이 지구 깊은 곳에서 형성된 이후, 화산 활동이나 지각판의 융기 현상에 의해 지표 근처로 이동하게 된다.\n화산 폭발의 경우, 고온에서 형성된 보석이 지표로 튕겨 나오고 지각판의 융기 현상에서는 지구 표면 근처에서 보석이 드러나면서 발견된다.\n결정화 과정 보석은 시간이 지나면서 안정적인 결정 구조를 형성하는 과정을 겪는다. 이 과정에서 특정 원소들이 결합하여 결정체를 이루게 된다.\n원자들은 일정한 규칙을 따라 배열되면서 고유의 결정 구조를 형성한다. 이 과정은 천천히 진행되며, 시간이 지나면서 구조가 안정화된다.\n이러한 구조들은 보석의 경도, 광택, 색상 등 물리적 특성에 큰 영향을 미친다. 이는 더 높은 경도와 빛 반사 능력을 가질 수 있게 되며, 미적 가치가 크게 증가한다.\n03 다이아몬드 Diamond 또는 금강석(金剛石) 4월의 탄생석\n18세기, 프랑스의 화학자 앙투안로랑 드 라부아지에\n(Antoine-Laurent de Lavoisier, 1743–1794)\n그는 다이아몬드, 석탄, 흑연을 태우는 실험을 통해 이들 모두에서 동일한 기체인 이산화탄소(CO₂) 가 발생함을 발견했다.\n이 실험은 그 당시 다이아몬드와 흑연이 서로 다른 물질로 여겨졌던 시대에 중요한 의미를 가졌다.\n질량 보존의 법칙 (Law of Conservation of Mass)\n화학 반응에서 반응물의 총 질량은 생성물의 총 질량과 항상 같다는 법칙. 물질은 화학 반응 중 새로운 형태로 변환되더라도 생성되거나 소멸되지 않으며 단지 배열만 바뀌기 때문에 총 질량은 변하지 않는다.\n라부아지에는 연소 과정에서 반응한 산소(O₂)의 질량과 생성된 이산화탄소(CO₂) 질량을 정확히 측정했다.\n그 결과 반응 전후의 총 질량이 같다는 것을 확인했다.\n그는 다이아몬드, 석탄, 흑연은 서로 다른 형태의 탄소 동소체임을 입증했다.\n모든 연소 과정에서 동일한 산물이 생성되므로 동일한 원소 탄소로 이루어져 있음을 밝혔다.\n04 합성 다이아몬드\n1955년, 미국의 다국적 기업 GE는 합성 다이아몬드를 최초로 제조하였다.\n(General Electric)\n이는 흑연을 고온 고압 조건에서 처리하는 방식으로 이루어졌다.\n이 실험은 다이아몬드가 자연에서 형성되는 것과 유사한 조건에서 합성될 수 있음을 보여준 중요한 사건이었다.\n이 실험에서는 주요 원료로 흑연을 사용하였으며, 다이아몬드 형성에 필요한 1,400 ℃ 의 고온과 55,000atm 의 고압 환경을 모방하였다.\n이 조건에서 탄소 원자들이 정밀하게 결합하여 다이아몬드 결정 구조가 형성하였다.\n인공 다이아몬드 다이아몬드는 자연적으로 형성되거나 인공적으로 합성될 수 있으며, 다음과 같은 두 가지 주요 특성으로 인해 다양한 산업 분야에서 활용된다.\n① 최고의 경도\n자연에서 발견되는 물질 중 가장 높은 경도를 가진다\n모스 경도(Mohs Hardness): 10\n이 특성 덕분에 산업용 절단 공구, 연마재, 드릴 비트 표면 코팅 등 고강도 작업이 필요한 도구에 사용된다.\n단단한 재료를 절단하거나 천공하는 과정에서도 쉽게 마모되지 않아 효율적이다.\n② 높은 열 전도율\n비금속 물질 중 최고 수준의 열 전도율을 가지며, 구리보다 약 5배 높은 열전도성을 보인다.\n이러한 특성은 전자 기기나 고성능 반도체 장치에서 발생하는 열을 빠르게 방출하는 데 유용하다.\n이로 인해 CPU, 레이저 다이오드, 고출력 광학 시스템 등에서 열 관리 재료로 사용될 수 있다.\n두 가지 특성의 활용\n\n드릴의 표면에 매우 적은 양의 인공 다이아몬드를 덩어리 형태로 뭉쳐서 붙여 사용한다.\n\n이 방식은 다이아몬드의 경도를 활용하여 천공, 절단, 연마, 미세 가공 등 다양한 작업에서 효율적으로 쓰인다.\n① 천공: 금속, 암석, 콘크리트 등 다양한 소재를 뚫는 데 사용된다. 이는 드릴의 수명이 길어지며, 고속으로 작업을 할 수 있다.\n② 절단: 다이아몬드 절단 휠이나 절단기를 이용하여 강철, 콘크리트, 대리석 등을 손쉽게 절단할 수 있다.\n③ 연마: 반도체 제조나 고급 광택이 필요한 장비 연마에 다이아몬드를 연마재로 사용한다.\n④ 미세 가공: 고정밀 부품 가공이나 미세한 표면 처리가 필요한 작업에서도 사용된다.\n\n전자 제품의 열 방출을 효율적으로 도와 과열을 방지한다.\n\n전자 제품에서 온도는 매우 중요한 요소이다.\n전자 기기나 컴퓨터 칩 등의 고속 작동 장치에서는 전류가 흐를 때 발생하는 열을 효율적으로 방출해야 하며, 그렇지 않으면, 부품이 과열되어 성능 저하나 손상이 발생할 수 있다.\n그래서 보통 구리와 같이 열전도율이 높은 물질이 열을 분산시키는 데 사용된다.\n다이아몬드는 구리보다 열전도율이 훨씬 높기 때문에, 열을 빠르게 전달하고 기기 내부의 온도를 균등하게 분배하는 데 매우 유용하다.\n다이아몬드를 손에 올리면 열을 빠르게 전달받아 따뜻하게 느껴지고, 얼음 위에 놓으면 빠르게 차가워지는 효과를 볼 수 있다.\n이러한 빠른 열 전달 특성 덕분에 고성능 전자 기기나 반도체 장치에서 열 관리에 사용되며, CPU, GPU와 같은 장치의 과열을 방지하고 효율적인 쿨링 시스템을 제공한다.\n① 고성능 전자 제품: 반도체나 컴퓨터 칩에 사용된다.\n② 레이저와 광학 장비: 레이저 다이오드나 광학 장비에서 활용된다.\n③ 고급 열 방출 장치: 고성능 엔진이나 전력 변환 장치 같은 분야에서도 활용된다.\n05 다이아몬드의 역사 다이아몬드는 고대부터 희귀하고 아름다운 보석으로 여겨졌다.\n1840년, 영국의 빅토리아 여왕이 앨버트 공과의 결혼식에서 하얀 드레스를 입은 것이 이후 결혼식 드레스 전통에 큰 영향을 미쳤다.\n당시 하얀색 드레스는 순수와 결백을 상징하게 되었으며, 이후 많은 유럽 귀족들이 이를 따르기 시작했다.\n다이아몬드 반지 빅토리아 여왕은 결혼식에서 다이아몬드보다 금과 사파이어 장식이 된 반지를 사용했다.\n그러나 다이아몬드가 결혼 반지로 인기를 끌게 된 것은 그보다 후대의 일이다.\n다이아몬드가 본격적으로 “사랑과 불멸”의 상징으로 자리 잡게 된 것은 1930년대 미국에서 De Beers 회사의 마케팅 캠페인 때문이다.\n“A Diamond is Forever”라는 광고 문구가 대중적으로 다이아몬드를 결혼 반지로 사용하는 문화에 크게 기여했다.\n다이아몬드의 상징 다이아몬드는 어떤 물질로도 긁히지 않으므로, “영원성”을 상징하기도 한다.\n하지만 깨짐(취성, Brittleness) 성질을 가지므로 큰 충격을 받으면 쉽게 깨질 수 있다.\n또한, 수억 ~ 수십억 년의 시간이 지나면 흑연으로 변할 가능성이 있다.\n이는 열역학적으로 흑연이 다이아몬드보다 에너지가 낮고 더 안정적인 상태이기 때문이다.\n다만, 자연 상태에서는 이러한 변화가 매우 느리게 진행되며, 고온·고압 환경에서는 이 변환이 가속화될 수 있다.\n이러한 다이아몬드의 특성은 사랑의 속성과 비유되기도 한다.\n색상의 다양성\n\n순수한 다이아몬드는 투명하다.\n\n① 결정 구조\n다이아몬드는 탄소 원자들이 정사면체 구조로 강하게 결합된 결정체이다. 이 구조는 매우 균일하여 빛이 산란되지 않고 내부를 통과할 수 있도록 한다.\n② 전자 대역 구조\n다이아몬드는 전자 대역폭이 매우 넓어 가시광선 영역의 빛을 흡수하지 않기 때문에 빛이 대부분 투과한다.\n③ 불순물 없음\n다른 원소가 거의 포함되지 않아 빛을 방해하거나 색을 띠게 하는 요인이 없다.\n\n불순물을 첨가하면 색상을 바꿀 수 있다.\n\n① 옐로 다이아몬드 소량의 질소(N)를 첨가하면 노란색을 띈다.\n영화 ’티파니에서의 아침’에서 등장하면서 대중에게 더욱 잘 알려졌다.\n(Breakfast at Tiffany’s, 1961)\n대표적인 옐로 다이아몬드는 Sun-Drop(태양의 눈물)로 110.3캐럿의 크기를 가진다.\n2011년 11월 소더비(Sotheby’s) 제네바 경매에서 약 1억 240만 달러(당시 약 140억 원)에 낙찰되었다.\n② 블루 다이아몬드 소량의 붕소(B)를 첨가하면 푸른색을 띈다.\n일반 무색 다이아몬드보다 10배 이상의 가치를 지닌다고 알려져 있다.\n대표적인 블루 다이아몬드는 Blue Hope(블루 호프)로 약 45.52 캐럿의 크기를 가진다. 현재 시세로 추정되는 가치는 약 2억 5천만 달러(약 3,300억 원)이상이다.\n이 다이아몬드를 가진 사람들은 불행이나 재앙에 휘말린다는 미신이 있다. 현재 워싱턴 D.C. 국립 자연사 박물관에 전시되어 있다. (National Museum of Natural History)\n\n결정구조를 바꾸면 색상을 바꿀 수 있다.\n\n③ 핑크 다이아몬드 특정한 압력으로 결합 구조가 살짝 뒤틀리면 분홍색을 띈다.\n내부의 미세한 압력 변화나 격자 구조의 결함에 의해 발생하는 것으로, 이 결함들이 특정 빛을 흡수하고 반사하는 특성을 만들어낸다.\n대표적인 핑크 다이아몬드는 Pink Legacy(분홍빛 유산)는 18.96 캐럿의 크기를 가진다.\n2018년 11월 스위스 제네바에서 진행된 크리스티 경매에서 5,037만 5,000스위스프랑(당시 약 574억 원)에 낙찰되었다.\n영화 ‘색, 계’(‘色·戒’, 2007)에 등장한 핑크 다이아몬드 반지는 까르띠에에서 제작된 소장품이다.\n다이아몬드는 단순히 물리적 특성을 넘어, 사랑과 희귀함을 상징한다.\n시대에 따라 다양한 영화나 역사적 사건 속에서 특별한 의미를 부여받으며 그 가치를 지속적으로 인정받고 있다."
  },
  {
    "objectID": "misc/mse_04_1.html",
    "href": "misc/mse_04_1.html",
    "title": "전통 세라믹스 (Classical Ceramics)",
    "section": "",
    "text": "대표적인 전통 세라믹스인 도자기, 유리 그리고 시멘트의 전반적인 개념에 대해 다루고자 한다.\n\n01 서론\n주로 자연에서 얻은 원료를 사용하며, 불순물이 많이 포함된 상태로 가공된다. 전통 세라믹스는 크게 도자기, 유리, 시멘트로 나눌 수 있다. 각 물질은 독특한 가공 방법을 통해 완성된다.\n\n\n02 도자기 (陶瓷器, Ceramics)\n(소결 세라믹스, Sintered Ceramics) 미세한 세라믹 분말을 높은 온도에서 가열하여 입자들이 서로 결합하도록 만든 재료. 이 과정에서 분말은 녹지 않고 고체 상태로 치밀하게 응축되어 강도와 내구성이 향상된다.\n신석기 시대부터 사용된 가장 오래된 세라믹의 형태.\n고온에서 원료를 가열하여 입자들이 서로 결합하면서 단단해지는 과정. 소결은 ’불로 굽는다’는 의미로, 흙을 구워 도자기 제품을 만드는 전통적인 방식이다.\n도자기는 도기와 자기의 합성어로, 일반적으로 젖은 흙을 빚어 원하는 형태를 만든 후 가마에서 적당히 구워 형태를 고정시키는 방식.\n\n도기 (陶器, Earthenware)\n\n서민들이 주로 사용하는 그릇과 용기에 사용되었으며, 예를 들어 뚝배기와 항아리 같은 것이 대표적이다.\n비교적 낮은 온도에서 구워내고, 가마에서 구워지는 시간도 짧다. 이로 인해 기공(빈 공간)이 많이 남아 물을 흡수하기 쉽고, 단단함이 낮다.\n또한, 도기를 두드리면 둔탁한 소리가 나는데 이는 내부 구조가 치밀하지 않기 때문이다.\n\n자기 (磁器, Porcelain)\n\n높은 온도와 긴 시간 동안 구워내면서 도기보다 훨씬 단단하고 치밀한 구조를 가진다.\n이는 더욱 정교하고 내구성이 높은 그릇이나 기물 제작에 적합하다.\n가마에서 하루 정도 긴 시간을 높은 온도로 구워 충분한 열을 가함으로써, 물질이 수축하여 빈 공간이 채워지고, 내구성과 치밀성이 크게 향상된다.\n자기를 두드리면 경쾌한 소리가 나며, 이는 내부가 치밀하게 구성되어 도기보다 훨씬 단단함을 나타낸다.\n이러한 치밀함과 경쾌한 소리 특성 덕분에 편경이나 편종과 같은 전통 악기 제작에도 사용된다.\n따라서 도기와 자기는 쓰임새뿐 아니라 소리와 촉감, 외관에서도 구별되는 독특한 특징을 가지고 있다.\n\n도자기 제작 과정 ① 초벌구이 (도기 단계) 구멍이 많고 물을 쉽게 흡수하므로, 물이나 국을 담기에는 적합하지 않다.\n\n② 유약(釉薬) 바르기 이는 도기의 표면을 매끄럽게 하고 물이 새지 않게 한다. 유약은 도자기를 가마에 다시 구울 때 표면에 얇은 유리막을 형성하는 물질이다.\n③ 재벌구이 유약을 바른 후 가마에 다시 넣어 고온에서 구우면, 유약이 녹아 끈적한 상태가 되고 시간이 지나면서 도자기의 표면에 유리막이 형성된다.\n이로 인해 도자기가 단단해지며 물이 스며들지 않는 표면을 갖게 된다.\n\n도자기의 발달 배경\n\n① 연료와 점토\n고온에서 도자기를 구워야 하므로, 연료의 종류와 양이 중요한 역할을 한다.\n전통적으로 도자기와 자기를 구울 때 많은 연료가 필요했으며, 나무가 주로 사용되었다.\n초창기 도자기 가마는 나무를 땔감으로 사용했으며, 이는 많은 양의 연료를 소모하게 된다.\n② 중동 지역\n사막이 많고 나무가 부족하여 도기를 만들 때 주로 낮은 온도에서 구운 제품을 생산할 수밖에 없었다.\n그래서 중동에서는 자기보다는 상대적으로 저온에서 구워내는 도기가 주를 이었다.\n③ 중국의 도자기\n중국은 나무가 풍부하여 고온을 지속적으로 유지할 수 있었고, 이를 통해 고온에서 구워내는 단단한 자기를 생산할 수 있었다.\n이로 인해 중국의 자기는 기술적으로 우수한 특성을 지니게 되었고, 중동 상인들이 중국을 방문했을 때 놀라움을 느꼈다고 전해진다.\n④ 고품질 점토\n고령토(Kaolin)와 같은 고품질 점토는 자기를 만드는 데 중요한 자원이다.\n이는 고온에서도 형태를 유지하며 치밀하게 구워낼 수 있는 특성이 있다. 중국과 한국은 이러한 고품질 점토가 풍부하게 발견되는 지역이다.\n한국과 중국은 오랜 역사 동안 자기를 대량 생산할 수 있었던 중심 지역이였다.\n\n임진왜란 (1592–1598) (壬辰倭亂 / Imjin War, Japanese invasions of Korea)\n\n한국은 자기 생산에 큰 타격을 입었으며, 일본의 자기 기술이 발전하는 계기가 되었다.\n① 도자기 장인의 강제 이주\n임진왜란 동안 일본은 조선의 도자기 기술에 큰 관심을 가졌다. 전라남도와 경상도 지역의 도자기 장인들은 일본에 끌려가거나 강제로 이주하게 되었다.\n이들은 가마와 도자기 제조 기술을 일본에 전수했으며, 히데요시의 명령에 따라 이들은 일본에서 극진히 대우받았으나, 돌아가지 못하도록 강제되었다.\n② 일본 도자기의 발전\n조선의 도자기 기술은 일본의 사쓰마 도자기(Satsuma ware)와 아리타 도자기(Arita ware) 같은 일본의 독자적인 도자기 문화 발전에 큰 영향을 미쳤다.\n일본은 조선에서 전수받은 기술을 바탕으로 새로운 도자기 스타일을 발전시켰다. 특히 아리타 지역은 일본에서 백자와 같은 고급 자기를 생산하는 중요한 중심지로 자리 잡았다.\n일본은 유럽과 아시아로 수출하며 세계적인 도자기 제작 강국 중 하나로 성장하게 되었다.\n\n중국의 도자기\n\n① 중국 도자기와 무역\n중국의 도자기는 고대부터 비단과 함께 중요한 수출 품목이었다. 고대 중국은 중동과 유럽으로 도자기를 수출하며 상업적 거래를 활발히 했다.\n비단과 도자기는 중국의 대표적인 고급 무역품으로, 실크 로드를 통해 여러 지역에 전해졌다.\n② 중국 도자기의 가치\n중국의 도자기는 중동과 유럽에서 매우 귀하게 여겨졌다. 그 이유는 중국의 도자기가 지닌 독특한 아름다움과 내구성 때문이다.\n당시 유럽에서는 중국의 도자기를 쉽게 모방할 수 없었고, 그로 인해 수입품으로서의 가치가 더욱 높아졌다.\n유럽에서는 이를 “차이니스” 또는 “차이니스 머니”라고 불렀다.\n“차이니스”는 중국의 고유한 스타일을 의미하며, “차이니스 머니”는 그 당시 중국의 도자기나 상품이 귀중한 자산으로 여겨졌다는 뜻으로 사용되었다.\n③ 도자기 운송의 험난함\n해상과 육로를 통한 도자기 운송 과정은 당시로서는 험난했고 운송 중에 도자기가 깨지는 일이 흔했다.\n고대와 중세 시대에는 도로와 해상의 교통 수단이 발달하지 않았음으로, 도자기와 같은 깨지기 쉬운 제품을 운송하는 데에는 많은 위험이 따랐다.\n손상된 도자기는 운송 도중에 빈번히 발생한 문제였고, 이는 상인들이 비용을 부담해야 하는 중요한 사항이었다.\n④ 도자기의 높은 수익성\n도자기는 그 자체로 고급 소비재로서 가치가 높았기 때문에, 운송 도중에 일부가 파손되더라도, 나머지 제품이 충분히 이익을 가져올 수 있었다.\n⑤ 국제적 영향력 확대\n도자기와 비단 무역을 통해 중국의 국제적 영향력이 확대되었다.\n실크 로드를 통해 중국은 유럽, 중동, 아시아와의 무역 네트워크를 구축하며 경제적, 정치적 영향력을 확대했다.\n도자기와 비단은 중국의 대표적인 수출품으로, 이들의 무역은 중국의 국제적인 위상 강화에 큰 역할을 했다.\n\n비단길의 유래 (실크로드, Silk Road)\n\n① 한나라와 흉노족의 갈등\n한나라(漢)는 흉노족의 위협에 대응하기 위해 월지국과 연합하려 했다.\n흉노족은 한나라 북부에 지속적인 위협을 가했으며, 한 무제(漢武帝)는 서쪽의 월지국(月氏國)과 연합하여 흉노족을 견제하려는 전략을 세웠다.\n월지국은 과거 흉노족의 침략으로 인해 큰 피해를 입었기 때문에, 한 무제는 그들과의 동맹 가능성을 타진하고자 했다.\n② 장건의 서역 사절 파견\n기원전 139년경, 장건(張騫)은 한 무제의 명령으로 서역으로 파견되었다.\n그는 이동 중에 흉노족에게 포로로 붙잡혀 약 10여 년간 억류되었으며, 탈출 후 서역으로 향하였다.\n③ 월지국과의 연합 실패\n장건이 월지국에 도착했을 때, 월지족은 흉노족의 위협을 피해 아프가니스탄 지역으로 이주한 상태였다.\n이로 인해 장건의 동맹 제안은 실패로 끝났다.\n④ 장건의 서역 정보 수집\n장건은 이 여정에서 서역의 풍부한 자원, 문화, 교역 가능성에 대한 정보를 수집하였고, 이는 한나라와 서역 간 교역로 확립에 기여했다.\n이후, 중국과 서역(중앙아시아, 페르시아, 로마 제국) 간의 교역로가 활발히 열리면서 비단길(실크로드)이 탄생하게 되었다.\n⑤ 당나라 (唐, 618–907)\n중국 역사상 가장 부유하고 국제 교류가 활발했던 시기.\n비단길을 통해 페르시아, 아라비아, 유럽까지 무역이 활발하게 이루어졌고, 다양한 외국 문화와 종교가 장안(현재의 시안)으로 들어왔다.\n⑥ 장안의 화제 (長安之話題)\n당나라 시기 장안(시안)이 중국의 정치적, 문화적 중심지였다는 데에서 비롯된 표현으로, 특히 모든 사람들이 주목하는 이야기를 뜻하는 은유적 표현이다.\n장안은 당나라 시기 수도였으며 실크로드의 중요한 중심지였다. 현재 이곳은 삼성의 반도체 공장이 위치해 있는 곳으로도 알려져 있다.\n03 유리 (琉璃, Glass) (용융 세라믹스, Fused Ceramics) 고온에서 용융 상태로 만들어진 세라믹스.\n고대 이집트에서 시작되어 이후 유럽으로 전파된 세라믹 형태로, 용융(Melting) 및 급랭(Quenching)을 통해 제조된다.\n고온에서 원료를 녹여 끈적하고 유동성 있는 상태로 만든 후 냉각하여 고체 상태로 굳히는 과정.\n이 과정에서 원료는 완전히 녹아 결정을 이루지 않는 비정질 구조의 유리(Amorphous material)가 된다.\n① 주요 성분\n주로 이산화규소(SiO₂)로 구성되며, 일반적으로 모래에서 얻어진다. 실리카(Silica)는 유리의 투명성, 내열성, 내화학성을 부여하는 중요한 성분이다.\n② 용융 및 가공\n유리는 고온에서 용융되어 흐물흐물한 상태로 변하며, 이 상태에서 다양한 형태로 성형하거나 가공할 수 있다.\n이 과정은 유리 제조의 기본적인 특징이다.\n③ 블로잉법 (Blowing)\n유리 덩어리를 불어서 공기 주머니를 만들어 속이 비게 만드는 방법.\n베네치아는 1세기경부터 유리 공예로 유명했으며, 이 기술을 바탕으로 섬세한 유리 제품을 생산했다.\n오늘날 이 기술은 기계화되어 자동화된 생산 공정이 이루어지고 있다.\n④ 과거 유리의 활용\n유리는 전기 절연체로써 송전탑에서 사용되었으며, 고온과 날씨 변화에 강한 특성을 지녔기 때문이었다.\n유리 절연체는 전력 산업에서 중요한 역할을 하고 있다.\n⑤ 유리의 대체제 최근에는 알루미나(Al₂O₃)가 유리 대신 사용되는 경우가 많다. 알루미나는 유리보다 더 높은 절연 성능과 내구성을 제공하며, 특히 전력 산업에서 많이 사용된다.\n\n투명한 유리 유리는 고대 로마와 이집트에서 사용되었지만, 현대적인 투명 유리의 생산 방식은 17세기 초 네덜란드에서 시작되었다. 이 시기에 유리 제조 기술이 발전하면서 투명한 유리가 본격적으로 등장했다.\n\n① 빛의 투과\n빛을 통과시키는 성질이 있어 광학 기기에서 중요한 역할을 한다. 유리는 빛이 지나갈 때 일정한 방식으로 굴절되거나 반사되며, 이는 광학 기기나 렌즈 제작에 사용된다.\n② 유리의 굴절\n유리의 곡면을 통해 빛의 경로를 조정할 수 있다.\n빛은 유리의 굴절률에 따라 경로가 변하는데, 이는 빛을 퍼뜨리는 오목 렌즈(Concave Lens)와 빛을 모으는 볼록 렌즈(Convex Lens)에서 발생하는 현상이다.\n③ 렌즈의 활용\n렌즈는 유리를 사용하여 빛의 굴절을 이용해 여러 가지 광학 효과를 나타내는 기기이다.\n사람의 눈에 맞는 곡률을 가진 렌즈를 사용하여 빛을 정확하게 망막에 집중시킨다.\n이를 통해 시력을 교정하는 데 사용된다.\n볼록 렌즈는 원시 교정에, 오목 렌즈는 근시 교정에 사용된다.\n\n망원경 (Telescope)\n\n1608년, 네덜란드의 안경업자인 한스 리퍼셰이. (Hans Lippershey, 1570–1619)\n그는 두 개의 렌즈를 결합하여 원거리 물체를 확대하는 장치를 만들었고, 이는 망원경의 초기 형태이다.\n① 상업적 활용\n상인들은 망원경을 사용하여 배의 도착 시점을 예측하고, 후추의 가격 변동을 예측하여 거래하였다.\n가격이 가장 높을 때인 배가 도착하기 전에 후추를 판매하고, 도착 후 가격이 떨어질 때는 후추를 구매하여 이윤을 추구하는 전략을 썼다.\n② 특허 불허\n당시 망원경의 발명은 독창성이 부족하다고 판단되어, 특허를 인정받지 못했으며, 발명자로서의 지위도 인정되지 않았다.\n이 소식은 유럽 전역으로 퍼졌고, 갈릴레오 갈릴레이가 이에 관심을 가졌다.\n1609년, 이탈리아의 천문학자인 갈릴레오 갈릴레이 (Galileo Galilei, 1564–1642)\n그는 베네치아에서 유리를 구매한 뒤, 망원경의 성능을 수학적으로 최적화하여 14 ~ 20배의 배율을 가진 망원경으로 천체 관측을 시도했다.\n그는 이를 통해 달과 목성의 위성을 관찰하며 지동설을 지지하는 중요한 증거를 발견한다.\n이를 통해 당시의 지구 중심 우주관을 깨뜨리고 헬리오센트리즘(Heliocentrism)을 지지했다.\n종교 재판\n그는 베니스 공화국에서 망원경의 군사적, 상업적 가능성을 제시하기 위해 시연을 했으며, 베니스 귀족들에게 직접 망원경을 보여주었다.\n이후 그의 지동설 지지는 지구 중심의 우주관을 따르는 교회 교리와 충돌하며 이단으로 정죄받았다.\n갈릴레오는 교회에 의해 1633년 종교 재판을 받았고, 자신의 이론을 철회하라는 명령을 받았다. 그 후 가택연금 상태로 여생을 보냈다.\n\n유리의 깨짐 원리 유리는 인장력에 약한 특성을 가지고 있어 외부에서 압력이 가해지면 미세한 균열이 생기고 점차 깨지게 된다.\n\n유리의 표면은 보통 압축력을 받아 내부에서 외부로 당겨지는 인장력을 받는다. 예를 들어, 앞면에서 외부의 충격이 가해지면, 뒷면에서 이 균형을 잃고 쉽게 깨진다.\n특히, 뾰족한 물체는 유리의 한 부분에 집중적인 힘을 가해 더 쉽게 깨진다.\n이는 유리의 내부 응력과 외부 힘이 상호작용하는 방식이다.\n① 강화유리 (Tempered Glass)\n고온 상태에서 빠르게 냉각시키는 방법으로 표면에 압축 응력을 형성하여, 외부 충격을 받았을 때 깨지지 않고, 인장력에 강한 내구성을 보인다.\n② 강화유리의 깨짐 방식\n강화유리가 깨질 때 날카로운 파편 대신 둥글고 뭉툭한 조각으로 깨지는 이유는 안전성을 고려한 설계이다.\n강화유리는 작은 파편으로 부서져 큰 부상을 방지하고, 특히 자동차 유리나 창문에 사용될 때 안전성을 제공한다.\n③ 고층 건물의 유리\n두 개의 강화유리 사이에 탄력성을 가진 투명한 접착 필름을 넣은 구조로, 이를 통해 내구성을 높이고, 유리가 깨졌을 때 산산조각이 나는 것을 방지한다.\n이 접착 필름은 유리를 보호하며 충격에 의해 유리가 분해되는 것을 막는 역할을 한다. 외부 충격이나 자연 재해에도 튼튼한 보호 기능을 제공하여, 건물의 안전성을 높인다.\n④ 유리 표면의 매끄러움\n유리의 표면은 흠집 없이 매우 매끈하게 만들어져야 한다. 그 이유는 표면의 흠집이 충격을 받았을 때 유리가 약해지기 때문이다.\n먼저, 유리 원료를 고온에서 용융시켜 액체 상태로 만든다. 그 후, 액체 유리를 평평한 표면에 붓거나 롤러로 눌러 평평하게 만든다.\n이 과정에서 중력은 유리를 평평하게 눌리는 데 중요한 역할을 하며, 유리 표면은 고르게 펴지게 된다.\n이후 유리는 냉각되어 단단하게 굳어지며, 이때 흠집이 없는 고른 표면을 형성할 수 있다.\n04 시멘트 (Cement) (Setting and Hydraulic Ceramics, 응결경화 세라믹스)\n물이나 기타 액체와 반응하여 반죽이 단단해지는 초기 단계인 응결 및 구조적으로 강해지는 경화를 거쳐 단단한 고체 상태로 변하는 세라믹 재료이다.\n\n석고 (Gypsum / CaSO₄·2H₂O, 이수화황산칼슘)\n\n피라미드 건축 시 돌과 돌 사이를 결합하는 모르타르(Mortar)로 사용되었다. 이는 빠르게 굳고 작업이 용이해 고대 건축물에서 널리 사용되었다.\n석고를 이용해 내부 벽을 부드럽게 다듬거나, 회반죽(Plaster)으로 마감하는 데도 사용되었다.\n석고는 CaSO4·2H2O 형태로 자연적으로 존재하며, 간단한 가공으로 사용 가능한 재료였으므로, 고대 문명에서 쉽게 활용되었다.\n\n수경성 석회 (Hydraulic Lime / CaCO₃, 석회석)\n\n점토 성분이 포함된 석회석을 800~1000°C의 온도로 소성하여 제조한 건축 재료.\n이러한 점토 성분(알루미나, 실리카)이 석회와 함께 반응하여 물과 결합하는 능력을 제공한다.\n이로 인해 습한 환경이나 물 속에서도 경화가 가능해졌다.\n고대 이집트와 로마에서는 일반 소성 석회(비수경성 석회)와 함께 수경성 석회를 혼합해 초기 건축물에 사용했다.\n르네상스와 산업 혁명기에 수경성 석회는 중요한 건축 재료로 발전했다. 특히 다리, 항구 등 습한 환경에서 필요한 구조물에 사용되었다.\n\n포졸라나 시멘트 (Pozzolana Cement / 석회(CaO)+포졸란)\n\n기원전 2세기경, 이탈리아 포추올리(Pozzuoli) 지역에서 발견된 화산재를 석회와 혼합해 최초로 사용되었다.\n수경성 석회의 발전된 형태로, 석회와 화산재 또는 인공 포졸란 물질의 혼합으로 제작된다.\n(인공 포졸란 물질의 예: 플라이 애시, 고로 슬래그)\n화산재의 주요 성분인 규산(SiO₂)과 알루미나(Al₂O₃)가 석회와 화학 반응하여 수경성을 부여한다. 내화학성, 내구성, 습기 저항력이 뛰어나 특정 환경에서 효과적이다.\n이 기술로 판테온, 로마 수도교 등 내구성이 뛰어난 건축물이 완성되었다.\n19세기에 포틀랜드 시멘트가 개발된 이후, 포졸라나 시멘트는 포틀랜드 시멘트에 포졸란 물질을 혼합하는 방식으로 발전했다.\n에디스톤 등대의 역사 (Eddystone Lighthouse)\n영국 남서부의 플리머스(Plymouth)에서 남서쪽으로 약 22.5km 떨어진 에디스톤 암초(Eddystone Rocks)에 위치한다.\n1698년, 영국의 발명가인 헨리 윈스타니.\n(Henry Winstanley, 1644–1703)\n세계 최초의 해상 등대(1대 등대)를 건설했다. 1703년, 대폭풍으로 등대가 파괴되고 윈스탠리도 그 자리에서 목숨을 잃었다.\n1709년, 영국의 조선 기술자인 존 루디어.\n(John Rudyerd, 1650–1718)\n목재 구조로 2대 등대를 건설했다. 그러나 1755년 화재로 인해 소실되었다.\n1759년, 영국의 토목 기술자인 존 스미턴.\n(John Smeaton, 1724–1792)\n수력학적 원리를 적용하여 바다 위에서 견딜 수 있는 내구성이 뛰어난 3대 에디스톤 등대를 설계했다.\n특히 바람과 파도의 압력을 분산시키는 둥근 형태의 등대 구조는 그의 혁신적인 설계 중 하나였다.\n스미턴은 실험을 통해 점토가 포함된 석회석을 소성한 수경성 석회를 발견하고 이를 결합재로 사용했다.\n스미턴의 수경성 석회 사용 기술은 이후 포틀랜드 시멘트 개발에 영감을 주어 현대 시멘트 기술의 기초가 되었다.\n1882년, 영국의 등대 건설 기술자인 제임스 닉홀슨 더글러스.\n(James Nicholas Douglass, 1826–1898)\n기존의 스미턴 등대가 침식으로 위험해지자 새롭게 건설했다. 현재 사용 중인 등대이며, 스미턴이 건설한 3대 등대의 기초는 기념물로 남아 있다.\n\n포틀랜드 시멘트 (Portland Cement)\n\n1824년, 영국의 발명가인 조셉 애스프딘.\n(Joseph Aspdin, 1778–1855)\n포틀랜드 시멘트를 개발 및 특허받았으며, 이는 수경성 세라믹의 대표적인 예이다.\n물과 혼합되어 화학 반응을 일으키며 경화가 일어난다. 이 과정에서 열을 방출하며 단단해진다.\n이 이름은 영국 도싯(Dorset) 지방의 포틀랜드 섬의 석회암과 경화된 시멘트의 색상과 질감이 비슷하다는 이유로 붙여졌다.\n\n재료 및 제조 방식 석회석(Limestone)과 점토(Clay)를 원료로 사용하며, 이들을 고온에서 소성(Calculate)하여 제조한다.\n\n이 혼합물은 물을 더하면 수화작용을 일으켜 단단하게 굳으며, 이는 현대 건축에서 필수적인 특성으로 간주된다.\n\n사용과 발전 포틀랜드 시멘트는 개발 이후 건축 및 토목 공사에서 널리 사용되며, 이후 현대적인 건축 재료의 표준으로 자리 잡았다.\n\n특히, 철근 콘크리트와 같은 구조물의 발전에 크게 기여했다.\n\n콘크리트 (Concrete)\n\n시멘트, 물, 골재(모래, 자갈 등) 및 경우에 따라 다양한 혼화제를 혼합하여 만든 인공적인 석재.\n혼합된 재료가 응결 및 경화하면서 단단해져 건축 및 토목 구조물에 널리 사용된다.\n\n주요 구성 성분 ① 골재 (Aggregate) 모래(세골재)와 자갈(굵은 골재)이 포함되며, 콘크리트의 부피 대부분을 차지하고 구조적 강도를 제공한다.\n\n② 결합재 (Binder) 시멘트가 주요 결합재로 사용되며, 물과 화학 반응을 통해 골재를 단단히 고정하는 역할을 한다.\n③ 물 (Water) 시멘트와 반응해 수화작용(Hydration)을 통해 콘크리트를 굳히고, 반응이 완전히 진행되면 콘크리트가 단단해진다.\n\n콘크리트 경화 과정 ① 응결 (Set) 시멘트와 물의 화학 반응으로 콘크리트가 점차 형태를 유지할 수 있는 상태로 변하는 초기 과정.\n\n② 경화 (Hardening) 시간이 지나면서 콘크리트가 점점 강도를 높이는 과정. 이 수화 작용은 콘크리트의 특성인 강도와 내구성을 결정짓는 핵심 과정이다.\n\n콘크리트 제조 및 시공\n\n① 레미콘 (Ready-mix Concrete)\n공장에서 미리 혼합된 콘크리트를 회전 드럼이 장착된 트럭으로 현장에 운송하는 방식으로, 이는 응결을 방지하고 일정한 품질을 유지할 수 있게 한다.\n이는 현대 건설 현장에서 널리 사용되는 방식이다.\n② 거푸집 (Formwork)\n콘크리트를 특정 모양으로 굳히기 위해 사용되는 틀이며, 콘크리트가 충분히 경화된 후 제거된다.\n구조물의 형태와 치수를 결정하며, 콘크리트가 경화될 때까지 이를 지지한다."
  },
  {
    "objectID": "misc/mse_03_3.html",
    "href": "misc/mse_03_3.html",
    "title": "섬유",
    "section": "",
    "text": "섬유의 전반적인 개념과 합성 섬유에 대해 다루고자 한다.\n01 천연 섬유\n\n면 (Cotton, 綿) 목화나무의 열매에서 얻어지며, 부드럽고 통기성이 뛰어난 특성을 가지고 있다.\n\n그러나 물에 젖으면 구김이 잘 생기고 주름이 쉽게 지기 때문에 관리가 필요한다.\n\n모 (Wool, 毛) 양털에서 얻어지며, 뛰어난 보온성과 따뜻함을 제공한다. 하지만 빨래 후 사이즈가 줄어드는 경향이 있다.\n비단 (Silk, 絲) 누에고치에서 얻은 섬유로, 부드럽고 고급스럽고 아름다우며 고급 의류에 자주 사용된다. 그러나 내구성이 비교적 약하고, 세탁 시 손상되기 쉬운 단점이 있다.\n\n또한, 가격이 상대적으로 높다.\n02 합성 섬유 석탄이나 석유를 원료로 하여 화학적 과정을 통해 제조된다.\n이 과정은 환경에 영향을 미칠 수 있지만, 내구성이 뛰어나고 다양한 용도로 사용된다. 천연섬유에 비해 땀과 습기를 잘 흡수하지 않으며 정전기가 발생하기 쉽지만, 세탁 후 빨리 마르는 장점이 있다.\n1935년, 듀폰에서 월리스 캐로더스와 그의 연구팀이 최초의 합성섬유인 나일론(Nylon)을 개발한다. 웰스(S. Wells)와 로젠타울(E. Rosenbaum) 등 과학자들도 이 연구에 기여했다.\n이는 합성섬유 산업에 큰 전환점을 가져왔다.\n1938년 2월, 듀폰 회사는 기존의 돼지 털 대신 나일론을 처음으로 칫솔의 솔 소재로 상용화였다.\n위생적이고 내구성이 뛰어난 특성으로 칫솔 산업에 큰 영향을 미쳤다.\n돼지 털을 사용한 칫솔은 위생에 문제가 있을 수 있었고, 알레르기나 세균 번식 우려도 존재했기 때문에,\n나일론은 훨씬 개선된 대체물로 자리잡았다.\n1940년 5월 15일, 나일론을 사용한 스타킹이 판매되었다.\n단순히 피부를 가리는 용도에 그쳤던 이전의 스타킹과 달리, 각선미를 강조하고, 더욱 편안하며 내구성이 뛰어났다.\n나일론 스타킹은 여성들로부터 큰 호응을 얻었고, 출시 몇 시간 만에 400만 켤레가 팔리는 기록을 세웠다.\n또한, 피부를 완전히 가리지 않는 특성 때문에, 미용과 패션에 대한 새로운 문화를 창출했다.\n발래방에서는 나일론을 사용한 빨래망 등이 실용적으로 사용되었으며, 특히 나일론 스타킹은 여성들의 패션 아이템으로 자리잡았고, 그로 인해 미용과 패션에 대한 관심이 높아졌다.\n군사적인 용도로도 큰 변화를 일으켰다.\n낙하산의 천, 군용 텐트, 밧줄, 타이어 등 다양한 군사 장비에 사용되었다. 특히, 내구성과 경량성이 중요한 군사 분야에서 중요한 역할을 하였다.\n그러나, 나일론은 일상적인 의류에는 다소 불편할 수 있다. 이는 통기성이 부족하고, 유연성이나 편안함이 떨어지기 때문이다.\n1941년, 석유에서 추출하여 폴리에스터(Polyester) 섬유를 개발한다. 1946년, 영국에서 첫 상품화가 되었고, 1953년, 미국의 듀폰에서 본격적으로 상업화가 시작했다.\n일반적인 옷감으로 널리 사용되며, 내구성, 주름 방지 특성, 색상 유지력이 뛰어나 일상복 및 여러 의류에 많이 사용된다.\n생산이 용이하고, 세탁 후 형태가 잘 유지되며 내마모성이 높아 의류 산업에서 중요한 역할을 한다.\n또한, 의류용으로 적합한 특성을 보유하고 있어, 현재 합성섬유 시장의 큰 비중(약 3분의 2)을 차지하고 있다.\n1950년, 미국의 듀폰사는 석유에서 추출한 아크릴(Acrylic) 섬유를 “Orlon”이라는 제품명으로 생산하기 시작했다. 가볍고 부드러우며, 보온성이 뛰어나 겨울 의류나 따뜻한 옷감에 많이 사용된다. 이는 양털의 특성을 가지면서 가격이 저렴하고 세탁이 용이한 장점이 있다.\n그러나 여름 의류나 다양한 용도에 적합하지는 않으며, 통기성이 떨어질 수 있다.\n결과적으로, 합성 섬유는 천연 섬유에 비해 생산이 용이하고, 관리와 유지보수가 쉬워 대중적인 선택이 되고 있다.\n03 기타 합성 섬유\n\n폴리우레탄 (Polyurethane)\n\n신축성이 뛰어나며, 적은 양으로도 우수한 효과를 낼 수 있다.\n이 소재는 활동성과 편안함을 중시하는 스포츠웨어나 레깅스, 수영복과 같은 기능성 의류에 많이 사용된다.\n’스판덱스(Spandex)’라는 상표명으로도 알려져 있으며, 이는 고가의 상표명으로 사용된다. 이는 폴리우레탄 기반이지만 특수한 가공을 통해 더욱 높은 탄성과 내구성을 제공한다.\n고무보다 뛰어난 내구성을 가지며, 최대 5배까지 늘어나도 원래 상태로 복귀하는 우수한 신축성을 가졌다.\n또한, 산과 알칼리에 대한 높은 내화학성을 가지며, 주로 다른 섬유와 혼합되어 사용된다.\n\n아라미드 (Aramid, Aromatic Polyamides)\n\n극한 환경에서도 잘 견디는 내열성과 강한 물성을 가진 합성 섬유. 고온과 마모에 강하며, 내화학성이 높고, 강철의 5배에 달하는 강도, 나일론의 10배에 이르는 신축성, 경량성, 그리고 우수한 난연성을 가진다.\n이로 인해 중장비용 로프, 산업용 보호 의류와 장갑, 소방복, 방탄 재킷, 방탄 헬멧 등 다양한 용도에 활용되고 있다.\n1973년, 듀폰에서 개발한 파라 아라미드 섬유 제품인 케블라(Kevlar)가 있다.\n일반 아라미드 섬유의 강화 버전인 파라 아라미드 섬유는 방탄 조끼, 방검복, 우주 장비 등에서 활용된다.\n머시룸 효과 (Mushroom Effect)\n재료가 특정 충격을 받을 때 형태가 버섯 모양으로 퍼지는 특성.\n주로 방탄복과 같은 보호 장비에서 충격을 완화하고 흡수하는 데 쓰이며, 충격이 가해지는 순간 표면적을 넓혀 피해를 최소화하는 원리로 작용한다.\n\n부직포 (不織布, Non-woven Fabric)\n\n섬유를 직조하지 않고 랜덤하게 배치한 섬유를 접착제로 붙여 만든 소재.\n직물과 달리 부직포는 신축성이 거의 없으며, 내구성이 낮아 약한 힘에도 쉽게 찢어질 수 있다.\n그러나, 가볍고 통기성이 뛰어나며 보온성도 우수한 특성으로 인해 다양한 1회용 용도로 많이 사용되며, 대표적으로 마스크에 활용된다.\n이는 직물에 비해 구조가 불규칙하고 미세한 면적이 많아 미세 이물질이 잘 통과하지 못하기 때문이다.\n부직포는 미세먼지를 막는 2차 필터 역할을 하며, 정전기 처리된 3차 필터가 초미세먼지를 걸러낸다.\n정전기는 약 8 ~ 9시간이 지나면 사라져 초미세먼지 차단 효과가 떨어지므로, 미세먼지가 심한 날에는 새 마스크를 사용하는 것이 좋다.\n이와 같은 원리로 포장지에도 정전기 유지 기능을 적용하여 이물질 방지 효과를 높이고 있다.\n원료로는 주로 폴리에스터(Polyester)와 폴리프로필렌(Polypropylene) 같은 섬유가 사용된다.\n\n탄소 섬유 강화 플라스틱 (CFRP, Carbon Fiber Reinforced Polymer)\n\n강도 대비 가벼운 무게가 특징으로, 자동차 경량화 및 에너지 효율성을 위해 널리 사용되고 있다.\n예를 들어, 전기차 배터리는 평균 400 ~ 540kg에 달하는데, 이처럼 무거운 배터리는 차량의 전기 소모를 늘리고 타이어 마모를 가속화시킨다.\n이를 해결하기 위해 자동차 제조업체는 CFRP를 사용해 차량의 무게를 줄이고 있다.\nCFRP는 플라스틱에 탄소 섬유를 직물 형태로 붙이는 과정을 반복해 복잡한 모양을 제작할 수 있다.\n초기에는 골프채와 낚싯대 같은 스포츠 장비에 주로 사용되었으나, 현재는 F1 경주용 자동차, 고압 가스통, 보잉 787에도 활용되고 있다.\n예를 들어, BMW i7(2022)에서는 충돌 시 강한 힘을 받는 부위를 제외한 나머지 차체에 탄소 섬유를 적용해 경량화를 실현했다.\n이처럼 탄소 섬유는 차량 경량화뿐 아니라 항공, 스포츠, 우주 기술 분야에서도 중요한 역할을 하고 있다.\n자율 비행 개인 항공기(OPPAV)와 같은 차세대 항공기에서도 활용되지만, 배터리 소모량이 많아 현재 상용화에는 한계가 있다.\n또한, 내구성을 활용하여 수소연료 보관용 고압 수소통 제작에도 사용된다.\n런닝화의 경우 나이키(Nike)가 탄소 섬유 플레이트를 최초로 도입하였으며, 현재 전 세계 운동화 생산 시장에서 약 42%의 점유율을 보유하고 있다."
  },
  {
    "objectID": "misc/mse_03_1.html",
    "href": "misc/mse_03_1.html",
    "title": "범용 플라스틱",
    "section": "",
    "text": "범용 플라스틱의 종류와 특징을 전반적으로 다루고자 한다.\n범용 플라스틱 (Universal Plastic)\n다양한 용도로 사용될 수 있는 플라스틱.\n01 폴리염화비닐 (Polyvinyl Chloride, PVC)\n가장 먼저 개발된 소재로, 19세기에 우연히 합성된 물질이다.\n이 소재가 개발되기 전에는 베이클라이트가 사용되었다.\n1872년, 독일의 화학자 하인리히 알트\n(Heinrich Adolf Gustav Meyer)\n그는 vinyl chloride라는 화합물을 합성하는 실험을 진행했다. 이 실험에서 그는 불완전한 반응을 통해 물질이 변하는 모습을 관찰했는데,\n이 변형된 물질이 바로 PVC였다.\n당시 알트는 PVC의 성질을 제대로 이해하지 못했으며, 발견 당시에는 물질이 단단하거나 견고하지 않았기 때문에,\n그는 이 물질에 대해 실험을 계속 진행하지 않았다.\n1912년, 독일의 화학자 프리드리히 윌헬름\n(Polyvinyl Chloride)\n그는 PVC를 가열하고 압력을 가함으로써, 경화되고 단단한 형태로 변하는 성질을 발견하게 되었다.\n그 덕분에 PVC는 다양한 산업에서 활용될 수 있는 소재로 발전하게 되었다.\n이 소재는 대량 생산이 어려웠으며, 제작 과정에서 여러 개의 틀을 사용하고 오븐에서 경화시키는 절차가 필요했다.\n그러나 값이 저렴하고 충격에 강하며, 열과 바람에 잘 견디는 특성 덕분에 플라스틱은 다양한 용도로 널리 사용되었다.\n특히 급변하는 환경에서도 우수한 내구성을 보이는 특성으로 실외 부품에 적합한 플라스틱 소재로 자리 잡았다.\n1927년부터 대량 생산이 시작된 경질 PVC는 내구성과 내후성이 뛰어나, 실외 부품에 활용되는 대표적인 소재가 되었다.\n활용 많은 강의실이나 상업 건물에서 볼 수 있는 하얀색 유리 창틀 은 PVC 창틀로 만들어진 경우가 많다.\n단열성이 뛰어나 에너지 효율을 높일 수 있으며, 유지 보수가 쉬우므로 실내외 모두에서 인기가 높다.\n이러한 특성 덕분에 건축 자재뿐만 아니라 배관, 지붕 커버 등에도 널리 사용되고 있다.\nPVC의 문제점\n\n환경적 문제 염소(Cl)를 포함하고 있어 연소 시 염화수소(HCl) 가스가 발생한다.\n\n이 가스는 유독하며, 대기 중에 방출되면 환경에 악영향을 미칠 수 있다. 염화수소가 물과 반응하면 염산이 형성되어 부식성 물질이 될 수 있다.\n또한, PVC를 소각하는 과정에서 다이옥신(Dioxin)이 생성될 위험이 있다.\n이는 매우 유독한 화학물질 그룹을 지칭하는 용어로, 주로 폴리염화디벤조-파라-다이옥신(PCDDs) 계열의 화합물을 의미한다.\n다양한 산업 공정과 화학 반응에서 생성되는 부산물로, 자연적으로는 화재나 화산 활동에서도 소량 생성될 수 있다.\n이러한 이유로, PVC 폐기물은 신중하게 처리해야 하며, 일부 지역에서는 PVC 사용을 제한하거나 금지하고 있다.\n재활용 문제 PVC는 재활용이 어렵고, 재활용 과정에서의 분리 및 정제 작업이 복잡하여 비용이 많이 든다.\n\n물리적 문제 기본적으로 단단한 재질로 유연성이 부족하다.\n\n이를 개선하기 위해 가소제를 첨가하여 유연성을 높인 연질 PVC는 전선 피복, 바닥재, 페스티벌 천막, 포장 필름 등 다양한 용도로 활용된다.\n그러나 가소제의 종류에 따라 인체에 유해할 가능성이 있으며, 특히 저가의 가소제를 사용할 경우 내구성과 안전성 문제가 발생할 수 있다.\n프탈레이트 (Phthalate) 소재의 유연성을 증가시키기 위해 첨가되는 물질 중 하나.\n이것은 환경 호르몬으로, 인체 내에서 호르몬 작용을 방해하고 교란시킬 수 있다.\n이러한 이유로, 이것이 포함된 제품은 어린이 물품에서 사용이 금지되어 있으며, 특히, 장난감이나 아기용품에 사용되지 않도록 엄격히 규제된다.\n비프탈레이트 플라스티사이저 (Non-Phthalate Plasticizer 또는 Phthalate-Free Plasticizer)\n가장 널리 사용되는 플라스티사이저는 프탈레이트 계열이지만,\n비프탈레이트 플라스티사이저는 프탈레이트의 대체 물질로 개발되어, 상대적으로 안전성이 높다고 평가받는다.\n프탈레이트와 같은 역할을 하면서도 독성 및 환경적 영향을 최소화한 대체 물질로, 시트르산 에스테르, 아디프산 에스테르, 고분자 플라스티사이저 등이 있다.\n납 (Lead, Pb) 내구성을 높이기 위해 일부 제품에는 납이 첨가되기도 한다. 예를 들어, 농구공에 납 성분이 포함될 수 있다.\n따라서 농구공과 같은 납 성분이 포함된 제품을 만진 뒤에는 반드시 손을 씻어야 하며,\n특히 어린이들이 이러한 제품을 입에 대거나 제품에서 떨어진 이물질을 섭취하지 않도록 주의해야 한다.\n\nPVC 비닐 탕수육 같은 뜨거운 음식을 포장할 때 사용하는 비닐은 고온에서 유해 물질이 나올 수 있으므로, 주의해야 한다.\n\n또한, 재활용이 어려우므로, 종량제 봉투에 버려야 한다.\n반면, 가정용 비닐은 주로 폴리에틸렌(PE)으로 만들어지며, 상대적으로 안전하고 환경적 영향이 적다.\n\n친환경 가소제 최근에는 ECO 가소제를 사용하여 인체에 무해하고 친환경적인 대체제를 찾는 노력이 진행 중이다.\n\n이러한 친환경 가소제는 환경에 덜 해롭고, 건강에 대한 위험성이 낮아 어린이 용품과 식품 포장재에 적합하다.\n02 폴리스티렌 (Polystyrene, PS)\n무색투명하여 색상을 입히기 쉽고, 가공성이 우수한 소재.\n1839년, 독일의 화학자 하인리히 하이만\n(Heinrich Heinrich)\n그는 스티렌(styrene)이라 불리는 유기 화합물을 사용하여 폴리머의 첫 번째 형태를 만들었으나, 당시에는 실용적 용도로 사용되지는 않았다.\n이것은 벤젠(Benzene)과 에틸렌(Ethylene)을 반응시켜 생성되는 화합물로, 물질 그 자체로는 액체이지만, 일정 조건에서 고체 형태로 중합될 수 있다.\n폴리머화 (Polymerization) 스티렌 분자가 중합 반응을 거쳐 폴리머 형태로 변환되는 과정이다.\n이때 생성된 폴리스티렌은 원래의 스티렌과 비교하여 물리적 성질이 크게 달라지게 된다.\n상업적 생산 1930년대 후반에 IG Farben사는 스티렌을 중합하여 투명하고 단단하며 가공하기 쉬운 플라스틱을 생산했다.\n이 물질은 특히 포장재나 일회용 용기와 같은 소비재의 제조에 적합하였고, 그 결과 폴리스티렌이 전 세계적으로 사용되는 주요 플라스틱이 되었다.\n70 ~ 90℃의 비교적 낮은 온도에서 부드러워지는 특징(연화)을 가지고 있어,\n요거트 통, 카세트테이프 필름, 요구르트 용기, 빨대, 1회용 용기 뚜껑 등 다양한 용기에 자주 사용된다.\n그러나 내구도가 낮아 충격에 약한 단점이 있다.\n03 발포 폴리스티렌 (Expanded Polystyrene, EPS)\n1954년, 미국의 화학 회사 Dow Chemical(현재 Dow Inc.)\n이곳에서 발포제로 폴리스티렌을 50배 팽창시켜 98%의 빈공간을 가진 초경량 소재를 개발했다.\n이 소재는 주로 스티로폼(Styrofoam) 으로 알려져 있으며, 탁월한 단열성과 충격 흡수 특성 덕분에 주로 완충제, 단열재, 흡음재로 사용된다.\n활용 이 소재는 경량화와 단열 성능을 높이는 데 유용한 특성을 가지고 있어, Japan Dome House와 같은 구조물에 사용된다.\n일부 건축물에서는 20배 정도 팽창된 EPS가 사용되며, 특히 모듈형 건축물이나 동그란 형태의 구조물에 적합하다.\nEPS의 팽창 정도를 조절할 수 있어, 다양한 건축적 요구에 맞는 특성을 제공한다.\n04 폴리에틸렌 (Polyethylene, PE)\n1933년, 영국의 화학자인 해롤드 스튜어트와 프레드릭 K. 고드리치 연구팀에 의해 처음 합성되었다.\n(Harold Stuart, Frederick K. Godfrey)\n이들은 고압 합성법을 통해 폴리에틸렌을 처음 만들었고, 이는 당시에는 상대적으로 중요한 기술로 여겨지지 않았다.\n그러나 1939년, 독일의 화학자 하인리히 웬셀이\n(Heinrich Wenzel)\n이를 상용화하려는 작업을 본격적으로 시작했으며, 이를 통해 폴리에틸렌은 여러 산업에서 빠르게 채택될 수 있었다.\n이 소재는 합성 플라스틱의 대표적인 제품 중 하나로, 특성상 부드럽고 유연한 소재이다.\n1941년, 미국의 듀폰사와 몬산토 등의 대기업들이\n(DuPont, Monsanto)\n대량 생산 기술을 개발하여 폴리에틸렌을 플라스틱 시장에서 주요 소재로 자리 잡게 되었다.\n특히 플라스틱의 혁신적 증가에 크게 기여했으며, 이는 PVC보다 저렴한 가격과 다양한 용도로 인해 더욱 각광을 받았다.\n두 가지 주요 형태로 생산된다.\n\n저밀도 폴리에틸렌 (Low-Density Polyethylene, LDPE)\n\n0.94 g/cm³ 미만의 소재.\n부드럽고 유연한 특성을 가지고 있으며, 주로 가정용 비닐봉지와 식품 포장재에 사용된다.\n이 소재는 뛰어난 가공성과 유연성, 가벼움, 투명성을 자랑하지만, 내구성은 상대적으로 낮다.\n\n고밀도 폴리에틸렌 (High-Density Polyethylene, HDPE)\n\n0.94 g/cm³ 이상의 소재.\n뛰어난 강도와 내구성을 자랑한다.\n이 소재는 반투명하며 질기고 내약품성이 뛰어나, 약품 통, 용기, 파이프 등 다양한 분야에서 널리 사용된다.\n두 가지 형태 모두 비교적 쉽게 재활용이 가능한 특성도 가지고 있어 지속 가능한 플라스틱 대체재로 자리 잡았다.\n05 폴리프로필렌 (Polypropylene, PP)\n1953년, 이탈리아의 화학자 고르디오니와 루이지 보르기 교수에 의해 개발되었다.\n(Giorgio Natta, Luigi Borghi)\n이후 1959년부터 본격적으로 상업 생산이 시작되었다.\n비교적 안전한 플라스틱으로, 연소 시 유해가스를 발생시키지 않는 것이 큰 장점이다.\n약 170°C 정도에서 연화가 시작되며, 그 온도 이하에서는 형태가 잘 유지된다.\n반투명하고 견고하며 내약품성이 뛰어난 특성을 가져 음식물 쓰레기통, 약품통, 음식 식기 등 다양한 용도로 널리 사용된다.\n06 폴리에틸렌 테레프탈레이트 (Polyethylene Terephthalate, PET)\n1941년, 영국의 화학자 존 렉스 윈필드와 제임스 테넌트 딕슨에 의해 처음 합성되었다.\n(John Rex Whinfield, James Tennant Dickson)\n1973년, 듀폰사의 화학자 나다니엘 와이어스\n(Nathaniel Wyeth)\n그는 PET를 이용한 병 제조 기술을 개발했다.\n1977년, 미국의 허쉬(Hershey) 사가 PET병을 채택하여 음료를 담기 시작했다.\n기체 차단 능력 우수 외부 기체의 침투를 막아주므로, 음료수 병(콜라)과 같은 용도로 적합하다.\n화약 및 약품 보관 부적합 화학적 안정성이 낮아 화약이나 강한 약품을 담기에는 적합하지 않다.\n복잡한 내부 구조 재질의 내부 구조가 복잡하여 재활용이 어려운 복합재료는\n대신 물리적인 형태를 변경하여 다른 용도로 활용하는 새활용(Upcycling)이 주로 이루어진다.\nPET의 정보\n밀도 차이 페트병(PET)의 밀도는 약 1.38 g/cm³로 물보다 무겁기 때문에 공기가 없으면 가라앉는다.\n반면 뚜껑 소재로 주로 사용되는 폴리프로필렌(PP)의 밀도는 약 0.9 g/cm³로 물보다 가벼워 물에 뜬다.\n이 원리를 이용해 물리적으로 분리하는 방법이 일반적이다.\n뚜껑 닫아서 배출하기 페트병은 최대한 압축한 후 뚜껑을 닫아야 하며, 이를 통해 이물질 유입을 방지하고 분리 공정이 원활해질 수 있다.\n접착제로 붙은 상표 일부 페트병은 재활용을 고려해 접착제를 사용하지 않거나 쉽게 제거 가능한 상표를 사용한다.\n접착제로 붙어 있어 쉽게 제거되지 않는 상표는 굳이 제거하지 않고 그대로 배출해도 무방하다.\n페트병 재활용 분리된 페트병은 잘게 부숴 PET 칩으로 만들어지며, 이를 활용해 섬유(폴리에스터 실), 충전재, 직물 등으로 재활용된다.\n참고로, 다양한 종류의 플라스틱을 일일이 구별하기 어렵기 때문에, ’플라스틱’이라는 하나의 카테고리로 묶어서 배출하는 것이다.\n페트병 색상 무색일수록 재활용이 용이하다.\n색상이 섞이면 재활용 후 소재 활용도가 낮아져, 검은색이나 어두운 색으로 한정된다.\n과거에는 다양한 색상의 페트병이 있었지만, 현재는 재활용성을 높이기 위해 무색 PET를 선호하는 추세이다.\n테트라팩(Tetra Pak) 종이처럼 보이지만 실제로는 종이, 플라스틱, 알루미늄 층이 복합된 다층 포장재이다.\n테트라팩 전용 인증 마크가 있는 제품은 재활용에 유리한 소재로 만들어졌을 가능성이 높다.\n07 폴리카보네이트 (Polycarbonate, PC)\n1956년, 독일의 화학 회사인 바이엘(Bayer AG)에서 유리를 대체하기 위해 개발된 플라스틱 소재.\n뛰어난 투명성과 내구성을 자랑하며, 유리보다 가볍고 충격 저항성이 높아 다양한 산업에서 활용된다.\n대표적으로 자동차 부품, 건축 자재, 전자기기, 그리고 보호 헬멧과 같은 안전 장비에 사용된다.\n이는 얇고 가벼우면서도 충격에 강하고, 투명도가 높아 시야 확보에도 유리하기 때문이다.\n강화유리보다 150배 강해 잘 깨지지 않아서 고강도, 고탄성이 요구될 때 유리 대체용으로 사용된다.\n단점 다른 플라스틱에 비해 가격이 비싸며, 재질이 두꺼워질수록 투명도가 떨어지는 경향이 있다.\n연소 시 유독가스를 배출할 수 있어, 안전한 소각이 어렵다.\n뜨거운 물이나 기름과 접촉할 경우, 비스페놀 A(BPA)라는 환경호르몬이 검출될 수 있어 식품 용기로는 적합하지 않다.\n활용 1969년: 닐 암스트롱의 달 착륙 우주복 헬멧 소재로 사용되어, 충격에 강하고 시야를 확보하는 역할을 했다.\n1998년: Apple사가 출시한 iMac G3 컴퓨터의 투명 본체에 사용되어, 독특한 디자인과 내구성을 동시에 제공했다.\n2008년: 베이징 올림픽의 선양 경기장에서는 투명한 지붕 소재로 폴리카보네이트가 사용되어, 가볍고 튼튼한 구조물로 시공되었다.\n08 폴리 메타아크릴산 메틸 (Poly Methyl MethAcrylate) 또는 (아크릴, Acrylic)\n투명도가 높은 열가소성 플라스틱.\n흔히 ’아크릴 유리’라고 부르기도 한다.\n1928년, 독일의 화학자 오토 뢴트겐(Otto Röhm)\n그는 메타크릴산을 폴리머화하는 연구를 진행하면서 아크릴의 기반 물질인 PMMA를 개발했다.\n그는 “뢰흐름과 하스(Röhm & Haas)”라는 회사를 설립하고 상업화에 주력했다.\n1933년, 이 회사는 PMMA를 상업적으로 생산할 수 있는 공정을 개발하고, 이를 “플렉시글라스(Plexiglas)”라는 이름으로 상표 등록했다.\n이는 투명하고 가벼운 소재로 특히 2차 세계대전 당시 항공기의 캐노피와 잠수함의 관찰창 등에 사용되었다.\n비슷한 시기에, 듀폰은 독자적으로 PMMA를 개발하여 “루사이트(Lucite)”라는 이름으로 생산을 시작했다.\n이 역시 상업적으로 매우 성공하여 아크릴 산업의 주요 브랜드로 자리 잡았다.\n유리보다 가볍고 뛰어난 투명도를 자랑하며, 열가소성이 높다. 가격이 저렴하고, 내후성과 내충격성이 우수하다.\n또한, 연소 시 유독가스를 발생시키지 않으며, 환경호르몬을 포함하지 않아 안전하게 사용 가능하며, 환경에 미치는 영향도 적다.\n유리와 유사한 수준의 투명도를 가지고 있어, 높은 투명도가 요구되는 곳에서 유리 대체용으로 사용한다.\n특히, 수족관의 유리 대체용으로 자주 사용되는데, 이는 큰 규모의 수조를 안전하게 유지할 수 있기 때문이다.\n폴리카보네이트(PC)에 비해 강도와 탄성은 다소 떨어지는 특징이 있다. 따라서, 고강도와 고탄성이 필요한 용도에서는 적합하지 않을 수 있다.\n09 에폭시 (Epoxy)\n1946년경부터 상용화된 열경화성 수지.\n가열하면 신속하게 경화되어 작업 시간이 단축되며, 경화된 후 매우 단단하져 강한 충격과 하중을 견딜 수 있다.\n또한, 물과 날씨 변화에 잘 견디며, 외부 환경에 대한 저항력이 높다.\n주로 선박용 및 방수 도료, 보호 코팅 및 접착재, 토목 건축용 바닥재, LED 봉지 소재 등으로 다양한 분야에서 사용된다.\n\n소음의 원인 주차장 바닥과 타이어가 마찰하여 발생하는 소음의 주된 원인은 바닥이 에폭시로 코팅되어 있기 때문이다.\n\n이 소음은 바닥 공사(코팅 처리)가 완료된 지 얼마 안 되었을 때 더욱 크게 나타난다.\n만약 소음이 들리지 않는다면, 바닥이 마모되어 코팅이 다 닳은 상태일 수 있으며, 이 경우, 콘크리트와의 마찰로 인해 가루가 날릴 수 있다.\n\n개구리 장갑 장갑의 개발자는 전 쇼트트랙 선수 김기훈(金琪焄)과 이준호(李準鎬)이다.\n\n선수 시절 장갑에 에폭시 액을 붙인 것이 시초였으며, 그 과정에서 개구리 장갑을 고안했다고 알려져 있다.\n이 장갑은 마찰력을 높여주고, 코너링 시 손목의 유연성을 유지하며 안정성을 증가시키는 특징이 있다.\n당시 규정상 장갑에 추가적인 부착물에 대한 제한이 없었으며, 코너링 시에 기존 장갑의 안정성이 부족해 이를 개선하고자 했다.\n이후 장갑은 특허를 받지 않은 채 제작되어, 규제 없이 자유롭게 사용될 수 있었다.\n10 멜라민 (Melamine)\n1936년 독일에서 발명된 도기 느낌의 열경화성 수지.\n멜라민(Melamine)과 포름알데히드(Formaldehyde)를 합성하여 제조되었으며, 이 소재는, 멜라민 수지(Melamine Resin)라고 불리기도 한다.\n주로 식기, 특히 고기집에서 사용하는 하얀색 플라스틱 그릇 에 널리 사용된다.\n가격이 저렴하고 무색 투명하며, 착색이 가능하다. 이 그릇은 가벼우면서도 내구성이 높아 일상적인 사용에 적합하다.\n\n온도 제한 110도 이상의 뜨거운 음식을 담는 데는 적합하지 않다.\n\n고온 환경에서 멜라민이 변형되거나 유해 물질이 방출될 가능성이 있으므로,\n오븐이나 전자레인지와 같은 고온 조리기구에서는 사용을 피하는 것이 바람직하다.\n\n자외선 소독 불가 멜라민 소재는 자외선 소독이 불가능하여 위생 관리에 주의해야 한다.\n\n오랜시간 할 경우, 너무 딱딱해져 쉽게 부스러지거나 갈라질 수 있다. 제품에 따라서 다를 수 있으나 보통 3시간 이내를 권장한다.\n11 바이오 플라스틱 (Bio plastics)\n분해가 잘 되지 않는 전통적인 플라스틱 문제를 해결하기 위해 개발된 소재. 바이오플라스틱은 크게 두 가지 유형으로 나뉜다.\n\n천연 물계 바이오플라스틱 (Biomass Plastic)\n\n녹말에 옥수수나 감자 전분을 혼합하여 제조한 바이오플라스틱으로, 제조 비용이 낮고 탄소 배출을 추가로 유발하지 않는다.\n일반적으로 강도가 낮아 단단하지 않지만, 높은 유연성과 생분해성을 지닌다.\n이 바이오플라스틱은 미생물에 의해 자연적으로 분해되므로, 환경에 미치는 부정적인 영향을 최소화할 수 있다.\n\n생분해성 바이오플라스틱 (Biodegradable Plastic)\n\n특정 미생물이 선호하는 첨가제를 혼합하여 제작된 플라스틱으로, 약 50°C 이상의 환경에서 유해물질 방출 없이 1 ~ 2년 내에 완전히 분해된다.\n천연물계 바이오플라스틱에 비해 강도가 높고 단단하며, 사용 후 분해 과정에서 미세플라스틱 생성을 억제한다.\n이러한 특성 덕분에 다양한 생활용품과 어업에 사용되는 바닷그물 등에 활용되고 있다.\n다만, 가격이 다소 높은 점이 단점으로 지적된다."
  },
  {
    "objectID": "misc/mse_02_3.html",
    "href": "misc/mse_02_3.html",
    "title": "중금속 (Heavy Metal)",
    "section": "",
    "text": "중금속 및 희소 금속에 대해 다루고자 한다.\n\n01 서론\n일반적으로 비소(As), 카드뮴(Cd), 납(Pb), 수은(Hg) 등과 같은 무거운 금속을 말하며, 이들은 경금속인 철(Fe)이나 마그네슘(Mg)에 비해 상대적으로 밀도가 높다.\n중금속은 체내에서 쉽게 배출되지 않으며, 이로 인해 축적되고 독성을 유발할 수 있다.\n경금속 (Light Metal) 중금속에 비해 인체에 더 적합하고 안전한 경우가 많다. 일반적으로 밀도가 5 g/cm³ 미만으로 낮으며, 부식에 강한 특성을 지니고 있다.\n인체에 유익한 금속을 무기질이라고 하며, 아연, 철, 구리 등은 필수적인 미네랄로, 여러 생리적 과정에서 중요한 역할을 한다.\n\n수은 (Hg, Mercury) 유일하게 상온에서 액체 상태로 존재하는 금속이다. 또한, 기체 상태로 쉽게 변할 수 있는 휘발성을 가진다.\n\n미나마타병 水俣病 (みなまたびょう)\n이 병은 1956년 일본의 미나마타 지역에서 처음 보고되었으며, 주로 수은에 오염된 물고기나 해산물을 섭취한 사람들에게 발생했다.\n미나마타병은 중추신경계에 심각한 영향을 미쳤다.\n특히 이 질병은 생태계의 생물 농축 현상으로 인해, 먹이사슬의 최상위에 있는 인간에게 더 큰 피해를 미쳤다.\n수은 사용의 감소 초기 산업화 과정에서 특히 전자 산업에서 자주 사용되었으나, 독성과 환경 피해에 대한 인식이 확산되면서 사용은 점차 줄어들고 있다.\n\n카드뮴 (Cd, Cadmium) 과거에 니켈-카드뮴(NiCd) 배터리에 주로 사용되었다. 그러나 카드뮴의 독성으로 인해 현재는 사용이 제한되고 있다.\n\n이타이이타이병 (痛い痛い病) 카드뮴 중독으로 인해 발생하는 질병으로, 뼈에 빈 공간이 생기고, 움직일 때마다 극심한 통증을 유발한다.\n카드뮴은 주로 식수를 통해 인체에 유입되며, 1912년에 일본 도야마(富山) 현의 진쓰 강 유역에서 처음으로 보고되었다.\n\n납 (Pb, Lead) 납은 밀도가 11.36 g/cm³으로 매우 무겁고 가격이 저렴하여 다양한 산업에서 사용되었다.\n\n특히 자동차 휠 벨런스 및 스포츠 용품 등에 활용되었다. 과거 휘발유와 페인트에는 납 성분이 포함되었으며, 전기회로의 땜납에도 사용되었다.\n클레어 패터슨 (Clair Patterson) 납이 환경과 인체에 미치는 해로움을 연구하여, 납 중독이 심각한 건강 문제를 일으킬 수 있다는 사실을 밝혀냈다.\n이를 통해 납 사용에 대한 규제의 필요성을 강조하게 되었으며, 결과적으로 납 중독 예방을 위한 정책적 변화가 일어났다.\n호흡기 위험 납이 증발하여 호흡기로 들어오면 심각한 건강 문제를 유발할 수 있다. 특히 신경계에 미치는 영향이 크며, 어린이에게는 발달 지연을 초래할 수 있다.\n뼈의 취약성 납이 체내에 축적되면 뼈를 취약하게 만들어 골절 위험을 증가시킨다.\n대체물질 납이 포함된 휘발유는 환경과 건강에 악영향을 미치는 것으로 판단되어 사용이 금지되었으며, 현재는 무연 휘발유가 사용되고 있다.\n안전 조치 납을 사용하는 산업에서는 반드시 환기를 해야 하며, 산업용 마스크를 착용하여 노출을 최소화해야 한다.\n02 희소금속 (Rare Metal) 지구의 지각에서 비교적 낮은 비율(0.8%)로 존재하는 금속들로, 추출 및 정제 과정이 복잡하고 비용이 많이 드는 경우가 많다.\n이들 금속은 전자기기, 항공우주, 첨단 기술 제품 등에서 중요한 역할을 하며, 따라서 국제적으로 수요가 높다.\n희소금속을 국제적으로 거래할 경우 국가의 승인을 받아야 하며, 이는 환경 보호와 자원 관리의 측면에서 중요하다.\n\n희소 금속의 정의 경제 및 산업에서 사용되는 “희소 금속”은 국가나 연구자에 따라 다를 수 있다.\n\n이는 각 나라의 산업 구조, 자원 상황, 기술 발전 정도에 따라 희소 금속의 정의가 달라지기 때문이다.\n예를 들어, 한국은 35개의 희소 금속을 지정하여 관리하고 있으며, 일본은 그보다 더 많은 47개의 희소 금속을 지정한 것으로 알려져 있다.\n\n지각 내 존재량이 적은 금속\n\n인듐 (In), 이리듐 (Ir), 로듐 (Rh)\n이들은 상대적으로 희귀하며, 경제적으로 중요한 자원으로 취급된다.\n이들은 전자 산업, 자동차 촉매, 고온 고압 환경에서의 내구성이 필요한 분야 등에서 사용된다.\n\n존재량은 많지만 추출이 어려운 금속\n\n바나듐 (V), 백금 (Pt), 티타늄 (Ti)\n\n바나듐은 0.0230%로 존재하며, 주요 사용처는 강철 합금.\n백금은 풍부한 광산이 있지만, 광석에서의 추출이 복잡하고 비용이 많이 든다.\n티타늄은 지각에서 9번째로 풍부하게 존재하지만, 이를 추출하는 과정이 복잡하고 에너지 소모가 크다.\n\n\n주요 희소 금속과 그 용도\n\n\n리튬 (Li)은 2차전지, 특히 전기차 배터리와 같은 분야에서 매우 중요하다.\n백금 (Pt), 로듐 (Rh)은 배기가스 정화장치에서 중요한 역할을 한다. 특히 자동차의 촉매 변환기에 사용된다.\n네오디뮴 (Nd), 디스프로슘 (Dy)은 고성능 자석을 만드는 데 사용된다. 이는 전기 모터, 발전기, 하이브리드 차량 등에서 중요한 역할을 한다.\n인듐 (In)은 투명전극, LED 디스플레이, 터치스크린 등에 사용된다.\n우라늄 (U), 플루토늄 (Pu), 토륨 (Th)은 원자력 발전의 연료로 사용된다.\n\n\n희토류 금속 (Rare Earth Metal)\n\n스칸듐 (Sc), 이트륨 (Y), 그리고 란탄족 (57 ~ 71)의 17개 금속을 포함한다. 이 금속들은 전자, 통신, 재료 과학 등 다양한 분야에서 사용된다.\n란탄족 금속은 네오디뮴 (Nd)와 디스프로슘 (Dy) 외에도, 프라세오디뮴 (Pr), 세륨 (Ce), 터븀 (Tb) 등 다양한 원소들이 포함된다.\n이들은 전기적, 자성적 특성이 뛰어나며, 강력한 자석, 배터리, 전자 기기에 널리 사용된다.\n자연에서 쉽게 발견되지 않으며, 주로 특정 광물에서 추출해야 한다. 추출 과정이 복잡하여 기술적 장벽이 존재한다.\n\n중국의 영향력 현재 희토류 금속의 상당량이 중국에서 생산되며, 중국의 희토류 자원 점유율은 세계에서 가장 높다.\n\n이로 인해 다른 국가들은 중국의 수출 제한 조치에 영향을 받을 수 있다.\n희소금속 공급망 희소금속과 희토류 금속의 공급망은 국제적인 정치와 경제의 영향을 받는다. 특히, 무역 갈등이나 지정학적 긴장 상황에서 이러한 금속의 수출 제한이 논의되곤 한다.\n대체 기술 개발 전 세계적으로 희소금속에 대한 의존도를 줄이기 위한 대체 물질 및 기술 개발이 진행되고 있다."
  },
  {
    "objectID": "misc/mse_02_1.html",
    "href": "misc/mse_02_1.html",
    "title": "구리(Cu)",
    "section": "",
    "text": "구리의 특성 및 철의 제조 방법에 대해 다루고자 한다.\n01 구리 (Cu, Copper) 청동기 시대 이전부터 인류가 사용해 온 금속 중 하나.\n청동기 시대(Bronze Age)는 구리와 주석을 합금하여 청동을 만들면서 시작되었으며, 이는 약 기원전 3300년 ~ 1200년 사이의 시기로 여겨진다.\n\n전기 전도도 구리는 전기 전도도가 매우 우수하여 전선으로 널리 사용된다.\n\n금과 은도 전기 전도성이 뛰어나지만, 구리에 비해 가격이 비싸므로, 실용적인 측면에서 구리가 더 선호된다.\n\n열 전도도 구리는 열 전도도가 높아, 열을 효과적으로 전달할 수 있다. 따라서 열 배관이나 히트 싱크 같은 열 관리가 중요한 곳에서 많이 사용된다.\n내부식성 구리는 철에 비해 부식에 강하여 천천히 녹이 슨다.\n\n이는 구리가 갖는 내부식성 덕분으로, 물리적 화학적 환경에서 더 오래 견디며 녹슬지 않는 특징이 있다.\n\n가공성 구리는 녹는 점이 낮아 낮은 온도에서도 쉽게 녹일 수 있다. 이러한 특성 덕분에 다양한 형태로 가공하기 쉬워 산업적으로 많이 사용된다.\n살균 효과 구리는 강력한 살균 효과를 가지고 있다. 구리 입자는 밀입자(나노 사이즈의 입자)로서 세균이나 바이러스를 빠르게 사멸시킨다.\n\n구리의 표면에 닿은 세균은 구리 입자가 파괴하면서 증식을 막아 빠르게 사멸하게 된다. 이 때문에 병원의 손잡이나, 공공장소의 특정 표면에 구리를 사용하는 경우가 많다.\n\n나노 구리 입자 투명한 비닐에 나노 사이즈의 구리 입자를 뿌려 놓은 물체는 짧게는 2일, 길게는 일주일 동안 교차 감염을 막아준다.\n\n구리 입자에 닿는 세균이나 바이러스는 몇 분에서 몇 시간 이내에 사멸된다.\n02 활용 및 한계\n병원 및 공공장소 병원의 손잡이와 같은 공공장소의 표면은 구리를 사용하여 교차 감염을 줄일 수 있다. 하지만 구리는 상대적으로 비싼 소재이므로, 대규모로 사용하기에는 비용 부담이 있다.\n박테리아 및 바이러스 제어 구리는 박테리아의 DNA를 파괴하여 증식을 막아주는 특성이 있지만, 항생제의 발전으로 인해 이 기능은 일부 잊혀지기도 했다.\n최근 나노 기술과의 결합으로 구리의 살균 효과가 재조명되고 있으며, 병원과 공공장소에서 다시 구리에 대한 관심이 증가하고 있다.\n03 구리의 대체 금속\n\n구리 구리는 산업적으로 탁월한 소재이지만, 비용이 상대적으로 높아 대량으로 사용하기에는 부담이 될 수 있다.\n\n이에 따라 구리의 사용량을 줄이고 대체 금속을 사용하는 경우가 많다.\n\n아연 아연도 구리와 비슷하게 비싼 금속이므로, 비용적인 측면에서 부담이 될 수 있다.\n\n하지만, 아연은 구리와 혼합할 때 특정 물성을 강화시킬 수 있어 여전히 산업적으로 중요한 역할을 한다.\n\n구리 + 아연의 합금 구리에 아연(Zn)을 첨가하면 색상이 가미되어, 시간이 지남에 따라 누리끼리한 색상으로 변할 수 있다.\n\n아연을 더 많이 추가할수록 구리 특유의 붉은 색상이 줄어들고, 누런 느낌의 색상이 더 강해진다.\n구리 합금에서 아연의 비율을 조정하면 다양한 색상과 물성을 얻을 수 있어, 장식품이나 화폐 등에 널리 사용된다.\n\n청동과 황동의 차이 청동과 황동은 금속 혼합물로, 청동은 주로 구리와 주석의 합금이고, 황동은 구리와 아연의 합금이다.\n\n황동은 시간이 지나면 초록색으로 변할 수 있다. 이는 황동 표면에서 일어나는 산화 반응으로 인해 발생하는 색 변화이다.\n\n알루미늄 구리나 아연에 비해 비용이 저렴하고, 경량이며 내식성이 뛰어나 다양한 산업 분야에서 구리와 아연을 대체하는 재료로 사용된다.\n\n예를 들어, 1원짜리 동전은 100% 알루미늄으로 제작되어 있다. 이는 알루미늄의 가벼운 무게와 저렴한 비용 때문이다.\n알루미늄은 전기 전도도와 가공성이 우수하여, 구리의 역할을 대체하는 경우도 많다.\n\n동전 가치 변화 500원을 제외한 모든 동전은 현재 액면가보다 제작 비용이 더 높아졌다. 이는 사용된 금속의 원재료 가격 상승 때문이다.\n\n이로 인해 동전 제작에는 값싼 금속을 내부에 사용하고, 겉 표면에는 다른 금속을 덧씌우는 방식이 사용된다.\n04 철과 구리의 특성\n\n기본 강도 철(Fe)은 구리(Cu)에 비해 기본적으로 더 높은 강도를 가진다.\n\n하지만 순수한 철은 연성(잘 늘어나고 휘어지는 성질)과 전성(얇게 늘어나는 성질)이 높아 약간 부드러워\n실질적으로 사용하기엔 약점이 될 수 있다.\n\n주철 탄소 함량이 약 2~4%로 높은 철 - 탄소 합금이다.\n\n매우 단단하고 압축 강도가 크지만, 탄소 함량이 높아 취성(충격에 약해 쉽게 부러지는 성질)을 가지므로 내구성이 떨어진다.\n이러한 특성 때문에 주로 기계 부품이나 고정 구조물에 사용된다.\n\n강철 주철에 비해 탄소 함량이 낮고(0.02~2%), 특성이 조절 가능하다.\n\n탄소와 함께 망간(Mn), 니켈(Ni), 크롬(Cr) 등의 합금을 추가하면 더 단단하고 충격에 강하며 유연성을 갖춘 소재로 변한다.\n강철은 내구성이 뛰어나고 쉽게 휘거나 부러지지 않아 건축, 교량, 자동차, 칼 등 다양한 분야에서 사용된다.\n\n강철로 만든 칼의 우수성 강철로 만든 칼은 순수한 철로 만든 칼보다 훨씬 더 강력하다.\n\n이는 탄소 함량과 추가 합금 요소 덕분에 강도, 경도, 내마모성 등이 뛰어나기 때문이다.\n예를 들어, 일본의 사무라이 검(카타나)은 고탄소강과 저탄소강을 조합하여 내구성과 날카로움을 모두 만족시킨 사례로 유명하다.\n05 제선 공정\n\n철광석과 원료 철광석은 보통 누렇거나 붉은색을 띄며, 철을 추출하기 위한 주원료이다. 철광석과 탄소 덩어리(코크스)를 이용해 용광로에서 철을 추출한다.\n\nFINEX와 같은 공정도 활용되는데, 이는 철광석을 보다 효율적으로 처리하기 위한 최신 기술 중 하나이다.\n\n용광로 내부 공정 철광석과 코크스는 용광로(고로) 내부로 들어간다. 코크스는 연료 역할을 하며, 용광로에서 뜨거운 바람과 만나면 불타면서 강한 열을 생성한다. 코크스가 불에 타면서 용광로 내부의 온도가 상승하고, 철광석의 온도도 함께 올라간다.\n산소와의 반응 철광석에는 산소가 포함되어 있다.\n\n용광로 내부는 산소가 부족한 환경이기에, 코크스가 산소와 완전히 결합하지 못해 일산화탄소(CO)가 생성된다.\n일산화탄소는 매우 불안정한 상태이며, 안정적인 이산화탄소(CO₂)로 변하려는 성질이 있다.\n\n환원 반응과 철 추출 일산화탄소는 철광석에 포함된 산소와 결합하여 이산화탄소로 변한다.\n\n이 과정에서 철광석에 있던 산소가 제거되며, 순수한 철이 남게 된다. 이를 이산화탄소의 환원 반응이라고 부른다.\n최종적으로, 이러한 환원 반응을 통해 선철이 생성되고, 용광로 하단에서 쇳물 형태로 추출된다.\n\nHyREX 공정 전통적인 고로 공정을 개선한 최신 제선 기술.\n\n이 기술은 탄소 배출을 줄이고 에너지 효율을 높이기 위한 방식으로, 철광석의 환원 과정에서 수소를 활용할 수도 있다.\n06 실리카 제거 공정\n\n철광석과 실리카(SiO₂) 지구상의 거의 모든 암석에는 SiO₂(실리카)가 포함되어 있다. 철을 추출하기 위해서는 이 실리카를 제거해야 한다.\n석회석의 사용 실리카를 제거하기 위해 석회석(탄산칼슘, CaCO₃)을 사용한다.\n\n석회석을 용광로에 넣고 열을 가하면, 석회석은 열풍에 의해 이산화탄소(CO₂)가 분리되어 날아가고, 산화칼슘(CaO)이 남게 된다.\n산화칼슘은 실리카와 반응하여 슬래그(불순물을 제거하기 위한 부산물)를 형성한다. 이 슬래그는 용광로 상단에 떠오르며, 이를 갈고리로 긁어내어 제거한다.\n\n이산화탄소 배출 문제 이 공정에서 이산화탄소가 발생하는 것이 환경 문제로 지적된다. 따라서, 탄소 배출을 줄이기 위한 새로운 기술이 필요하다.\n수소 환원 기술 철광석을 환원할 때 기존의 탄소 대신 수소를 사용하는 방식을 수소 환원 반응이라고 한다. 수소는 철광석의 산소와 결합해 물(H₂O)로 변하므로, 이산화탄소 배출 없이 철을 추출할 수 있다.\n\n이러한 기술은 미래의 탄소 중립 목표를 달성하기 위한 핵심 기술로 주목받고 있다. 현재 이 기술은 아직 완성되지 않았으며, 수소 환원 철 기술은 개발 중인 상태이다.\n우리나라는 포스코와 같은 기업을 통해 이 기술에 대한 연구를 진행하고 있으나, 개발 속도는 다소 뒤처진 상황이다.\n07 제철 공정 철광석에서 철을 추출하여 다양한 철 제품을 만드는 공정을 단계별로 설명한다.\n\n선철 쇳물의 생성 철광석을 용광로에서 가열하여 불순물이 많이 포함된 선철 쇳물을 만든다.\n\n이 과정에서 산소를 주입하여 탄소와 결합시켜 이산화탄소(CO₂) 형태로 제거하는 것이 목표이다.\n탄소가 제거되면서 선철의 성질이 개선된다.\n이 과정은 고온 상태를 유지해야 하며, 용광로가 꺼지면 다시 점화하는 데 오랜 시간이 걸리므로 연속적인 공정이 중요하다.\n\n연주 공정 (Continuous Caster) 완제품이 아닌 반제품을 만드는 단계이며, 선철 쇳물을 일정한 형태로 만드는 공정이다.\n\n쇳물을 연속적으로 붓고 길을 따라 이동시키며, 유동성 고체 상태에서 서서히 경화되어 슬라브라는 넓은 판형태의 철 덩어리가 만들어진다.\n슬라브는 필요한 길이로 잘라서 식힌 후 다음 공정으로 이동한다.\n\n압연 공정 (Rolling) 슬라브를 다양한 형태로 가공하는 과정이다.\n\n두 개의 롤러 사이에 슬라브를 넣어 눌러주면, 얇고 긴 판 형태로 변한다. 이 과정을 압연 공정이라고 하며, 크게 두 가지 방식으로 나뉜다.\n3-1. 열간 압연 (열연) 슬라브를 가열한 상태에서 압연한다.\n슬라브를 화장지 두루마리처럼 말아서 저장할 수 있으며, 이 과정을 줄여서 열연 강판이라고 한다. 롤러의 굴곡을 조절하여 휘어지지 않도록 두꺼운 상태로 마무리한다.\n조선업체, 자동차 산업, 파이프, 전자레인지 등의 소재로 활용된다.\n3-2. 냉간 압연 (냉연) 열간 압연된 강판을 다시 가열하여 풀어낸 후, 더욱 얇게 눌러주는 과정이다. 이 과정을 통해 표면이 매끄럽고 정밀도가 높은 냉연 강판이 만들어진다.\n자동차 외장, 가전제품 등 매끈한 표면이 요구되는 제품에 사용된다.\n\n제철 공정의 전체 작업 제철회사에서는 이러한 과정을 통해 철광석에서 시작하여 여러 형태의 철 제품을 생산한다. 생산된 철 제품은 다양한 산업에 맞춰 가공되어 사용된다.\n선재 공정 (Wire Rod) 선형 제품을 생산하는 공정이다.\n\n빌렛(작은 직사각형 철 덩어리)을 가열하여 특정한 기계에 통과시키고, 철사를 뽑아내는 방식이다.\n철사, 케이블, 스프링 등의 얇고 긴 제품을 제작할 때 사용된다.\n\n주조 공정 (Casting) 쇳물을 몰드(틀)에 부은 후 식히고 틀을 제거하여 원하는 형태를 만든다. 복잡한 형태나 내부 구조가 중요한 제품을 제작할 때 사용됩니다.\n\n자동차 엔진 블록, 기계 부품 등 복잡한 물체를 효율적으로 제조할 수 있다.\n\n단조 공정 (Forging) 가열된 철 덩어리를 망치로 두들겨서 원하는 모양으로 만드는 공정이다.\n\n철의 내구성과 강도를 높이는 데 유리하며, 비교적 간단한 형태의 강한 제품을 제작할 때 사용된다.\n공구, 무기, 기계 부품 등 강도가 중요한 제품을 만드는 데 활용된다."
  },
  {
    "objectID": "misc/mse_01_3.html",
    "href": "misc/mse_01_3.html",
    "title": "철과 알류미늄",
    "section": "",
    "text": "베서머 전로, 알루미늄 및 두랄루민의 발전, 그리고 정유 산업의 역할에 대해 다루고자 한다.\n01 17세기 영국 아이작 뉴턴은 17세기 과학 혁명을 이끈 중요한 인물로, 그의 이론과 발견은 전 유럽의 과학 기술 발전에 큰 영향을 미쳤다.\n특히 중력 이론과 미적분학의 발전은 과학에 대한 투자를 촉진했다.\n산업혁명 18세기 후반에서 19세기 초에 걸쳐 영국에서 시작되었으며, 증기기관의 발명과 활용이 중요한 역할을 했다.\n기계가 증기기관의 힘으로 돌아가면서 생산성이 극대화되었고, 대량 생산된 제품을 철도와 같은 교통수단을 통해 빠르게 배송할 수 있게 되었다.\n강철 수요와 생산 기술 산업혁명으로 인해 금속과 강철의 수요가 급증하면서, 기존 제철 방법의 한계가 드러났다. 전통적인 제철 방법은 낮은 온도에서 쇳물을 만들 경우 탄소(C) 함량이 높아지는 문제가 있었다.\n이 문제를 해결하기 위해 망치질로 불순물을 제거하는 방법이 사용되었지만, 이는 비효율적이고 대량 생산에는 한계가 있었다.\n02 베서머 전로 (Bessemer) 영국의 헨리 베서머가 발명한 베서머 전로는 철광석과 석탄, 그리고 용광로 밑바닥에 열풍인 산소를 주입하여 제철 과정을 효율적으로 개선했다.\n이 과정에서 발생하는 탄소는 산소와 결합하여 이산화탄소로 변하며, 이를 통해 강철의 탄소 함량을 줄이는 데 성공했다.\n산업혁명 시기, 고품질의 강철을 생산하기 위해서는 철광석에서의 탄소 함량을 정확하게 조절하는 것이 중요했다.\n특히 탄소 함량이 1%로 적정해야 강철의 품질이 유지되는데, 변동성이 심한 과정에서 이를 일관되게 맞추는 것은 어려운 도전이었다.\n문제점 탄소 함량의 변동성이 크면 강철의 품질이 달라지며, 이는 공급자의 입장에서 큰 문제였다. 품질 불량으로 인해 고객의 신뢰를 잃거나 파산의 위험에 처할 수 있었다.\n해결책 무세트(Mushet)는 이 문제를 해결하기 위해 탄소를 완전히 제거한 후, 필요한 양의 탄소(1%)를 정확하게 첨가하는 방법을 제안했다.\n이 방식은 강철의 품질을 일정하게 유지할 수 있게 해주었다.\n무세트는 이 해결책에 대해 300파운드를 지불받았다.\n이는 품질 유지의 중요성을 강조하는 사례로, 고품질 강철의 생산이 산업의 발전에 어떻게 기여했는지를 보여준다.\n03 알루미늄 - Al 자연상태에서 산소와 결합하여 존재하며, 이를 분리하기 위해서는 3000도 이상의 높은 온도가 필요하다.\n나폴레옹 3세는 알루미늄 식기를 사용하여 접대했으며, 이는 알루미늄이 녹슬지 않고 가벼우며, 철보다 밝은 특성 덕분에 인기를 끌었다.\n그러나 초기에는 비싼 금속이었다.\n전기 분해를 이용한 생산 1800년대 초, 배터리가 발명되었고, 1800년대 후반에는 발전소가 등장하여 전기 분해의 가능성이 열렸다.\n이 시기에 다른 극에 다른 원소가 붙는 전기 분해 현상을 경험하면서 알루미늄의 활용을 고민하게 되었다.\n바륨와 알루미늄 산화물이 썩인 빙정석에 1000도의 열을 가하면 액체가 되며, 여기에 알루미늄 산화물을 넣으면 녹아 섞인다.\n마지막으로, 전기 분해를 통해 음극에 알루미늄이 분리된다. 이 과정을 통해 알루미늄을 효율적이고 저렴하게 생산할 수 있게 되었다.\n04 두랄루민 (Duralumin) 알루미늄은 철만큼 강하지 않다는 단점이 있었다.\n이를 해결하기 위해 합금을 만들어 보았지만, 초기에는 쓸만할 정도로 단단해지지 않았다.\n그런데, 시간이 지나면서 단단해지는 시효경화 현상을 우연히 발견하여, 결국 강철만큼은 아니지만 사용 가능한 알루미늄 합금을 개발하게 되었다.\n이 합금은 ” Duralumin ” 이라 불리며, 이는 내구성을 강조하는 이름이다.\n활용 독일 탱크는 힘을 받지 않는 부분에 두랄루민을 대체하여 가벼워졌다. 이로 인해 연합군 탱크보다 빠른 기동력을 갖게 되어 압도적인 전력 차이를 보였다.\n전쟁 후, 두랄루민이 내장된 비행기가 처음으로 여객기로 사용되었다. 이 여객기는 탄생 이후 약 72년 정도의 역사를 가지고 있다.\n사고 비행기가 고도에 오르면 공기압력이 0.26 atm으로 감소하기 때문에, 내부 압력을 약 0.74 atm으로 유지해야 했다.\n그러나 이러한 압력차로 인해 비행기 몸체가 지속적으로 팽창과 수축을 반복하며 큰 스트레스를 받았고, 결국 이를 견디지 못해 비행기에 구조적인 손상이 발생하는 사고가 있었다.\n사고 원인을 조사한 결과, 비행기 유리창 모서리에서 힘의 집중 현상이 발견되었다.\n이를 해결하기 위해 유리창을 외부는 원형, 내부는 사각형으로 설계하여 압력차를 효과적으로 분산시키는 방식을 도입했다.\n이러한 개선은 비행기 안전성을 크게 향상시키는 중요한 전환점이 되었다.\n05 정유 산업 (Refining Industry)\n증류탑의 원리 증류탑에서는 300도에서 액체 상태로 변하는 물질을 걷어내고, 기체는 위로 상승한다. 이 과정에서 온도를 조금 줄이며, 각 성분의 끓는점에 따라 분리되는 원리를 사용한다.\n증류탑의 원리를 활용한 정유 산업은 석유에서 다양한 화학 물질을 추출하고 정제하는 과정이다.\n나프타(naphtha)는 초기에는 쓸모가 없던 물질이었으나, 이를 화학자들이 활용하여 기초 원료를 추출하고 연결 및 조절하여\n새로운 물질을 만들어내는 데 성공했다.\n인공소재 나프타와 같은 원료를 바탕으로 만들어진 인공소재는 석유화학 산업 분야에서 중요한 역할을 한다.\n이러한 소재는 다양한 제품의 기초 원료로 사용되며, 현대 산업에서 필수적인 요소가 되었다.\n첨단세라믹 개발 나프타를 활용한 화학적 조합 및 변형 과정을 통해 첨단세라믹과 같은 고기능성 물질들이 개발되었다. 이러한 첨단소재는 전자기기, 항공우주, 의료기기 등 다양한 분야에서 혁신적인 성능을 발휘한다."
  },
  {
    "objectID": "misc/mse_01_1.html",
    "href": "misc/mse_01_1.html",
    "title": "청동",
    "section": "",
    "text": "청동의 영향 및 기술에 대해 다루고자 한다.\n01 청동 (Bronze) 구리(Cu)와 주석(Sn)의 합금으로, 고대에서 중요한 금속으로 사용되었다.\n’청동’이라는 이름은 오랜 시간 동안 노출되어 녹이 슬어 푸르스름한 색을 띠게 되는 것에서 유래되었다.\n구리는 자연에서 흔히 발견되는 원소는 아니지만, 그 자체로도 유용하다. 그러나 청동을 만들기 위해서는 구리와 주석이 함께 필요하다.\n제조 및 특성 청동은 구리와 주석을 혼합하여 만든다. 주석은 은백색 으로 빛나는 금속이며, 청동의 특성을 더욱 향상시킨다.\n이러한 합금은 주석의 비율에 따라 다양한 색상과 성질을 나타낼 수 있다. 주석이 포함된 비율이 많아질수록 청동은 더욱 비싸게 평가된다.\n황동은 구리(Cu)와 아연(Zn)의 합금으로, 청동보다 상대적으로 저렴하다. 금 의 색을 띠기도 하여 가끔 청동과 혼동되기도 한다.\n02 청동의 영향력 사회적 영향 청동을 사용할 수 있게 된 부족들은 더 강력한 무기를 만들 수 있었고, 이는 그들의 사회 구조와 정치적 권력에 큰 변화를 가져왔다.\n청동을 보유한 부족들은 다른 부족들을 정복하거나 병합하여 그들의 세력을 확장했다.\n이러한 무력 확장과 더불어,\n더 많은 사람을 통제할 수 있는 힘을 가진 지도자들은 초기에 법과 규칙을 제정했다.\n이는 사회의 조직화와 복잡한 구조 발전에 기여했으며, 결국 고대 문명의 기초를 형성하는 데 중요한 역할을 했다.\n청동기 사용은 또한 정치적 구조의 변화를 촉진했다. 청동을 다루는 기술과 자원을 보유한 소수의 귀족과 왕은 그들의 권력을 강화시켰고, 이를 통해 계급 구조가 형성되었다.\n소수의 귀족 계층이 대다수의 사람들 위에 군림하는 형태로 사회적 차별과 계급이 명확해졌다.\n이와 함께, 인구의 증가와 강가 주변에서의 농업과 정착 생활이 이루어짐에 따라 초기 국가가 형성되기 시작했다.\n경제적 기반이 마련되면서, 왕과 귀족들이 통치하는 사회가 등장하게 되었으며, 이는 국가 조직의 발전을 가속화했다.\n문화적 영향 청동은 또한 예술작품과 장식품, 의식용 도구로 사용되며, 고대 문명에서 중요한 문화적 상징으로 여겨졌다.\n청동으로 제작된 조각품과 장식물은 그 시대의 기술적 성과와 미적 감각을 나타내는 중요한 유물로 남아 있다.\n경제적 영향 청동기 시대에는 청동을 생산하고 교환하는 과정에서 무역이 활성화되었다. 청동을 가진 부족들이 다른 부족들과의 교류를 통해 자원을 확보하고, 그 과정에서 초기 상업 활동이 이루어졌다.\n03 기술적 맥락 청동의 성질 청동은 두 금속의 특성을 결합하여 더욱 강하고 내구성이 뛰어난 소재로 변모했다. 구리보다 강한 주석이 포함됨으로써, 청동은 더욱 견고한 도구와 무기를 만드는 데 이상적이었다.\n원자 이동과 구조 구리 원자는 상대적으로 큰 크기를 가지고 있으며, 주석 원자는 구리보다 작다.\n이 두 원자는 합금 상태에서 서로 배위 결합을 형성하게 되며, 그 결과 원자들이 고르게 분포되어 미세한 결정 구조를 이루게 된다.\n이 배열은 결정질 구조로, 이는 원자들이 규칙적으로 반복되는 방식으로 배치된 것이다. 이 규칙적인 배열은 물질의 기계적 특성에 큰 영향을 미친다.\n청동에서는 구리 원자와 주석 원자가 결합하면서, 주석이 구리의 결정 구조를 방해하거나 강하게 결합시켜 구리 원자의 이동을 제한한다.\n이러한 구조적 특성으로 인해, 청동은 원자 이동이 제한되어 단단하고 강한 물질이 되는데, 이는 물질의 변형이 어려울 때, 강도가 높아지는 특성을 가지기 때문이다.\n주석이 구리에 결합하면 청동의 기계적 강도가 증가하고, 이 합금은 단단하고 강한 특성을 지니게 된다.\n주석이 결합된 청동은 구리보다 더 높은 경도와 강도를 가지며, 이는 주석이 결정 구조 내에서 원자 간의 결합을 더욱 견고하게 만들기 때문이다.\n주석의 효과 청동에서 주석이 함유되면 외관상 더욱 매력적인 금속으로 여겨진다.\n청동은 금속 특유의 광택과 함께 은색 또는 황금빛 을 띠며, 주석이 포함되면 청동이 더욱 빛나고 고급스러워 보이게 된다.\n또한, 주석은 구리와 결합하면서 내식성도 증가시켜, 청동이 외부 환경에 노출되었을 때 더 오래 지속될 수 있도록 도와준다."
  },
  {
    "objectID": "misc/mse_01_0.html",
    "href": "misc/mse_01_0.html",
    "title": "신소재",
    "section": "",
    "text": "신소재의 정의와 물질의 분류 체계 전반에 대해 다루고자 한다.\n01 재료기술의 발전 소재 (재료, Material) 어떤 것을 만드는 데 바탕이 되는 재료. \n신소재 (Advanced Material) 기존에 존재하지 않았거나, 기존의 소재에 비해 새로운 특성, 기능을 가진 소재. 이러한 소재는 현대 산업의 다양한 요구에 부응하기 위해 개발된다.\n산업적 중요성 신소재는 새로운 기술이나 제품의 개발에 기초가 되는 소재로 사용된다. 전자기기, 항공 우주, 생명공학 등 여러 분야에서 신소재의 활용이 두드러진다.\n기존 소재의 재발견 신소재의 범위는 단순히 완전히 새로운 물질에 국한되지 않는다. 기존의 소재(철, 알루미늄 등)도 새로운 특성이 발견되거나 첨단 산업에서 활용될 경우 신소재로 간주될 수 있다.\n이는 과거에 비해 더 효율적이거나 혁신적인 사용 방식으로 발전시키는 것을 포함한다.\n기존 소재의 새로운 활용 이전에는 쓸모없던 소재가 첨단 산업에서의 새로운 용도로 사용될 경우, 이러한 소재 또한 신소재의 범위에 포함된다.\n특정 폐기물이 재활용되어 새로운 제품으로 재탄생하는 경우가 이에 해당된다.\n신소재의 발전과 응용 신소재의 개발은 기술 혁신을 통해 이루어지며, 이는 새로운 물리적, 화학적, 기계적 성질을 통해 가능해진다.\n연구와 개발을 통해 기존의 한계를 넘어서고, 다양한 산업의 요구에 맞춘 새로운 응용 가능성을 여는 것이 주된 목표이다.\n신소재는 과학적 원리와 공학적 응용이 결합하여 발전한다.\n과학 (科學, Science) 자연 현상과 물질의 성질을 이해하고 설명하는 학문.\n이를 위해 실험과 관찰을 통해 데이터와 원리를 수집하고, 이론을 개발하여 자연의 규칙을 파악하는 것이 주요 목표이다.\n주로 ” 왜? ” 라는 질문에 답하고, 현상을 설명하는 데 중점을 둔다.\n공학 (工學, Engineering) 과학적 원리를 실제 문제를 해결하는 데 응용하는 분야.\n이는 주어진 자원과 기술을 활용하여 인류의 필요를 충족시키는 제품, 시스템, 과정 등을 설계하고 개발하는 과정을 포함한다.\n주로 ” 어떻게? ” 라는 질문에 초점을 맞추며, 실용적이고 기능적인 솔루션을 창출하는 데 중점을 둔다.\n03 고체의 구분 - 구성성분 유기재료 (有機材料, Organic Materials) 주로 탄소를 포함한 화합물로 구성되며, 생명체에서 유래하는 물질.\n동물의 배설물로 자란 식물에서 얻어지는 화학 비료 없이 자연적으로 성장한 유기체에서 비롯된 물질을 포함한다.\n유기물은 탄소, 수소, 산소 등의 원소로 이루어져 있다.\n무기재료 (無機材料, Inorganic Materials) 일반적으로 탄소와 수소의 결합을 포함하지 않는 물질.\n무기물은 금속, 광물, 세라믹, 그리고 일반적으로 자연계에서 발견되는 다양한 물질을 포함한다.\n기 (基) ’기’라는 개념은 여러 맥락에서 사용될 수 있는데, 여기서는 물질이나 기계, 또는 장기의 기본 원리와 역할을 의미한다.\n기계의 기(機)는 기능이나 역할을 나타내고, 생명체의 기(氣)는 생명 활동에 필수적인 원리와 물질적 기반을 의미한다.\n유기화학과 무기화학 유기화학은 탄소 화합물의 구조, 성질, 반응을 연구하는 분야이며, 무기화학은 탄소 화합물을 제외한 물질의 화학을 연구하는 분야이다.\n두 분야는 소재의 출발점으로 중요한 역할을 한다.\n04 물질의 분류 계통 금속 (Metals) 전도성이 뛰어나고, 강도와 연성이 좋은 특성을 지니고 있다. 일반적으로 기계적 성질에 따라 여러 종류로 나눌 수 있다.\n금속은 독립적으로 연구되며, 다양한 산업에서 기초 재료로 활용된다.\n섬유 (Fibers) 천연 섬유 (면, 울)와 합성 섬유 (나일론, 폴리에스터)로 나눌 수 있다. 주로 직물, 의류, 그리고 복합 재료로 사용된다.\n반도체 (Semiconductors) 전기적 성질이 금속과 절연체 사이에 위치한 물질. 일반적으로 실리콘과 같은 원소가 사용된다.\n전자 기기에서 중요한 역할을 하며, 이 분야는 전문적이고 복잡하여 다루기 어려운 경우가 많다.\n세라믹 (Ceramics) 비금속 무기물로, 일반적으로 내열성, 경도, 내식성이 우수하다. 도자기, 타일, 전자기기 부품 등 다양한 용도로 사용된다.\n고분자 (Polymers) 반복적인 단위로 구성된 분자로, 플라스틱과 같은 합성 고분자가 포함된다. 다양한 특성을 가질 수 있으며, 생활용품에서 산업용 자재까지 폭넓게 사용된다.\n바이오소재 (Biomaterials) 생체적합성을 갖춘 물질로, 의학 및 생명공학 분야에서 사용된다. 인공장기, 조직 공학, 약물 전달 시스템 등에 활용된다.\n나노소재 (Nanomaterials) 나노미터 규모의 물질로, 독특한 물리적, 화학적 성질을 가지고 있다. 다양한 산업 분야에서 혁신적인 응용이 가능하다.\n05 반도체 (Semiconductor) 전도체와 절연체의 중간 성질을 가지며, 전기 전도도가 특정 조건에 따라 변하는 물질이다.\n일반적으로 전기가 잘 통하지 않지만, 특정한 조건이나 불순물을 추가함으로써 전기 전도성을 조절할 수 있다.\n부도체 → 도체 순수한 실리콘(Si)과 같은 반도체는 기본적으로 부도체이다. 하지만 여기서 불순물(붕소, 인)을 도핑함으로써 전기 전도성을 조절할 수 있다.\n붕소를 추가하면 p형 반도체가 생성되고, 인을 추가하면 n형 반도체가 생성된다. 이들 두 가지 유형의 반도체가 결합하여 다양한 전자기기를 만든다.\n전류 제어 반도체는 전류의 흐름을 제어하는 데 매우 중요한 역할을 한다. 이를 통해 스위치, 다이오드, 트랜지스터 등의 다양한 전자 소자를 만들 수 있다. 전자 산업에서 반도체 소자는 신호 증폭, 스위칭, 전력 관리 등 다양한 기능을 수행한다.\n다양한 성질 구현 과거에는 금속 재료를 대량으로 사용하여 회로를 구성해야 했지만, 현재는 극소량의 불순물을 도핑함으로써 다양한 특성을 가진 반도체 소자를 제조할 수 있게 되었다.\n이러한 변화는 전자 기기의 소형화와 성능 향상에 기여하고 있다.\n06 고체의 구분 - 결정질 (Crystalline) 원자가 규칙적으로 배열되어 3차원 구조를 형성한 물질. 이러한 배열의 규칙성을 ” 크리스탈 ” 이라고 부른다.\n결정을 구성하는 원자 간의 결합이 일정한 패턴을 유지하는 것이 특징이다.\n단결정 (Single Crystal) 원자 구조가 하나의 규칙적 배열로 이루어진 결정. 즉, 결정의 모든 부분이 동일한 결정 격자 구조를 가진다.\n다이아몬드, 실리콘 등이 있다. 이들은 특정한 방향으로 잘라내어도 동일한 성질을 유지한다.\n다결정 (Multicrystalline) 여러 개의 단결정들이 집합하여 형성된 결정이다. 각 결정이 규칙성이 다르며, 여러 개의 결정이 혼합되어 있는 상태.\n대부분의 금속, 세라믹 등에서 다결정 형태가 흔히 나타난다.\n비정질 (Amorphous) 원자 배열에 규칙성이 없는 상태. 즉, 원자가 무작위로 배열되어 있어 일정한 구조를 가지지 않는다.\n결정과 구분하기 위해 ” 비결정 ” 이라고도 부르지 않는다. 유리, 고분자 물질 등이 있으며, 이들은 특정한 형태나 구조를 갖지 않는다.\n결정 분포 생명체를 제외한 자연에서 존재하는 모든 물질은 일반적으로 다결정 상태로 발견된다. 이는 자연에서 형성되는 과정에서 여러 결정이 섞이고 각기 다른 결합을 이루기 때문이다.\n단결정을 만들기 위해서는 강압적인 제약을 가해야 하므로 제조 비용이 비싸다. 비정질 물질은 단결정에 비해 제조가 덜 복잡하지만 여전히 어려움이 따른다.\nLED 발광 성능 결정 구조 및 원소의 산소(O) 포함 여부에 따라 다르며, 이는 결정질 특성과 관련이 있다.\n07 역사적 배경 프로메테우스 (Προμηθεύς, Promētheus) 그리스 신화에서 인간에게 불 을 가져다준 신으로, 인간 문명의 상징적인 역할을 한다.\n불의 상징성 인간이 자연환경을 극복하고 발전하는 데 기여한 중요한 요소이다.\n불의 최초 사용 (약 50만 년 전) 인간이 불을 처음 사용하게 되면서 생긴 독보적인 3 가지 역할이 있다.\n\n식품의 변질과 살균\n\n불로 고기와 생선을 조리함으로써, 미생물의 생존을 막고 식품이 더 부드럽고 맛있어지는 효과를 얻었다.\n\n영양 섭취 증가\n\n조리된 식사는 소화가 용이하고 영양소의 흡수를 촉진하여, 인류의 식생활을 풍요롭게 만들었다.\n\n뇌의 성장\n\n더 나은 영양 상태는 뇌의 발달을 촉진하여 인지 능력이 향상되고, 결과적으로 인간의 수명이 연장되는 데 기여했다.\n농경생활의 시작 강가 주변의 촉촉한 흙에 떨어진 식물의 씨앗이 잘 자라는 것을 관찰한 초기 인류는, 강가에서 농사를 시작하게 되었다.\n이러한 경험은 인간이 정착 생활을 하게 되는 중요한 계기가 된다. 농사를 잘하는 사람들이 중심이 되어 점차 부족이 형성되었고, 이는 사회적 구조와 협력의 시작을 알렸다.\n도자기 제조의 혁신 강가 주변의 흙을 이용해 만든 그릇이 햇볕 에 말려 단단해지는 것을 발견하였고, 불로 가열했을 때 그 성질이 획기적으로 변화함을 알게 되었다.\n이로 인해 최초의 토기가 만들어지게 되었으며, 이는 재료 공학의 출발점이 되었다.\n신석기 시대 토기에서 발견된 곡물의 잔해들은 농경생활과 강가의 정착 생활을 추론하는 데 도움을 준다. 이러한 발견은 신석기 시대의 특징을 나타내며, 인류의 생활 방식에 큰 변화를 가져온 시기를 알린다."
  },
  {
    "objectID": "misc/mse_01_2.html",
    "href": "misc/mse_01_2.html",
    "title": "철기",
    "section": "",
    "text": "철의 영향 및 기술에 대해 다루고자 한다.\n01 철기 시대 철의 성질 주로 산소(O), 규소(Si), 알루미늄(Al)과 결합하여 자연에서 존재한다. 철광석에서 철을 추출하기 위해 고온에서 열에너지를 가해야 한다.\n구리는 약 900 ~ 1000도에서 얻어지지만, 철은 약 1500도에서 추출되어야 한다. 이는 더 복잡한 제련 과정을 요구함을 의미한다.\n철 생산의 도전 과제 초기 철 제련 과정에서는 나무를 태워서 불을 얻어야 했는데, 이로 인해 화력이 약해 생산이 어려웠다.\n그러나 불을 가두고 공기를 주입할 수 있는 가마가 개발됨으로써 온도를 효과적으로 조절할 수 있게 되었다.\n이러한 혁신은 철 생산의 효율성을 크게 향상시켰다.\n기술적 발전 공기를 집어넣는 장치는 철광석의 산화 과정을 가속화하여 더 많은 철을 추출할 수 있게 했다. 이는 더욱 강하고 내구성이 뛰어난 철 제품을 만드는 데 기여했다.\n가마를 개선하고 불을 가두는 장치를 통해 철의 제련 효율이 높아지면서, 대량 생산이 가능해졌다.\n이는 무기, 도구, 건축 자재 등 다양한 분야에서 철의 사용을 증가시켰다.\n사회적 및 경제적 영향 철로 만든 도구와 무기는 농업 생산성을 증가시켰으며, 전쟁에서도 우위를 점하게 해주었다.\n철제품의 수요 증가로 인해 상업 활동이 활발해지고, 여러 지역 간의 교역이 활성화되었다.\n철기의 사용은 더 강력한 군대와 정복의 가능성을 열어 주었고, 이는 사회 구조와 정치적 권력에 큰 변화를 가져왔다.\n02 목탄(숯)을 이용한 철 제련 목탄은 탄소(C) 함량이 높아 철광석을 제련하는 데 중요한 역할을 했다.\n철광석에 목탄을 함께 넣고 가열하면 철광석의 산소가 제거되며, 쇳물이 형성된다. 이때 탄소는 철의 강도와 경도에 중요한 영향을 미친다.\n선철 (Pig Iron) 목탄을 이용해 초기 제련된 철은 탄소 함량이 높아 선철이 된다. 선철은 매우 단단하지만, 탄소가 많아 취성이 강해 힘을 가하면 쉽게 부러진다.\n이러한 이유로 주조 공정에서 주로 사용되었으며, 특정한 모양을 찍어내는 데 적합했다.\n연철 (Wrought Iron) 선철의 탄소를 줄이기 위해 재가열한 뒤 철을 망치로 두들겨 불순물을 제거한다. 이 과정을 반복하면 탄소 함량이 감소하고 철이 더 연성을 가지게 된다.\n여기서 연성(軟性)이란 유연하고 질긴 성질을 의미한다. 즉, 철의 탄소 함량을 낮추면 더 질기고 유연해져\n깨지지 않는 연철을 만들 수 있다.\n열처리 및 내부 구조 변화 철을 가열하면 내부 구조가 변하며 재배치가 일어난다.\n열처리는 철의 강도와 경도를 조절하는 데 중요한 역할을 하며, 제련 과정에서 원하는 특성을 얻기 위해 다양한 열처리 기술이 개발되었다.\n열처리 후 고온의 철을 찬물에 급속 냉각하면, 철 내부의 미세 구조가 꼬이면서 강도가 높아진다.\n이 과정을 통해 강하고 내구성이 뛰어난 철 제품을 만들 수 있다.\n03 철기의 도입과 농업 확장 이전에는 석기를 사용하여 농사를 지었지만, 석기는 약하고 효율이 낮았다. 철은 강하고 저렴하며, 날카롭고 내구성이 뛰어나 농기구를 만들기에 적합했다.\n철제 농기구는 흙을 더 깊이 파고, 단단한 땅을 개간하는 데 효과적이라 더 많은 땅에서 농사를 지을 수 있게 되었다.\n이로 인해 농사는 강가 주변에 국한되지 않고 내륙으로 확장될 수 있었다.\n인구 증가 농업 생산성이 높아지면서 식량 생산이 늘어났고, 더 많은 인구를 부양할 수 있었다. 노동력이 더 필요해지면서 많은 가정에서 자식을 더 많이 낳게 되었다.\n이로 인해 인구가 급격히 증가하였고, 사회의 규모가 커지면서 보다 정교한 법과 규칙이 필요해졌다.\n국가의 형성과 발전 철기 시대에 들어서면서 다양한 지역에서 정복 전쟁이 활발하게 일어났고, 이를 통해 국가가 형성되고 통치 시스템이 정립되었다.\n춘추와 전국 시대 춘추 시대는 청동기의 사용이 주를 이루었지만, 전국 시대에 철기가 본격적으로 사용되면서 군사력과 농업 생산성이 크게 증가했다.\n국가 간의 경쟁이 심화되었고, 복잡한 법 체계와 정치 시스템이 등장하여 국가의 기반을 다지게 되었다.\n백제와 고구려 우리가 역사에 대한 의문을 가지는 것 중 하나가 바로 왜 고구려가 삼국을 통일하지 않았는가이다.\n이것이 이뤄졌다면 우리는 보다 넓은 영토를 보유할 수 있었을지도 모른다. 이것은 철기에 영향으로 그 이유를 추측할 수 있다.\n철기 도입으로 인해 농업이 확장되면서 인구 증가가 가속화되었다. 이에 따라 백제는 상대적으로 인구가 많았을 것으로 추정된다.\n이는 농업 생산량과 직결되며, 백제는 비교적 비옥한 지역에 위치하여 농업이 발달할 수 있는 조건이 더 좋았다.\n반면, 고구려는 식량 문제와 인구수에서 어려움을 겪었을 가능성이 크다. 넓은 영토를 보유하고 있었지만, 대부분의 영토는 고원 지대나 산악 지대가 많아 농업에 적합하지 않은 지역이었다.\n이로 인해 고구려는 군사력과 영토 확장에 주력하면서 외부에서 식량을 확보하려는 경향이 있었다."
  },
  {
    "objectID": "misc/mse_02_0.html",
    "href": "misc/mse_02_0.html",
    "title": "금(Ag)",
    "section": "",
    "text": "금속과 금의 특성을 중심으로 다루고자 한다.\n01 금속 (Metal) 대부분의 금속은 상온에서 고체 상태로 존재한다. 다만, 수은(Hg)은 상온에서 유일하게 액체 상태로 존재하는 금속이다.\n\n광택 (Luster) 금속은 일반적으로 매끄럽고 반짝이는 표면을 가지고 있어 빛을 반사한다. 이러한 특성 때문에 금속은 아름다움과 장식성을 제공하는 데 사용된다.\n열과 전기 전도성 (Thermal & Electrical Conductivity)\n\n금속은 열과 전기를 잘 전달하는 성질을 가지고 있다. 이로 인해 전선, 난방기구 등에서 광범위하게 사용된다.\n\n연성 (Ductility) 금속은 연성이 뛰어나다.\n\n이는 쉽게 늘어나거나 변형될 수 있음을 의미하며, 이로 인해 형태만 변할 뿐 쉽게 부서지지도 않는다.\n\n전성 (Malleability) 금속은 얇고 넓게 퍼질 수 있는 특성을 가지고 있다. 즉, 금속을 두드리거나 압축하면 넓은 면적을 유지하면서 형태가 변할 수 있다.\n\n02 금 (Gold)\n\n순수한 형태 금은 다른 원소와 쉽게 결합하지 않으며, 금끼리만 뭉쳐지는 특징이 있다.\n\n이는 금이 산화되지 않고 녹슬지 않기 때문이다.\n\n낮은 녹는 점 금은 비교적 낮은 온도(1064.18℃)에서 녹아 액체 상태로 변한다. 이 특성 덕분에 다양한 형태로 가공하기 용이하다.\n낮은 화학 반응성 금은 화학적으로 매우 안정적이며, 대부분의 산화물이나 산과 반응하지 않는다.\n\n이로 인해 금니와 같은 치과용 재료로 사용된다.\n\n높은 연성 금은 다른 금속보다 부드럽다. 금메달 을 깨물었을 때 남는 자국이 그 예이다.\n\n이는 금이 다른 금속보다 더 연한 성질을 가지기 때문이다.\n\n우수한 전기 전도도 반도체 및 전자기기에서 접촉재로 사용된다. 이는 신뢰성과 내구성이 필요한 전자 장치에 적합한다.\n높은 적외선 반사율 금으로 코팅된 표면은 태양열을 차단하면서도 적외선을 반사하여 분석하는 시스템에 적합한다.\n\n예를 들어, 천체 망원경의 반사 거울이 있다.\n03 경제적 가치 금의 수요가 증가하면서 금화가 널리 사용되었고, 금은 경제 체제의 중요한 요소가 되었다. 유럽에서는 금을 대량으로 채굴하고, 이는 대항해 시대에 나침반과 천문학의 발전과 함께 다른 지역에서 금을 찾기 위한 탐험으로 이어졌다.\n신흥 세력의 부상 신대륙에서는 금의 가치도 특정 시기에 낮아지는 경우가 있었다. 이는 주로 신대륙에서 대량으로 금이 발견되고 채굴되면서 금 공급이 급격히 증가했기 때문이다.\n예를 들어, 유럽에서는 금 1개의 가치가 은 100개에 해당했지만, 신대륙에서는 이보다 낮은 시세로 은 100개로 금 10개를 구매할 수 있었다.\n이로 인해 신흥 세력이 등장하게 되었고, 그들은 귀족이 아님에도 불구하고 상당한 부를 축적하며 권리를 주장하였다.\n민주주의의 발전 금과 은을 통한 부의 집중은 초기 민주주의의 기틀을 마련하는 데 기여했다. 유럽으로 부가 유입되면서 사회가 변화하고 선진국으로 발전하게 되었다.\n04 제임스 웹 우주망원경(JWST)\n\n적외선 분석 시스템 적외선은 가시광선보다 긴 파장을 가지며, 천체들이 방출하는 열을 측정하는 데 사용된다. 시스템은 태양열을 차단하고 나서, 적외선만을 반사하는 특수한 재료를 사용하여 적외선을 반사한다.\n\n이 과정은 불필요한 열이나 빛이 기기에 영향을 미치지 않도록 하여, 적외선만 정확히 분석할 수 있도록 돕는다.\n중심 구멍은 관측 장비가 원하는 천체의 이미지를 집중시키는 역할을 한다. 이 구멍을 통해 반사된 적외선만이 기기에 통과하여 분석된다.\n이렇게 함으로써 특정 파장의 적외선만을 정확하게 감지할 수 있다. 그 외의 불필요한 빛이나 열은 차단되거나 반사된다.\n\n도플러 효과 천체가 멀어지면 주파수가 낮아지는 도플러 효과를 이용하여, 천체의 움직임을 관찰할 수 있다.\n\n이로 인해 파란색에서 녹색, 노란색, 빨간색(적외선)으로 변하는 현상을 통해 최초의 천체와 은하를 탐구할 수 있다.\n\n허블 망원경의 한계 허블 망원경은 주로 가시광선과 자외선을 대상으로 설계되었기에 적외선 영역을 효율적으로 관측하려면 별도의 적외선 전용 망원경이 필요하다.\n\n제임스 웹 우주망원경(JWST)의 거울은 금으로 코팅되어 있어, 적외선을 효과적으로 반사하고, 고해상도의 적외선 이미지를 얻을 수 있도록 돕습니다.\n05 순금과 합금 금의 가격은 금 함유량에 따라 계산된다. 예를 들어, 18K 금은 순금보다 저렴하며, 금의 비율에 따라 가격이 결정된다.\n\n순금 (24K) 99.9%의 금으로 이루어진 금. 매우 연성이 뛰어나고 부드러우며, 쉽게 변형된다.\n18K 금 75%의 금과 25%의 다른 금속(주로 구리, 은)으로 이루어진 합금. 금 색상이 더 강하고 내구성이 향상된다.\n14K 금 58.3%의 금으로 이루어져 있다. 이는 연성을 줄이고 내구성을 높이기 위해 선택된 함량이다.\n\n금은 화학적 안정성, 연성, 전도성 등 여러 면에서 우수하여 귀금속 중에서 최고의 가치를 지니고 있다.\n이러한 특성 덕분에 보석 및 치과 치료에서 널리 사용된다.\n\n색상 다양성 금을 다른 금속과 혼합하면 색상이 달라질 수 있다.\n\n금에 구리를 추가하면 붉은 색상이 나타난다. 은과 니켈을 혼합하여 은색의 금반지를 만들기도 한다."
  },
  {
    "objectID": "misc/mse_02_2.html",
    "href": "misc/mse_02_2.html",
    "title": "건축",
    "section": "",
    "text": "건축, 부식 방지, 특수 합금, 폭죽 및 마그네슘에 대해 다루고자 한다.\n01 H 형강\n\n철근 콘크리트 건축물 취성을 가진 콘크리트와 연성을 가진 철근을 결합하여 강도를 높인 구조물이다.\n\n하지만 초고층 건물에서는 밑바닥이 막대한 하중을 받으므로, 이러한 구조로만 지을 경우 바닥이 견디지 못할 수 있다.\n이를 해결하기 위해 H 형강을 사용한다.\n\nH 형강 건축물 철을 H자 형태로 만든 구조물로, 강도가 뛰어나며 하중을 효과적으로 분산시킬 수 있다.\n\n덕분에 초고층 건물에서 안전성을 보장할 수 있는 중요한 자재로 활용된다.\n역할 고층 건물에서는 기둥이 1차적인 지지 역할을 담당하고, 내력벽 형태의 콘크리트가 이를 보조하여 2차적인 지지를 제공한다.\n특히 H형강은 각 부분이 균등한 하중을 받으므로 구조적으로 안정성이 뛰어나다.\n활용 이러한 구조를 채택하면 건물의 옆면을 얇은 유리나 금속판으로 마감할 수 있어 전체적인 무게를 줄이는 동시에 미관을 개선할 수 있다.\n그 결과, 고층 건물의 외관은 투명하고 세련된 모습이 되며, 이러한 설계 방식은 일반 아파트보다 상업용 건물에서 더욱 자주 사용된다.\n유리 사용의 장점으로는 자연광을 극대화할 수 있다는 점과 도시 경관과의 조화를 이루어 시각적 효과를 더할 수 있다는 점이 있다.\n\n인장강도 시험 재료의 인장강도를 측정하기 위해 시험편을 일정한 속도로 인장하여 파단되는 힘을 측정한다.\n\n이로써 재료의 강도와 변형 특성을 평가할 수 있다.\n02 희생 양극법 (Sacrificial Anode Method)\n이 방법은 금속 구조물의 부식을 방지하기 위해 사용된다.\n철이 산소와 결합하는 과정에서 전자를 주기에 부식이 발생하는데, 이를 막기 위해 마그네슘과 같은 더 전자를 잘 주는 금속을 사용한다.\n마그네슘이 철 대신 산화됨으로써 철 구조물의 부식이 방지된다.\n\n강철 기름통 강철로 만들어진 기름 통은 희생 양극법을 적용하여 산소 이온과 마그네슘 이온 (Mg²⁺)이 결합하여 부식이 방지된다.\n선박 선박의 경우, 아연판을 부착하여 아연이 희생 양극으로 작용하게 하여 선체의 부식을 방지한다.\n\n아연(Zn)이 산화되면서 철의 부식을 보호하는 원리이다.\n03 내후성강 (Atmosphere Corrosion Resisting Steel)\n녹이 쓰는 것에 대한 저항성이 뛰어난 금속이다.\n일반적인 철강은 산소와 수분에 노출될 경우 표면에서부터 산화되어 부식이 시작되지만,\n내후성강은 특수한 성질을 가지고 있어 부식 방지가 가능하다.\n\n원리 내후성강의 표면에 녹이 생기면, 초기에는 갈라진 틈으로 산소가 침투하여 부식이 계속될 수 있다. 그러나 내후성강은 특정 조건에서 치밀한 산화층을 형성한다.\n\n이 산화층은 표면을 덮는 막처럼 작용하며, 단단하고 촘촘하여 산소나 습기가 금속 내부로 침투하는 것을 차단한다.\n그 결과, 강철 내부는 산소와 접촉하지 않아 부식이 멈추게 된다.\n이러한 자기보호 특성 덕분에 내후성강은 일반 강철보다 부식에 강하고, 훨씬 더 오래 사용할 수 있다.\n\n색 변화 내후성강의 초기 색상은 노란색에서 시작하여 시간이 지나면서 갈색으로 변하는데,\n\n이는 디자인적으로도 매력적인 요소가 될 수 있다.\n\n환경적 장점 내후성강은 부식이 발생해도 물과 접촉할 경우 환경을 오염시키지 않는 장점이 있다. 반면, 석유화학 제품으로 만들어진 페인트는 물과 접촉할 때 오염을 유발할 수 있다.\n\n따라서, 내후성강은 외부 환경에 대한 내구성을 제공하면서도 환경 보호 측면에서도 긍정적인 선택이 될 수 있다.\n04 스테인리스 (Stainless)\n\n해리 브리얼리(Harry Brearley) 그는 1913년 영국에서 12 ~ 25%의 크롬(Cr)을 첨가해 녹이 잘 슬지 않는 강철을 발견하며 스테인리스강 개발의 시초가 되었다.\n전쟁에서의 부적합성 총기는 화약의 폭발로 발생하는 강렬한 압력이 탄환을 발사하는 원리이다. 이 과정에서는 강철이 필요했으며, 스테인리스강은 주요 재료로 사용되지 않았다.\n\n일부 부품(예: 총열, 총기 부속물 등)에서 활용된 사례도 존재하지만, 그 사용은 전반적인 총기 생산에서 큰 비중을 차지하지는 않았다.\n\n스테인리스강의 활용 제2차 세계대전이 끝난 후 외과 수술에 필수적인 도구로서 사용되기 시작했다. 스테인리스강은 부식에 대한 저항성이 뛰어나 외과 수술 도구에 적합했다.\n\n은, 황동 식기 및 높은 온도의 소독이 필요한 의료기구는 모두 스테인리스강으로 대체되었다.\n하지만 관리가 소홀해지면 여전히 녹이 쓸 수 있다.\n\n우주 탐사에서의 사용 현대 우주선에서는 스테인리스강이 일부 구조물에 사용되지만, 더 강력한 출력과 성능을 제공하는 합금이 필요하다.\n\n예를 들어, 스타쉽(Starship)의 엔진은 누리호 엔진보다 10배 더 높은 성능을 자랑하며,\n이를 위해 값싸고 무거운 스테인리스강 대신 고온에서 잘 견디는 고성능 합금이 사용된다.\n05 티타늄 (Titanium) 가벼우면서도 강도가 철과 비슷하여, 강력하면서도 경량의 특성을 가지고 있다.\n이러한 특성 덕분에 다양한 분야에서 응용되고 있다. 내식성이 뛰어나고, 오래 보존될 수 있는 장점을 가지고 있다.\n비용 티타늄은 TiO₂에서 산소(O₂)를 분리하는 과정이 어려워 비싸다. 이 때문에 티타늄을 활용한 제품은 가격이 상대적으로 높다.\n용도 (1) 티타늄은 생체적합성이 뛰어나 의료용 임플란트 재료로 많이 사용된다.\n\n항공기와 전투기에서도 티타늄이 사용된다. 이는 강도와 가벼운 무게가 중요한 비행기 구조에 적합하기 때문이다.\n티타늄을 활용한 형상 기억 합금은 특정 온도에서 원래 형태로 되돌아가는 특성이 있어,\n\n의료 기기 및 로봇 공학 등 다양한 분야에 응용된다.\n06 니티놀 (Ni ti nol) 니켈(Ni)과 티타늄(Ti)의 합금으로, 두 원소가 1:1의 비율로 조합된 고급 소재이다.\n1961년 미 해군병기연구소(NOL) W. Buehler가 개발하였다.\n이름은 니켈(Nickel)과 티타늄(Titanium)의 조합과 그 개발이 이루어진 Navy의 Naval Ordnance Laboratory에서 유래되었다.\n이 소재는 형상 기억 합금으로 분류된다.\n즉, 특정 온도에서 원래의 형태로 돌아갈 수 있는 특성을 가지고 있다. 니티놀은 100만 번 변형되어도 형태를 회복할 수 있는 뛰어난 특성을 보여준다.\n범용성 및 비용 니티놀은 그 뛰어난 특성에도 불구하고, 가격이 비싸서 범용성이 제한적이다. 대신, 더 저렴한 대체 합금으로 구리(Cu)와 같은 금속을 사용하여 유사한 성질을 구현하는 경우가 많다.\n용도 (1) 우주 발사체의 접이식 안테나에 사용된다. 발사 후 열을 가하면 펼쳐지는 특성을 이용한다.\n\n치열 교정기와 같은 의료 기기에서 사용되며, 형태 기억 특성을 활용하여 변형이 용이하다.\n자동차의 스노우 체인에도 사용되며, 추운 환경에서도 유연하게 작동할 수 있는 장점을 제공한다.\n니티놀의 특성을 이용하여 라인을 잡아주는 핀 등 다양한 패션 아이템에도 응용된다.\n\n07 폭죽 하단에 화약이 위치해 있으며, 내부에는 다양한 화학 물질이 포함되어 있다.\n작동 원리 폭죽이 점화되면 하단의 화약이 연소하며 가스를 발생시킨다. 생성된 가스가 폭죽을 하늘로 쏘아 올린다.\n폭죽이 일정 고도에 도달하면 내부의 나트륨과 같은 화학 물질이 점화되어 화학 반응을 일으키며 폭발한다.\n폭죽을 둘러싼 외부 껍질은 폭발 시 부서지거나 쪼개지며, 불꽃을 일정한 방향으로 방출하는 역할을 한다.\n껍질의 형태와 두께, 재료는 불꽃의 분포와 모양을 결정하는 데 영향을 미친다.\n또한, 폭발 시 다양한 색깔의 불꽃과 효과는 포함된 화학 물질의 종류에 따라 달라지며, 그 예시는 다음과 같다.\n알칼리금속 (1족, 수소 제외)\n리튬(Li), 나트륨(Na), 칼륨(K), 루비듐(Rb), 세슘(Cs)\n각 금속의 불꽃 색상이다.\n전기적 성질 이들 금속은 전자를 쉽게 잃고 양이온이 되며, 반응성이 매우 높다. 특히 수분이나 공기 중의 산소와 격렬하게 반응한다.\n반응 물과 반응하여 수소 기체를 발생시키며, 이 반응은 열을 방출한다. 이로 인해 수소가 연소할 수 있는 위험이 있다.\n질소(N)와 같은 비활성 가스 환경에서 실험할 경우 더 안전하게 다룰 수 있다.\n알칼리토금속 (2족)\n베릴륨(Be), 마그네슘(Mg), 칼슘(Ca), 스트론튬(Sr), 바륨(Ba) / + 구리(Cu) (11족)\n각 금속의 불꽃 색상이다.\n전기적 성질 이들 금속도 전자를 잘 주어 양이온이 되지만, 알칼리금속보다 반응성이 덜한다.\n반응 알칼리토금속은 물과 반응할 수 있으나, 그 반응은 알칼리금속보다 덜 격렬하다.\n가루 상태로 만들거나 열을 가할 경우 반응성이 증가한다.\n08 마그네슘(Magnesium) 철보다 약 5분의 1 가벼운 금속으로, 뛰어난 경량성과 강도를 가지고 있다.\n반응성이 높아 산소와 쉽게 결합하여 산화되기에, 사용 시에는 반응성을 억제해야 한다.\n용도 마그네슘은 주로 가벼운 구조물이나 부품에 사용된다. 예를 들어, 항공기, 자동차 부품, 스포츠 장비 등에 적용된다.\n플라스틱 코팅이나 기타 재료와 결합하여 무게를 줄이면서도 필요한 강도를 유지할 수 있다.\n주파수 방해 금속 케이스를 사용할 경우 전자기파에 의한 주파수 방해가 발생할 수 있어, 이로 인해 무선 신호가 방해받을 수 있다.\n최근의 스마트폰에서는 아이폰은 듀랄루민을, 삼성 갤럭시는 마그네슘 합금을 사용하여 강도를 유지하면서 무게를 줄이고 있다."
  },
  {
    "objectID": "misc/mse_03_0.html",
    "href": "misc/mse_03_0.html",
    "title": "플라스틱",
    "section": "",
    "text": "플라스틱의 역사와 분류에 대해 다루고자 한다.\n01 기초 유기물질 유기화합물의 기본 단위로, 일반적으로 탄소(C)와 수소(H)로 구성된다. 이러한 물질들은 단순한 분자 구조를 가지고 있으며, 다양한 화학적 성질을 지니고 있다.\n\n단일 유기물질 (모노머, Monomer)\n\n하나의 유기 분자로, 다른 분자와 결합하여 고분자 구조를 형성할 수 있는 능력을 가진 물질.\n예를 들어, 에틸렌(ethylene)이나 프로필렌(propylene) 같은 작은 분자들이 모노머에 해당된다.\n\n다중 유기물질 (폴리머, Polymer)\n\n여러 개의 모노머가 결합하여 형성된 고분자.\n예를 들어, 폴리에틸렌(polyethylene)이나 폴리프로필렌(polypropylene) 같은 플라스틱이 이에 해당된다.\n플라스틱 (Plastic) 일반적으로 합성고분자로, 다양한 형태와 물리적 특성을 가지고 있다.\n그러나 모든 고분자가 플라스틱은 아니며, 자연에서 유래된 고분자(예: 셀룰로오스)도 존재한다.\n합성수지 (Synthetic Resin) 인위적으로 합성된 고분자로, 플라스틱의 한 종류로 분류될 수 있다. 다양한 화학 구조를 가질 수 있으며, 응용 분야에 따라 다양한 특성을 발휘한다.\n탄소 결합 탄소 원자는 4개의 결합을 형성할 수 있는 능력이 있어, 다양한 화합물을 만들 수 있다.\n탄소 원자 간에는 단일결합, 이중결합, 삼중결합이 존재할 수 있으며, 이로 인해 다양한 분자 구조가 생성된다.\n\n단일결합 (Single Bond) 두 원자가 한 쌍의 전자를 공유하는 결합.\n\n수소 분자(H₂), 메테인(CH₄) 등. 가장 약한 결합이지만, 가장 길며, 회전이 자유롭다.\n\n이중결합 (Double Bond) 두 원자가 두 쌍의 전자를 공유하는 결합.\n\n이산화탄소(CO₂), 에틸렌(C₂H₄) 등.\n단일결합보다 강하지만, 결합 길이는 더 짧고 회전이 제한된다(결합의 평면 구조 유지).\n\n삼중결합 (Triple Bond) 두 원자가 세 쌍의 전자를 공유하는 결합.\n\n질소 분자(N₂), 아세틸렌(C₂H₂) 등.\n가장 강하고, 가장 짧은 결합을 가지며, 회전이 불가능하다(직선형 구조 유지).\n02 석유화학 산업 석유와 천연가스를 원료로 하여 다양한 화학 물질과 소재를 생산하는 산업. 이 과정에서 생성되는 다양한 화합물들은 여러 산업에서 원료로 사용된다.\n\n저비용 석유화학 산업에서 생산되는 플라스틱 및 기타 화합물들은 대량 생산이 가능하여, 원가가 상대적으로 저렴한다.\n\n이는 산업 전반에서 비용 효율적인 재료로 인식되게 한다.\n\n내화학성 석유화학 제품은 대개 화학적 안정성이 높아 산, 알칼리 등 다양한 화학 물질에 대한 저항력이 크다.\n\n이로 인해 부식이나 변질이 잘 일어나지 않다. 예를 들어, 화학 저장 탱크와 같은 산업용 장비에서 많이 사용된다.\n\n내충격성 석유화학 제품은 충격에 대한 저항성이 뛰어나며, 이는 자동차 범퍼와 같은 안전 장비에 활용된다.\n\n내충격성을 가지는 플라스틱은 변형이 가능하며, 충격 흡수능력이 뛰어난 특성을 가지고 있다.\n\n절연성 석유화학 소재는 전기 절연성이 우수하여 전선 피복재, 전기 기기 및 전자기기에서 많이 사용된다.\n\n이는 전기 전도성을 방지하고 안전성을 높이는 데 기여한다.\n03 셀룰로이드 (Celluloid) 존 웨슬리 하야트(John Wesley Hyatt)라는 미국의 발명가가 1869년에 개발한 최초의 열가소성 합성수지이다.\n셀룰로오스는 나무나 잎에서 추출된 천연 물질로, 이를 화학 물질과 결합시켜 셀룰로이드라는 합성 물질을 만든다.\n보통 니트로셀룰로스와 플라스틱 가소제(Izers)가 주요 화학 성분으로 사용된다. 이러한 과정을 통해 유연성과 내구성이 뛰어난 플라스틱으로 변형된다.\n초기에는 코끼리 상아의 대체물질로 당구공 제작에 사용되었으며, 이후에는 복잡한 공예품 제작에도 활용되었다.\n\n몰딩 (Molding) 이 방식은 셀룰로이드가 열과 압력을 받으며 성형되어 원하는 형태로 만들어지는 공정이다.\n\n가열하면 부드럽게 변형되어 몰딩할 수 있으며, 냉각 후에는 굳어져 일정한 형태를 유지한다.\n\n물리적 특성 셀룰로이드는 80도 이상의 온도에서 변형되는 문제점이 있어 열에 민감한 특성을 가지고 있다.\n\n또한, 담배 불에 의해 쉽게 불이 붙거나 심지어 폭발할 수 있는 위험이 있었다. 셀룰로이드는 발화점이 낮아 170 ~ 190도에서 쉽게 불이 붙는다.\n그래서 주로 장식품과 같은 불에 잘 타지 않아도 되는 제품에만 사용되었다.\n탁구공의 경우 2014년까지 셀룰로이드가 사용되었으며, 현재 다른 재료로 대체되었다.\n\n영화 산업에서의 사용 1900년대 초, 영화 필름(Kodak)의 소재로 사용되었다.\n\n이는 열을 가해 명암 차이를 조절할 수 있어, 많은 양의 필름을 저렴하게 생산할 수 있었기 때문이다.\n오늘날 영화 산업의 발전에 중요한 기여를 한 소재로 알려져 있다.\n\n논쟁 셀룰로이드는 천연 물질을 사용하여 만들어졌지만, 화학 처리를 통해 물리적 성질을 변화시키는 방식으로 제조되기에 ’합성 플라스틱’이라고 불리기 어렵다고 주장하는 학자도 있다.\n\n전통적인 합성 플라스틱은 완전히 합성된 물질로,\n자연에서 직접 얻을 수 있는 성분이 아니라 인위적으로 화학 반응을 통해 처음부터 끝까지 만들어진다.\n이런 점에서 셀룰로이드는 전통적인 합성 플라스틱과는 다른 범주로 간주된다.\n04 베이클라이트 (Bakelite) 레오 헨드릭 베이클랜드(Leo Hendrik Baekeland)라는 벨기에 출신의 화학자가 1907년에 개발한 최초의 합성 플라스틱이다.\n그의 이름을 따서 이 소재에 “베이클라이트”라는 이름이 붙여졌다. 이 물질을 개발한 공로로 “플라스틱의 아버지”라는 별명을 얻었다.\n그는 페놀(Phenol)과 포름알데히드(Formaldehyde)의 중합 반응을 통해 만들어진 합성수지이다.\n100% 합성 물질로, 천연 원료가 아닌 인공적으로 합성된 플라스틱이다. 이는 다른 합성 물질들과는 달리, 자연에서 유래하지 않는 점에서 특징적이다.\n이 개발은 플라스틱 및 고분자 산업의 기초를 다지는 계기가 되었다.\n\n내열성 및 내구성 내열성이 뛰어나고, 전기 절연성이 좋아 전자제품의 케이스 등 다양한 분야에서 사용된다.\n가공성 경화 과정을 통해 단단한 플라스틱으로 변형되며, 가공 후에는 변형되지 않는 특성을 가지고 있다.\n\n이는 복잡한 형태로 성형하기에 적합하다.\n\n환경 저항성 화학물질과의 반응성이 적어, 내구성이 우수하여 다양한 환경에서도 사용될 수 있다.\n사용 용도 초기에는 전기 기기, 주방용품, 자동차 부품 등 다양한 산업 분야에서 사용되었으며, 특히 당시 호황이었던 전기 산업에서 전자기기의 절연체로 많이 활용되었다.\n\nLP판(레코드판)의 제작에도 사용되었으며, 이는 음향 품질을 높이는 데 기여했다. 시간이 지나면서 수요가 증가함에 따라 합성 플라스틱의 사용은 더욱 확대되었다.\n쉘락은 인도락깍지벌레(Laccifer lacca)의 분비물에서 얻어지는 천연 수지로, 본래 LP판의 제작에 사용되는 소재였다.\n그러나 내구성이 낮고 가격이 비쌌기 때문에, 당시 LP판을 즐겨 듣던 레오 베이클랜드는 베이클라이트라는 새로운 소재를 개발한 것이다.\n05 플라스틱의 구분\n\n열가소성 수지 (Thermoplastic Resin)\n\n가열 시에 부드러워져 흐물흐물하게 변하며, 냉각하면 다시 단단해지는 성질을 가진 합성 수지이다.\n대표적인 예로는 폴리에틸렌(PE), 폴리프로필렌(PP), 폴리염화비닐(PVC) 등이 있다.\n사슬 형태 긴 사슬 형태의 분자 구조로 되어 있어, 열을 가할 때 분자 간의 결합이 풀리면서 흐물해진다.\n재활용 가능성 대부분의 플라스틱 물질이 열가소성 수지로 만들어지며, 이들은 재활용 마크를 가지고 있는 경우가 많다.\n화학처리를 통해 기초 물질로 분리(회귀)할 수 있어, 재활용이 용이하다.\n범용 플라스틱 다양한 용도로 사용되며, 일상생활에서 자주 볼 수 있는 플라스틱 제품의 대부분이 열가소성 수지로 제작된다.\n예를 들어, 포장재, 용기, 가전 제품 부품 등이 있다.\n가공 방법 (1) 사출 성형 (Injection Molding)\n다양한 형태의 제품을 만들 수 있으며, 이는 산업 생산에서 매우 일반적인 방법이다.\n\n압출 성형(Extrusion Molding)\n\n연속적인 형태로 성형하는 방식으, 주로 파이프, 필름, 시트, 와이어 코팅 등의 제품 제작에 사용된다.\n\n열경화성 수지 (Thermosetting Resin)\n\n가열 시에 경화되는 성질을 가진 합성 수지로, 한 번 경화되면 다시 가열하더라도 형태를 변경할 수 없는 특성을 가지고 있다.\n대표적으로 베이클라이트와 같은 물질이 이에 해당한다.\n불가역적 경화 열경화성 수지는 가열 후 화학 반응을 통해 경화가 이루어지며, 이 과정은 불가역적이다.\n한 번 경화되면 다시 녹이지 못하고 원래의 형태로 되돌릴 수 없다.\n그물 형태 열경화성 수지는 분자가 그물망처럼 연결된 구조를 가지고 있다. 이 구조 덕분에 열과 화학적 스트레스를 잘 견디며, 높은 강도와 내열성을 가진다.\n재활용 불가능 일단 경화되면 재활용이 어려워, 폐기 시 환경 문제를 일으킬 수 있다.\n가공 방법 (1) 사출 성형 (Injection Molding)\n초기 단계에서 보통 액체 상태로 가공된 후, 고온에서 경화 과정을 거쳐 최종 제품을 만들어지는 공정이다.\n\n압축 성형 (Compression Molding)\n\n주로 복합 재료 제작에 사용되며, 압력을 가해 경화시키는 방식이다.\n\n전사 성형 (Transfer Molding)\n\n섬유 강화 플라스틱(FRP) 같은 제품을 만들 때 주로 활용되는 성형 방법이다."
  },
  {
    "objectID": "misc/mse_03_2.html",
    "href": "misc/mse_03_2.html",
    "title": "고무",
    "section": "",
    "text": "고무의 전반적인 개념과 합성 고무에 대해 다루고자 한다.\n01 발견\n기원전 1600년경, 중남미 지역의 올멕 문명에서 고무를 처음으로 사용한 흔적이 발견된다.\n(이 고무는 Hevea brasiliensis에서 추출한 라텍스를 의미.)\n고무는 의식용 물품, 방수 재료, 공놀이 등에 활용되었다.\n기원전 200년 ~ 기원후 900년경, 마야 문명이 고무를 더욱 광범위하게 활용했다.\n마야인들은 천연 라텍스를 채취한 뒤, 나무의 수액과 섞어 탄성과 내구성이 높은 고무를 만들어 사용했다.\n이것은 주로 방수 의복, 공놀이(Pok-Ta-Pok), 의식적 도구 제작 등에 사용되었다.\n14세기 ~ 15세기경, 아즈텍 문명이 마야 문명을 계승해 고무 사용을 지속했다.\n아즈텍인들은 고무로 만든 공을 종교적 의식과 여가 활동에 사용했으며,\n고무나무가 자생하는 열대 지역과 활발히 교류했다.\n1493 ~ 1496년, 크리스토퍼 콜럼버스의 두 번째 항해 중\n(Christopher Columbus, 1450–1506)\n카리브해 지역(히스파니올라, 쿠바, 자메이카 등)을 탐험하며 원주민들이 사용하는 고무 공을 목격했다.\n그는 이를 유럽으로 가져갔지만, 당시 유럽에서는 고무의 실질적 용도를 이해하지 못했다.\n16세기 초반, 스페인 탐험가들이 중앙아메리카와 남아메리카에서 아즈텍 및 마야 문명을 접하며 고무 사용법과 그 제작 과정을 보고했다.\n스페인 사제와 학자들이 고무의 특징과 활용법을 유럽에 기록으로 남기기 시작했다.\n02 고무의 활용 및 어원\n1770년, 영국의 화학자 조지프 프리스틀리\n(Joseph Priestley, 1733–1804)\n고무를 지우개로 사용하는 법을 발견하며 유럽에서 고무의 실용성이 주목받기 시작했다.\n‘Rubber’ 의 어원 고무의 영어 명칭인 ’Rubber’는 18세기 후반에 처음 사용되었다.\n이는 고무의 주요 초기 용도가 연필 자국을 문질러 지우는 도구(지우개)였기 때문이다.\n당시 연필로 쓴 글씨를 지우기 위해 고무를 문질러 사용하는 방식에서 ‘Rub’이라는 단어가 연관되었고, 이로 인해 고무는 ’Rubber’ 라는 이름을 얻었다.\n지우개의 영어 명칭 영국식 영어에서는 ’Rubber’라는 단어가 여전히 지우개를 의미한다.\n반면, 미국식 영어에서는 ‘Eraser’ 가 지우개를 뜻하며, ’Rubber’는 일반적으로 고무를 가리키는 단어로 사용된다.\n‘Eraser’ 의 어원 라틴어 ‘Eradere’ 에서 유래했다.\n’ e - ’ 는 ‘out’ (밖으로), ‘radere’ 는 ‘scrape’(긁다, 문지르다)라는 의미이다.\n합쳐서 ‘eradere’는 ’긁어서 없애다’ 를 뜻하며, 영어로는 ‘erase’(지우다)로 발전했다.\n‘Eraser’ 는 ‘지우는 도구’ 를 의미하며, 18세기 후반부터 사용되기 시작했다.\n‘고무’ 의 어원 프랑스어 ‘gomme’(고므), 네덜란드어 ‘gomm’(곰), 그리고 일본어 ‘ゴム’(고무)에서 유래되었다.\n여기서 ’gomme’는 고무나무 수지를 뜻하는 ’rubber’와 관련이 있으며, 이 단어들이 한국어로 넘어오면서 현재의 ’고무’라는 단어로 정착하게 되었다.\n영어에서 ’gum’과 독일어의 ’Gummi’는 고무와 유사한 성질을 가진 물질을 의미하는 단어로,\n고무와 관련된 의미에서 유래하였지만, 이들 단어는 고무의 원래 의미인 ’고무나무 수지’와는 다소 차이가 있다.\n고무나무의 수지나 그것을 인공적으로 흉내낸 합성 고무는 한국어로 고무라고 불리며, 일반적인 나무 수지에는 껌이나 검이라는 단어를 사용한다.\n씹는 껌(gum)은 치클나무의 수지에서 추출된 물질로, 영어의 gum과 그 어원은 동일하지만, 우리는 이를 껌이라고 따로 구분하여 부르고 있다.\n즉, ‘껌’ 과 ‘고무’ 의 어원은 같지만, 그 의미와 사용에 차이가 있다.\n03 가황 고무 (Vulcanized Rubber)\n1823년, 영국의 발명가인 찰스 매킨토시\n(Charles Macintosh, 1766–1843)\n그는 석탄 타르에서 얻은 나프타(naphtha)를 사용해 고무를 녹이고, 이를 천 사이에 삽입하는 방법으로 방수 섬유을 만들었다.\n매킨토시 코트 (Macintosh Coat)\n매킨토시가 발명한 방수 섬유는 곧 의류 제작에 활용되어 최초의 비옷(Raincoat)가 탄생했다.\n이 코트는 특히 비가 자주 내리는 날씨에서 유용한 의류로 인기를 끌었다. 그러나 천연 고무를 이용한 방수 섬유는 너무 뻣뻣해 실용성이 떨어졌다.\n미국의 발명가인 찰스 굿이어\n(Charles Goodyear, 1800–1860)\n그는 고무의 가황법을 발명하며 고무의 산업적 활용이 본격화되었다.\n1839년, 천연 고무에 유황을 첨가하여 가황 고무를 발명했으며, 이로 인해 고무의 내구성, 탄력성, 안정성이 크게 향상되었다.\n1844년, 미국에서 가황고무 기술에 대한 특허를 획득했다. 그러나 당시 영국에서는 특허를 등록하지 못했다.\n영국에서는 이미 핸콕이 가황 고무와 관련된 특허를 등록한 상태였기 때문이다.\n영국의 발명가 토머스 핸콕\n(Thomas Hancock, 1786–1865)\n1820년대, 핸콕은 고무를 더 잘 가공할 수 있는 기술을 연구하면서 “매스티케이터(masticator)”라는 고무 연화 기계를 개발했다.\n1843년, 핸콕은 독립적으로 가황고무를 발명했다.\n그는 유황과 고무를 가열하여 고무의 내구성과 탄성을 개선하는 방법을 발견하고, 영국에서 이를 특허로 등록했다.\n이후 가황 고무를 매킨토시 코트에 적용하여 악취를 제거하고 기능을 개선하였다.\n굿이어와 핸콕의 분쟁 굿이어는 가황 고무 발명의 권리를 주장했으나,\n토머스 핸콕은 가황 고무 기술을 독립적으로 개발해 1843년 영국에서 특허를 먼저 등록한 상태였다.\n이로 인해 굿이어는 영국에서 법적 우위를 확보하지 못했으며, 유럽 시장에서 가황 고무 기술을 상업화할 권리를 잃게 되었다.\n결국 재정적인 문제와 특허 분쟁으로 인해 그의 회사는 파산했고, 굿이어는 한 많은 삶을 살다 1860년 쓸쓸히 생을 마감했다.\n1830년대 초, Liverpool Rubber사는 고무로 만든 신발 밑창인 Sand Shoes를 출시했다.\n이는 고무 산업 초기의 중요한 상업적 성공 사례로 여겨진다.\n04 고무의 응용\n방수용품과 타이어 고무는 그 탄력성과 방수 특성 덕분에 방수용품에 많이 사용된다.\n특히, 비옷이나 우산 등에 사용되는 천연 고무는 비로부터 보호하는 데 큰 역할을 한다.\n또한, 고무는 자동차 타이어와 같은 고성능 제품에도 필수적으로 쓰인다.\n1887년, 스코틀랜드의 발명가인 존 보이드 던롭\n(John Boyd Dunlop, 1840–1942)\n최초의 공기 주입식 자전거 타이어를 발명한다.\n자신의 아들이 자전거를 타는 동안 불편을 겪자, 공기 주입식 타이어를 발명하여 충격을 완화하고 주행 성능을 개선하는 방법을 고안했다.\n이는 자전거 타이어 역사에 큰 획을 그은 발명이었습니다.\n1891년, 프랑스의 기업가인 앙드레 미쉐린과 에두아르 미쉐린\n(André Michelin, 1853–1931) (Édouard Michelin, 1859–1940)\n그들은 탈착식 공기 주입식 자전거 타이어를 발명한다.\n같은 해 9월, 프랑스의 사이클 스타 샤를 테롱이\n(Charles Terront, 1857–1932)\n파리-브레스트-파리(Paris-Brest-Paris) 자전거 경주에서 사용하여 우승함으로써 그 우수성이 입증되었다.\n1889년, 형제는 프랑스 클레르몽페랑에서 미쉐린(Michelin) 회사를 설립하며 타이어 산업의 선구자로 자리 잡았다. 이후 자동차용 타이어를 포함한 다양한 혁신을 통해 타이어 기술을 발전시켰다.\n그들이 개발한 타이어는 공기를 주입하여 탄력을 제공하고, 탈부착이 가능한 설계로 유지보수가 편리한 점에서 혁신적이었다.\n오늘날 우리가 사용하는 현대적인 타이어와 유사한 형태로, 자동차와 자전거의 타이어 기술을 혁신적으로 발전시켰다.\n1895년, 미쉐린 형제는 최초의 공기 주입식 자동차용 타이어를 발명한다.\n이 타이어는 차량의 무게를 효과적으로 지탱하고 주행 중 충격을 흡수하여 안정성과 편안함을 크게 향상시켰다.\n같은 해, 이 타이어를 장착한 차량이 파리-보르도(Paris-Bordeaux) 자동차 경주에 출전하며 기술력을 입증했다.\n타이어 생산의 중요성 자동차 기업들이 타이어 생산에 중점을 두는 이유는 차량의 성능과 안전에 직접적인 영향을 미치기 때문이다.\n타이어는 도로와의 접촉을 담당하며, 자동차의 주행 성능, 연비, 제동력 등에 중요한 역할을 한다.\n스니커즈 (Sneakers)\n고무 밑창을 사용한 운동화로, 20세기 초반부터 인기를 끌었다.\n고무 밑창은 착용감을 높이고 내구성을 강화하며, 스포츠와 일상적인 활동에서 모두 사용되기 좋은 특성을 가지고 있다.\n스니커즈의 발전은 패션 아이템으로도 큰 영향을 미쳤다.\n05 석유 및 정유 산업의 발전 19세기 말부터 20세기 초, 석유 산업과 정유 산업이 급격히 발전하면서, 다양한 화학 제품이 생산되기 시작했다.\n특히, 석유화학 산업의 발전은 합성 고무를 비롯한 여러 고분자 물질의 생산을 가능하게 했다.\n합성 고무는 천연 고무에 비해 가격이 저렴하고, 다양한 특성을 갖출 수 있어 산업 전반에서 중요한 역할을 하게 되었다.\n1909년, 독일의 화학자 프리드리히 호프만\n(Friedrich Hofmann, 1866–1956)\nBayer에서 그의 동료들과 함께 합성 고무인 폴리이소프렌(Polyisoprene) 개발한다.\n폴리메틸이소프렌(Polyisoprene) 기반으로 만들어졌으며, 천연 고무를 대체하려는 시도의 시작이었다.\n초기 합성 고무는 천연 고무에 비해 품질이 낮고, 물리적 특성이 제한적이었다.\n특히, 내구성과 탄성이 부족하여 상용화에 어려움을 겪었다.\n제1차 세계 대전(1914-1918) 동안, 주로 동남아시아 지역에서 공급되는 천연 고무의 수급이 어려워졌다.\n전쟁으로 인해 교역이 제한되었고, 이를 대체할 수 있는 합성 고무의 수요가 급증했다.\n1차 세계 대전 후, 석유화학 공업이 발전하면서 합성 고무 생산이 가능해졌다.\n특히, 석유에서 얻을 수 있는 화학 물질을 통해 더 나은 품질의 합성 고무를 만들 수 있는 기술이 발전하였다.\n1920년대 초, 부타디엔(butadiene)을 이용한 합성 고무 기술이 개발되었다.\n’스타이렌-부타디엔 고무’와 같은 고무 제품의 생산이 가능해졌다.\n(SBR, Styrene-Butadiene Rubber)\n이로 인해 고무의 특성이 개선되었고, 자동차 타이어 등 다양한 산업 분야에서 사용되기 시작했다.\n1930년대, 미국의 듀폰(DuPont)사의 윌리스 캐러더스가 개발한\n(Wallace Carothers, 1896–1937)\n네오프렌(Neoprene)이 등장하며 내구성과 특성이 크게 개선되었다.\n이는 합성 고무의 일종으로, 주로 자동차 타이어와 같은 산업용 제품에 사용될 수 있는 물질로 상업화되었다.\n또한, 내열성, 내화학성, 내유성 등을 갖춘 고무로, 다양한 산업 분야에서 널리 사용된다.\n골프공 소재 (1) 폴리부타디엔 (Polybutadiene) 고무의 탄력성과 내구성을 향상시켜 골프공의 충격을 잘 흡수하도록 돕는다. 이는 골프공의 비거리와 안정성에 중요한 영향을 미친다.\n\n아이온 노머 (Ionomer) 골프공의 표면에 사용되며, 내구성이 뛰어나고 스핀이 잘 걸리게 해준다. 고강도와 내마모성이 뛰어나 골프공의 성능을 높이는 데 기여한다.\n엘라스토머 (Elastomer) 고무의 유연성과 탄성을 높여 골프공의 품질을 향상시킨다. 이는 골프공의 내구성과 비거리 향상에 중요한 역할을 한다."
  },
  {
    "objectID": "misc/mse_04_0.html",
    "href": "misc/mse_04_0.html",
    "title": "세라믹(Ceramic)",
    "section": "",
    "text": "세라믹에 대한 개념에 대해 다루고자 한다.\n\n01 서론\n일반적으로 비금속 무기 물질로 구성되며, 고온 소결 처리를 통해 물리적 강도가 강화된 소재.\n비금속 무기 물질(Non-Metallic Inorganic Material)은 주로 산화물(Oxides), 질화물(Nitrides), 탄화물(Carbides)로 분류된다.\n고온 소결(Sintering)이란, 분말형태의 물질을 높은 온도로 가열하여 고체 상태에서 입자들이 서로 결합하도록 만드는 과정.\n\n\n02 어원\nΚεραμος (Keramos) 이 단어는 “굽다”라는 의미를 포함한다.\n고대에는 점토를 가마에 넣어 고온에서 구워 도자기나 벽돌과 같은 물건을 제작했으므로, 점토와 도자기 그리고 굽는 과정이 자연스럽게 연결되었다.\nΚεραμικός (Keramikos) 점토와 관련된 작업(도자기 제작)을 지칭하며, 여기에서 현대의 “Ceramic”이 유래했다.\n세라믹 (Ceramic) 특정한 성질(내열성, 내마모성, 내화학성 등)을 가진 비금속 무기 소재 자체를 의미하며, 산화알루미늄(Al₂O₃), 질화규소(Si₃N₄) 등이 있다.\n세라믹스 (Ceramics) 세라믹 소재로 만든 제품 또는 공예품을 의미하며, 도자기, 타일, 반도체 칩, 인공관절 등이 있다.\n\n\n03 窯業 (가마 산업)\n세라믹 재료를 제작하거나 가공하는 산업. 이는 전통 도자기 제작부터 현대의 고기능 세라믹 생산에 이르는 광범위한 분야를 포함한다.\n窯(가마 요): 세라믹 제조에서 원료를 굽는 고온의 설비인 “가마”를 의미한다. 전통적으로 도자기와 같은 세라믹 재료를 제작할 때 사용하는 장치이다.\n業(일 업): 산업이나 일, 작업을 뜻한다.\n이 단어는 일본어에서 유래된 표현으로, 특히 전통적으로 도자기를 굽거나 세라믹 제품을 생산하는 업계를 지칭할 때 사용된다.\n한국에서는 도자기 제조와 관련된 산업을 가리킬 때 종종 이 표현을 사용하기도 하지만, 일반적으로 “도예”나 “세라믹 산업”으로 표현된다.\n04 장단점\n장점 (1) 마모에 강한 성질인 내마모성 덕분에 기계 공구, 특히 절삭 공구(Cutting Tools)에서 자주 사용된다.\n\n화학적 반응에 대한 저항성인 내화학성 덕분에 생체재료(인공 관절, 치아 보철물)로 사용된다.\n높은 녹는점인 내열성 덕분에 고온 환경을 견뎌야 하는 항공기 부품 등에 사용된다.\n금속에 비해 밀도가 낮아 경량화가 필요한 응용 분야에서 사용된다.\n다양한 전자기적 특성을 활용하여 세라믹 콘덴서, 반도체, LED 등의 전자 부품으로 활용된다.\n\n단점 (1) 인성이 낮고 취성이 강해여 충격에 쉽게 깨진다.\n\n매우 단단하기 때문에 절삭이나 가공이 어렵다. 이를 위해 레이저 가공 등 첨단 가공 기술이 사용된다.\n녹는점이 매우 높아 금속처럼 녹여서 틀에 부어 형태를 만드는 것이 어렵다.\n도자기처럼 구울 수는 있지만, 굽는 과정에서 수축이 발생하여 원래 크기보다 작아진다. 이로 인해 정밀한 크기로 제작하기가 까다롭다.\n\n05 물질의 강함과 단단함\n강함은 물질이 파괴나 변형에 저항하는 정도를 의미하는데, 단단한 물질이 반드시 강한 것은 아니다.\n예를 들어, 유리는 매우 단단하지만 충격에 약해 잘 깨지는 특성을 가지고 있다.\n\n단단함 (Hardness) 표면이 긁히거나 변형되는 것에 대한 저항성을 나타내며, 단단한 물질은 일반적으로 쉽게 긁히지 않는다.\n강함 (Strength) 외부의 힘에 저항하는 능력으로, 재료가 파괴되거나 영구 변형되기까지의 저항 정도를 나타낸다.\n\n따라서, 유리처럼 단단하지만 깨지기 쉬운 재료는 취성(Brittleness)이 높은 예로 들 수 있다.\n반대로 무른 물질(Soft Materials)은 쉽게 긁히거나 변형되지만 깨지는 경우는 적다."
  },
  {
    "objectID": "misc/mse_04_2.html",
    "href": "misc/mse_04_2.html",
    "title": "첨단 세라믹스 (Fine Ceramics)",
    "section": "",
    "text": "첨단 세라믹스의 전반적인 개념에 대해 다루고자 한다.\n\n01 서론\n기존의 세라믹스보다 고도의 기술과 정밀한 공정이 요구되는 세라믹 소재로, 일반적으로 높은 성능과 특수한 기능을 갖춘 재료를 의미한다.\n제2차 세계 대전 (1939–1945) 第二次 世界 大戰, World War II\n전쟁 중 군사적 필요로 인해 금속, 세라믹, 고분자 등 다양한 재료에 대한 연구가 급속히 발전했다.\n특히 고온 내구성, 내마모성, 경량화 등 군사 장비의 성능 향상을 위한 세라믹 기술이 개발되었다.\n고순도 재료 전자산업의 초기 발전은 고순도 재료가 필수적이었으며, 이는 레이더, 진공관, 반도체 등 첨단 전자기기의 개발로 이어졌다.\n이러한 기술은 전후 민간 산업으로 확산되면서 현대의 첨단 세라믹스 발전에 큰 영향을 미쳤다.\n주요 분류 및 특징 ① 전자 세라믹스 전기적·자기적 성질을 이용해 커패시터, 센서 등 전자 부품에 사용된다.\n② 기계구조 세라믹스 경량, 내마모성, 내열성이 뛰어나 터빈 블레이드, 베어링 등 기계 부품에 적합하다.\n③ 에너지·환경 세라믹스 광학적·전기화학적 성질로 연료 전지, 태양광 패널, 배기가스 정화 시스템에 활용된다.\n④ 바이오 세라믹스 경량과 내화학성이 뛰어나 치아 임플란트, 인공 뼈 등에 사용된다.\n01 ITO 투명 전극 (Indium Tin Oxide, 인듐 주석 산화물)\n인듐(In)과 주석(Sn)을 혼합하여 만든 산화물로, 전기 전도성과 투명성을 동시에 가지고 있어 전자기기에서 중요한 역할을 한다.\n전기가 흐르는 물질과 투명한 물질의 합성.\n구성 및 원리 인듐 산화물(Indium Oxide, In₂O₃)과 주석 산화물(SnO₂)이 결합한 물질로, 주로 90%의 인듐 산화물과 10%의 주석 산화물 비율로 혼합된다.\n도핑(Doping) 과정을 통해 전도성을 증가시키는데, 주석(Sn)을 인듐 산화물에 첨가함으로써 전자들이 쉽게 흐를 수 있도록 만든다.\n이 기술은 터치스크린, 디스플레이, 광학 장치 등에서 널리 사용된다.\n주요 특징 가시광선 영역에서 투명하여 화면 시야를 방해하지 않고, 동시에 전기가 흐르는 특성 덕분에 터치스크린과 디스플레이 장치에 중요하게 사용된다.\n전기 전도성이 뛰어나 전극이나 배선으로 활용되며, 터치스크린에서는 사용자 손가락이 생성하는 전기 신호를 ITO 전극 배열이 감지하여 동작을 인식한다.\n활용 분야 ① 스마트폰이나 태블릿의 터치스크린. ② OLED, LCD 등 다양한 디스플레이. ③ 태양광 패널의 전극. ④ 헤드셋, 스마트워치와 같은 소형 전자기기.\n02 방탄복 (防彈服, Bulletproof Vest)\n총알, 파편, 칼날 등 외부의 물리적 충격이나 공격으로부터 사용자를 보호하는 특수한 방호복.\n방탄복은 여러 층의 소재로 구성되어 있으며, 그중 세라믹 소재는 충격을 분산시키고 관통을 방지하는 역할을 한다.\n\n총알 방어 과정 ① 속도 감소와 에너지 분산\n\n세라믹 판에 총알이 부딪히면, 세라믹의 단단한 표면이 총알의 속도를 급격히 감소시킨다.\n세라믹이 총알의 에너지를 분산시키면서 총알이 더 이상 원래의 속도로 침투할 수 없게 만든다.\n② 총알의 변형\n세라믹의 강한 표면에 의해 총알의 앞부분이 깨지거나 변형된다. 이로 인해 총알이 더 이상 집중된 힘을 발산하지 않게 되고, 그 힘이 방어판에 분산되어 몸에 미치는 충격을 줄인다.\n\n사용 이유 ① 경량화 효과\n\n세라믹은 금속보다 강도가 높고 상대적으로 가볍기 때문에, 방탄복을 덜 무겁고 더 효율적으로 만들 수 있다.\n이는 착용자의 이동성을 유지하면서도 효과적인 방어를 제공할 수 있게 한다.\n② 내열성과 내환경성\n고온과 환경 변화에 강한 특성을 가지고 있어 방탄복에 적합한 재료로 사용된다.\nNIJ Level IV (National Institute of Justice)\n미국의 법 집행 기관을 위한 방탄 성능 기준을 제공한다. 이 기준은 방탄 장비가 어느 정도의 위협으로부터 보호할 수 있는지를 나타낸다.\n이 레벨에서는 고속으로 날아오는 .30-06 탄환(풀 메탈 자켓)과 같은 고강도 금속 총알을 방어할 수 있는 능력을 요구한다.\n이러한 총알은 일반적인 방탄복으로는 막을 수 없다.\n세라믹 방탄복 Level III 또는 Level IV에 해당하며, 특히 Level IV는 방탄 성능에서 최상위 수준을 의미한다.\n이 방탄복은 약 3kg이며, 두께는 약 14mm이다.\n이 정도의 무게는 사람이 착용하기에 불편하지 않으며, 높은 보호 성능을 유지하면서도 실용적이다.\n이 방탄복은 주요 인사들의 양복 속에 숨겨서 입을 수 있도록 설계된다.\n03 세라믹 브레이크 디스크 (Ceramic Brake Disc)\n주철(Cast Iron) 브레이크 디스크에 비해 많은 장점을 제공하며, 특히 고성능 차량과 자동차 경주에서 중요한 역할을 한다.\n주철 브레이크 디스크의 한계 ① 주철은 불순물이 많이 포함된 소재로, 마찰에 의해 표면이 쉽게 손상된다.\n② 급브레이크를 사용하거나 오랜 시간 지속적으로 브레이크를 밟을 경우, 디스크 온도가 800 ~ 900 °C 까지 온도가 상승할 수 있다.\n이때 주철의 성질 변화로 디스크가 물러지거나 찌그러져 브레이크 성능이 저하될 수 있으며, 심각한 경우 브레이크가 작동하지 않을 수 있다.\n장점 ① 주철보다 마찰에 대한 내구성이 훨씬 뛰어나며, 표면에 흠집이 거의 생기지 않는다.\n② 세라믹은 고온에서도 강도가 그대로 유지된다. 특히 경주와 같은 고속, 고열 상황에서 매우 중요한 장점이다.\n③ 주철보다 가벼워 자동차 전체 중량을 줄이고 연비 효율성을 향상시킨다. 또한, 가속과 제동에 유리하여 특히 경주용 차량에서 큰 이점을 제공한다.\n단점 ① 고급 재료로 제조되며, 비용이 매우 높아 경주용 차량이나 고성능 스포츠카에 주로 사용된다. 대중적인 차량에서는 비용 효율성 면에서 채택되기 어려운 경우가 많다.\n② 차가운 상태에서 마찰력과 브레이크 성능이 일시적으로 떨어질 수 있어, 일정 시간이 지나야 최적의 성능을 낼 수 있다.\n③ 상대적으로 부서지기 쉬운 특성이 있어, 충격에 취약할 수 있다. 특히 운전 중 불규칙한 노면에서 충격을 받으면, 파손될 수 있는 위험이 존재한다.\n04 에어로젤 (Aerogel)\n1931년, 미국 캘리포니아 공과대학교의 사무엘 스티븐스 키슬러 박사가 발명했다.\n(California Institute of Technology, Caltech) (Samuel Stephens Kistler, 1900–1975)\n그는 젤에서 액체 성분을 제거하면서 고체 구조를 유지하는 방법을 연구했다.\n이 과정에서 초임계 건조(Supercritical Drying) 기법을 사용해 액체를 제거하고, 고체의 다공성 구조를 보존하는 기술을 개발하여 오늘날 에어로젤의 기초가 되는 기술을 완성했다.\n에어로젤은 100 나노미터(100 nm) 굵기의 실리카(Silica)로 구성되어 있으며, 이는 머리카락 굵기의 1000 분의 1 크기이다.\n100 나노미터: 1 천만분의 1 미터 (10⁻⁷ m) 이 크기는 코로나 바이러스(약 120 nm)보다 작은 크기이다.\n이 물질은 3차원 구조로 흩뿌려지며, 98%의 공기로 이루어져 있다. 이로 인해 에어로젤은 우수한 단열 효과와 방음 성능을 제공하며, 공기 밀도의 약 3 ~ 15배 정도에 해당되는 가벼운 소재이다.\n(실제로 플라스틱 통 안에 든 에어로젤을 들었을 때, 내용물이 전혀 없는 빈 통을 드는 것과 같은 느낌이었다.)\n어떤 용도로 사용할 것인가?\n에어로젤의 제조 공정은 매우 복잡하고 기술적으로 어려우며, 개발에 필요한 동기도 부족했던 탓에 상용화에도 많은 어려움이 따랐다.\n그럼에도 불구하고, 우주 산업에서는 에어로젤의 특성이 중요한 역할을 할 수 있다는 가능성에 주목했다.\n일반적인 인공위성이나 로켓과 달리 우주 왕복선은 지구와 우주를 왕복해야 하므로, 지구 대기권 재진입 시 강한 마찰열을 견뎌야 했다.\n이때, 에어로젤의 가벼운 무게와 뛰어난 단열 성질이 중요한 요소로 작용했다. 에어로젤은 고온의 마찰열을 효과적으로 차단하면서도 무게가 가벼워 우주 왕복선의 효율을 높이는 데 기여할 수 있었다.\nNASA는 에어로젤을 활용하기 위해 개발 및 생산하는 회사와 계약을 체결하여, 우주 왕복선에서 사용될 수 있도록 했다.\n에어로젤의 상용화는 제조 공정의 어려움과 비용 문제로 제한적이었지만, 우주 산업의 발전과 우주 여행의 상용화가 이루어지면, 그 사용이 더욱 증가할 가능성이 크다.\n예를 들어, 우주 산업, 항공 분야에서의 단열 재료나 고온 환경에서의 보호 재료로서의 역할이 더욱 중요해질 것이다.\n05 압전 세라믹스 (Piezoelectric Ceramics)\n외부에서 가해지는 기계적 압력이나 변형에 의해 전하를 생성하거나, 반대로 전기 신호를 가했을 때 변형을 일으키는 성질을 가진 세라믹 소재.\n압전 효과 (Piezoelectric Effect) 압전 세라믹스를 포함한 특정 물질들이 기계적인 압력을 받았을 때 전기를 생성하는 현상이다.\n외부 압력이나 기계적 힘이 가해지면 물질 내부에서 전하가 분리되며, 그로 인해 전압이 발생한다.\n이 원리를 활용하여 다양한 장치에서 전기를 생성하거나, 반대로 전기를 흐르게 하면 형태가 변하는 특성을 보인다.\n\n가스 점화기의 원리\n\n① 압축 및 기계적 힘 작용\n스프링이나 레버를 돌리거나 눌러서 압전 세라믹스에 기계적인 힘을 가한다. 외부 압력을 받으면 결정 구조 내에서 양(+)과 음(-) 전하가 분리된다.\n이 과정에서 전기 에너지가 생성된다.\n② 전하 발생과 전압 생성\n압력을 받는 동안 내부 전기장 변화를 통해 높은 전압을 생성한다. 이 전압은 일반적으로 수백에서 수천 볼트(Volt)에 이르며, 짧은 순간 동안 매우 높은 전압을 출력할 수 있다.\n③ 스파크 생성\n생성된 높은 전압은 전극 간에 스파크(방전)를 발생시킨다. 이 스파크는 가스나 인화성 물질에 점화되어 불이 붙는 역할을 한다.\n\n카메라 초점 조정 기술\n\n① 모터 기반\n전통적인 카메라 렌즈는 모터를 사용해 렌즈 위치를 물리적으로 이동시켜 초점을 맞춘다.\n이 방식은 비교적 넓은 범위의 초점 조정이 가능하지만, 크기가 커지고 소음이나 에너지 소모가 있을 수 있다.\n② 압전 세라믹스 기반\n전기 신호로 렌즈 위치를 정밀하게 제어하고 진동을 통해 초점을 조정하는 기술은 고해상도 이미지 촬영에 유리하다.\n또한, 빠른 반응 속도와 저전력 동작으로 배터리 소모를 줄이며 소형 카메라 장치에 적합한 소형화가 가능하다.\n활용 ① 안경 세척기 초음파를 사용하여 안경에 묻은 이물질을 깨끗하게 세척하는 데 사용된다.\n② 초음파 검사 의료 분야에서 몸속을 검사하는 데 사용된다.\n③ 해저 깊이 측정 소나(Sonar) 장치에서 초음파를 사용하여 해저의 깊이를 측정한다.\n④ 어문 탐지기 초음파를 이용한 탐지기에서 물고기와 잠수함을 구분하는 데 초음파의 반사 속도와 회전 속도를 이용한다.\n빠른 회전은 물고기, 느린 회전은 잠수함을 나타낸다.\n06 네오디뮴 자석 (Neodymium Magnet, Nd₂Fe₁₄B)\n철(Fe), 붕소(B), 네오디뮴(Nd)으로 구성된 강력한 희토류 영구 자석. 공식적으로는 NdFeB 자석이라고도 불린다.\n초강력 자석으로 알려진 네오디뮴 자석은 NdFeB 합금으로 제조되며, 매우 높은 자기력을 제공한다.\n철(Fe): 자석의 강도 네오디뮴 자석의 강도는 철의 원자들이 얼마나 잘 정렬되었는지에 달려 있다. 즉, 자석의 세기는 원자의 정렬 상태에 따라 달라진다.\n네오디뮴(Nd): 영구자화 (Permanent Magnetization)\n네오디뮴 원자를 첨가하면 자석은 영구자석이 된다. 이는 자석의 자기력이 시간이 지나도 변하지 않는 특성을 가진다.\n문제점 온도가 올라가면 자석의 성질이 변화한다.\n높은 온도에서는 자석의 정렬이 흐트러지고, 그로 인해 자석의 자기력이 사라질 수 있다.\n디스프로슘(Dy): 자력 유지 성능 향상 디스프로슘을 첨가하면 자석의 자력 유지 성능이 향상되며, 고온 환경에서도 강한 자기력을 유지할 수 있다.\n다만, 디스프로슘을 사용하면 자석의 자기 강도나 에너지 밀도가 다소 감소할 수 있다.\n07 초전도체 (超傳導體, Superconductor)\n전기 저항이 0에 가까운 상태로 떨어지는 물질.\n초전도 현상 (超傳導現象, Superconductivity)\n1911년, 네덜란드의 물리학자 헨드릭 카메를링 오네스가 발견했다.\n(Hendrik Casimir Onnes, 1853–1926)\n그는 수은을 영하 269도 냉각하면서 전기 저항이 갑자기 0으로 떨어지는 현상을 관찰했다.\n이로써 그는 전기가 흐를 때 에너지 손실이 발생하지 않는 초전도 현상을 최초로 관찰하였다.\n전기 저항 (Electrical Resistance)\n전기 회로에서 전류가 흐를 때 발생하는 에너지 손실을 의미한다.\n이 손실은 전류가 흐를 때 전자가 물질 내에서 다른 입자와 충돌하여 발생하는데, 충돌 과정에서 에너지가 열로 변환되어 발생하는 것이다.\n따라서 전선의 저항은 전기 에너지가 열로 변환되는 정도를 나타낸다.\n저항과 전선의 관계 저항은 길이에 비례하고, 단면적에 반비례한다. 즉, 전선이 길어지면 저항이 커지고, 두께가 두꺼워지면 저항이 감소한다.\n이 원리는 옴의 법칙(Ohm’s Law)에 의해 설명된다.\n전기 에너지의 송전에서 에너지 손실은 주로 열로 발생하는데, 전선의 길이가 길면 길어질수록 손실도 커진다.\n이런 이유로, 전력 송전의 효율을 높이기 위해선 송전 거리를 짧게 해야 한다.\n화력과 수력 발전소에 경우, 대체로 바닷가나 내륙의 전력 소비지와 가까운 곳에 위치하며, 이는 전선이 길어지면 손실이 커지기 때문이다.\n원자력 발전소는 생산 단가가 낮고, 멀리까지 전력을 송전할 수 있기 때문에 상대적으로 더 먼 거리에 위치할 수 있다.\n초전도체의 이점 기존의 전선은 저항에 의해 에너지 손실이 발생한다.\n① 반면, 초전도체는 이러한 손실이 없으므로 송전 효율이 매우 높고, 송전 거리의 제한이 사라진다.\n② 고온에서도 자석의 자기 성능을 유지할 수 있으며,\n③ 합선이나 과열로 인한 문제도 발생하지 않기 때문에, 송전과 관련된 성능의 제한을 해소할 수 있다.\n이는 전력 산업에서 매우 중요한 혁신적인 변화이다.\n마이스너 효과 (Meissner Effect)\n초전도체가 특정 온도 이하로 냉각될 때 외부 자기장을 내부에서 완전히 배제하는 현상.\n이로 인해 초전도체 내부에는 자기장이 존재하지 않으며, 초전도체 표면에서는 자기장이 급격히 감소한다.\n이 현상 때문에 초전도체 위에 자석을 띄우는 자기 부상(Magnetic Levitation)이 가능해진다.\n자기부상열차의 한계\n① 구조적 한계\n궤도와 차량의 간격이 매우 얇아 정밀한 제어가 필요하다. 차량이 지나치게 무거우면 부상 유지가 어려워 많은 승객을 수용하기 힘들다.\n② 전력 공급 문제\n자기 부상 유지와 추진을 위해 막대한 전력이 필요하다. 초전도체 기술을 활용하더라도 전력을 공급할 수 있는 한계가 존재한다.\n③ 자석의 한계\n자석의 성능에도 물리적 한계가 있어 더 강력한 부상 시스템 구현이 어렵다. 자석의 크기와 무게가 증가하면 시스템 효율이 감소한다.\n④ 저항 문제\n저온을 유지하는 냉각 시스템이 복잡하고 비용이 높다. 고온 초전도체 기술은 개발 중이지만 상용화에는 어려움이 있다.\n⑤ 공기 저항 문제\n열차 속도가 증가할수록 공기 저항이 크게 작용한다.\n이를 극복하기 위해 진공 상태의 튜브 시스템 즉, 하이퍼루프 개념이 제안되고 있지만 구현이 어렵다."
  },
  {
    "objectID": "misc/mse_04_4.html",
    "href": "misc/mse_04_4.html",
    "title": "보석들",
    "section": "",
    "text": "다이아몬드 이외의 다른 4종의 보석에 대해 다루고자 한다.\n\n01 에메랄드\nEmerald, Be₃Al₂Si₆O₁₈ 베릴알루미늄실리케이트 5월의 탄생석\n고대부터 귀한 보석으로 여겨져 왔으며, 그 이름은 그리스어 ’스멜라두스(σμάραγδος)’에서 유래한다. 이는 ’초록색’을 의미한다.\n에메랄드는 주로 베릴(Be0) 계열의 광물로, 베릴륨(Be), 알루미늄(Al), 실리콘(Si), 산소(O)가 결합된 구조를 가지고 있다.\n이 보석의 색은 주로 크로뮴(Cr)과 철(Fe)의 함량에 의해 결정된다. 또한, 기둥 모양의 형태를 가지므로, “녹주석”이라고 불리기도 한다.\n\n상징성 고대 이집트인들은 초록색을 “생명의 색”으로 여겼다. 이는 농작물의 풍요로운 색과 닮아 있어 생명과 재생을 상징했기 때문이다.\n\n당시 고대 이집트에서 귀중한 보석으로 여겨졌으며, 클레오파트라는 에메랄드를 특히 사랑했다고 전해진다.\n클레오파트라 7세 필로파토르 (Cleopatra VII Philopator, 69BCE–30BCE)\n\n에메랄드의 사용 왕족과 귀족들은 에메랄드를 주로 장식품이나 부적에 사용했다.\n\n에메랄드는 당시 죽음 이후의 세계에서 보호를 제공한다고 믿어져 무덤에 함께 매장되기도 했다.\n\n역사적 가치 클레오파트라 시대에는 에메랄드 광산이 존재했다.\n\n이집트의 제벨 자바라(Jebel Zabara)와 제벨 세케이트(Jebel Sikait)는 클레오파트라 시기에 운영되던 중요한 에메랄드 광산들로 알려져 있다.\n이러한 이유로 에메랄드는 당시 문화와 신앙에서 큰 상징적 의미를 지녔다.\n\n스페인 정복자들의 침략 16세기 초, 스페인과 포르투갈이 아메리카 대륙과 남미를 탐사하면서 자원을 약탈하고 정복을 진행했다.\n\n그들은 아즈텍, 잉카와 같은 고대 문명을 정복하고, 금, 은, 에메랄드 등 다양한 자원을 유럽으로 가져왔다.\n\n뮤이즈 (Muzo) 콜롬비아의 보야카(Boyacá) 주에 위치한다.\n\n안데스 산맥의 한 지역으로, 세계에서 가장 오래된 에메랄드 광산 중 하나로 유명하다.\n이 지역은 에메랄드의 최고 품질을 자랑하며, 오늘날에도 여전히 중요한 에메랄드 생산지로 알려져 있다.\n\n유럽에서의 가치 에메랄드는 유럽에서 귀한 보석으로 자리잡게 되었다.\n\n그 당시 상류층과 왕족에게 큰 선물이었으며, 그 가치는 금, 은, 다이아몬드와 같은 다른 귀금속과 비견될 만큼 높았다.\n\n영화 ‘클레오파트라’ (Cleopatra, 1962)\n\n당시 엘리자베스 로즈먼 테일러는 클레오파트라 역을 맡았고, 리차드 서튼 버튼은 마르쿠스 안토니우스 역을 맡았다.\n(Elizabeth Rosemond Taylor, 1932–2011) (Richard Sutton Burton, 1925–1984)\n에메랄드는 영화 촬영 중에 발생한 둘의 로맨스와 관련이 있으며, 버튼은 테일러에게 콜롬비아산 에메랄드로 만든 화려한 보석을 선물했다.\n이 보석은 매우 크고 값비싼 에메랄드로, 에메랄드가 고대 이집트와 연관된 보석이기 때문에 클레오파트라와의 역사적 연관성을 더욱 강조한 선물이었다.\n이 사건은 헐리우드 역사에서 가장 유명한 로맨틱한 선물 중 하나로 기억되며, 엘리자베스 테일러의 아름다움과 클레오파트라 역할과 맞물려 더욱 큰 의미를 가진다.\n02 아쿠아마린 (Aquamarine, Be₃Al₂Si₆O₁₈ 베릴알루미늄실리케이트) 3월의 탄생석\n베릴광(beryl)의 일종으로, 이 광물은 기본적으로 알루미늄과 베릴륨이 포함된 규산염 미네랄이다.\n화학적으로는 에메랄드와 동일한 구조를 가졌지만, 이 베릴광에 소량의 철이 섞여 있어 푸른색 또는 청록색을 띈다.\n\n어원 라틴어로, “aqua”(물)와 “mare”(바다)의 결합에서 유래했다.\n\n이 이름은 아쿠아마린이 바다의 색을 연상시키는 맑고 푸른 색을 띤다는 특성에서 비롯되었다.\n\n그리스 신화의 포세이돈 Poseidon / Ποσειδῶν(Poseidôn)\n\n그리스 신화에서 포세이돈은 바다의 신으로, “바다의 지배자” 또는 “바다와 지진의 신”으로 알려져 있다.\n고대 그리스에서는 아쿠아마린이 포세이돈의 선물로 여겨졌다. 이는 아쿠아마린이 바다와 관련된 신화적 의미를 지닌 보석이라는 전통을 반영한 것이다.\n또한, 고대 항해자들이 아쿠아마린을 호신부로 사용하여 바다에서의 안전을 기원하였다.\n03 진주 (Pearl, CaCO₃ 탄산칼슘) 6월의 탄생석\n조개나 연체동물의 몸속에서 만들어진다.\n이물질(모래알 등)이 조개 안으로 들어오면 연체동물은 이를 감싸기 위해 보호 물질인 진주질(Nacre)을 분비한다.\n시간이 지나면서 탄산칼슘(CaCO₃)과 콘키올린(Conchiolin)이라는 단백질이 여러 겹 쌓여 진주가 생성된다.\n자연산 진주는 오랜 시간에 걸쳐 형성되며, 다결정질 구조를 가지기 때문에 빛을 산란시켜 은은한 광택을 낸다.\n특징 주로 불투명하거나 반투명한 구조를 가지며, 광택이 독특하다. 색상은 흰색, 크림색, 분홍색, 회색, 검은색까지 다양하다.\n\n엘리자베스 1세 엘리자베스 튜더 (Elizabeth Tudor, 1533–1603)\n\n잉글랜드와 아일랜드의 여왕으로 1558년부터 1603년까지 통치했다.\n그녀는 “버진 퀸(The Virgin Queen)”이라는 별명을 가졌다. 이는 그녀가 결혼하지 않고 평생 독신으로 지냈기 때문이다.\n그녀는 스스로 “나는 국가와 결혼했다”고 선언하며, 다른 나라와의 정략결혼을 통한 정치적 동맹을 거부했다.\n\n초상화 속 진주 엘리자베스 1세의 초상화에는 종종 진주 목걸이나 귀걸이가 등장한다. 진주는 그녀의 순결, 권위, 그리고 신성함을 상징했다.\n버지니아 명칭의 유래 (Commonwealth of Virginia)\n\n미국 동부에 위치한 주로, 북쪽으로 메릴랜드와 워싱턴 D.C., 동쪽으로 대서양, 서쪽으로 웨스트버지니아와 접하고 있다.\n미국 초기 13개 식민지 중 하나로, 미국 독립과 역사에서 중요한 역할을 했다.\n엘리자베스 1세의 별명인 “The Virgin Queen”에서 버지니아(Virginia)라는 이름이 유래했다.\n1584년, 영국의 탐험가 월터 롤리 경.\n(Sir Walter Raleigh, 1552–1618)\n엘리자베스 여왕의 승인을 받아 오늘날 미국 동부 해안에 버지니아 식민지를 설립했다.\n이는 엘리자베스 여왕에 대한 헌정의 의미로 식민지 이름이 정해졌다.\n\n납 중독 엘리자베스 1세는 당시 사용되던 화장품에 포함된 납 성분으로 인해 건강이 악화된 것으로 알려져 있다.\n\n그녀는 흰 피부를 유지하기 위해 납과 식초 혼합물을 사용했다.\n\n영국의 과학 기술 발전\n\n① 100년 전쟁 (The Hundred Years’ War, 1337–1453)\n영국과 프랑스 사이에 벌어진 일련의 군사적 충돌로, 주로 왕위 계승 문제와 영토 분쟁을 둘러싼 갈등이 원인이었다.\n이 전쟁은 여러 차례의 전투와 평화 협정이 혼합된 복잡한 형태였으며, 두 나라 간의 지속적인 전투로 이어졌다.\n결과적으로, 프랑스가 승리하게 되었다.\n② 장미 전쟁 (War of the Roses, 1455–1487)\n100년 전쟁이 끝난 후, 영국 내에서 발생한 내전.\n영국 왕위의 계승권을 두고 두 주요 가문인 랭커스터 가문(빨간 장미)과 요커 가문(흰 장미) 사이에 벌어졌다.\n이 전쟁은 영국의 정치적 혼란을 가중시키며, 결국 튜더 왕조가 등장하는 계기가 되었다.\n③ 절대왕정의 등장 영국은 주요 전쟁에서 패배하고 정치적 혼란을 겪으면서 절대왕정으로의 전환이 본격적으로 이루어졌다.\n이는 왕이 국가의 모든 권력을 중앙집권적으로 지배하는 정치 체제로, 법, 군대, 경제, 종교 등 국가의 모든 부분에 대해 전면적인 통제력을 행사하는 것이 특징이다.\n튜더 왕조의 창시자인 헨리 7세\n헨리 튜더 (Henry Tudor, 1457–1509)\n그는 장미 전쟁을 끝내고 안정된 정치적 환경을 만들었으며, 중앙 정부의 권력을 강화하고 왕권을 확립했다.\n④ 상인 우대 정책 엘리자베스 1세의 통치 초기인 16세기 중반부터 시작되었다. 이 정책은 상업과 무역의 발전을 도모하고, 영국의 경제를 활성화하려는 목적이었다.\n특히, 상인들에게 세금 면제나 혜택을 제공하며 영국 상업을 발전시키는 데 큰 역할을 했다.\n⑤ 스페인의 무적함대 격파 무적함대 (Armada Invencible, 1588)\n스페인의 왕 필립 2세.\n필리페 2세 데 아우스트리아 (Philippe II de Austria, 1527–1598)\n그는 영국과의 종교적 갈등과 정치적 대립을 해결하기 위해, 영국을 침공하기 위해 대규모 해군을 구성했다.\n당시 영국은 가톨릭과 프로테스탄트 간의 갈등이 심했고, 엘리자베스 1세는 프로테스탄트를 지지하는 입장에서 스페인과 적대적 관계를 유지하고 있었다.\n스페인 왕국은 해양 패권을 장악하려는 목표로 무적함대를 조직하여 1588년에 영국을 공격하기 위한 대규모 해군을 보냈다.\n그러나 영국 해군은 기동성과 전술적 우위를 통해 스페인 함대를 격파하였다.\n이 전투를 통해 엘리자베스 1세는 영국의 해상권을 강화하고, 스페인 제국의 해상 지배력을 극복할 수 있었다.\n이 승리는 엘리자베스 1세 시대의 중요한 성과 중 하나로 기록된다.\n엘리자베스 1세 시대 이후, 영국은 정치적 안정과 해상권 확립을 기반으로 상업과 과학 기술이 발전하였다.\n이는 뉴턴이 활동할 수 있는 기반을 마련한 측면이 있다.\n영국의 과학자 아이작 뉴턴 경.\n(Sir Isaac Newton 1642 – 1727)\n엘리자베스 1세의 시대 이후로 과학 혁명 시대의 중심 인물로 떠오른다.\n그는 만유인력의 법칙과 미적분학의 기초를 세운 인물로, 엘리자베스 1세가 다스리던 시기의 정치적 안정과 상업의 발전이 과학 기술 발전에 중대한 기여를 했다.\n과학기술 발전을 위한 지원과 분위기가 이 시기에 형성되었으며, 이는 뉴턴과 같은 위대한 과학자의 업적을 가능하게 했다.\n04 호박 (Amber, C₁₀H₁₆O, 디터펜 알콜) 11월의 탄생석\n수백만 년 전에 나무에서 분비된 수지가 굳어져 형성된 고대의 유기 화합물이다.\n황금색에서 갈색, 노란색, 녹색, 붉은색까지 다양한 색을 띠며, 때로는 내부에 고대의 곤충이나 식물 잔해가 갇혀 있는 경우도 있다.\n\n역사적 가치 오래된 나무 수액이 시간이 지나면서 탄화되어 매우 단단해지고, 자연광에서 투명한 색을 띠기 때문에 보석으로 가치가 높다. 특히 고대에는 향수를 만드는 재료로도 사용되었다.\n파이톤 (Phaethon)\n\n태양신 헬리오스(Helios)와 클리메네라(Clymene)는 여신 사이에서 태어난 아들.\n헬리오스는 하늘을 가로지르는 불마차를 몰고 태양을 이동시키는 신이다.\n어느 날, 파이톤은 먼 곳에 있는 아버지의 신전을 찾아왔고, 이에 아버지는 기뻐하였다. 그는 스튁스 강에 맹세를 걸고 소원을 들어주겠다고 약속한다.\n파이톤은 불마차를 운전하겠다고 소원을 빌었고, 이에 헬리오스는 다른 소원을 빌 수 없냐며 설득했지만, 이미 스튁스에 걸었던 맹세 때문에 이 소원을 들어줄 수밖에 없었다.\n파이톤은 불마차를 운전할 실력이 부족했다.\n이후 불마차는 불안정하게 움직였고, 파이톤은 점점 상황을 통제할 수 없게 되었다.\n그로 인해 불이 대지에 떨어져, 여러 가지 재난을 일으켰다.\n결국, 대지의 여신 데메테르(Demeter)는 이 문제를 해결하기 위해 제우스(Zeus)에게 도움을 청했다.\n제우스는 이 문제를 해결하기 위해 불마차를 향해 번개를 쏘았고, 결국 파이톤은 마차에서 떨어져 죽게 된다.\n그의 죽음 이후, 그의 누이들은 깊은 슬픔에 빠져 나무로 변했고, 그들의 눈물은 호박으로 변했다고 하는 전설이 있다.\n\n상징 이 전설은 호박이 “눈물”로 변하는 이야기와 결합되어, 호박이 상징적으로 슬픔, 애도, 그리고 죽음을 나타내기도 한다.\n\n또한, 고대 그리스 신화에서는 호박을 신성한 과일로 여겼다.\n\n한국에서의 호박 한국에서는 호박이 전통적으로 노리개와 같은 장식용 소품으로 사용되었다.\n\n특히, 귀한 여성들이 노리개로 호박을 사용하며 아름다움을 강조하고, 호박이 상징적으로 행운, 부유함, 장수와 연결되기도 했다."
  },
  {
    "objectID": "misc/mse_05_1.html",
    "href": "misc/mse_05_1.html",
    "title": "나노 소재",
    "section": "",
    "text": "나노기술 및 자연에서 영감을 받은 구조적 특성에 대해 다루고자 한다.\n01 풀러렌 (Fullerene)\n1985년, 발견된 탄소 동소체의 한 종류로, 주로 60개의 탄소 원자가 결합하여 구형 모양을 형성하는 C₆₀ 분자로 알려져 있다.\n풀러렌이라는 이름은 미국의 건축가 버크민스터 풀러에서 유래했다.\n(Buckminster Fuller, 1895–1983)\n그가 개발한 지오데식 돔 구조와 유사한 형태를 가졌기 때문이다.\n(Geodesic Dome, 1946)\n플러렌 분자의 구조가 구형이고 지오데식 돔처럼 다수의 삼각형 면이 결합된 형태를 띄고 있어 이를 반영한 이름이다.\n플러렌의 발견자는 다음과 같다:\n하라르드 크라우스(Harold W. Kroto, 1939–2016) 리처드 스몰리 (Richard E. Smalley, 1943–2005) 제이 로버트 커천 (J. Robert Curl, 1933–2022) 이들은 플러렌을 발견한 공로로 1996년 노벨 화학상을 공동 수상했다.\n\n초기 풀러렌 연구 풀러렌은 구형 구조의 높은 안정성으로 주목받았다.\n\n(Buckminsterfullerene, C₆₀)\n강한 공유 결합 구조 덕분에 화학적으로 매우 안정하며, 고온과 고압에서도 뛰어난 내성을 보였다.\n그러나 이러한 안정성은 다른 물질과의 반응이나 새로운 화합물 생성에 어려움을 초래했다.\n\n초기의 인기 부족 발견 초기에는 안정성 등 여러 물리적 특성이 주목받았지만, 구체적인 상업적 활용 사례가 부족했다.\n\n대량 생산 기술이 부족하고, 가격이 너무 비싸서 실용적인 응용이 어려웠다.\n주로 학술적 관심에서만 다뤄졌으며, 실생활에서 널리 사용되지 못했기에 발견 초기에는 “인기 없는 물질”로 여겨졌다.\n\n상업적 활용 한계 풀러렌이 상업적으로 널리 활용되지 못한 이유는 비용 효율성 문제 때문이다.\n\n풀러렌의 합성 과정은 매우 복잡했고, 고순도의 물질을 대량으로 생산하려면 높은 비용이 들었다.\n초기에 풀러렌은 윤활유로 사용될 가능성에 대한 논의도 있었다.\n풀러렌은 독특한 구조와 물리적 특성 덕분에 윤활 특성이 있을 것으로 예상되었으며, 특히 나노스케일에서의 고체 윤활성에 대한 연구가 활발히 이루어졌다.\n풀러렌 분자는 매우 매끄러운 표면을 가지고 있어 마찰을 줄이고, 미세한 마모를 감소시킬 수 있다는 가능성이 제기되었기 때문이다.\n그러나 플러렌은 일반적인 윤활유에 비해 생산 단가가 훨씬 높았다.\n고체 윤활유로 사용할 수 있는 흑연이나 몰리브데넘 디설파이드(MoS₂) 등의 대체재는 이미 저렴하고 효과적이었다. 기존 대체재가 충분히 성능을 충족하므로, 고가의 풀러렌을 선택할 이유가 적었다.\n풀러렌을 윤활제로 상업화하기 위해서는 가격 대비 성능에서 현저한 차별화가 필요했기 때문이다.\n그 결과, 경제성 부족으로 인해 기업들은 대규모 투자를 주저하게 되었다.\n\n역사적 의의 그럼에도 불구하고, 풀러렌은 상업적 성공 여부를 넘어 탄소 소재 연구의 혁명적인 전환점을 마련한 중요한 발견이었다.\n\n이전까지 탄소는 다이아몬드와 흑연 같은 단순한 형태로만 알려져 있었다.\n풀러렌은 3차원 구형 구조로, 탄소가 새로운 방식으로 배열될 수 있음을 보여준 최초의 사례였다.\n풀러렌의 발견은 이후 탄소 나노튜브와 그래핀 연구로 이어져, 나노소재 과학의 기초를 다지는 계기가 되었다.\n\n기술적 가능성 시간이 지나면서 연구자들은 풀러렌을 화학적으로 변형하거나 다른 물질과 복합체를 형성하는 기술을 개발했다.\n\n예를 들어, 다양한 작용기를 결합하는 화학적 반응 (예: 친전자성 첨가 반응)이나 나노소재 복합체 제작 기술이 발전했다.\n풀러렌의 전도성은 유기 태양전지나 전자 디바이스에 활용되고 있으며, 구형의 구조는 약물을 포획하고 전달하는 것에 유용했다.\n또한, 뛰어난 안정성과 낮은 마찰 계수 덕분에 고체 윤활제로도 사용되었다. 비록, 풀러렌이 상용 윤활유로 널리 사용되지는 않았지만, 고급 산업 및 연구 분야에서 실험적으로 사용되었다.\n기존 윤활유가 극한 조건에서 분해되거나 마찰 성능이 떨어지는 문제를 풀러렌이 보완할 수 있다.\n또한, 높은 성능을 요구하는 특정 기계나 장치에 적합하며, 고효율, 저마찰 특성이 필요한 실험 환경에서 사용되었다.\n02 흑연 (Graphite)\n지구상에서 가장 흔하게 발견되는 탄소 동소체로, 탄소 원자들이 2차원 평면 구조를 이루며 결합되어 있다.\n이 구조는 층상 구조로, 각 층은 서로 약한 반데르발스 힘에 의해 결합되어 있다.\n(Van der Waals forces)\n이 힘은 결합력이 비교적 약하기 때문에, 각 층은 쉽게 분리되거나 미끄러질 수 있다.\n연필 사용의 원리 연필로 글을 쓸 때 흑연의 층들이 종이에 부드럽게 떨어져 나가는 원리도 바로 이 약한 층간 결합 덕분이다.\n글을 쓸 때 연필의 압력으로 흑연 층들이 종이 표면에 남게 된다. 이 때문에 연필로 쓴 자국은 종이에 흔적을 남기게 되는 것이다.\n03 탄소나노튜브 (Carbon Nanotube, CNT)\n구조적으로 나노미터 크기의 직경과 높은 종횡비(길이 대비 직경 비율)를 가지고 있다.\n\n전계 강화 효과 (Field Enhancement)\n\nCNT의 끝부분에서는 전계 강화 효과가 나타난다.\n이는 전기장이 좁은 영역에서 극도로 강해지는 현상으로, 나노미터 크기의 좁은 끝에서 특히 두드러진다.\n이 효과 덕분에 CNT는 기존의 평평한 금속 음극보다 전자를 방출하기 위해 필요한 전압이 훨씬 낮다.\n\n전계 방출 (Electron Field Emission)\n\nCNT는 낮은 전압으로도 전자를 효율적으로 방출할 수 있다.\n전계 강화 효과로 인해 전자가 강하게 방출되며, 방출된 전자는 고속으로 이동한다. 이러한 특성은 CNT가 기존 금속 음극보다 더 낮은 에너지 소모를 가능하게 한다.\n또한, 전자 방출이 즉각적으로 이루어지기 때문에 빠른 스위칭이 필요한 응용에서도 매우 효과적으로 활용될 수 있다.\n\n환경적 이점 이러한 CNT 음극의 특성은 애너지 소모를 줄이고 전력 효율성을 높인다. 이는 에너지 절약과 이산화탄소 배출 감소에 기여할 수 있다.\n\nCNT는 높은 열 안정성과 내구성을 갖추고 있어, 기존 금속 음극보다 교체 주기가 길어질 수 있다.\n이로 인해 자원의 사용량이 줄어들고, 폐기물 발생도 감소시킬 수 있다.\n또한, 나노 크기의 특성을 활용해 소형화된 전자 기기를 개발할 수 있으며, 생산 과정에서 사용되는 물질의 양을 줄일 가능성이 있다.\n\n전자총 (Electron Gun)\n\n전자를 방출하고 가속하여 특정 방향으로 발사하는 장치. CNT 음극을 사용하면 방출 전류가 안정적이고 전력 소비를 줄일 수 있다.\n이는 전자 현미경, 전자빔 리소그래피 등에 사용된다.\nElectron Microscope Electron Beam Lithography\n\nX-ray 튜브 음극 CNT는 기존 금속 음극보다 작은 크기와 높은 전류 밀도를 제공한다.\n\n치과에서 사용하는 소형 X-ray 장비에 적합하며, 이는 낮은 전압으로도 강력한 X-ray를 생성할 수 있기 때문이다. 또한, 방출되는 전자의 제어가 용이해 고해상도 이미징이 가능하다.\n\n치과 X-ray 장비 환자의 치아 및 뼈를 고해상도로 촬영하는 데 사용된다.\n\nCNT 음극은 기존 장비 대비 더 작고 가벼운 X-ray 장비를 제작할 수 있어 치과 진료의 효율성을 높인다.\n낮은 전력으로도 충분한 X-ray 강도를 제공해 환자와 의료진의 방사선 노출을 줄이는 장점이 있다.\n\n전망 CNT는 소형 의료 장비뿐 아니라, 대형 산업용 장비, 디스플레이 기술(FED, Field Emission Display), 우주 장비 등 다양한 분야에서도 적용 가능성이 연구되고 있다.\n\n특히, CNT의 고효율 전자 방출 특성은 미래의 에너지 절약형 장비와 고성능 영상 기술의 핵심이 될 것으로 보인다.\n\n우주 항공 분야 CNT는 매우 강하고 가벼운 특성 덕분에 우주 항공 분야에서도 활발하게 연구되고 있다.\n\n우주 정거장 재료나 케이블 등 구조적 요소로 적용할 수 있으며, 우주 엘리베이터와 같은 미래 기술에도 활용할 가능성이 논의되고 있다. 극한의 강도를 요구하는 분야에서 그 요구를 충족시킬 수 있는 잠재력을 지니고 있다.\n04 그래핀 (Graphene)\n2004년, 영국 맨체스터 대학의 안드레 가임과 콘스탄틴 노보셀로프 교수 연구팀이 그래핀을 발견하였다.\n(Andre Geim, 1958–현재) (Konstantin Novoselov, 1974–현재)\n그 공로로 2010년 노벨물리학상을 수상하였다.\n연구팀은 흑연에서 테이프를 이용해 단일 원자층을 분리하는 방식으로 그래핀을 최초로 얻었다.\n\n단층 구조 그래핀은 흑연에서 한 층을 떼어낸 형태로, 탄소 원자들이 벌집 모양의 평면 배열을 이루고 있다.\n\n이 단일 층은 2차원적인 구조를 형성하며, 그래핀을 구성하는 기본적인 구조 단위이다.\n\n결합 특성 그래핀의 각 탄소 원자는 sp² 혼성화(이중 결합)를 통해 세 개의 탄소와 강한 공유 결합을 형성한다.\n\n이러한 결합은 평면을 형성하며, 그래핀의 강도와 안정성을 제공한다.\n나머지 한 개의 전자는 π 전자로, 이 전자는 각 탄소 원자 간에 겹쳐져 자유롭게 움직인다.\n이 전자의 이동은 그래핀의 전기적 특성을 결정짓는 중요한 요소이며, 전도성을 높이는 역할을 한다.\n3.주요 특성\n① 전기전도성 전자 이동성(Electron Mobility)은 매우 뛰어나며, 구리의 약 100배에 달한다.\n이는 그래핀이 무질서도가 거의 없는 2차원 구조로, 전자가 충돌 없이 빠르게 이동할 수 있기 때문이다.\n② 기계적 강도 강철보다 약 200배 강한 강도를 가지면서도, 매우 얇고 유연하다. 이러한 특성 덕분에 유연한 전자기기(Flexible Electronics) 개발에 적합하다.\n③ 열전도성 그래핀은 우수한 열전도체로, 열전도율이 다이아몬드보다 더 높다. 이는 전자 장치의 열 관리에 활용될 가능성을 열어준다.\n④ 투명성 그래핀은 97.7%의 가시광선을 투과할 정도로 투명하다. 이는 투명 전극이나 디스플레이 기술에 유용하다.\n05 도꼬마리 열매 (Cocklebur)\n열매에 작은 갈고리 모양의 구조가 있어 다른 물체나 동물의 털에 달라붙는 특징을 가지고 있다.\n1941년, 스위스의 엔지니어 게오르그 드 메스르탈.\n(George de Mestral, 1907–1990)\n이 열매를 현미경으로 관찰하고, 그 구조가 갈고리처럼 생긴 돌기들이 다른 표면에 부착되는 원리를 발견했다. 이러한 원리를 바탕으로, 옷에 붙었다 떼는 간단하고 강력한 결합 방법을 개발하게 된다.\n\n벨크로의 원리 (Velcro)\n\n두 개의 스트립(조각)을 서로 맞대었을 때,\n하나는 갈고리(hook) 형태의 작은 돌기가, 다른 하나는 루프(loop) 형태의 고리가 되어 서로 맞물려 결합하는 방식이다.\n갈고리는 고리 모양의 구조로 되어 있어 루프에 걸려 붙는다. 이렇게 갈고리와 루프가 결합되면 매우 강력한 고정력을 발휘하며, 당기면 쉽게 분리된다.\n\n친수성 (Water-Attracting)\n\n벨크로는 물과 결합하는 특성을 가지고 있기도 하며, 이 경우 친수성을 나타낼 수 있다.\n친수성이란 물과 쉽게 결합하고, 물에 잘 붙는 성질을 의미한다. 이는 벨크로의 루프 부분이 물에 의해 일부 보강되거나 부착력이 달라질 수 있다는 것을 의미한다.\n\n벨크로의 활용 옷, 신발, 가방 등 다양한 의류뿐만 아니라, 의료 장비나 인공 보조기기에도 벨크로가 활용되어 착용과 조정이 용이하다.\n\n또한, 우주복이나 우주선에서 물품을 고정하는 데 사용되기도 한다.\n\n특허 및 창업 게오르그 드 메스트랄은 벨크로 기술에 대한 특허를 획득한 후, 1952년 Velcro SA라는 회사를 설립한 뒤, 이를 상업화하며 큰 성공을 거두었다.\n\n벨크로는 이후 전 세계적으로 유명한 고정 장치로 자리 잡게 되었다.\n06 연잎 미세 돌기 (Lotus Leaf)\n연잎의 표면은 미세한 나노 크기의 돌기와 미세한 홈들로 덮여 있다.\n(Nano-Protrusions)\n이 나노 구조는 연잎이 물을 매우 잘 튕겨내고, 물방울이 표면에 머물지 않도록 한다.\n\n초소수성 (Superhydrophobicity)\n\n물방울이 표면에 닿았을 때 물방울이 구슬처럼 표면 위에서 굴러다니는 현상.\n연잎의 나노 구조는 표면에 물이 접촉할 때, 물방울이 표면과 거의 접촉하지 않고 공기층을 형성하게 만든다. 이로 인해 표면에 물이 고이지 않고, 물방울이 쉽게 구르며 떨어진다.\n연잎은 물에 대한 접촉각(Contact Angle)이 150도 이상으로, 물이 흘러내리기 쉽다.\n\n자기세정효과 (Self-Cleaning Effect)\n\n연잎의 초소수성 표면은 물방울이 표면 위를 구르면서 먼지나 오염물질을 쉽게 밀어낸다.\n이 현상은 자기세정효과라고 불리며, 오염물질이 연잎 표면에 붙지 않거나 쉽게 제거되도록 돕는다.\n물방울이 표면을 구를 때, 표면에 있는 작은 먼지나 오염물질은 물방울에 의해 밀려 나가거나 떨어져 나간다.\n결과적으로 연잎은 항상 깨끗하게 유지된다.\n\n생물학적 영감 및 응용 나노기술을 이용해 초소수성 표면을 만들거나, 자기세정 기능을 부여하는 다양한 산업적 응용이 가능하다.\n\n건물 외벽, 창문, 자동차, 의류 등에 활용되어 오염물질을 쉽게 제거할 수 있으며, 물에서 불순물을 제거하는 수처리 기술 개발에도 기여하고 있다. 또한, 방수 및 오염 방지 기능이 필요한 전자기기에 적용될 수 있다.\n07 규조토 (Diatoms)\n규조류의 껍질이 퇴적되어 형성된 물질로, 그 껍질은 실리카(SiO₂)로 되어 있다.\n매우 미세한 식물 플랑크톤으로, 크기는 대개 μm ~ mm 사이이며, 실리카 껍질의 표면에는 나노 크기의 기공(구멍)이 다수 존재한다.\n이 기공은 규조토의 전체 부피의 80 ~ 95%를 차지한다. 이 나노 크기의 기공은 규조토의 특수한 성질을 만들어낸다.\n\n규조토의 특징 고품위 규조토(&gt;90%)는 순수 실리카가 주 성분인 규조토로, 기공이 많은 구조를 가지고 있어, 다공성과 우수한 흡수성을 특징으로 한다.\n\n이러한 특성 덕분에 여과제로 사용된다.\n규조토는 매우 가볍고 미세한 입자로 이루어져 있어 물리적, 화학적으로 안정적이며, 고온에서도 잘 분해되지 않는다.\n\n주요 활용 규조토는 뛰어난 여과성을 가지고 있어 맥주, 음료, 식수 등의 여과에 사용된다. 특히, 고품질의 규조토는 음료수 여과 과정에서 중요한 역할을 한다.\n\n농업 분야에서는 비료와 토양 개량제로 사용되며, 전기 산업에서 전선의 절연재로도 이용된다.\n또한, 유분 흡수 능력이 우수하여, 오일 흡수제로도 활용된다.\n\n환경적 이점 규조토는 자연에서 얻을 수 있는 물질로, 화학적인 처리 없이도 환경에 미치는 영향이 적은 물질이다.\n\n또한 재활용이 가능하고, 지속 가능한 자원으로 여겨진다.\n08 다이너마이트 (Dynamite)\n1867년, 스웨덴의 화학자 알프레드 노벨이 발명하였다.\n(Alfred Nobel, 1833–1896)\n니트로글리세린(Nitroglycerin)을 주요 성분으로 사용하여 만든 폭발물이다.\n이 물질은 매우 강력한 폭발력을 지닌 액체 폭약으로, 불안정하고 취급하기 어려운 물질이었다.\n따라서 이를 안전하게 다룰 수 있는 방법이 필요했다.\n\n규조토의 활용 그는 규조토의 나노 크기 기공 구조를 활용하여 니트로글리세린을 흡수시키는 방법을 생각해냈다.\n\n규조토는 다공성이며 나노 크기의 기공을 다수 포함하여, 니트로글리세린을 효과적으로 흡수할 수 있다.\n규조토는 니트로글리세린의 3배에 달하는 무게를 흡수하여, 이를 안정화시키고 폭발의 위험을 줄일 수 있었다.\n\n니트로글리세린의 안정화 규조토는 니트로글리세린을 안전하게 흡수하여 폭약이 폭발하지 않도록 안정화시켰다.\n\n고체 상태로 만들어져 저장과 운반이 용이하며, 니트로글리세린의 강력한 폭발력을 유지하면서도 안전한 취급이 가능해졌다.\n\n특허 및 상업화 1867년, 알프레드 노벨은 다이너마이트 발명에 대해 특허를 획득했다. 이후 다이너마이트는 건설, 채굴, 군사 등 여러 분야에서 널리 사용되었다.\n\n다이너마이트의 발명은 노벨의 이름을 널리 알리게 되었으며, 노벨상 설립의 배경이 되기도 했다.\n09 제올라이트 (Zeolite) (Na₂, K₂, Ca₂​)Al₂Si₃​O₁₀ ​⋅ 4H₂​O\n알루미늄(Al), 실리콘(Si), 산소(O)로 이루어진 결정성 알루미노실리케이트이다.\n이 물질은 매우 규칙적인 나노 크기의 구멍(기공)을 가지고 있다.\n이 기공의 크기는 대체로 0.3 ~ 10 nm 범위로, 매우 작은 크기의 분자들도 선택적으로 통과할 수 있다.\n이 규칙적인 기공 구조 덕분에 제올라이트는 흡착, 이온 교환, 분자 크기 분리 등에 유용하게 사용된다.\n\n개발과 산업적 활용 1990년대 초, 미국의 Mobil에서 개발되어 양산이 시작되었다.\n\n원유 정제 과정에서 매년 70억 배럴 이상의 원유가 제올라이트를 활용해 정제된다.\n제올라이트는 분해, 구성 변화 및 성분 분리 등 다양한 과정에서 촉매 역할을 하여 원유의 품질을 개선하고, 정제 과정을 보다 효율적으로 만들었다.\n또한, 제올라이트는 공기에서 산소(O₂)와 질소(N₂)를 분리하는 데 사용된다.\n제올라이트의 기공 구조는 산소와 질소 분자의 크기 차이를 이용하여 선택적으로 분리하는 특성을 지닌다.\n\n다른 활용 분야 제올라이트는 흡착제로 사용되어, 가스, 화학 물질, 수분 등을 효율적으로 흡착할 수 있다. 이 특성 덕분에 수처리, 공기 정화, 환경 보호 분야에서 유용하게 활용된다.\n\n또한, 뛰어난 이온 교환 능력으로 수질 개선, 비료의 흡수, 의약품 제조 등에서도 활용되며, 화학 촉매로서 화학 반응의 효율성을 높인다.\n특히 석유화학 산업에서 다양한 화합물의 합성 및 분해에 중요한 역할을 한다.\n10 도마뱀붙이 (Gecko foot, Gecko adhesion)\n도마뱀의 발이 가지는 특수한 구조로, 물체에 강하게 붙을 수 있는 능력을 말한다.\n보통 11 ~ 12 cm, 1 ~ 2 kg 정도의 크기를 가지는 도마뱀붙이는 벽이나 천장을 자유자재로 이동할 수 있으며, 그 원리는 발가락 구조에 있다.\n도마뱀붙이의 발에는 Seta(셉타)라는 구조가 있다.\n각 발가락에는 50만 개의 Seta가 있으며, 각 Seta의 끝에는 수백 개의 나노 크기의 Spatula(스패출러)가 있다.\n이 Seta와 Spatula는 반데르발스 힘(Van der Waals force)을 이용해 표면에 강하게 부착할 수 있게 만들어 준다.\n이 물리적 현상 덕분에 도마뱀붙이는 매우 미세한 표면에도 쉽게 부착할 수 있고, 벽을 탈 때도 떨어지지 않는다.\n\n게코테이프 (Gecko Tape)\n\n도마뱀붙이의 발가락 구조를 모방하여 만든 스티키 테이프.\n이 테이프는 도마뱀붙이가 벽을 붙잡는 방식인 나노 크기의 Spatula를 이용하여 접착력을 발휘한다.\n반데르발스 힘을 활용하여 강력하면서도 재사용 가능한 접착력을 제공한다. 이 힘은 분자 수준에서 발생하는 약한 상호작용으로, 물리적 접착이 가능하게 한다.\n일반적인 접착 테이프와 달리 한 번 붙이고 떼더라도 접착력이 그대로 유지되며, 여러 번 재사용할 수 있는 장점이 있다.\n\n게코테이프의 활용 로봇이 벽이나 천장을 자유롭게 이동할 수 있도록 돕는 기술로 발전할 수 있다.\n\n또한, 피부에 자극을 최소화하면서도 강력한 접착력을 제공하는 의료용 테이프로 활용될 수 있으며, 표면에 손상 없이 부착할 수 있는 재료로 건축 분야에도 응용될 수 있다.\n11 구조색 (Structural Color)\n물질이 염료나 색소 없이 나노구조나 미세한 표면 구조에 의해 색을 나타내는 현상.\n이 현상은 물질의 물리적 특성에 따라 빛의 반사, 굴절, 분산 등이 다르게 일어나며, 그 결과로 특정 색이 나타난다.\n\n멜라닌 (Melanin)\n\n주로 피부, 머리카락, 눈 등에 존재하는 색소.\n손톱과 같은 부위에서 멜라닌은 투명한 특성을 가지며, 나노 구조에 따라 반사되는 빛이 달라져 색을 나타낸다.\n멜라닌 자체는 색이 없는 물질이지만, 물질과 빈 공간이 반복적으로 배열되면 외부 빛에 의해 구조색 효과가 나타날 수 있다.\n즉, 구조색은 물질과 빈 공간의 반복적인 배열, 나노 크기의 구조에 따라 색이 달라 보이는 현상이다.\n예를 들어, 녹색은 약 100nm의 나노구조와 55nm의 빈 공간에서 나타날 수 있다. 갈색은 약 125nm의 나노구조와 70nm의 빈 공간에서 나타날 수 있다.\n\n모르포 나비 (Morpho Butterfly)\n\n브라질, 페루 등 중남미 지역에 서식하며, 빛의 반사를 이용한 구조색에 의해 푸른빛을 띈다.\n자연에서 색소에 의한 순수한 파란색은 매우 드문 현상이다.\n초기 연구자들은 모르포 나비 날개에서 파란색을 만들어내는 색소를 찾으려고 했지만, 색소가 존재하지 않음을 확인했다.\n이후 연구에서 날개 표면의 나노 구조가 빛의 반사를 조절하여 색을 만들어낸다는 것을 알게 되었다.\n나비의 날개 표면은 미세한 나노 주름과 뽀족한 돌기로 덮여 있으며, 이 구조가 빛을 분산시키고 반사하여 파란색을 만들어낸다.\n이처럼 나노 구조는 특정 빛의 파장을 선택적으로 반사하여 색을 형성하며, 동시에 물과의 접촉을 최소화해 물방울을 쉽게 튕겨내는 초소수성 효과도 발휘한다.\n\n카멜레온 (Chameleon)\n\n주로 아프리카와 마다가스카르를 포함한 열대 지역에 서식하며, 특유의 색상 변화 능력, 독립적으로 움직이는 눈, 그리고 긴 혀를 이용한 먹이 포획 능력을 가진 파충류.\n카멜레온은 몸의 색을 변화시킬 수 있는 능력을 지니며, 이를 통해 다양한 환경에 적응하거나 의사소통을 한다.\n이 색 변화는 단순한 색소 변화가 아니라, 몸 표면의 미세 구조를 조절해 빛의 반사와 분산을 변화시킴으로써 발생한다.\n이를 통해 주변 환경과 조화를 이루는 색을 나타낼 수 있다.\n\n오팔 (Opal)\n\n실리카(SiO₂)와 물이 결합된 광물.\n오팔의 색상 변화는 주로 빛의 간섭에 의해 발생하는데, 이는 오팔의 내부 구조에 있는 실리카 구슬들이 특정한 방식으로 배열되어 빈 공간을 형성하고, 이 구조가 빛의 파장을 선택적으로 반사하고 분산시키기 때문이다.\n오팔의 내부 구조는 나노미터 크기의 실리카 입자들이 규칙적으로 배열되어 있어서, 빛이 이들 사이를 지나갈 때 다양한 색이 반사된다.\n이 반사는 빛의 각도에 따라 달라져, 오팔의 독특한 색상 변화(예: 파랑, 초록, 보라)가 나타나게 되며, 이러한 색상 변화를 “화려한 불꽃”(play-of-color)이라고도 부른다.\n\n구조색의 활용 여권과 같은 보안 문서에는 구조색을 활용하여 보안 기능을 강화하는 경우가 많다.\n\n이러한 구조색은 자외선(UV) 빛을 비추면 숨겨진 색상이나 패턴이 드러나도록 설계된다.\n이는 구조적 배열이 특정 파장의 빛을 반사하거나 흡수하는 원리를 이용하며,\n일반적인 색소나 염료를 사용하는 방식과 달리 빛의 간섭이나 회절을 통해 색이 형성된다.\n이러한 특수한 구조색 효과는 문서의 위조를 방지하는 데 중요한 역할을 하며,\n자외선 등의 특정 조건에서만 나타나는 색상이나 패턴을 통해 진위 여부를 확인할 수 있다."
  },
  {
    "objectID": "pd/od/od_01.html",
    "href": "pd/od/od_01.html",
    "title": "Git 사용법",
    "section": "",
    "text": "VS코드로 Git 사용법에 대해 다루고자 한다.\n\n01 버전 관리 시스템\nVCS, Version Control System 소프트웨어 개발 및 문서 작업에서 변경 사항을 기록, 추적, 관리, 수정, 복구할 수 있도록 돕는 도구.\n\n로컬 버전 관리 시스템 Local VCS 파일 변경 이력을 로컬 컴퓨터에서만 관리하는 시스템을 의미. 대표적인 로컬 버전 관리 시스템은 RCS이며, 이는 파일의 변경 이력을 개별적으로 저장하는 방식이다.\n\nRevision Control System 과거 macOS 및 UNIX 시스템에서 사용되었으나, 현재는 Git과 같은 분산 버전 관리 시스템(DVCS)의 등장으로 거의 사용되지 않는다.\n\n중앙집중식 버전 관리 시스템 CVCS, Centralized VCS 여러 사용자가 협업할 수 있도록 설계된 중앙 서버 기반의 버전 관리 시스템.\n\n모든 변경 사항이 중앙 서버에 저장되며, 사용자는 중앙 서버에서 데이터를 가져와 수정한 후 다시 업로드하는 방식으로 동작한다.\n대표적으로, SVN (Subversion), Perforce, CVS (Concurrent Versions System) 등이 있다.\nCVCS의 단점 ① 모든 변경 이력이 중앙 서버에 저장되므로, 서버가 다운되거나 손상되면 데이터를 잃거나 작업이 불가능할 수 있다.\n② 파일을 가져오거나 변경 사항을 기록하려면 항상 중앙 서버에 연결해야 한다. 네트워크 연결이 불안정하면 효율적으로 작업하기 어렵다.\n3 . 분산 버전 관리 시스템 (DVCS, Distributed VCS)\n각 개발자가 히스토리가 포함된 전체 저장소를 로컬에 복제하여 관리하는 방식.\nGit, Mercurial이 대표적인 DVCS이다.\nGit의 장점 ① 모든 변경 이력을 로컬 저장소(Local Repository)에 저장한다. 따라서 네트워크 연결 없이도 히스토리를 조회하고, 로컬에서 자유롭게 작업할 수 있다.\n② 가벼운 브랜칭(branching) 기능을 제공하여, 여러 작업을 독립적으로 진행할 수 있다. 브랜치를 쉽게 병합(merging)할 수 있어, 팀 협업 시 코드 충돌을 최소화할 수 있다.\n③ 모든 개발자의 로컬 저장소에 프로젝트의 전체 히스토리를 저장한다. 중앙 서버에 문제가 발생해도, 로컬 저장소를 활용하여 데이터를 복구할 수 있다.\n이러한 장점 덕분에 Git은 현대적인 소프트웨어 개발에서 가장 널리 사용되는 버전 관리 시스템이 되었다.\n02 Git 사용법\n1 . Git 설치하기\nGit - Downloads\nDownloads macOS Windows Linux/Unix Older releases are available and the Git source repository is on GitHub. Latest source Release 2.48.1 Release Notes (2025-01-13) Download Source Code GUI Clients Git comes with built-in GUI tools (git-gui, gitk), but ther\ngit-scm.com\n설치가 완료되었다.\nGit Bash를 관리자 권한으로 실행하기.\n다음과 같은 창이 나오는 것을 확인할 수 있다.\n2 . VS Code 실행하기 Git 창을 닫고 VS Code를 실행하기.\nVS Code가 설치되지 않았다면, 먼저 설치한 후 실행하세요.\nVisual Studio Code - Code Editing. Redefined\nVisual Studio Code redefines AI-powered coding with GitHub Copilot for building and debugging modern web and cloud applications. Visual Studio Code is free and available on your favorite platform - Linux, macOS, and Windows.\ncode.visualstudio.com\n첫 화면.\n새 파일 만들기.\nOpen Folder &gt; New &gt; Folder\n파일명이 왼쪽 상단에 생성된 것을 확인할 수 있다. 이제 오른쪽 상단의 Toggle Panel을 클릭한다.\n3 . 터미널 (Terminal) 터미널에 Git 저장소를 로컬로 복제(clone)하는 명령어 입력하기.\n\n\n기본구조: git clone + 복제할 원격 저장소의 URL\n예제: git clone https://github.com/onlybooks/python-algorithm-interview.git\n해당 저장소의 모든 파일과 이력이 현재 작업 디렉토리로 다운로드된다.\n즉, python-algorithm-interview 폴더가 생성되며, 해당 저장소의 내용을 그대로 가져오게 된다.\n4 . GitHub 저장소 클론\nGithub 업로드를 허용한다.\n원하는 계정을 선택한다.\n계정이 없으면 새 계정을 생성한다.\n열기 버튼 클릭.\nPrivate는 본인만 접근할 수 있고, Public는 모든 사용자가 접근할 수 있다.\n필요에 따라 적절한 옵션을 선택한다.\n커밋 내역 확인하기.\nREADME.md 파일 확인하기.\n\n파일의 수정 과정을 알 수 있다."
  },
  {
    "objectID": "pd/od/od_03.html",
    "href": "pd/od/od_03.html",
    "title": "개발 일지",
    "section": "",
    "text": "01 제목1 [ID 사용하기]\n1 . 제목2 본문1\n본문2\nSign In\nSign in to AILab ID Authenticate yourself with your passkey to access the admin panel. Authenticate\nsso.llms.kr\n키 값.\n비밀번호 입력.\nSign in to Services\nSign in to Services Do you want to sign in to Services with your AILab ID account? Cancel Sign in\nsso.llms.kr\n재희 작품\n깃허브 주소 보내면 이메일로 옴\ndjailab"
  },
  {
    "objectID": "pd/od/od_05.html",
    "href": "pd/od/od_05.html",
    "title": "개발 일지3",
    "section": "",
    "text": "재실감지 프로젝트의 2월 24일 오후 9시 회의에 대해 다루고자 한다.\n01 API 정의 응용 프로그램 인터페이스, Application Programming Interface.\n다른 프로그램과 소통할 수 있도록 만들어진 인터페이스. 사용하는 대상과 방식에 따라 다르게 분류될 수 있다.\nAPI의 역할을 두 가지 측면에서 구분해서 봐야 한다.\n1 . HTTP 통신을 위한 API 웹 애플리케이션이나 모바일 앱에서 백엔드 서버와 데이터를 주고받기 위한 API.\n주로 HTTP 프로토콜을 사용하여 데이터를 요청하고 응답을 받는다.\n예를 들어, 클라이언트(웹 브라우저, 모바일 앱 등)가 서버의 특정 URL에 요청을 보내면 서버가 JSON 형식으로 데이터를 응답하는 방식.\n보통 REST API, GraphQL API 등이 이에 해당된다.\n2 . 백엔드 내부에서 DB와 통신하는 API 백엔드 시스템 내부에서만 사용되는 API. 프로그램 내부에서 함수 호출을 통해 사용된다.\n예를 들어, 백엔드 코드에서 DB에서 유저 정보를 가져오는 함수 같은 것이 이에 해당된다.\nHTTP 요청 없이, 애플리케이션 내부의 모듈 간 통신을 위한 API라고 할 수 있다.\n02 재실 확인 정의 연구실 내의 사람들의 재실 즉, 출입 여부를 확인하는 시스템을 구축하는 방식.\n이를 이해하려면 프론트엔드(사용자 인터페이스)와 백엔드(서버) 간의 데이터 흐름을 살펴봐야 한다.\n1 . 재실 상태 업데이트 방식 연구실 구성원이 출입할 때마다 해당 정보를 서버인 백엔드에 전송해야 한다.\n이를 위해 POST 요청을 사용하여 현재 상태(입실/퇴실)를 백엔드에 전달한다. 백엔드는 이를 받아서 DB에 재실 정보를 저장한다.\n2 . DB 직접 접근 불가 프론트엔드는 웹 브라우저에서 실행되는 화면(UI).\n보안상 프론트엔드가 DB에 직접 접근하면 안 된다. 대신, 백엔드 서버에 GET 요청을 보내서 데이터를 가져와야 한다.\n3 . GET 요청 연구실의 재실 상태가 바뀌어도 프론트엔드에서는 이를 자동으로 알 방법이 없다.\n따라서 1초마다 백엔드에 GET 요청을 보내 최신 상태를 확인해야 한다. 이를 통해 연구실 출입 상태가 바꿜 때마다 UI가 자동으로 업데이트된다.\n연구실 인원 수가 많지 않다면, 1초마다 GET 요청을 보내도 서버가 충분히 감당할 수 있다.\n4 . 추가 최적화 방법 위 방식은 불필요한 트래픽이 발생할 수 있다.\n웹소켓(WebSocket)을 사용하면 재실 상태가 변경될 때만 프론트엔드로 알림을 보낼 수 있다.\n하지만 구현이 조금 더 복잡할 수 있다.\n03 DB 설계\n1 . DB의 정의 연구실 출입 정보를 저장해야 하므로 DB가 필요하다.\n연구실 출입 기록을 관리하지 않고 메모리(RAM)에서만 처리하면 서버를 재시작하면 데이터가 사라진다.\n따라서 DB에 출입 기록을 저장해야 한다.\n2 . SQLite 연구실 시스템은 트래픽이 많지 않다.\nSQLite는 파일 기반 경량 DB로, 연구실 같은 소규모 시스템에서 사용하기 적절하다.\n대형 시스템이면 MySQL, PostgreSQL 같은 DB를 고려해야 하지만, 현재 상황에서는 SQLite로 충분하다.\n3 . 로그 기록 남기는 방법 연구실 출입 기록을 DB에 어떻게 저장할지 고민하는 부분이다. 로그를 남기는 방식은 여러 가지가 있지만, 최소한의 정보만 저장하면 된다.\n즉, 재실 상태를 실시간으로 저장하는 것이 아니라, “출입 로그만 남기면 충분하다”는 의미이다.\n예를 들어, 한 사람이 하루에 5번 드나들면, 그 5번의 출입 로그만 기록하면 된다.\n“현재 연구실에 있는 사람” 이 누구인지 확인하려면, status = ’ in ’ 중 가장 최신 로그를 보면 된다.\n출입할 때만 기록을 남기므로 불필요한 데이터 저장을 최소화할 수 있다.\n4 . DB 갱신 보류 데이터가 계속 쌓이면 DB 크기가 커지고 성능 저하 가능성이 있다. 그래서 “1달마다 오래된 로그를 삭제할까?” 라는 고민을 했지만, 결론적으로 필요 없다고 판단하였다.\nSQLite는 소규모 데이터 저장에는 충분히 빠르고 효율적이다. 연구실 출입 기록을 하루 수십 ~ 수백 개 수준으로 가정해도 1년치 데이터는 몇 MB가 안 될 것이다.\n따라서 데이터가 많아서 생기는 성능 문제는 걱정할 필요 없다.\n5 . DB 제약사항 최소화 테이블을 만들 때, 외래키, 복잡한 관계 등 너무 많은 제약을 설정하지 말자는 의미.\nSQLite는 간단한 DB이므로, 최대한 단순한 구조로 유지하는 것이 좋다. 최소한의 필드만 저장 (user_id, status, timestamp)\nJOIN 남발하는 등 불필요한 복잡한 테이블 관계를 자제한다. ON DELETE CASCADE 같은 것을 최소화하여 강한 외래키 제약한다.\n04 DB 라이브러리 선택 데이터베이스와 소통하려면 라이브러리(의존성)가 필요하며 다음과 같은 후보가 있다.\n1 . SQLAlchemy Python에서 가장 널리 사용되는 ORM(Object-Relational Mapping) 라이브러리.\nSQLAlchemy\nThe Database Toolkit for Python\nwww.sqlalchemy.org 트랜잭션, 복잡한 쿼리, DB 최적화 등 강력한 기능 제공하지만 설정이 복잡하고 학습 곡선이 가파르다.\n장점 대규모 프로젝트에서 사용하기 좋다.\n다양한 데이터베이스 지원 (PostgreSQL, MySQL, SQLite 등) ORM뿐만 아니라 직접 SQL을 실행하는 기능도 제공한다.\n단점 문법이 복잡하고 설정이 어려우며, 초보자가 배우기에 부담스러울 수 있다.\n추천 대상 ① 대규모 프로젝트에서 강력한 데이터베이스 기능이 필요한 경우 ② SQL을 잘 알고 있고 세부적인 최적화가 필요한 경우\n2 . Peewee peewee — peewee 3.17.9 documentation\n경량 ORM으로, 코드가 단순하고 배우기 쉽다. SQLAlchemy보다 기능이 적지만, 기본적인 CRUD 작업에는 충분하다.\n장점 코드가 간결하고 직관적이며, 설정이 간단해 빠르게 시작 가능하다.\nSQLite, PostgreSQL, MySQL을 지원한다.\n단점 SQLAlchemy보다 기능이 부족하다. 대규모 프로젝트에는 부적절할 수 있다.\n추천 대상 ① 작은 프로젝트나 빠르게 개발해야 하는 경우 ② 복잡한 DB 연산이 필요하지 않은 경우\n3 . SQLModel SQLAlchemy를 기반으로 만든 최신 ORM. Pydantic과 통합되어 데이터 검증이 쉽다.\nSQLModel\nhttps://www.sqlalchemy.org/ SQLAlchemy의 강력한 기능을 유지하면서도 더 간단한 사용법 제공한다.\n장점 공식 지원을 받으며, 최신 기술을 반영한다.\nSQLAlchemy보다 더 간단한 문법 가지며, Pydantic과 연계되어 데이터 모델링과 검증이 쉽다.\n단점 아직 SQLAlchemy만큼 성숙하지 않다. (상대적으로 신생 라이브러리) 복잡한 DB 기능을 다루려면 결국 SQLAlchemy의 일부 기능을 사용해야 한다.\n추천 대상 ① FastAPI 같은 최신 프레임워크와 함께 사용할 경우 ② SQLAlchemy의 복잡함을 줄이고 싶지만, 강력한 기능이 필요한 경우\n최종 결정으로 SQLModel을 선택하였다.\n05 SQLite과의 비교\n1 . 일반적인 DBMS MySQL, PostgreSQL 등이 해당된다.\n독립적인 서버로 실행되며, 클라이언트가 네트워크를 통해 서버에 접속하여 데이터 요청한다.\n여러 사용자가 동시에 접근할 수 있다.\n2 . SQLite 서버가 없으므로, 별도의 프로그램을 실행할 필요 없다. 데이터를 ’ .db 파일 ’ 로 저장한다.\n프로그램이 직접 파일을 읽고 쓰는 방식으로 동작한다. 가볍고 간단해서 로컬 데이터 저장용으로 적합하다.\n06 쿼리\n1 . 유저 테이블 랩원(사용자)에 대한 기본 정보를 저장한다.\n2 . 로그 테이블 (presence)\n입실 및 퇴실 시간을 기록한다.\n각 로그에는 입실/퇴실 여부를 나타내는 정보와, 어떤 유저인지에 대한 정보가 포함된다.\n이 쿼리는 presence 테이블에서 유저의 입실 및 퇴실 기록을 조회하는 쿼리이다.\nSELECT DISTINCT ON (user_id) * FROM presence ORDER BY at_time, user_id DESC; 이 쿼리는 각 유저별로 가장 최근의 입실/퇴실 기록을 가져온다.\n예를 들어, 유저 A가 오전 9시에 입실하고 오후 5시에 퇴실한 경우, presence 테이블에서 유저 A의 두 개의 레코드가 있을 수 있다.\n이 쿼리는 유저 A의 가장 최근 퇴실 시간 또는 입실 시간을 선택하게 된다.\nSELECT DISTINCT ON (user_id) DISTINCT ON (user_id)는 유저별로 가장 최근의 기록만 선택한다. 즉, 유저가 여러 번 입실/퇴실한 경우, 각 유저의 가장 최근 입실/퇴실 정보만 가져오게 된다.\n’ * ’ 테이블의 모든 컬럼을 선택하겠다는 의미이다.\n예를 들어, at_time (시간), user_id (유저 ID), 입실/퇴실 여부 등을 포함하는 모든 컬럼을 가져온다.\nFROM presence presence 테이블에서 데이터를 가져온다. presence 테이블에는 입실과 퇴실 시간에 대한 기록이 저장되어 있다.\nORDER BY at_time, user_id DESC at_time으로 먼저 시간 순서대로 정렬한다. (입실/퇴실 시간을 기준으로 오름차순 정렬)\n그 다음 user_id DESC로 유저 ID를 내림차순으로 정렬한다.\n이는 같은 시간에 여러 번 입출입한 기록이 있을 수 있을 때, 가장 최신 기록을 가져오기 위함이다."
  },
  {
    "objectID": "pd/re/re_01.html",
    "href": "pd/re/re_01.html",
    "title": "React 설치",
    "section": "",
    "text": "https://github.com/kpyszkowski/kpyszkowski/blob/main/books/Alex%20Banks%20%26%20Eve%20Porcello%20-%20Learning%20React%3A%20Learning%20React%20Modern%20Patterns%20for%20Developing%20React%20Apps%20-%20O%E2%80%B2Reilly%20(2020).pdf\n구글 검색창에 Welcome to React - Learning React [Book]이라고 치면 원본 PDF 나온다. 이는 미번역본이다.\nhttps://ko.react.dev/learn 전문 사이트로 참고할 것.\n교재의 Chapter 1. Welcome to React를 참고하여 React(리엑트) 설치에 대해 다루고자 한다.\n1 . React 개발 도구 설치 React로 만든 웹 애플리케이션을 디버깅하고 검사하기 위한 브라우저 확장 도구이다.\nhttps://chromewebstore.google.com/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?pli=1\n이 도구는 크롬이나 파이어폭스와 같은 브라우저의 개발자 도구에 통합되어, React 컴포넌트 구조를 쉽게 탐색할 수 있도록 돕는다.\n이 앱을 크롬에 설치한 후 상태 표시를 확인하라.\n\nNode.js 설치\n\nhttps://nodejs.org/en\nNode.js는 JavaScript 런타임 환경으로, JavaScript를 서버 사이드에서 실행할 수 있게 해준다.\n다운로드한 후, 로컬 드라이브에 저장한다.\n(가장 아래)\n\nnpm 설치 (Node Package Manager)\n\nNode.js의 패키지 매니저로, 수많은 자바스크립트 라이브러리와 패키지가 모여 있는 저장소이다. 개발자는 필요한 라이브러리나 모듈을 npm을 통해 간편하게 설치하고 사용할 수 있다.\n예를 들어, React나 Express 같은 자주 사용하는 라이브러리도 npm으로 설치할 수 있다.\n\n프로젝트 초기화 명령어 프로젝트의 기본 설정을 자동으로 생성해 준다.\n\nnpm init -y // 기본 설정을 자동으로 생성\n이 명령어를 실행하면 package.json 파일이 생성되며, 프로젝트의 이름, 버전, 스크립트 등을 설정하는 데 사용된다.\n여기서 -y 옵션은 npm init 명령어의 질문에 대한 기본 값을 자동으로 입력해준다.\n만약 사용자가 직접 설정을 수정하거나 설정 내용을 변경하려면 아래 명령어를 입력하고 각 질문에 응답하면 된다.\nnpm init // 사용자가 직접 수정\n이 명령어 자체는 심각한 문제를 일으키지 않으며, 생성된 package.json 파일은 언제든지 수정할 수 있다.\n다만, 프로젝트를 관리하고 설정하는 과정에서는 신중함이 필요하다.\n\nYarn 설치 Yarn 패키지를 현재 프로젝트의 로컬 디렉터리에 설치한다. 이 경우 해당 프로젝트에서만 Yarn을 사용할 수 있다.\n\nnpm install yarn\nYarn 패키지를 전역(-g: global)으로 설치한다.\n이렇게 하면 특정 프로젝트와 상관없이, 컴퓨터의 모든 디렉터리에서 Yarn 명령어를 사용할 수 있다.\nnpm install -g yarn\n\n버전(version) 확인 명령어 현재 시스템에 설치된 Node.js의 버전을 확인하는 명령어이다.\n\nNode -v // v20.17.0\n현재 시스템에 설치된 npm의 버전을 확인하는 명령어이다.\nnpm -v // 10.8.2\n이 명령어들은 Node.js와 npm이 올바르게 설치되었는지, 그리고 최신 버전인지 확인할 때 자주 사용된다."
  },
  {
    "objectID": "pd/re/re_03.html",
    "href": "pd/re/re_03.html",
    "title": "비동기 자바스크립트",
    "section": "",
    "text": "(P.56)\n01 비동기 자바스크립트 (Asynchronous JavaScript) 프로그램이 멈추지 않고, 여러 작업을 동시에 처리할 수 있도록 설계된 개념. 기본적으로 자바스크립트는 싱글 스레드 언어이므로, 한 번에 하나의 작업만 처리할 수 있다.\n따라서 비동기 작업이 없으면 어떤 작업이 완료될 때까지 다른 작업들이 대기해야 하고, 이로 인해 화면이 멈추는 현상이 발생할 수 있다.\n\n비동기 처리의 필요성 초기에는 웹 페이지에서 동적인 변화가 적어 화면이 멈추는 일이 큰 문제가 되지 않았다.\n\n하지만 현대적인 웹 애플리케이션에서는 사용자 경험이 중요해졌고, 작업이 멈추면 페이지가 느려지거나 비정상적으로 작동하는 것처럼 보일 수 있었다.\n특히, 서버에서 데이터를 불러오거나 큰 계산 작업을 하는 동안 사용자 인터페이스가 멈추는 것을 방지하는 것이 중요해졌다.\n\n비동기 처리 방법 자바스크립트에서 비동기 처리를 위해 다양한 방법을 사용할 수 있다.\n\n\n콜백 함수 (Callback) 함수가 완료되면 그 다음에 실행할 작업을 함수로 전달하여 실행하는 방식.\n\n하지만 콜백 함수의 중첩이 많아지면 코드가 복잡해지고 관리하기 어려워지는 “콜백 지옥”이 발생할 수 있다.\n\n프로미스 (Promise) 콜백 함수의 단점을 보완한 비동기 처리 방식. 작업이 완료되면 성공(resolve)이나 실패(reject)에 대한 결과를 반환한다.\n\n.then()이나 .catch()로 결과를 처리할 수 있다.\n\nasync/await ES8에 도입된 방식으로, 프로미스를 더 직관적이고 순차적으로 처리할 수 있게 만들어준다.\n\nasync 키워드를 함수에 붙이고, 그 안에서 await를 사용하여 비동기 작업이 완료될 때까지 기다릴 수 있다.\n입력 함수와 콜백 함수 자바스크립트에서 입력 함수는 사용자로부터 데이터를 입력받는 작업을 수행하는 함수.\n비동기 작업에서는 입력이 완료된 후에도 다른 작업이 바로 실행되며, 이때 콜백 함수를 사용하여 입력이 완료된 후 특정 작업을 처리하게 됩니다.\n콜백 함수는 특정 이벤트가 발생하거나 작업이 완료되었을 때 호출되는 함수입니다.\n2초 뒤에 “첫 번째 작업 완료”라는 메시지를 출력하는 비동기 코드. 이 경우 콜백 함수가 사용됩니다.\n중요한 점은, 2초가 지나기 전에 다른 작업이 바로 실행된다는 것입니다. 즉, 프로그램은 기다리지 않고 계속 실행되며, 작업이 완료되면 콜백 함수가 호출됩니다.\n이것이 블로킹되지 않는다는 의미입니다.\nAPI 요청과 fetch 함수 과거에는 API 요청을 보내는 작업이 복잡하고 번거로웠지만, fetch 함수의 도입으로 간편하게 API 호출을 할 수 있게 되었다.\nfetch 함수는 기본적으로 프로미스를 반환하며, 비동기적으로 데이터를 받아옵니다. 이는 REST API를 사용하여 서버에서 데이터를 가져올 때 매우 유용합니다.\nPromise 비동기 작업의 상태와 결과를 나타내는 자바스크립트 객체\n이 개념을 비유적으로 설명하면, 브라우저가 데이터를 요청하면서 “최선을 다해 데이터를 가져올게. 성공하든 실패하든 그 결과를 알려줄게.” 라고 약속하는 것과 같다.\n기본 동작 진행 중 (pending): 비동기 작업이 아직 완료되지 않은 상태.\n성공 (fulfilled): 비동기 작업이 성공적으로 완료된 상태.\n실패 (rejected): 비동기 작업이 실패한 상태.\n\n.then() ⇨ 작업 체이닝(연결) 비동기 작업이 성공한 후 실행될 작업(콜백 함수)을 등록합니다. 이 함수는 이전 작업이 완료되면 호출됩니다.\n\n여러 then()을 연결하여 작업을 순차적으로 수행할 수 있다.\n\n.catch() ⇨ 예외 처리 Promise에서 발생한 오류를 잡아서 처리하는 메서드.\n\n데이터 요청 중 네트워크 문제나 서버 오류 등의 이유로 작업이 실패할 때, 그 에러를 처리할 수 있다.\nAsync 일반적인 함수를 비동기적으로 처리하게 해주는 함수. .then() 체인을 사용해 promise의 결과를 기다리는 대신, async 함수는 코드 실행을 promise가 해결될 때까지 기다리도록 함.\n앞서서 API 요청을 Async 함수로 처리하는 코드 async function getFakePerson() { try { const response = await fetch(‘https://api.randomuser.me/?nat=US&results=1’); const data = await response.json(); console.log(data.results); } catch (error) { console.error(‘Error:’, error); } } getFakePerson() 동시 접근이 안된다. 해결할 때까지 기다려 준다.\n개발자는 이를 미리 예측하여, 멈추지 않고 작업이 가능하도록 예외처리를 하는 것이 중요함. 패치 시 await를 해줌, 함수 호출을 하면 한 명의 데이터가\n뉴 바인딩, 뒤에 클래스 문법이 나옴, 클래스의 이름의 첫 글자는 대문자이다. 이는 객체로 사용 가능한으로 판단할 수 있다.\n괄호 안에 매개변수()에 각각 성공과 실패에 대한 정보가 담긴다. 웹 문서에서 데이터 요청할 때 쓰는 2가지 방식\nGET(노출하여 가져오는 것, 해킹 당할 위험이 큼) POST(노출하지 않아, 더 안전하다.)\n성공했다면(200), 상한 연산자 - 참(?)/거짓(:)\nClasses 2015이전에는 자바에서 클래스를 쓸 수 없었음 이후 클래스를 써서 개발자들에게 각광을 받았다.\n방학 때 어디로 놀러갈 것인가?\n// p.62\nfunction Vacation(destination, length) { this.destination = destination; this.length = length; } Vacation.prototype.print = function() { console.log(this.destination + ” | ” + this.length + ” days”); }; const maui = new Vacation(“Maui”, 7); maui.print(); // Maui | 7 days 생성자 호출, 마오이로 7일 간 자바 스크립트 클래스 문법 차별화를 두겠다고 따라한 걸 안 들키려고 함. 이는 파이썬도 마찬가지이다.\nC언어로 배운 사람은 혼란할 수 있음\nclass Vacation { constructor(destination, length) { this.destination = destination; this.length = length; } print() { console.log(${this.destination} will take ${this.length} days.); } }\nconst trip = new Vacation(“Santiago, Chile”, 7); trip.print(); // Chile will take 7 days.\n자바 기능의 모든 것.\nclass Expedition extends Vacation { constructor(destination, length, gear) { super(destination, length); this.gear = gear; } print() { super.print(); console.log(Bring your ${this.gear.join(\" and your \")}); } } super의 의미: 부모의\n매개변수 2개, 1.상속: 부모의 기능을 그대로 사용하려는 것 객체 지향에서는 가장 중요하다.\n2.다형성, 3.인터페이스\nconst trip = new Expedition(“Mt. Whitney”, 3, [ “sunglasses”, “prayer flags”, “camera”]); trip.print(); // Mt. Whitney will take 3 days. // Bring your sunglasses and your prayer flags and your camer\n스크립트도 모듈로 쓸 수 있도록 모듈은 쉽게 말해 부품이다.\nexport const print=(message) =&gt; log(message, new Date()) export const log=(message, timestamp) =&gt; console.log(${timestamp.toString()}: ${message}) 외부에서 가져다 쓸 수 있도록 인폴트시킨 것. C언어에서 블리컨파일 불러오기를 자바에서는 모듈이라는 기능으로 불린것 뿐\n객체도 외부에 공개할 수 있다.\nimport { print, log } from “./text-helpers”; import freel from “./mt-freel”; print(“printing a message”); log(“logging a message”); freel.print(); C언어는 인클루드이고, 자바나 파이썬은 인폴트를 사용한다. 모듈을 이용해서 외부의 기능을 사용한 것. 이는 관리하기 용이하고 이름이 중복되는 것을 방지.\n아바스틴줄 약"
  },
  {
    "objectID": "pd/re/re_05.html",
    "href": "pd/re/re_05.html",
    "title": "React를 이용한 Blog 만들기(1)",
    "section": "",
    "text": "(P.109)\nChapter 4 How React Works\n01 리액트 프로젝트 환경을 설정하기.\n\nNode.js 최신 버전 설치\n\nhttps://nodejs.org/en\n설치 후, 명령 프롬프트(또는 터미널)에서 node -v 와 npm -v 를 입력해 버전 확인이 가능해야 한다.\n\nVisual Studio Code (VS Code) 설치\n\nhttps://code.visualstudio.com/\n설치 후, VS Code를 열어 React 프로젝트 작업을 시작할 수 있다.\n\nReact 프로젝트 생성\n\n터미널을 열고 React 프로젝트를 생성하고자 하는 폴더로 이동한 후, 다음 명령어를 입력하여 기본 React 템플릿을 생성하기.\nnpx create-react-app my-app\n이 명령어는 my-app이라는 폴더에 React 프로젝트를 생성하며, 필요한 모든 패키지가 자동으로 설치된다.\n\n프로젝트 실행\n\n생성한 프로젝트 폴더로 이동한 후, 다음 명령어를 입력해 로컬 서버에서 앱을 실행하기.\ncd my-app npm start 이제 브라우저에서 http://localhost:3000 에 접속하여 React 애플리케이션을 볼 수 있다.\nReact 프로젝트 작업 폴더를 설정하기. C 드라이브 또는 보조 드라이브에 react class라는 폴더를 만들기.\n예시: C:class 또는 다른 드라이브에 react class라는 이름으로 폴더를 생성.\n\nVisual Studio Code를 열고 상단의 File 메뉴를 클릭하기.\nOpen Folder를 선택하고, 방금 만든 react class 폴더를 찾아 선택한다.\nSelect Folder 버튼을 누르면 VS Code에서 react class 폴더가 열리고, React 프로젝트를 생성하거나 파일을 추가하여 작업을 시작할 수 있다.\n\n초기 설치 시 오류가 발생할 수 있다. 1. React 프로젝트 생성 오류 해결\nnpx create-react-app 명령어 실행 시 오류가 나는 경우, 먼저 기존에 전역으로 설치된 create-react-app 패키지를 제거한다.\n// 명령프롬프트(터미널)에 입력하기. npm uninstall -g create-react-app\n\n다시 설치\n\nnpm install 명령어를 사용해 패키지 설치를 다시 확인하고, 업데이트한다.\nnpm install\n\n프로젝트 생성 재시도\n\n이제 npx create-react-app 명령어로 React 프로젝트를 다시 생성한다. 예시로 blog 라는 프로젝트를 생성하려면 아래와 같이 입력한다.\nnpx create-react-app blog\n실행 후, 아래와 같이 확인 메시지가 나오면 Enter 키를 눌러 설치를 계속 진행한다.\n이 과정을 통해 문제가 해결되고 정상적으로 React 프로젝트를 생성할 수 있을 것이다.\nReact 프로젝트 생성 명령어 작업 폴더에서 React 프로젝트를 생성하는 명령어.\nnpx create-react-app blog react class 폴더에서 실행하면 blog라는 이름의 폴더에 React 프로젝트가 생성된다.\n명령어 구성 요소 설명 1. npx\n라이브러리 실행 및 설치 명령어. Node.js가 설치되어 있어야 사용 가능하다.\n\ncreate-react-app\n\n리액트 프로젝트 설정이 완료된 보일러플레이트를 자동으로 만들어 주는 라이브러리. 개발자가 프로젝트 기본 구조를 설정할 필요 없이 빠르게 시작할 수 있다.\n\nblog\n\n생성할 프로젝트의 폴더명. blog 폴더가 생성되며, 블로그 개발을 위한 기본 구조가 포함된다.\n즉, blog 폴더에 기본적인 React 환경이 준비되어, 바로 블로그 프로젝트 작업을 시작할 수 있다.\n시간 지연 React 프로젝트를 생성하는 데 시간이 걸릴 수 있으며, 사용자에 따라 다소 차이가 있을 수 있다.\n(나의 경우, 약 5분 정도 걸렸다.)\nVS 코드의 왼쪽에 위치한 파일 탐색기에서 프로젝트 폴더와 파일들이 생성되는 것을 확인할 수 있다.\n모든 파일 생성이 완료되면 터미널에 “성공적으로 작업이 완료되었습니다”와 같은 메시지가 나타나며, 프로젝트 생성이 완료된 것을 알 수 있다.\n설치 완료 생성된 blog 폴더로 이동하여 프로젝트 파일들을 확인할 수 있다.\n아래 명령어로 blog 폴더로 이동한다.\n\n폴더 이동 명령어\ncd blog\n프로젝트 폴더 구성 설명 1. node_modules\n설치된 라이브러리들이 포함된 폴더.\n프로젝트의 모든 종속성(dependencies)을 담고 있으며, package.json에 명시된 라이브러리를 자동으로 설치해 준다.\n\npublic\n\n공용 파일들이 저장된 폴더로, index.html과 같은 파일이 여기에 포함된다.\n리액트 프로젝트에서 실제 HTML 페이지는 이 index.html을 기반으로 하며, 앱이 빌드될 때 이 파일을 활용하여 최종 사용자에게 보여진다.\n\nsrc\n\n실제 개발 작업을 하는 주요 코드들이 들어 있는 폴더.\n컴포넌트, 스타일시트, 이미지 등 프로젝트 코드 파일을 포함하며, App.js 등 주요 리액트 컴포넌트들이 여기에 위치한다.\n이 구조를 통해 프로젝트 파일들을 효율적으로 관리하고, 나중에 퍼블리시할 때 최적화된 파일들이 빌드된다.\nReact 프로젝트에서 각 폴더의 역할 (1) public 폴더 정적 리소스 폴더로, 앱이 빌드되거나 프로젝트가 퍼블리시될 때 파일이 압축되지 않고 그대로 유지되는 파일들이 위치한다.\nReact 프로젝트에서는 기본적으로 public 폴더를 정적 파일들을 위한 폴더로 사용하지만, 가끔 static이라는 이름의 폴더를 따로 추가해 정적 파일을 관리하는 경우도 있다.\npublic 폴더와 static 폴더의 차이는, 프로젝트의 규모와 필요에 따라 정적 리소스를 구분 관리하고 싶을 때 static 폴더를 사용한다.\nindex.html React 애플리케이션의 진입점으로, 가장 먼저 로드된다. 모든 React 컴포넌트가 이 HTML 파일 안의\n\n\n\n태그 안에 렌더링된다.\n보통 수정할 일이 거의 없다.\n주요 역할 1. 자주 바뀌지 않는 이미지, 파비콘, 폰트 파일 등 변하지 않는 리소스를 보관한다.\n\n이 폴더의 파일들은 빌드 시 별도로 최적화되지 않으며, 그대로 클라이언트에 전달된다.\n\n\nsrc 폴더\n\n코딩 작업의 중심으로, React 애플리케이션의 컴포넌트, 스타일, 로직 등이 여기에 위치한다.\nApp.js 애플리케이션의 메인 컴포넌트. 전체 앱 구조의 기본을 제공하고, 대부분의 기능 구현이 시작되는 핵심 파일.\nindex.js App.js를 DOM에 마운트하고 애플리케이션을 브라우저에 연결하는 역할을 한다. ReactDOM.render()로 App 컴포넌트를\n\n\n\n에 렌더링한다.\n주요 역할 1. 컴포넌트 파일, CSS 파일, JavaScript 로직 파일이 위치하며, 애플리케이션의 동적 기능이 모두 여기에서 구현된다.\n\nsrc 폴더의 파일에서 변경된 내용이 index.html을 통해 화면에 실시간으로 반영되므로, 앱의 동적 업데이트와 유지보수가 용이하다.\n\nsrc 폴더는 React 프로젝트의 핵심이며, 코딩과 개발의 중심이 된다.\n프로젝트 실행 npm start 명령어를 입력하여 개발 서버를 시작한다.\n코드에 변경 사항이 생기면 자동으로 웹 브라우저에 반영되어 실시간으로 확인할 수 있다.\n개발 서버는 기본적으로 Chrome 브라우저를 자동으로 열어주지만, 자동으로 열리지 않는 경우 Chrome 주소창에 localhost:3000 을 입력해 접속할 수 있다.\ndiv 태그 웹 페이지의 레이아웃을 설정하는 기본적인 요소이다. 여러 개의 화면 블록을 나누거나 레이아웃을 조정할 때 주로 사용된다.\nCSS 스타일 속성 1. padding\n요소 내부 여백으로, 내용(텍스트나 이미지)과 요소 테두리 간의 공간을 조절한다. 패딩 값을 주면 요소 내부의 글이나 이미지가 테두리와의 간격이 넓어진다.\n\nmargin\n\n요소 외부 여백으로, 요소와 다른 요소 사이의 간격을 설정한다. 마진을 통해 요소들이 서로 떨어져 보이게 만들 수 있다.\n이러한 속성을 조정하면서 웹 페이지 레이아웃을 구성하고, 원하는 배치와 간격을 설정할 수 있다.\n서버 서비스를 제공하는 쪽으로, 클라이언트(고객)로부터 요청된 데이터를 처리하고 응답한다.\n클라이언트 데이터를 요청하는 쪽으로, 일반적으로 웹 브라우저에서 사용자(고객)가 보는 인터페이스이다. React는 클라이언트에서 데이터를 받아 동적으로 화면에 보여주는 역할을 한다.\nJavaScript XML (JSX) JavaScript 코드 안에서 HTML 태그처럼 작성할 수 있도록 해주는 문법.\nHTML과 JavaScript를 한 파일에서 결합하여 작성하므로, 인터페이스와 로직이 밀접하게 연결된다.\n이로 인해, 코드의 가독성과 유지보수성이 높아진다.\n데이터 바인딩 변수를 템플릿 문자열({ })을 통해 UI에 연결하여 실시간으로 데이터를 반영하는 것이다. src, id, href와 같은 속성에 변수를 동적으로 연결하여, 콘텐츠가 바뀌면 UI도 즉시 반영되게 할 수 있다.\nconst logoUrl = “https://example.com/logo.png”; return ;\n사용 예시: 네이버 로고 기념일에 따라 로고가 바뀌는 것처럼, 디자이너가 제공하는 기념일 로고 이미지를 서버에 업로드하고, React 코드에서 해당 이미지를 불러온다.\n이름이나 URL을 바꾸기만 하면 새로운 로고가 적용되므로, 서버와 클라이언트에서 데이터만 업데이트해도 쉽게 반영된다.\nReact에서 style 속성은 객체 형태로 넣어야 하며, { } 안에 작성한다. 이처럼 React는 JSX와 데이터 바인딩을 통해 동적이고 유연하게 UI를 생성하고 관리할 수 있다."
  },
  {
    "objectID": "pd/re/re_07.html",
    "href": "pd/re/re_07.html",
    "title": "React를 이용한 Blog 만들기(3)",
    "section": "",
    "text": "React Component UI의 구성 요소.\n재사용 가능하고 독립적인 UI를 표현하는 함수 또는 클래스. 컴포넌트 기반 구조를 통해 UI를 작고 관리하기 쉬운 단위로 나눌 수 있어 유지보수성이 높아진다.\nIT 관련 업계에서 사용되는 용어는 종종 부품이나 퍼즐 조각으로 비유할 수 있다. 각 부품이나 조각은 독립적으로 기능을 하면서 전체 시스템의 일부분을 이루는 역할을 한다. 이러한 방식은 복잡한 시스템을 구성하고 유지보수하기 위해 필수적이다.\n파이썬에서는 이를 모듈로 부르며, 각 기능을 모듈 단위로 나누어 관리한다. 자바에서는 패키지라는 개념을 사용하여, 관련된 클래스를 묶어 관리한다. 이와 같은 구조적 분리는 코드를 더 읽기 쉽게 하고, 유지보수와 확장성을 향상시킨다.\n리액트(React)는 이러한 개념을 컴포넌트 단위로 나누어 관리하는 방식으로, UI를 더 작은 부품으로 나누어 관리하고 재사용할 수 있게 해줍니다. 이로 인해 복잡한 애플리케이션을 보다 효율적으로 개발할 수 있다.\n또한, C 언어에서는 분리 컴파일을 사용하여 소스 코드를 독립적인 단위로 나누고, 자바에서는 클래스를 사용하여 객체 지향적으로 코드의 재사용성과 모듈화를 제공한다. 이러한 방식들은 코드의 복잡도를 줄이고, 협업 시 충돌을 방지하며, 각 기능을 독립적으로 테스트하고 관리할 수 있게 해준다.\nComponent 종류 1. Functional Components (함수형 컴포넌트) 함수 형태로 정의된 컴포넌트로, 주로 useState, useEffect와 같은 React Hook을 사용해 상태와 생명주기를 관리한다.\n\nClass Components (클래스형 컴포넌트) ES6 클래스 문법을 사용해 정의되며, this.state와 this.setState를 통해 상태를 관리하고, 생명주기 메서드를 제공한다.\n\n생명주기, 온크레이트, 다른 페이지로 띄울 때, 온퍼지 일시 정지 상태 다시 원래 상태로 돌아올 때 까지의 상태.\n(React 16.8 이후로는 주로 함수형 컴포넌트를 권장한다.)\nProps 부모 컴포넌트에서 자식 컴포넌트로 전달하는 데이터. 컴포넌트 간의 데이터 전달을 담당하며 변경할 수 없는(read-only) 값\nState 컴포넌트 내에서 관리되는 동적 데이터. 상태가 변경될 때마다 컴포넌트가 다시 렌더링된다.\nHooks 함수형 컴포넌트에서 상태(state)와 생명주기(lifecycle)를 관리할 수 있게 해주는 기능이다. 이전에는 클래스형 컴포넌트에서만 사용할 수 있었던 상태 관리와 생명주기 관련 기능을 함수형 컴포넌트에서도 사용할 수 있도록 만들어준다.\n“Hooks”라는 이름은 후크나 갈고리를 의미하는데, 이는 말 그대로 어떤 기능을 “훅”으로 걸어 넣는다는 느낌으로 비유할 수 있다.\n외투의 후크: 외투의 끝부분에 달린 후크처럼, Hook은 특정 기능을 기존 코드에 “걸어놓는” 역할을 한다. 지퍼의 끝부분: 후크는 지퍼처럼, 여러 기능을 하나의 “구조” 안에서 잘 맞물리게 해준다. 피터팬의 후크 선장: 피터팬에서 악역으로 등장하는 후크 선장은 갈고리 손을 가진 인물로 알려져 있다.\n주요 React Hooks 1. useState\n상태를 관리하기 위한 기본 Hook 함수형 컴포넌트에서 로컬 상태를 선언하고 관리할 수 있게 합니다.\n초기값을 인자로 받아 상태값과 상태 갱신 함수 [state, setState]를 반환한다.\n\nuseEffect\n\n컴포넌트가 렌더링될 때나 상태가 변경될 때 특정 코드를 실행할 수 있게 하는 Hook. 두 번째 인자로 의존성 배열을 받으며, 이 배열에 명시된 값이 변경될 때마다 콜백 함수가 재실행된다.\n주로 데이터 가져오기, 구독, 타이머 설정 등에 사용된다.\n\nuseContext\n\nContext API와 함께 사용하여 컴포넌트 트리 전반에 데이터를 전달한다.\nprops를 계속 전달할 필요 없이 하위 컴포넌트에서 바로 데이터에 접근할 수 있어 전역 상태 관리에 유용하다.\n고급 Hooks 1. useReducer\n상태를 관리하기 위해 useState보다 복잡한 로직이 필요할 때 사용하는 Hook.\n리듀서 함수와 초기 상태를 받아 상태와 디스패치 함수를 반환하며, Redux와 유사한 패턴으로 상태를 관리할 수 있다.\n\nuseMemo\n\n특정 값의 계산을 메모이제이션하여, 의존성이 변하지 않는 한 동일한 값을 반환하도록 한다. 계산 비용이 큰 함수의 결과를 캐싱하여 성능 최적화에 유용하다.\n\nuseCallback\n\n메모이제이션된 콜백 함수를 반환한다.\n특정 함수가 재생성되는 것을 방지하고, 주로 의존성 배열이 변하지 않는 한 동일한 함수를 재사용하도록 해 성능을 개선할 수 있습니다.\n이 Hook들을 조합해 복잡한 상태 관리와 최적화 작업을 효율적으로 수행할 수 있다.\nuseEffect로 대체하기 함수형 컴포넌트에서는 생명주기 메서드를 직접 사용할 수 없으므로, useEffect Hook을 통해 각 생명주기 단계에서 필요한 동작을 구현한다.\n\nMounting 컴포넌트가 처음 DOM에 추가되는 단계.\n\ncomponentDidMount 메서드를 사용해 첫 번째 렌더링 후 실행할 코드를 작성한다.\n함수형 컴포넌트에서는 useEffect의 빈 배열 []을 의존성 배열로 설정하여, 컴포넌트가 처음 렌더링될 때 한 번만 실행되도록 한다.\n\nUpdating 상태나 props가 변경될 때 컴포넌트가 업데이트되는 단계.\n\ncomponentDidUpdate 메서드를 사용해 상태나 props가 변경될 때 실행할 코드를 작성한다.\n함수형 컴포넌트에서는 useEffect의 의존성 배열에 특정 상태나 props를 넣어 해당 값이 변경될 때마다 코드가 실행되도록 설정한다.\n\nUnmounting 컴포넌트가 DOM에서 제거되는 단계.\n\ncomponentWillUnmount 메서드를 사용해 컴포넌트가 제거되기 직전에 실행할 코드를 작성한다.\n함수형 컴포넌트에서는 useEffect의 반환 함수에 cleanup 코드를 작성하여 컴포넌트가 언마운트될 때 해당 코드가 실행되도록 한다.\n모바일 앱이나 복잡한 웹 애플리케이션의 경우, 생명주기는 8단계로 나눌 수 있다.\n실습\n\n\n제목\n\n\n날짜\n\n\n상세내용\n\n\nReact에서 HTML 코드를 줄이고 재사용하기 위해 컴포넌트를 활용할 수 있다. 특히 함수형 컴포넌트는 간단하게 특정 UI를 캡슐화하고, 다른 곳에서 쉽게 사용할 수 있어 유용하다.\n컴포넌트 작성 방법 1. 함수 이름 짓기 함수형 컴포넌트의 이름은 대문자로 시작해야 한다. (예시: Modal, Header, Footer 등)\n\nHTML 코드 축약 축약할 HTML 코드를 return() 안에 작성한다. 여러 태그가 있을 경우, 반드시 하나의 루트 태그로 감싸야 한다.\n\n여러 태그를 감싸기 위해\n\n태그를 사용할 수 있으며, 빈 태그(Fragment) &lt;&gt;&lt;/&gt;를 사용할 수도 있다. &lt;&gt;&lt;/&gt;(Fragment)을 사용할 경우 불필요한 HTML 구조가 생기지 않아 코드가 깔끔해진다.\n\n컴포넌트 적용하기 컴포넌트를 원하는 위치에 &lt;컴포넌트명 /&gt; 형태로 작성하여 삽입한다.\n\n위 규칙을 지키지 않으면 컴포넌트가 제대로 렌더링되지 않거나 에러가 발생할 수 있다.\n실습2\n\n  &lt;Modal/&gt;      \n&lt;/div&gt;\n); } function Modal(){ return(\n\n&lt;h2&gt;제목&lt;/h2&gt;\n&lt;p&gt;날짜&lt;/p&gt;\n&lt;p&gt;상세내용&lt;/p&gt;\n\n); }\nexport default App;\n반복적으로 사용되는 HTML UI 덩어리들을 컴포넌트로 만들어서 관리하는 것은 매우 효율적이다. 컴포넌트를 재사용 가능한 작은 단위로 만들면 코드 유지보수가 용이해지고, UI 요소를 다른 페이지에서도 손쉽게 사용할 수 있기 때문이다.\n장점 동일한 UI를 여러 페이지에서 사용하려면 컴포넌트를 만들어 재사용할 수 있다. 한 번 정의한 컴포넌트는 여러 곳에서 사용할 수 있어 코드 중복을 줄일 수 있다. 앱을 여러 작은 컴포넌트로 분리하여 앱을 더 구조적으로 관리할 수 있다.\n단점 1. State 관리 복잡성\n컴포넌트에서 상태(state)를 관리할 때, 상위 컴포넌트와 하위 컴포넌트 간의 데이터 흐름이 복잡해질 수 있다.\n여러 컴포넌트에서 상태를 변경해야 할 때, 상태를 올바르게 관리하는 것이 중요한데, useState와 props를 잘 이해하고 사용해야 한다.\n\nprops를 통한 상태 전달\n\n상위 컴포넌트에서 관리하는 상태를 하위 컴포넌트에서 사용하려면 props를 사용해야 한다. 이때, props를 통해 전달된 데이터는 하위 컴포넌트에서 변경할 수 없고, 부모 컴포넌트에서만 업데이트할 수 있다. 이로 인해 상태 업데이트가 필요한 경우, 상태 변경을 위한 함수를 props로 전달해야 할 수도 있다.\n  &lt;/div&gt;\n  &lt;Modal title = {글제목[0]}/&gt;      \n&lt;/div&gt;\n); } function Modal(props){ return(\n\n  &lt;h2&gt;{props.title}&lt;/h2&gt;\n  &lt;p&gt;날짜&lt;/p&gt;\n  &lt;p&gt;상세내용&lt;/p&gt;\n\n); }\nexport default App;\nimport React, { useState } from ‘react’; import ‘./App.css’;\nfunction App() { let [글제목, 글제목변경] = useState([“소고기 맛집-토방”, “불고기 맛집-마포숯불갈비”, “한식 맛집-백만석”]); let [따봉, 따봉변경] = useState(0); let [선택된제목, 선택된제목변경] = useState(글제목[0]); // 초기값 설정\nreturn (\n\n  &lt;div className=\"black-nav\"&gt;\n    &lt;div&gt;대진대 맛집 소개&lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; 선택된제목변경(글제목[0])}&gt;\n        {글제목[0]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 4일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; 선택된제목변경(글제목[1])}&gt;\n        {글제목[1]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 5일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;hr/&gt;\n  &lt;div&gt;  \n    &lt;div className='list'&gt;\n      &lt;h4 onClick={() =&gt; 선택된제목변경(글제목[2])}&gt;\n        {글제목[2]} &lt;span onClick={() =&gt; 따봉변경(따봉 + 1)}&gt;👍&lt;/span&gt; {따봉}\n      &lt;/h4&gt;\n      &lt;p&gt;11월 6일 발행&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  {선택된제목 && &lt;Modal title={선택된제목} /&gt;}\n&lt;/div&gt;\n); }\nfunction Modal(props) { return (\n\n  &lt;h2&gt;{props.title}&lt;/h2&gt;\n  &lt;p&gt;발행 날짜: {props.title === '소고기 맛집-토방' ? '11월 4일' : props.title === '불고기 맛집-마포숯불갈비' ? '11월 5일' : props.title === '한식 맛집-백만석' ? '11월 6일' : ''}&lt;/p&gt;\n  &lt;p&gt;이곳은 {props.title}에 대한 상세내용입니다.&lt;/p&gt;\n&lt;/div&gt;\n); }\nexport default App;\n이런 식으로 컨퍼넌트를 만들어서 관리할 수 있다."
  },
  {
    "objectID": "pd/re/re_09.html",
    "href": "pd/re/re_09.html",
    "title": "React를 이용한 Blog 만들기(5)",
    "section": "",
    "text": "개발자는 수동화로 하는 방법의 불편함을 알아야 함 자동화 도중 발생하는 오류를 해결하기 위해 필수적임 빅데이터의 기본은 데이터베이스임\n1 . Node.js 설치하기 (설치했음) 작업 폴더(todoapp)를 만들고 오픈하기\n\nExpress 라이브러리 사용하기 (Terminal) npm init 셋팅하기-entry point : server.js로 설정하고 나머진 그냥 Enter키\n\n(Terminal) npm install express로 설치하기\nnode_modules 폴더가 생성되고 패키지가 설치된다.\n(VS-왼쪽탐색창) new file로 server.js 만들기\n서버가 뭔가요? 기술면접, 고기집 서빙-서버(서비스를 제공해주는 것), 서브 포트 번호는 통로를 열어주도록 서버에 지정하는 것, 네트워크 프로그램(채팅, 웹서비스, 이메일) 같은 통로로 들어오면 안 됨, 다 달라야 함, 그래서 포트 번호가 서로 다름 이것을 포함한 것이 URL이고, 현재는 암묵적으로 80을 씀 지금 실습은 8080이라는 특별한 번호를 쓴 것 100이하는 예약이 있어서 함부로 쓰면 안 됨,\n(VS-왼쪽탐색창) new file로 server.js 만들기. Server.js 파일을 열고 아래 코드 작성하기.\nGet 요청 처리하기:특정 URL을 방문하면 보여지는 것.\n’/pet’에 방문하면 ’펫용품 쇼핑할 수 있는 페이지입니다.’라는 메시지가 보이도록 요청해보자!\nServer.js 수정후 ctrl + c로 서버 중지한 뒤, 다시 서버 가동한다. (node server.js) 브라우저에서 localjost:8080/pet을 작성하여 확인한다.\n응용 해보기 /beauty를 방문하면, ‘뷰티용품을 쇼핑할 수 있는 페이지입니다’ 가 나오도록 작성하기.\n매번 수정할 때마다 서버를 중지하고 재실행하는건 매우 번거로운일이다.\n자동화로 바꾸는 방법 (nodemon 설치)\nnpm install –g nodemon\n설치 후 실행할 경우, 에러가 날 것이다.\nnodemon server.js\n스크립트 실행 제한으로 인한 현상이다.\n스크립트 실행 제한 풀기 Windows Power shell을 관리자 권한으로 실행한다. executionpolicy 입력 시, Restricte 라고 나온다.\nset-executionpolicy unrestricted 실행한 뒤, 실행 화면에서 y(yes)를 입력한다.\nGet 요청 변경\nLocalhost:8080/까지만 브라우저에 입력하면 띄울 페이지 만들기-index.html\nGet 요청 변경\nindex.html 만들기, 코드 작성(기본 템플릿코드임)\n우선 template.html 파일을 생성하고 아래 코드를 작성한다. 작성된 코드를 복사하여 index.html파일에 붙여넣는다.\nGet 요청 변경\n\n태그\n웹 페이지가 다양한 장치와 브라우저에서 어떻게 렌더링될지를 제어한다.\n\n\n\n이 태그는 웹 페이지가 모바일 디바이스나 다양한 화면 크기에 적응하도록 브라우저가 뷰포트를 설정하는 방식을 지정한다.\nname=“viewport”: 뷰포트 설정을 지정하는 태그임을 나타낸다.\ncontent 속성:\ndevice-width: 디바이스의 물리적인 화면 너비(픽셀 크기)에 맞추어 뷰포트를 설정.\ninitial-scale=1.0: 초기 확대/축소 비율을 1로 설정합니다. 즉, 1픽셀은 디바이스의 1픽셀로 보여지도록 함.\n역할:\n모바일 장치에서 적절한 크기로 콘텐츠를 보여줌.\n화면 크기에 관계없이 콘텐츠가 스크롤 없이 잘 보이게 함.\n현재는 edge 브라우저로 바꿔었는데, 이러한 변화는 보안상에 문제가 잦았기에 발생한 것이다. 그래서 사람들이 크롬을 쓰기 시작했고, 이후 크롬 브라우저로 몰리게 된다.\n마이크로소프트웨어는 급하게 edge 브라우를 개발했으나 사람들은 돌아오지 않았다. 현재는 크롬에서 파이어 폭스로도 이동하는 추세이며, 크롬에서도 보안상에 문제가 있다.\n개인정보를 가져가는 이유는 무료로 마케팅에 사용하기 위해서이다. 파이어 폭스는 비영리 기업이기에 더욱 안전하다.\n\n\n\n이 태그는 Internet Explorer 브라우저에서 웹 페이지의 호환성 모드를 설정.\nhttp-equiv=“X-UA-Compatible”: 브라우저가 페이지를 어떻게 렌더링할지 지시.\ncontent=“ie=edge”: 최신 렌더링 엔진(Edge 모드)을 사용하도록 지정한다. 즉, IE가 과거 호환성 모드(예: IE7, IE8)를 사용하지 않고 최신 기능을 활용한다.\n역할:\nIE에서 최신 웹 표준을 최대한 활용하게 한다. 레거시(구형) IE 모드로 인해 발생할 수 있는 스타일 깨짐이나 비표준 동작을 방지한다.\nHTML/CSS 디자인 – bootstrap 4\nBootstrap\nPowerful, extensible, and feature-packed frontend toolkit. Build and customize with Sass, utilize prebuilt grid system and components, and bring projects to life with powerful JavaScript plugins.\ngetbootstrap.com\n우측 상단의 버전을 4.6.x로 선택한다.\n왼쪽 탐색창에서 Getting started 클릭한다.\n해당 페이지 하단 아래에 Started template 코드가 있다. 복사하여 Index.html 코드에 붙여넣는다.\n&lt;!doctype html&gt;\n\n\n\n\n\n&lt;!-- Bootstrap CSS --&gt;\n&lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css\" integrity=\"sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N\" crossorigin=\"anonymous\"&gt;\n\n&lt;title&gt;Hello, world!&lt;/title&gt;\n\n\n\nHello, world!\n\n\n\n\n\n\n\n\n\nButton, Navbar 검색하여 붙여넣기 해보기\nol, ul, il의 정의\nNavbar 수정하기\nNav -&gt; ToDo app\nFeature -&gt; Write\nPricing -&gt; 임시\n메인 글 수정하기\nJumbotron 검색\n첫번째 내용 복사 붙여넣기- 아래 처럼 수정\nWrite 발행 메뉴 수정하기\nWrite.html파일을 만들어 form 만들기"
  },
  {
    "objectID": "pd/re/re_11.html",
    "href": "pd/re/re_11.html",
    "title": "React를 이용한 Blog 만들기(6)",
    "section": "",
    "text": "목록을 적성하기.\n1 . 자 료 의 입 력\nCloud: MongoDB Cloud\naccount.mongodb.com\nHtml 파일에 서버데이터 넣는 방법? 데이터가 왔다가는 건 매우 어려움\n–template engine 사용 Pug/ejs/Handlebars…… 가장쉽고 간단한 ejs를 사용 다른 엔진은 자체 문법을 공부해야하지만, ejs는 자바스크립트 코드를 그대로 쓸 수 있음.\n–Ejs 설치 •&gt;&gt;npm install ejs 설치후 server.js 안에 ejs 설정하기\n•Html 파일에 데이터 넣기 –Ejs파일 만들기 •views폴더를 만들고 그 안에 list.ejs 생성 List.ejs코드와 main.css 코드 작성\n•Main.css파일 작성 –우선 프로젝트 폴더에 public 폴더 생성 -&gt; main.css 생성후 작성 –Server.js에 다음 코드 추가하기\n•데이터베이스 데이터를 ejs에서 사용하려면 –&lt;%=데이터 이름 %&gt;로 감싸서 사용 –HTML 코드를 ejs에서 사용하려면 위 특수태그를 사용하면 된다. –Html안에 &lt;%=데이터 이름 %&gt;을 쓰면 object 자료형이 깨져서 나옴 –&lt;%=JSON.stringify(데이터 이름) %&gt;으로 사용하면 데이터가 나옴.\n•/time 이라고 접속하면 현재 서버의 시간을 보내주는 기능을 만들어보자. –Ejs로 웹페이지를 만들어서 거기 안에 서버의 시간을 박아넣어서 보내주면 됨. –(팁) 서버의 시간은 server.js 파일 아무데서나 new Date() 라고 쓰면 나옴\n•데이터베이스 데이터를 ejs에서 사용하려면 –&lt;%=데이터 이름 %&gt;로 감싸서 사용 –HTML 코드 및 자바스크립트를 ejs에서 사용하려면 위 특수태그(&lt;% %&gt;)를 사용하면 된다. –Html안에 &lt;%=데이터 이름 %&gt;을 쓰면 object 자료형이 깨져서 나옴 –&lt;%=JSON.stringify(데이터 이름) %&gt;으로 사용하면 데이터가 나옴. –자바스크립트 반복문을 사용하여 여러 개의 게시물을 화면에 보이도록 하자.\n•지금까지 코드엔 네비게이션 바가 안보인다?! –Index.html의 네브바 코드만 복사에서 붙여넣으면 되지만, 모든 페이지마다 이러한 작업을 하는건 매우 비효율적이다. –Nav.ejs를 만들고, 네브바 코드를 넣은뒤, 이걸 포함시키는 방법이 더 효과적이다. –이를 위해 &lt;%- include(‘nav.ejs’) %&gt;를 사용하자.\n서버에 HTTP 요청하는법 서버로 진짜 요청 날리려면\nREST원칙\n좋은 URL 작명방법\nMethod : GET, POST, PUT, UPDATE, DELETE\n\nGET : 서버에 데이터 출력요청할때\nPOST : 서버에 데이터 입력요청할때\nPUT,UPDATE: 서버에 데이터 수정요청할때\nDELETE : 서버에 데이터 삭제요청할때\nURL : /어쩌구\n\n–POST요청은 어떻게?\n\n태그 이용 –REST(Representation state transper) API •좋은 API 디자인하는 원칙 6개 1. 일관성 있는 URL이 좋음 - 2가지 역할 금지 »하나의 URL+method는 하나의 데이터를 보내야 좋음\n\n유저에게 서버역할을 맡기지 마라\n요청끼리 서로 의존성이 있으면 안된다. - 독립적\n요청은 캐싱이 가능해야 한다.\n\n여기서, 캐싱이란 자주 수신되는 자료들은 요청을 날리지 않고 하드에 저장해놓고 쓰도록 하는 기술\nLayered System :요청하나는 최종 응답전까지 여러 단계를 거쳐도 된다.(잘 몰라도 됨)\nCode on demand : 서버는 유저에게 실행가능한 코드를 보내줄 수도있음. (잘 몰라도 됨)\n좋은 URL 작성법 1. 동사보다는 명사 위주로\n\n띄어쓰기는 언더바_대신 대시-기호\n파일 확장자 쓰지말기(.html)\n하위 문서들을 뜻할땐 /기호를 사용함(하위 폴더같은 느낌)\n\n다음과 같은 문제를 풄 있어야 된\nQ1.facebook.com/bbc/photos로 GET요청하면 어떤 데이터 보여줄까?\nBBC뉴스 이미지를 보여준다.\nQ2.Instagram.com/explore/tags/food로 GET요청하면 어떤 데이터 보여줄까?\n글 작성 페이지와 form 만들고, 서버는 전송받은 글 확인\n글 작성기능? 1.글작성 페이지에서 글 써서 서버로 전송 2.서버는 글을 검사 3.이상없으면 DB저장 –Views/write.ejs 생성\nwrite.ejs 코드-list.ejs코드를 복사 붙여넣고 아래처럼 수정\n•글 작성페이지와 form 만들기 –버튼을 클릭한다고 입력이 되는가? NO!!\nUrl과 method를 지정해야 함!! –form태그의 속성으로 action과 method를 지정해야 한다 –또한 input태그에 name속성을 지정해야 입력된다.\n서버로 글 전송(server.js) res.body를 쓰면 유저가 보낸 데이터 출력가능 쓰기 전에 셋팅이 필요\n–Localhost/write페이지에서 글 입력하고 서버로 데이터 전송뒤 콘솔로 터미널에 출력하기 –DB 저장은? 앞서서 공부했음.알아서 해보자!! ^^;;;\nvjqmfflt"
  },
  {
    "objectID": "ch/os_01.html",
    "href": "ch/os_01.html",
    "title": "운영체제(OS)",
    "section": "",
    "text": "운영체제(OS)의 전반적인 개념에 대해 다루고자 한다.\n\n01 서론\n초기에는 운영체제가 단순한 하드웨어 제어에 집중했으나, 이후에는 소프트웨어 기능이 추가되면서 점점 발전해왔다.\nWindows 운영체제의 소스 코드는 공개되어 있지 않으며, 사용자는 단순히 라이선스를 구매하여 사용할 수 있다.\n반면, 최근에는 오픈 소스 코드가 활성화되면서 리눅스(Linux) 와 같은 오픈 소스 운영체제가 등장했다.\n즉, 운영체제의 내부 동작을 깊이 이해하고 싶다면, 리눅스를 활용해보는 것이 좋은 방법이다.\n이 챕터에서 배울 수 있는 것.\n① 컴퓨터 시스템의 일반적 구성과 인터럽트의 역할에 대한 기본 지식을 기술한다.\n② 현대 다중프로세서 컴퓨터 시스템에서의 구성요소에 대해서 기술한다.\n③ 사용자 모드로부터 커널 모드로의 변경에 대해서 기술한다.\n④ OS가 다양한 컴퓨팅 환경에서 어떻게 사용되는지 논의한다.\n⑤ Free와 Open Source OS 예제를 제공한다.\n02 운영체제 (Operating System)\n컴퓨터의 하드웨어를 관리하는 프로그램.\n응용 프로그램을 위한 기반을 제공하며, 컴퓨터 사용자와 컴퓨터 하드웨어 사이에서 중재자 역할을 수행하는 프로그램이다.\nOperating System Market Share Worldwide | Statcounter Global Stats\nThis graph shows the market share of operating systems worldwide based on over 5 billion monthly page views.\ngs.statcounter.com\nhttps://cdn.jsdelivr.net/npm/echarts/dist/echarts.min.js\nvar chart = echarts.init(document.getElementById('chart'));\nvar option = {\n    title: {\n        text: '전 세계 운영체제 시장 점유율',\n        subtext: '2025년 2월',\n        left: 'center'\n    },\n    tooltip: {\n        trigger: 'item'\n    },\n    legend: [\n        {\n            orient: 'horizontal',\n            left: 'center',\n            top: '93%',\n            itemWidth: 20,\n            itemHeight: 10,\n            data: ['Android', 'Windows', 'iOS']\n        },\n        {\n            orient: 'horizontal',\n            left: 'center',\n            bottom: '0%',\n            itemWidth: 20,\n            itemHeight: 10,\n            data: ['macOS', 'Unknown', 'Linux', 'Others']\n        }\n    ],\n    series: [\n        {\n            name: '전체 비율',\n            type: 'pie',\n            radius: '70%',\n            center: ['50%', '53%'],\n            data: [\n                { value: 45.49, name: 'Android', label: { show: true, position: 'inner', color: '#fff', fontSize: 25, fontWeight: 'bold', textBorderColor: '#000', textBorderWidth: 2, formatter: '{b}\\n{d}%' } },\n                { value: 25.35, name: 'Windows', label: { show: true, position: 'inner', color: '#fff', fontSize: 20, fontWeight: 'bold', textBorderColor: '#000', textBorderWidth: 2, formatter: '{b}\\n{d}%' } },\n                { value: 18.26, name: 'iOS', label: { show: true, position: 'inner', color: '#fff', fontSize: 20, fontWeight: 'bold', textBorderColor: '#000', textBorderWidth: 2, formatter: '{b}\\n{d}%' } },\n                { value: 5.67, name: 'macOS', label: { show: true, position: 'outside', formatter: '{b}: {d}%' } },\n                { value: 2.97, name: 'Unknown', label: { show: true, position: 'outside', formatter: '{b}: {d}%' } },\n                { value: 1.38, name: 'Linux', label: { show: true, position: 'outside', formatter: '{b}: {d}%' }, itemStyle: { color: '#DDA0DD' } },\n                { value: 0.88, name: 'Others', label: { show: true, position: 'outside', formatter: '{b}: {d}%' }, itemStyle: { color: '#808080' } }\n            ],\n            emphasis: {\n                itemStyle: {\n                    shadowOffsetX: 0,\n                    shadowColor: 'rgba(0, 0, 0, 0.5)'\n                }\n            }\n        }\n    ]\n};\nchart.setOption(option);\nOS의 목적 컴퓨터 시스템을 효율적으로 운영하고 사용자가 보다 편리하게 작업할 수 있도록 돕는 중요한 소프트웨어이다.\n1 . 사용자 프로그램 실행 지원 사용자가 원하는 프로그램을 실행할 수 있도록 지원하며, 이를 통해 다양한 소프트웨어를 활용할 수 있다.\n2 . 사용자 문제 해결 지원 파일 관리, 프로세스 제어, 네트워크 연결 등 여러 기능을 제공하여 사용자가 보다 쉽게 문제를 해결할 수 있도록 한다.\n3 . 컴퓨터 시스템 접근성 향상 하드웨어와 소프트웨어 간의 인터페이스 역할을 하며, 사용자가 명령어나 그래픽 인터페이스(GUI)를 통해 쉽게 시스템을 제어할 수 있도록 한다.\n4 . 하드웨어 자원 효율적 관리 CPU, 메모리, 저장 장치, 입출력 장치 등의 하드웨어 자원을 효율적으로 관리하여 여러 프로그램이 원활하게 실행될 수 있도록 한다.\n03 처리 방식의 분류\n1 . 아날로그 컴퓨터 연속적인 데이터를 처리하는 방식.\n물리적인 값(온도, 속도, 압력 등)을 전기적인 신호로 변환하여 처리한다. 연속적인 데이터의 변화를 실시간으로 계산하거나 제어하는 데 유용하다.\n연속적인 데이터를 기반으로 처리하는 특성 때문에 실시간 제어 시스템이나 복잡한 물리적 시스템 모델링에 사용된다.\n전자 회로 시뮬레이션, 항공기 비행 제어 시스템, 온도 조절 시스템 등이 있다.\n2 . 디지털 컴퓨터 이산적인 데이터를 처리하는 방식.\n데이터를 0과 1의 이진수로 변환하여 계산한다.\n대부분의 현대 컴퓨터는 이 방식을 사용하며, 고속 데이터 처리, 복잡한 계산, 소프트웨어 기반 작업을 처리할 수 있다. 특히 정보 처리 및 저장, 소프트웨어 개발, 대규모 데이터 분석 등 다양한 용도로 사용된다.\n일반적인 개인용 컴퓨터, 서버 컴퓨터, 슈퍼컴퓨터 등이 있다.\n04 OS의 분류\n1 . 대형 및 중형 컴퓨터 OS (Mainframe / Mid-range Computer, 미니컴퓨터)\n하드웨어 자원의 효율적인 활용에 중점을 둔 OS.\n금융, 공공기관, 연구소 등에서 대량의 데이터를 처리하는 데 사용되며, 다수의 사용자가 동시에 접속하여 작업할 수 있도록 관리한다.\n하드웨어의 성능을 최대한 끌어올리는 것이 주된 목표이다. 높은 안정성과 신뢰성을 제공하며, 대량의 데이터 처리 및 다중 사용자 지원한다.\n① IBM 기반 OS IBM z/OS; 메인프레임(기업용), → 금융, 은행, 대기업 서버에서 사용된다.\nIBM Blue Gene OS; 슈퍼컴퓨터(연산용). → 과학 연구, AI, 기후 분석, 유전체 연구에 사용된다.\n② Unix 계열 AIX (Advanced Interactive eXecutive) ; IBM POWER 아키텍처 기반 서버 → 고성능 컴퓨팅, 비즈니스 크리티컬 시스템에 사용된다.\nHP-UX (Hewlett Packard Unix) ; HPE PA-RISC, Itanium 아키텍처 서버 → 고성능 서버, ERP 시스템, 보안, 가상화에 사용된다. Solaris ; SPARC, x86 아키텍처 서버 → Oracle 데이터베이스, 네트워크 및 데이터 관리에 사용된다.\n③ Linux 기반 OS Cray Linux Environment (CLE) ; 슈퍼컴퓨터, 고성능 컴퓨팅(HPC) 환경 → 과학 연구, 대규모 데이터 분석, 고성능 컴퓨팅에 사용된다.\nRed Hat Enterprise Linux (RHEL) ; 서버, 클라우드 환경, 데이터 센터 → 빅데이터 분석, 기업 애플리케이션에 사용된다.\nSUSE Linux Enterprise Server (SLES) ; 서버, 클라우드 환, 가상화 환경, SAP 환경 → 고가용성 시스템, 컨테이너 및 오케스트레이션에 사용된다.\n2 . 개인용 컴퓨터 OS 개인 사용자를 위한 OS.\n업무용 소프트웨어, 게임, 멀티미디어 등 다양한 응용 프로그램을 지원하는 것이 특징이다.\n사용자의 편의성을 중점적으로 고려하여 GUI 기반의 인터페이스가 주를 이룬다.\n응용 프로그램 설치 및 실행이 용이하고, 하드웨어 및 주변기기와의 높은 호환성을 가진다.\n① Windows → 워크스테이션, 데스크톱 PC, 노트북.\n(Dell XPS, HP Pavilion, ASUS ROG)\n② macOS → 애플의 Mac 시리즈.\n(MacBook Air, MacBook Pro, iMac)\n③ Linux (Ubuntu, Fedora, Debian)\n→ 개발자 및 연구 목적의 개인용 컴퓨터 (커스텀 빌드 PC).\n3 . 휴대용 개인용 컴퓨터 OS 이동성을 강조한 OS.\n사용자가 물리적인 키보드 없이 터치스크린과 음성 인식 기능 등을 통해 쉽게 조작할 수 있도록 설계되었다.\n스마트폰, 태블릿, 일부 노트북(2-in-1)에서 사용되며, 모바일 환경에서 최적화된 UI와 앱 구동 환경을 제공한다.\n또한, 가벼운 무게와 뛰어난 이동성을 가지며, 네트워크 및 클라우드 기능과 연계하여 사용 가능하다.\n① Android → 삼성 갤럭시 시리즈, Google Pixel, 샤오미 스마트폰 및 태블릿.\n② iOS / iPadOS → 아이폰, 아이패드 시리즈.\n③ Chrome OS → 크롬북.\nGoogle Pixelbook, Samsung Chromebook.\n운영체제는 각 환경에 맞춰 최적화되어 있으며, 목적과 사용 방식에 따라 다양한 기능을 제공한다.\n05 컴퓨터 시스템의 구성 (Computer System)\n1 . 시스템 (System)\n상호 연결된 요소들이 유기적으로 작용하여 특정 목표를 수행하는 구조.\n물리적, 생물학적, 사회적, 기술적 분야 등 다양한 영역에서 적용된다.\n여러 개의 개별 요소들이 모여 형성되며, 이들은 서로 유기적으로 상호작용하여 각자의 기능을 수행한다.\n주어진 목적을 달성하기 위해 목표 지향적으로 운영되며, 요소들은 효율적으로 작동할 수 있도록 질서 있게 조직된다.\n또한, 시스템은 외부 환경과 상호작용하고 정보를 주고받으며, 필요에 따라 환경에 적응할 수 있는 능력을 가진다.\n예를 들어, 컴퓨터 시스템, 기업 조직 시스템, 생태계 시스템 등이 있다.\n2 . 하드웨어 (Hardware)\n컴퓨터 시스템의 기본 계산 자원을 제공하며, CPU, 메모리, I/O 장치 등으로 구성된다.\nCPU는 데이터 처리 및 계산을 수행하는 역할을 하며, 메모리는 데이터 저장 및 처리 속도를 지원한다.\nI/O 장치는 사용자와 시스템 간의 상호작용을 가능하게 하여, 외부 장치와 데이터를 주고받을 수 있도록 한다.\n이러한 요소들이 유기적으로 결합되어 시스템의 기본적인 기능을 수행하는 역할을 한다.\n3 . 소프트웨어 (Software)\n① 운영 체제 Operating System, 시스템 프로그램 또는 시스템 소프트웨어.\n다수의 사용자가 다양한 응용 프로그램을 사용할 수 있도록 하드웨어 자원의 사용을 제어 및 조정한다.\n이를 통해 하드웨어와 소프트웨어 간의 상호작용을 관리하고, 시스템 자원의 효율적인 분배를 담당한다.\n② 응용 프로그램 (Application Programs)\n사용자의 특정 문제를 해결하기 위해 설계된 프로그램으로, 자원의 사용 방식을 정의한다.\n예를 들어, 워드 프로세서, 컴파일러, 웹 브라우저, 데이터베이스 시스템, 비디오 게임 등이 있다.\n이들은 각각 사용자의 요구를 만족시키기 위한 다양한 기능을 제공한다.\n4 . 사용자 (Users)\n사람, 기계, 다른 컴퓨터 등을 포함하며, 소프트웨어와 하드웨어를 직접 또는 간접적으로 활용하는 주체를 말한다.\nSCRIPT https://cdn.jsdelivr.net/npm/echarts@5.3.8/dist/echarts.min.js\nSCRIPT\nvar chartDom = document.getElementById('main');\nvar myChart = echarts.init(chartDom);\nvar option = {\n  title: {\n    text: '컴퓨터 시스템 구성 요소에 대한 개략적 구성도',\n    left: 'center'  // 제목을 중앙에 배치\n  }\n};\nmyChart.setOption(option);\nSCRIPT https://fastly.jsdelivr.net/npm/echarts@5/dist/echarts.min.js SCRIPT\nvar dom = document.getElementById('container');\nvar myChart = echarts.init(dom, null, {\n  renderer: 'canvas',\n  useDirtyRect: false\n});\nvar app = {};\n\nvar option;\n\nconst data = {\n  name: '1. 사용자\\nUsers',\n  children: [\n    {\n      name: '2. 응용 프로그램\\nApplication Programs',\n      children: [\n        {\n          name: '3. 운영체제\\nOperating\\nSystem',\n          children: [\n            { name: '4. 하드웨어\\nHardware', children: [\n                { name: 'CPU' },\n                { name: 'RAM' },\n                { name: 'I/O' }\n              ]\n            },\n            { name: 'Android'},\n            { name: 'Windows'},\n            { name: 'IOS'},\n          ]\n        },\n        {name: 'Microsoft\\nOffice'},\n        {name: 'Google\\nChrome'}\n      ]\n    },\n  ]\n};\n\noption = {\n  series: [\n    {\n      type: 'tree',\n      data: [data],\n      top: '20%',\n      left: 0,\n      bottom: '20%',\n      right: 0,\n      symbolSize: 0,\n      edgeShape: 'polyline',\n      center: ['60%', '40%'],\n      edgeForkPosition: '80%',\n      initialTreeDepth: 50,\n      lineStyle: {\n        width: 3,\n      },\n      label: {\n        backgroundColor: '#fff',\n        padding: [5, 0],\n        position: 'inside',\n        verticalAlign: '',\n        align: 'center',\n        borderRadius: 5,\n        borderColor: '#ccc',\n        borderWidth: 2,\n        formatter: function (params) {\n          return `{label|${params.data.name}}`;\n        },\n        rich: {\n          label: {\n            lineHeight: 20,\n            padding: [5, 5],\n            align: 'center',\n            verticalAlign: 'middle',\n          }\n        }\n      },\n      leaves: {\n        label: {\n          position: 'inside',\n          verticalAlign: '',\n          align: 'center',\n          backgroundColor: '#f0f0f0',\n          padding: [5, 0],\n          borderRadius: 5,\n        }\n      },\n      emphasis: {\n        focus: 'descendant'\n      },\n      expandAndCollapse: true,\n      animationDuration: 550,\n      animationDurationUpdate: 750,\n      orient: 'vertical',  // 트리 방향을 세로로 설정\n    }\n  ]\n};\n\nif (option && typeof option === 'object') {\n  myChart.setOption(option);\n}\n\nwindow.addEventListener('resize', myChart.resize);\n06 실행 과정\n1 . 프로그램 실행 요청 사용자가 바탕화면이나 시작 메뉴에서 응용 프로그램을 클릭한다. 운영 체제는 해당 프로그램을 실행해야 한다는 요청을 받는다.\n2 . 프로그램을 찾기 프로그램 실행 파일(.exe)은 하드디스크(SSD 또는 HDD)에 저장되어 있다. 운영 체제의 파일 시스템이 이를 찾아 메인 메모리(RAM)로 로드한다.\n이 과정에서 디스크 스케줄링 알고리즘(예: FCFS, SSTF)이 사용될 수도 있다. 실행 파일뿐만 아니라 필요한 라이브러리(.dll 파일 등)도 함께 메모리로 적재된다.\n3 . 명령어 가져오기 및 실행 프로그램이 메모리에 올라오면, CPU가 실행을 시작한다. PC가 첫 번째 명령어의 주소를 가리킨다.\nCPU는 명령어 사이클(Fetch-Decode-Execute-Cycle)을 반복하면서 한 줄씩 명령을 가져와 실행한다.\n① Fetch CPU가 메인 메모리(RAM)에서 명령어를 가져온다.\n② Decode 가져온 명령어를 해석하여 어떤 작업을 해야 하는지 결정한다.\n③ Execute 해당 명령을 실행하여 결과를 만든다.\n4 . 프로그램 조작 사용자가 키보드로 문서를 작성하면, 운영 체제가 이를 받아서 응용 프로그램으로 전달한다.\n작성한 문서를 저장할 때는 운영 체제가 파일 시스템을 통해 하드디스크에 기록한다.\n5 . 프로세스 관리 사용자는 특정 응용 프로그램을 사용하면서 인터넷 브라우저, 음악 플레이어 등 다른 프로그램도 실행할 수 있다.\n운영 체제는 스케줄러(CPU 스케줄링)를 통해 여러 프로세스를 관리하며, Round Robin, Priority Scheduling 등을 사용하여 CPU 시간을 나누어 사용한다.\n6 . 프로그램 종료 사용자가 응용 프로그램을 닫으면, 운영 체제는 프로세스 테이블에서 해당 프로세스를 제거한다.\n프로그램이 사용하던 메모리를 해제하고, 저장된 데이터를 하드디스크에 기록한다.\nCPU는 다른 작업을 수행할 준비를 한다.\n운영 체제가 할 일은 관점(View)에 따라 달라진다.\n07 사용자 관점 (User View)\n편의성, 사용의 용이성, 좋은 성능을 원한다. ⌎ Convenience, Ease of Use, Good Performance\n이 과정에서 자원 활용(Resource Utilization)에 대한 고려는 크지 않다.\n최대의 활용 연결된 여러 대의 터미널을 통해 접근할 수 있는 대형 컴퓨터(Mainframe)나 미니 컴퓨터(Minicomputer)의 경우\n사용자의 자원 활용을 극대화하기 위해서 설계된다.\n이처럼 운영 체제는 범용 컴퓨터에서도 다양한 요구를 반영하여 작동한다.\n① 워크스테이션 사용자 (Workstations)\n전용 자원을 자유롭게 사용할 수 있지만, 서버의 공유 자원도 빈번하게 활용할 수 있도록 설계된다.\n즉, 사용 용이성과 자원 활용이 균형을 이루는 구조를 갖는다.\n② 휴대용 컴퓨터 자원이 제한적이므로 사용성과 배터리 효율을 최적화하는 방식으로 설계된다.\n③ 가전제품 및 임베디드 시스템 가전제품이나 자동차 내의 임베디드 컴퓨터처럼 일부 시스템에서는 사용자 관점이 거의 없거나, 매우 제한적으로 적용된다.\n08 시스템 관점 (System View)\nOS는 하드웨어와 가장 밀접하게 연관된 프로그램.\n1 . 자원 할당자 (Resource Allocator)\nOS는 모든 자원의 관리자로서 동작하며, 필요한 자원을 적절히 할당하는 역할을 수행한다.\n또한, 자원을 둘러싼 상충하는 요청들을 공정하고 효율적으로 조정하여 시스템을 원활하게 운영하도록 결정한다.\n운영 체제에 대한 또 다른 관점으로는, 다양한 I/O(입·출력) 장치와 사용자 프로그램을 효과적으로 제어하는 역할이 강조될 수 있다.\n2 . 제어 프로그램 (Control Program)\n운영 체제는 컴퓨터의 오류 및 부적절한 사용을 방지하기 위해 사용자 프로그램의 실행을 관리한다.\n특히, 입·출력 장치의 작동 및 제어와 밀접한 관련이 있다.\nOS의 개념과 진화\nOS는 한마디로 정의하기 어렵다. 다양한 역할을 포괄하며, 수많은 디자인과 사용 형태가 존재한다.\n오늘날 운영 체제는 선박, 우주선, 게임기, TV, 산업 제어 시스템뿐만 아니라 토스터기까지 다양한 기기에 적용되고 있다.\n이는 과거 군사용으로 사용되던 고정형 컴퓨터가 점차 범용화되었으며, 자원 관리와 프로그램 제어의 필요성이 증가했기 때문이다.\n전통적으로, OS 설치 CD에 포함된 것이 운영 체제이고, 그 외는 응용 프로그램이라는 단순한 관점이 있었다.\n그러나, 사용자가 OS를 주문했을 때 공급업체가 제공하는 모든 것이 OS로 간주되기도 하며,\n포함된 기능은 시스템에 따라 크게 다르고, IT 환경의 변화에 따라 OS에 대한 개념도 진화하고 있다.\n특히 Google, VMware, Microsoft가 새로운 OS 개념을 이끌고 있다.\n예를 들어, 검색 엔진과 함께 웹 기반 애플리케이션, 캘린더, Gmail, Google 드라이브 등의 서비스가 OS의 역할을 확장하는 방향으로 변화하고 있다.\n1 . Google OS를 단순히 PC를 실행하는 소프트웨어가 아닌, 사용자가 PC를 켠 후 Google Apps, 검색, 웨이브 등 웹 기반 애플리케이션을 이용하는 환경으로 바라본다.\n이를 반영한 대표적인 예가 데스크톱용 웹 OS인 Chrome OS이다. 즉, 과거에는 존재하지 않았던 웹 중심의 운영 체제 개념을 제시한 것이다.\n2 . VMware 클라우드 데이터 센터에서 애플리케이션과 인프라를 연결하는 역할을 클라우드 OS로 정의한다.\n대표적인 예로 vSphere 7이 있으며, 이는 가상화 환경에서 자원을 효율적으로 관리하고 배포하는 운영 체제 개념을 반영한 것이다.\n3 . Microsoft 현재 가상화 시장에서는 VMware에, 웹 기반 애플리케이션 분야에서는 Google에 밀리고 있다.\n이를 감안하여, MS는 데스크톱 지배력과 클라우드 컴퓨팅의 비전을 결합하는 데 주력하고 있다.\n이 과정에서 쉐어포인트 워크스페이스와 같은 서비스가 시도되었으나 실패하였고, 그 후 Azure라는 퍼블릭 클라우드 서비스를 제공하고 있다.\n현재 클라우드 시장에서 AWS가 32%, Azure가 20%의 점유율을 기록하고 있다.\n10 OS 일반적 정의 컴퓨터에서 메인 메모리에 상주하면서 항상 실행되는 시스템 프로그램을 일반적으로 커널(Kernel)이라고 한다.\n커널은 OS의 핵심 부분으로, OS의 부분집합에 포함된다. 시스템 자원 관리, 프로세스 관리, 메모리 관리 등을 담당한다.\nOS는 커널을 포함하여, 사용자 공간(User Space)에서 실행되는 응용 프로그램과 시스템 라이브러리, 미들웨어 등으로 구성된다.\n커널은 운영 체제의 핵심으로, 하드웨어와 직접적으로 상호작용하며 시스템 자원을 관리하고, 사용자 프로그램의 실행을 제어하는 중요한 역할을 한다.\n그 외의 다른 모든 프로그램은 응용 프로그램에 해당한다.\n모바일 운영 체제인 iOS와 Android는 데이터베이스, 멀티미디어 및 그래픽을 지원하는 미들웨어와 함께 코어 커널을 갖추고 있다.\n11 OS의 처리 공유 메모리에 접근을 제공하는 공통 버스를 통해 연결된 여러 개의 장치 제어기와 하나 이상의 CPU로 구성된다.\nCPU와 장치 제어기는 메모리 사이클을 얻기 위해 서로 경쟁하며 병행 수행된다.\n각 장치 제어기는 특정 장치를 관리한다. 또한, 각 장치 제어기는 자신만의 로컬 버퍼를 가지고 있다.\n공유 메모리에 대한 질서 있는 접근을 보장하기 위해 메모리 제어기가 제공된다. 메모리 제어기는 메모리 접근을 동기화하여 충돌을 방지하고 효율적인 자원 관리를 한다.\n1 . H/W 장치 시스템 버스를 통해 프로그램을 실행 중인 CPU에 신호를 보내 인터럽트(Interrupt)를 발생시킬 수 있다.\n예를 들어, 마우스 인터럽트, 키보드 인터럽트 등은 하드웨어 인터럽트에 해당한다.\n이러한 인터럽트는 액션을 취한 장치가 신호를 보내며, 이는 OS와 H/W 간의 상호작용의 핵심 부분이다.\n장치 제어기는 이러한 통신을 인터럽트를 통해 실행한다. 이를 인터럽트 구동식 컴퓨터라고 부른다.\n(Interrupt Driven Computer)\n인터럽트는 적절한 인터럽트 서비스 루틴(ISR)이 있는 시작 주소로 제어를 전달한다.\n프로세스는 신호를 받으면 CPU는 해당 작업을 처리할 수 없음을 알리고, 운영체제가 대신 처리하도록 한다.\n이 과정에서 인터럽트 벡터 테이블을 참조하여 ISR의 주소를 찾고, 이를 통해 해당 인터럽트를 처리한다.\n인터럽트가 요청되면, 인터럽트 벡터를 사용하여 인터럽트를 유발한 장치에 맞는 ISR의 주소를 제공하게 된다.\n인터럽트 벡터는 여러 인터럽트 서비스 루틴에 대한 주소 배열을 가지며, 각 인터럽트는 고유한 장치 번호로 색인화되어 인터럽트 요청을 처리한다.\n2 . 응용 소프트웨어 트랩(Trap) 또는 예외(Exception)는\n오류(예: 0으로 나누기, 유효하지 않은 메모리 접근)나 사용자 프로그램의 운영 체제 서비스 요청에 의해 유발되는 소프트웨어 인터럽트이다.\n이러한 인터럽트는 신호를 받으면 CPU는 이를 처리할 수 없다는 신호를 보내고, 운영체제가 대신 처리한다.\n그 후, 인터럽트 벡터 테이블을 참조하여 적절한 ISR의 주소를 찾아 이를 수행한다.\nOS는 인터럽트 구동(Interrupt Driven) 방식으로 동작한다.\nCPU가 인터럽트 요청을 받으면, 현재 실행 중인 명령을 완료한 후,\n현재 수행 중인 프로그램을 일시 중단한 다음 해당 프로그램의 상태를 안전한 장소에 저장한다.\n그 후, 인터럽트 처리 루틴을 실행하여 인터럽트 원인을 찾아내고 (인터럽트 벡터 테이블을 통해), 해당 ISR 또는 인터럽트 핸들러를 실행하여 인터럽트를 처리한다.\n3 . 인터럽트 원인 판별 방법 ① 폴링 방식 (Polling)\n주기적으로 상태를 체크하는 방법이다.\n하지만 이 방식은 시간이 오래 걸리고 비효율적이어서, 보통 16비트 이하의 단순한 컴퓨터 시스템에서만 사용된다.\n② 인터럽트 벡터 방식 (Vectored Interrupt)\n인터럽트 발생 시, ISR을 수행하여 인터럽트 원인에 대한 처리를 하며, 이를 통해 빠르게 인터럽트에 대응할 수 있다.\n만약 여러 작업이 동시에 이루어지면, 각 작업은 CPU의 의존성이 없이 독립적으로 실행되며, 작업이 완료된 후에는 각각의 결과가 처리된다.\n[교제] 운영체제 제 10판 [원그래프 출처] Examples - Apache ECharts"
  },
  {
    "objectID": "rd/od_05.html",
    "href": "rd/od_05.html",
    "title": "개발 일지3",
    "section": "",
    "text": "재실감지 프로젝트의 2월 24일 오후 9시 회의에 대해 다루고자 한다.\n01 API 정의 응용 프로그램 인터페이스, Application Programming Interface.\n다른 프로그램과 소통할 수 있도록 만들어진 인터페이스. 사용하는 대상과 방식에 따라 다르게 분류될 수 있다.\nAPI의 역할을 두 가지 측면에서 구분해서 봐야 한다.\n1 . HTTP 통신을 위한 API 웹 애플리케이션이나 모바일 앱에서 백엔드 서버와 데이터를 주고받기 위한 API.\n주로 HTTP 프로토콜을 사용하여 데이터를 요청하고 응답을 받는다.\n예를 들어, 클라이언트(웹 브라우저, 모바일 앱 등)가 서버의 특정 URL에 요청을 보내면 서버가 JSON 형식으로 데이터를 응답하는 방식.\n보통 REST API, GraphQL API 등이 이에 해당된다.\n2 . 백엔드 내부에서 DB와 통신하는 API 백엔드 시스템 내부에서만 사용되는 API. 프로그램 내부에서 함수 호출을 통해 사용된다.\n예를 들어, 백엔드 코드에서 DB에서 유저 정보를 가져오는 함수 같은 것이 이에 해당된다.\nHTTP 요청 없이, 애플리케이션 내부의 모듈 간 통신을 위한 API라고 할 수 있다.\n02 재실 확인 정의 연구실 내의 사람들의 재실 즉, 출입 여부를 확인하는 시스템을 구축하는 방식.\n이를 이해하려면 프론트엔드(사용자 인터페이스)와 백엔드(서버) 간의 데이터 흐름을 살펴봐야 한다.\n1 . 재실 상태 업데이트 방식 연구실 구성원이 출입할 때마다 해당 정보를 서버인 백엔드에 전송해야 한다.\n이를 위해 POST 요청을 사용하여 현재 상태(입실/퇴실)를 백엔드에 전달한다. 백엔드는 이를 받아서 DB에 재실 정보를 저장한다.\n2 . DB 직접 접근 불가 프론트엔드는 웹 브라우저에서 실행되는 화면(UI).\n보안상 프론트엔드가 DB에 직접 접근하면 안 된다. 대신, 백엔드 서버에 GET 요청을 보내서 데이터를 가져와야 한다.\n3 . GET 요청 연구실의 재실 상태가 바뀌어도 프론트엔드에서는 이를 자동으로 알 방법이 없다.\n따라서 1초마다 백엔드에 GET 요청을 보내 최신 상태를 확인해야 한다. 이를 통해 연구실 출입 상태가 바꿜 때마다 UI가 자동으로 업데이트된다.\n연구실 인원 수가 많지 않다면, 1초마다 GET 요청을 보내도 서버가 충분히 감당할 수 있다.\n4 . 추가 최적화 방법 위 방식은 불필요한 트래픽이 발생할 수 있다.\n웹소켓(WebSocket)을 사용하면 재실 상태가 변경될 때만 프론트엔드로 알림을 보낼 수 있다.\n하지만 구현이 조금 더 복잡할 수 있다.\n03 DB 설계\n1 . DB의 정의 연구실 출입 정보를 저장해야 하므로 DB가 필요하다.\n연구실 출입 기록을 관리하지 않고 메모리(RAM)에서만 처리하면 서버를 재시작하면 데이터가 사라진다.\n따라서 DB에 출입 기록을 저장해야 한다.\n2 . SQLite 연구실 시스템은 트래픽이 많지 않다.\nSQLite는 파일 기반 경량 DB로, 연구실 같은 소규모 시스템에서 사용하기 적절하다.\n대형 시스템이면 MySQL, PostgreSQL 같은 DB를 고려해야 하지만, 현재 상황에서는 SQLite로 충분하다.\n3 . 로그 기록 남기는 방법 연구실 출입 기록을 DB에 어떻게 저장할지 고민하는 부분이다. 로그를 남기는 방식은 여러 가지가 있지만, 최소한의 정보만 저장하면 된다.\n즉, 재실 상태를 실시간으로 저장하는 것이 아니라, “출입 로그만 남기면 충분하다”는 의미이다.\n예를 들어, 한 사람이 하루에 5번 드나들면, 그 5번의 출입 로그만 기록하면 된다.\n“현재 연구실에 있는 사람” 이 누구인지 확인하려면, status = ’ in ’ 중 가장 최신 로그를 보면 된다.\n출입할 때만 기록을 남기므로 불필요한 데이터 저장을 최소화할 수 있다.\n4 . DB 갱신 보류 데이터가 계속 쌓이면 DB 크기가 커지고 성능 저하 가능성이 있다. 그래서 “1달마다 오래된 로그를 삭제할까?” 라는 고민을 했지만, 결론적으로 필요 없다고 판단하였다.\nSQLite는 소규모 데이터 저장에는 충분히 빠르고 효율적이다. 연구실 출입 기록을 하루 수십 ~ 수백 개 수준으로 가정해도 1년치 데이터는 몇 MB가 안 될 것이다.\n따라서 데이터가 많아서 생기는 성능 문제는 걱정할 필요 없다.\n5 . DB 제약사항 최소화 테이블을 만들 때, 외래키, 복잡한 관계 등 너무 많은 제약을 설정하지 말자는 의미.\nSQLite는 간단한 DB이므로, 최대한 단순한 구조로 유지하는 것이 좋다. 최소한의 필드만 저장 (user_id, status, timestamp)\nJOIN 남발하는 등 불필요한 복잡한 테이블 관계를 자제한다. ON DELETE CASCADE 같은 것을 최소화하여 강한 외래키 제약한다.\n04 DB 라이브러리 선택 데이터베이스와 소통하려면 라이브러리(의존성)가 필요하며 다음과 같은 후보가 있다.\n1 . SQLAlchemy Python에서 가장 널리 사용되는 ORM(Object-Relational Mapping) 라이브러리.\nSQLAlchemy\nThe Database Toolkit for Python\nwww.sqlalchemy.org 트랜잭션, 복잡한 쿼리, DB 최적화 등 강력한 기능 제공하지만 설정이 복잡하고 학습 곡선이 가파르다.\n장점 대규모 프로젝트에서 사용하기 좋다.\n다양한 데이터베이스 지원 (PostgreSQL, MySQL, SQLite 등) ORM뿐만 아니라 직접 SQL을 실행하는 기능도 제공한다.\n단점 문법이 복잡하고 설정이 어려우며, 초보자가 배우기에 부담스러울 수 있다.\n추천 대상 ① 대규모 프로젝트에서 강력한 데이터베이스 기능이 필요한 경우 ② SQL을 잘 알고 있고 세부적인 최적화가 필요한 경우\n2 . Peewee peewee — peewee 3.17.9 documentation\n경량 ORM으로, 코드가 단순하고 배우기 쉽다. SQLAlchemy보다 기능이 적지만, 기본적인 CRUD 작업에는 충분하다.\n장점 코드가 간결하고 직관적이며, 설정이 간단해 빠르게 시작 가능하다.\nSQLite, PostgreSQL, MySQL을 지원한다.\n단점 SQLAlchemy보다 기능이 부족하다. 대규모 프로젝트에는 부적절할 수 있다.\n추천 대상 ① 작은 프로젝트나 빠르게 개발해야 하는 경우 ② 복잡한 DB 연산이 필요하지 않은 경우\n3 . SQLModel SQLAlchemy를 기반으로 만든 최신 ORM. Pydantic과 통합되어 데이터 검증이 쉽다.\nSQLModel\nhttps://www.sqlalchemy.org/ SQLAlchemy의 강력한 기능을 유지하면서도 더 간단한 사용법 제공한다.\n장점 공식 지원을 받으며, 최신 기술을 반영한다.\nSQLAlchemy보다 더 간단한 문법 가지며, Pydantic과 연계되어 데이터 모델링과 검증이 쉽다.\n단점 아직 SQLAlchemy만큼 성숙하지 않다. (상대적으로 신생 라이브러리) 복잡한 DB 기능을 다루려면 결국 SQLAlchemy의 일부 기능을 사용해야 한다.\n추천 대상 ① FastAPI 같은 최신 프레임워크와 함께 사용할 경우 ② SQLAlchemy의 복잡함을 줄이고 싶지만, 강력한 기능이 필요한 경우\n최종 결정으로 SQLModel을 선택하였다.\n05 SQLite과의 비교\n1 . 일반적인 DBMS MySQL, PostgreSQL 등이 해당된다.\n독립적인 서버로 실행되며, 클라이언트가 네트워크를 통해 서버에 접속하여 데이터 요청한다.\n여러 사용자가 동시에 접근할 수 있다.\n2 . SQLite 서버가 없으므로, 별도의 프로그램을 실행할 필요 없다. 데이터를 ’ .db 파일 ’ 로 저장한다.\n프로그램이 직접 파일을 읽고 쓰는 방식으로 동작한다. 가볍고 간단해서 로컬 데이터 저장용으로 적합하다.\n06 쿼리\n1 . 유저 테이블 랩원(사용자)에 대한 기본 정보를 저장한다.\n2 . 로그 테이블 (presence)\n입실 및 퇴실 시간을 기록한다.\n각 로그에는 입실/퇴실 여부를 나타내는 정보와, 어떤 유저인지에 대한 정보가 포함된다.\n이 쿼리는 presence 테이블에서 유저의 입실 및 퇴실 기록을 조회하는 쿼리이다.\nSELECT DISTINCT ON (user_id) * FROM presence ORDER BY at_time, user_id DESC; 이 쿼리는 각 유저별로 가장 최근의 입실/퇴실 기록을 가져온다.\n예를 들어, 유저 A가 오전 9시에 입실하고 오후 5시에 퇴실한 경우, presence 테이블에서 유저 A의 두 개의 레코드가 있을 수 있다.\n이 쿼리는 유저 A의 가장 최근 퇴실 시간 또는 입실 시간을 선택하게 된다.\nSELECT DISTINCT ON (user_id) DISTINCT ON (user_id)는 유저별로 가장 최근의 기록만 선택한다. 즉, 유저가 여러 번 입실/퇴실한 경우, 각 유저의 가장 최근 입실/퇴실 정보만 가져오게 된다.\n’ * ’ 테이블의 모든 컬럼을 선택하겠다는 의미이다.\n예를 들어, at_time (시간), user_id (유저 ID), 입실/퇴실 여부 등을 포함하는 모든 컬럼을 가져온다.\nFROM presence presence 테이블에서 데이터를 가져온다. presence 테이블에는 입실과 퇴실 시간에 대한 기록이 저장되어 있다.\nORDER BY at_time, user_id DESC at_time으로 먼저 시간 순서대로 정렬한다. (입실/퇴실 시간을 기준으로 오름차순 정렬)\n그 다음 user_id DESC로 유저 ID를 내림차순으로 정렬한다.\n이는 같은 시간에 여러 번 입출입한 기록이 있을 수 있을 때, 가장 최신 기록을 가져오기 위함이다."
  },
  {
    "objectID": "rd/od_03.html",
    "href": "rd/od_03.html",
    "title": "개발 일지",
    "section": "",
    "text": "01 제목1 [ID 사용하기]\n1 . 제목2 본문1\n본문2\nSign In\nSign in to AILab ID Authenticate yourself with your passkey to access the admin panel. Authenticate\nsso.llms.kr\n키 값.\n비밀번호 입력.\nSign in to Services\nSign in to Services Do you want to sign in to Services with your AILab ID account? Cancel Sign in\nsso.llms.kr\n재희 작품\n깃허브 주소 보내면 이메일로 옴\ndjailab"
  },
  {
    "objectID": "rd/od_01.html",
    "href": "rd/od_01.html",
    "title": "Git 사용법",
    "section": "",
    "text": "VS코드로 Git 사용법에 대해 다루고자 한다.\n\n01 버전 관리 시스템\nVCS, Version Control System 소프트웨어 개발 및 문서 작업에서 변경 사항을 기록, 추적, 관리, 수정, 복구할 수 있도록 돕는 도구.\n\n로컬 버전 관리 시스템 Local VCS 파일 변경 이력을 로컬 컴퓨터에서만 관리하는 시스템을 의미. 대표적인 로컬 버전 관리 시스템은 RCS이며, 이는 파일의 변경 이력을 개별적으로 저장하는 방식이다.\n\nRevision Control System 과거 macOS 및 UNIX 시스템에서 사용되었으나, 현재는 Git과 같은 분산 버전 관리 시스템(DVCS)의 등장으로 거의 사용되지 않는다.\n\n중앙집중식 버전 관리 시스템 CVCS, Centralized VCS 여러 사용자가 협업할 수 있도록 설계된 중앙 서버 기반의 버전 관리 시스템.\n\n모든 변경 사항이 중앙 서버에 저장되며, 사용자는 중앙 서버에서 데이터를 가져와 수정한 후 다시 업로드하는 방식으로 동작한다.\n대표적으로, SVN (Subversion), Perforce, CVS (Concurrent Versions System) 등이 있다.\nCVCS의 단점 ① 모든 변경 이력이 중앙 서버에 저장되므로, 서버가 다운되거나 손상되면 데이터를 잃거나 작업이 불가능할 수 있다.\n② 파일을 가져오거나 변경 사항을 기록하려면 항상 중앙 서버에 연결해야 한다. 네트워크 연결이 불안정하면 효율적으로 작업하기 어렵다.\n3 . 분산 버전 관리 시스템 (DVCS, Distributed VCS)\n각 개발자가 히스토리가 포함된 전체 저장소를 로컬에 복제하여 관리하는 방식.\nGit, Mercurial이 대표적인 DVCS이다.\nGit의 장점 ① 모든 변경 이력을 로컬 저장소(Local Repository)에 저장한다. 따라서 네트워크 연결 없이도 히스토리를 조회하고, 로컬에서 자유롭게 작업할 수 있다.\n② 가벼운 브랜칭(branching) 기능을 제공하여, 여러 작업을 독립적으로 진행할 수 있다. 브랜치를 쉽게 병합(merging)할 수 있어, 팀 협업 시 코드 충돌을 최소화할 수 있다.\n③ 모든 개발자의 로컬 저장소에 프로젝트의 전체 히스토리를 저장한다. 중앙 서버에 문제가 발생해도, 로컬 저장소를 활용하여 데이터를 복구할 수 있다.\n이러한 장점 덕분에 Git은 현대적인 소프트웨어 개발에서 가장 널리 사용되는 버전 관리 시스템이 되었다.\n02 Git 사용법\n1 . Git 설치하기\nGit - Downloads\nDownloads macOS Windows Linux/Unix Older releases are available and the Git source repository is on GitHub. Latest source Release 2.48.1 Release Notes (2025-01-13) Download Source Code GUI Clients Git comes with built-in GUI tools (git-gui, gitk), but ther\ngit-scm.com\n설치가 완료되었다.\nGit Bash를 관리자 권한으로 실행하기.\n다음과 같은 창이 나오는 것을 확인할 수 있다.\n2 . VS Code 실행하기 Git 창을 닫고 VS Code를 실행하기.\nVS Code가 설치되지 않았다면, 먼저 설치한 후 실행하세요.\nVisual Studio Code - Code Editing. Redefined\nVisual Studio Code redefines AI-powered coding with GitHub Copilot for building and debugging modern web and cloud applications. Visual Studio Code is free and available on your favorite platform - Linux, macOS, and Windows.\ncode.visualstudio.com\n첫 화면.\n새 파일 만들기.\nOpen Folder &gt; New &gt; Folder\n파일명이 왼쪽 상단에 생성된 것을 확인할 수 있다. 이제 오른쪽 상단의 Toggle Panel을 클릭한다.\n3 . 터미널 (Terminal) 터미널에 Git 저장소를 로컬로 복제(clone)하는 명령어 입력하기.\n\n\n기본구조: git clone + 복제할 원격 저장소의 URL\n예제: git clone https://github.com/onlybooks/python-algorithm-interview.git\n해당 저장소의 모든 파일과 이력이 현재 작업 디렉토리로 다운로드된다.\n즉, python-algorithm-interview 폴더가 생성되며, 해당 저장소의 내용을 그대로 가져오게 된다.\n4 . GitHub 저장소 클론\nGithub 업로드를 허용한다.\n원하는 계정을 선택한다.\n계정이 없으면 새 계정을 생성한다.\n열기 버튼 클릭.\nPrivate는 본인만 접근할 수 있고, Public는 모든 사용자가 접근할 수 있다.\n필요에 따라 적절한 옵션을 선택한다.\n커밋 내역 확인하기.\nREADME.md 파일 확인하기.\n\n파일의 수정 과정을 알 수 있다."
  },
  {
    "objectID": "ch/sql_01.html",
    "href": "ch/sql_01.html",
    "title": "DB 개론",
    "section": "",
    "text": "정형 및 비정형 DB에 대해 다루고자 한다.\n01 빅데이터의 등장 배경\n1 . 소셜미디어의 발전 페이스북, 트위터, 인스타그램 등 소셜미디어가 급격히 성장하면서 방대한 양의 데이터가 생성되었다.\n또한, 사용자 행동 패턴, 감성 분석, 추천 시스템 등에 활용되면서 데이터의 중요성이 증가하였다.\n2 . IoT 서비스의 증가 다양한 센서와 기기가 네트워크로 연결되면서 실시간으로 데이터를 수집할 수 있게 되었다.\n스마트홈, 스마트팩토리, 스마트시티 등에서 수많은 비정형 데이터가 생성되었다.\n3 . 데이터 가치의 재인식 기존에는 주로 정형 데이터(엑셀, DB 등 구조화된 데이터)가 중요했지만, 이제는 비정형 데이터(텍스트, 이미지, 영상 등)의 가치가 재조명되었다.\n이를 효과적으로 저장, 처리, 분석하는 기술(빅데이터 저장소, AI 기반 분석 기술 등)이 필요해졌다.\n4 . 클라우드 서비스 등장 과거에는 데이터 저장 및 처리를 위한 고가의 인프라가 필요했지만, 클라우드 기술의 발전으로 누구나 쉽게 대량의 데이터를 저장하고 분석할 수 있게 되었다.\nAWS, Azure, Google Cloud 등의 클라우드 서비스가 빅데이터 처리의 핵심 인프라로 자리 잡았다.\n02 빅데이터의 정의\n1 . 대용량 데이터 집합 기존 하드웨어와 소프트웨어의 처리 능력을 넘어선다.\n기존 데이터베이스 시스템으로 처리하기 어려운 방대한 데이터.\n대량(Volume), 다양(Variety), 속도(Velocity)의 특성을 가진다.\n2 . 의사결정 이전에는 얻을 수 없었던 통찰과 의사결정을 위한 정보 자산.\n단순한 데이터가 아닌, 분석을 통해 의미 있는 패턴과 인사이트를 도출할 수 있다.\n기업, 정부, 연구기관 등이 데이터 기반 의사결정을 내리는 데 활용된다.\nDIKW 피라미드와의 관계\nData(데이터): 가공되지 않은 원천 데이터. Information(정보): 데이터를 정리하고 구조화하여 의미를 부여한 것. Knowledge(지식): 정보를 분석하여 패턴과 관계를 발견한 것. Wisdom(지혜): 지식을 바탕으로 최적의 의사결정을 내리는 단계.\n빅데이터 분석은 데이터 → 정보 → 지식 → 지혜로 발전하는 과정에서 중요한 역할을 함.\n03 빅데이터의 5V 특성\n1 . 가치(Value) 단순한 데이터 저장이 아니라 분석을 통해 새로운 가치를 창출해야 한다. 기업, 연구, 행정 등 다양한 분야에서 활용된다.\n2 . 규모(Volume) 방대한 데이터 규모(대용량)를 의미하며, 페타바이트(PB) 또는 엑사바이트(EB) 단위의 데이터 처리 필요하다.\n즉, 기존 데이터베이스 시스템으로는 감당하기 어렵다.\n3 . 다양성(Variety) 정형 데이터(데이터베이스, 엑셀)뿐만 아니라, 반정형 데이터(XML, JSON), 비정형 데이터(이미지, 영상, SNS 데이터 등)까지 포함.\n4 . 속도(Velocity) 데이터가 실시간으로 생성되고 빠르게 처리될 필요가 있다.\n예: IoT 센서 데이터, 소셜미디어 스트리밍 데이터, 금융 거래 데이터.\n5 . 신뢰성(Veracity) 데이터의 오류, 불확실성, 노이즈를 줄이고 신뢰성과 정확성을 확보해야 한다.\n잘못된 데이터는 분석 결과의 왜곡을 초래할 수 있기 때문이다.\n04 빅데이터의 유형\n1 . 정형 데이터 (Structured Data) 고정된 필드를 가지며, 명확한 구조가 있다.\n테이블 형식(행과 열)으로 저장되며, 관계형 데이터베이스(RDBMS)에서 쉽게 관리 가능하다.\n구조 변경이 어렵고 제한적임.\n📌 예시: 엑셀 파일, SQL 데이터베이스(MySQL, PostgreSQL), ERP 시스템 데이터\n2 . 반정형 데이터 (Semi-Structured Data) 데이터를 저장할 때 고정된 필드를 가질 수 있지만, 구조 변경이 가능함. 스키마가 유연하며, 계층적 또는 키-값 형태로 저장될 수 있음.\n관계형 DB보다는 NoSQL(MongoDB, Cassandra) 같은 비관계형 데이터베이스에서 주로 활용됨.\n📌 예시: XML, JSON, YAML, 로그 파일, 이메일(헤더 구조 있음)\n3 . 비정형 데이터 (Unstructured Data) 고정된 필드나 명확한 구조가 없음. 데이터 크기가 크고, 분석 및 가공이 어려움.\n머신러닝 및 자연어 처리(NLP) 같은 고급 기술을 이용하여 의미를 추출해야 함.\n📌 예시: 이미지, 비디오, 오디오, PDF, 소셜미디어 포스트, 이메일 본문\n05 주요 기술 수집 - 데이터 처리 - 저장 - 분석 - 시각화\n1 . 수집 데이터의 충분성, 완전성, 일관성, 정확성\n정량적, 정성적, 내부, 외부, 일화성, 반복성, 배치, 실시간 고려\n웹 크롤링, 웹 스크래핑, 정보 추출, 단 타겟 웹 페이지의 유무, 제거의 실행 유무 차이가 있다.\n적재를 기준으로 전처리, 후처리,\n빅데이터 정제 - 결측값, 잡음값(평균값으로 처리), 이상값 처리\n2 . 처리 대화형, 일괄형, 실시간형\n아파치 하둡 - 분산 병렬 컴퓨팅 기술 - 저장 및 분석, 수평적 확장\n반면, 수직적 확장은 스케일 업, 일관성있다, 장애에 치명적이다.\nHDFS - 네임노드, 데이터노드\n맵리듀스 - 인풋 - 스플릿 - 맵핑 - 셔핑 - 뤼듀싱 - 아웃풋\n스파크(인메모리) - 맵리듀스 단점 보안\n3 . 저장 정형 - SQL(정의됨), 반정형 - SQL, NoSQL(고정된 스키마 X), 비정형 - NoSQL, 분산파일DB(내부구조X)\nSQL과 NoSQL 비교 자세히, 장단점\nSQL - 필드를 추가 및 확장 - 낭비되는 공간 생김 - 테이블 분리(정규화)\nNoSQL - 있는 걸 그대로 추가, 성능 저하 유형 - 키-값 형태, 컬럼 패밀리, 문서 형태, 그래프 형태\n4 . 분석 클레스페케이션 분석 - 지도합습\n클러스터링 분석 - 비지도 학습\n포러케스팅 분석, 어소시셰이션 분석 - 데이터 마이닝\n비정형 - 텍스트 마이닝, 웹 마이닝, 오피니언 마이닝, 소셜 마이닝\n5 . 시각화"
  },
  {
    "objectID": "ch/db_01.html",
    "href": "ch/db_01.html",
    "title": "DB 개론",
    "section": "",
    "text": "DB의 전반적인 개념에 대해 다루고자 한다.\n01 DB 응용 분야 데이터베이스(DB)는 현대 사회 전반에서 핵심적인 역할을 수행하고 있으며, 그 응용 분야는 매우 다양하다.\n1 . 금융 & 비즈니스 비용 절감과 수익 극대화를 위해 DB가 폭넓게 활용된다.\n고객 관계 관리(CRM)와 전사적 자원 관리(ERP)는 기업 운영의 효율성을 높이는 대표적 시스템이며,\n판매 시점 관리(POS)를 통해 거래 데이터를 실시간으로 기록·분석할 수 있다.\n또한 데이터 웨어하우스와 데이터 마이닝 기술은 방대한 거래 데이터를 체계적으로 저장하고, 의미 있는 정보를 도출하여 경영 의사결정에 활용된다.\n2 . 소셜 네트워크 트위터, 페이스북, 인스타그램과 같은 플랫폼은 방대한 사용자 데이터를 기반으로 운영되며, 이를 통해 긴밀한 커뮤니티 형성과 맞춤형 서비스 제공이 가능하다.\n특히 빅데이터 분석은 사용자의 행동 패턴을 파악하고 모바일 라이프로그(Life Log)와 같은 새로운 서비스를 가능하게 한다.\n3 . 사회 효율적인 시스템 구축의 기반이 되기도 한다. 교통정보 시스템은 실시간 데이터를 바탕으로 최적의 경로를 안내하며, 개인 일정 관리 시스템(PIMS)과 사물인터넷(IoT)은 개인화된 편의성을 제공한다.\n나아가 DB는 인공지능(AI)과 결합되어 다양한 사회 문제 해결과 지능형 서비스 구현에 기여하고 있다.\n4 . 클라우드 최근 DB는 삶의 가치를 높이는 핵심 인프라로 자리 잡았다.\n클라우드 DB와 데이터 분산 처리 기술은 대규모 데이터를 효율적으로 관리할 수 있게 하며, IaaS, PaaS, SaaS와 같은 클라우드 서비스 모델은 기업과 개인 모두에게 유연하고 확장 가능한 데이터 활용 방안을 제공한다.\n특히 빅데이터와 차세대 DB 기술의 융합은 향후 클라우드 생태계의 혁신을 가속화할 것으로 전망된다.\n02 발전 과정 전쟁은 과학과 기술의 발전을 촉진하는 강력한 동인이었다. 특히 군대 통제와 전술 운영에서 무전기와 같은 통신 기술의 발전은 필수적이었다.\n전쟁 중에는 주파수 도청을 방지하기 위한 암호 기술이 급격히 발전했으며, 이를 해독하는 과정에서 수학자들의 역할이 매우 컸다.\n암호 해독은 단순히 군사적 정보 확보에 국한되지 않고, 이후 컴퓨터 과학의 토대를 마련하는 계기가 되었다.\n대표적으로 제2차 세계대전 당시 컴퓨터는 암호 해독에 적극 활용되었으며, 전쟁이 끝난 뒤에는 이러한 컴퓨터 기술을 민간 산업에 어떻게 활용할지가 중요한 과제로 떠올랐다.\n전후 사회에서는 컴퓨터의 활용 범위가 군사 목적에서 문서와 텍스트 처리로 확장되었다. 행정과 기업 환경에서 대량의 문서를 효율적으로 관리해야 했으므로 저장 장치 개발이 필수적이었다.\n이에 따라 자기테이프, 자기디스크 등 문서용 저장 장치 기술이 집중적으로 발전하였으며, 컴퓨터는 단순한 계산 기계에서 대량의 데이터를 분류하고 처리하는 장치로 변모했다.\n이러한 변화는 컴퓨터의 상업적 성공을 견인하였고, 이후 정보화 사회로 나아가는 기반을 마련하였다.\n03 데이터 데이터는 가공되지 않은 원시적 사실(fact)로, 수치·기호·문자 등 다양한 형태로 존재한다.\n이러한 데이터는 단순히 수집된 상태에서는 의미가 제한적이지만, 가공과 처리 과정을 거치면 새로운 의미를 지닌 정보로 변환된다.\n이러한 과정을 개념적으로 표현하면 다음과 같이 나타낼 수 있다.\n이는 “데이터(D)에 처리(P)를 적용하면 정보(I)가 된다”는 의미의 비유적 수식이다. 여기서 처리(Process)는 단순 계산이나 집계뿐 아니라 분류, 요약, 분석, 의미 부여 등 다양한 가공 과정을 포괄한다.\n따라서 정보는 데이터 그 자체가 아니라, 데이터를 특정 목적에 맞게 해석 및 활용할 수 있도록 구조화한 결과물이라고 할 수 있다.\n컴퓨터 시스템은 이 과정에서 핵심적인 도구로 작동하며, 데이터를 효과적으로 수집·저장·처리하여 실질적인 결론을 도출한다.\n궁극적으로 데이터의 활용도를 높이는 것은 단순한 저장이 아니라, 조직과 개인의 의사결정에 유용한 정보로 가공하는 과정이다.\n04 정보 시스템 Information System\n이러한 정보를 생성 및 분배하는 구조적 체계를 의미한다. 조직 내외부의 데이터를 수집하고 이를 정보로 변환하며, 사용자에게 필요한 시점에 제공하는 역할을 한다.\n이 과정에서 다음의 시스템들이 중요한 하위 단위로 작동한다.\n1 . 응용 시스템 Application System\n특정 업무 영역에 초점을 맞춘 정보 처리 시스템으로, 예를 들어 학사 관리, 급여 관리, 회계 관리와 같이 개별적인 운영 업무를 지원한다.\n이러한 응용 시스템들이 통합적으로 구성되고 운영되는 상위 개념이 바로 정보 시스템이다.\n즉, 응용 시스템은 개별적이고 구체적인 업무 단위를 담당하고, 정보 시스템은 이를 아우르는 거시적 틀로서 조직 전체의 정보 흐름을 관리하는 역할을 수행한다.\n데이터의 규모가 커짐에 따라, 이를 효율적으로 관리 및 활용하야 한다.\n2 . 경영정보시스템 MIS: Management Information System\n조직 내외부의 데이터를 체계적으로 수집·처리하여, 경영 관리자가 기획(Planning), 운영(Operation), 통제(Control) 활동을 수행할 수 있도록 정보를 제공하는 시스템이었다.\n일정 부분 관리자 의사결정을 지원했으나, 제공되는 정보가 주로 정형화된 보고서 형태로 제한되어 있어 전략적이고 비정형적인 의사결정을 충분히 뒷받침하기에는 한계가 있었다.\n3 . 의사결정 지원 시스템 DSS: Decision Support System\nDB와 모델베이스(Model Base)를 결합하여 관리자가 직접 데이터를 탐색하고 분석할 수 있도록 지원하였다. 이 시스템은 특히 비정형적이고 전략적인 의사결정을 수행하는 관리자에게 유용했으며, 단순 보고서 제공을 넘어 시뮬레이션, 최적화 기법, 통계 분석을 통해 다양한 대안의 평가를 가능하게 했다. 따라서 DSS는 넓은 의미에서 정보시스템의 발전적 형태로 간주되며, 이후 ERP, 데이터 웨어하우스(DW), 데이터 마이닝(DM), 지식 관리 시스템(KMS) 등과 긴밀히 연결되는 기반이 되었다.\n전사적 자원 계획 관리 ERP: Enterprise Resources Planning\n이후 기업 활동이 복잡해짐에 따라 전사적 자원 관리(ERP)가 도입되었고, 이를 통해 조직 내 다양한 기능과 부서를 통합 관리할 수 있게 되었다.\n데이터 웨어하우스 Data Warehouse\n기업 내외부에서 발생하는 방대한 데이터를 체계적으로 저장·관리하기 위한 일종의 정보 저장소(information repository)로 발전하였다.\n이는 단순한 DB와는 달리, 분석과 의사결정을 목적으로 데이터를 구조화하고 보관하는 특징을 가진다.\n데이터 마이닝 Data Mining\n저장된 데이터로부터 의미 있는 규칙이나 패턴을 발견하는 지식 발견(knowledge discovery)의 핵심 기술로 자리 잡았다. 이후 빅데이터 분석의 기초가 되었으며, 예측 분석과 인공지능 기술로 확장되었다.\n지식 관리 시스템 KMS: Knowledge Management System\n조직 내 축적된 지식을 체계적으로 관리하고 공유함으로써 의사결정의 질을 높이는 데 기여하였다.\n더 나아가 의사결정 지원 시스템(DSS: Decision Support System)은 데이터 분석과 시뮬레이션 기능을 통해 경영진의 전략적 선택을 지원하였다.\n특히 최근에는 인공지능의 추론 기능이 결합되면서, 데이터의 활용성은 단순한 보고 차원을 넘어 예측과 문제 해결을 지원하는 수준으로 크게 향상되었다.\n02 자료 처리 시스템 자료를 정보로 생성해주는 시스템.\n1 . 일괄 처리 (Batch Processing)\n원시데이터로부터 사전작업 - 데이터 생성 -\n데이터를 한꺼번에 모아서 사전에 준비된 상태에서 한 번에 처리하는 방식. 처리 속도는 매우 빠르지만, 실시간 처리가 아닌 사전 준비가 필요하다.\n예시: 수능 채점과 같이 데이터를 순차적으로 접근하고 처리한 후 최종 결과를 출력한다.\n장점:\n대용량 데이터를 처리할 때 효율적이다. 부가적인 장치 필요 없으므로, 비용 절감적이다. 순차처리 기법에 유리하다.\n단점:\n실시간 처리가 불가능하고, 사전 준비 시간이 소요된다.\n2 . 온라인 처리 시스템 (Online Processing)\n사용자가 요청하기 전까지 기다림 - 사용자가 요청하기 쉽도록 단말 장치 설치 및 컴퓨터와의 연결 - 통신라인의 온라인 상태를 유지하고 있어야 한다.\n서버가 네트워크로 연결되어 실시간으로 데이터를 처리한다. - 리얼 타임 사용자는 즉각적인 고급 서비스를 받을 수 있다. 임의접근처리에 더 유리하다.\n예시: 현금 인출 시스템(ATM)처럼 실시간으로 데이터를 처리하고 응답한다.\n장점:\n실시간 처리로 사용자 경험이 개선된다.\n단점:\n사용자 서비스 위주.\n많은 경우 시스템이 웨이팅 상태에 있으므로 시스템 성능 활용도는 낮다.\n유지보수와 통신 제어가 필요하다.\n3 . 분산 처리 시스템 (Distributed Processing)\n2가지 컴퓨터 중 요청하는 컴퓨터는 클라이언트, 서비스 제공 컴퓨터는 서버가 되어 처리함.\n여러 노드나 사이트가 독립적으로 데이터를 처리하며, 각 시스템이 필요할 때 다른 시스템에 요청을 보낸다. 가장 많이 사용되는 시스템.\n예시: 분산 데이터베이스 시스템에서, 본인에게 없는 정보가 필요할 때 클라이언트가 되어 다른 서버에 요청하는 방식.\n장점\n벡업 기능, 분담 기능, 복구 및 정상화 기능을 통해 안정성이 향상된다.\n단점\n통신 연결 및 데이터 일관성을 유지하는 데 추가적인 제어가 필요한다.\n03 용어의 기원\n1 . 시대별 1960년: 데이터 관리 개념이 처음으로 소개됨. 1965년: 2차 회의에서, 시스템화 및 개념 정립이 이루어짐.\n2 . 정의 한 조직(enterprise)의 여러 응용 시스템들이 공용(shared)하기 위해 통합 (integrated)하여 저장(stored)한 운영 데이터(operational data)의 집합 불편함 없이 모든 부서가 편리하고 쉽게 공유할 수 있어야 한다.\n통합 데이터를 묶어 집합으로 관리하며, 중복을 최소화하여 관리함.\n중복 제거 필요 없는 중복은 제거할 수 있으며, 서버가 장애를 일으켰을 때 백업본을 통해 복구 가능.\n의도적 중복 최소화된, 제어된 중복, 통제된 중복은 시스템 성능을 향상시킬 수 있다.\n예: 캐시\n저장 및 공용 데이터를 통합적으로 저장하고, 여러 사용자나 시스템에서 공용으로 사용할 수 있음.\n운영 데이터가 실시간으로 운영되는 것을 강조.\n3 . 특징 실시간 응답 및 요청\n데이터를 실시간으로 처리하고 응답할 수 있는 능력.\n동적 데이터 특성\n데이터를 동적으로 관리하고 변경할 수 있음.\n동시 공유\n여러 사용자가 동시에 데이터를 접근하고 사용할 수 있음.\n멀티 트랜잭션\n다중 트랜잭션을 처리하여 여러 작업을 동시에 처리 가능.\n내용에 의한 참조\n데이터 내부에 있는 또 다른 내용을 참조하여 효율적인 데이터 접근이 가능.\n03 개념적 구성 요소\n1 . 논리적 구성 요소 개체 (Entity) 와 관계 (Relationship) 로 이루어 졌다.\n개체 DB에 기록되는, 반드시 저장해야 하는 중요한 데이터. 정보의 단위. 예, 학생 정보\n그러나 이것만으로는 가치가 떨어진다 그러므로 이 가치를 높이히 위해 다른 정보를 주입.\n관계 개체 간의 연관성. 이를 통해 데이터를 고급 검색하거나 다양한 방식으로 활용할 수 있다.\n2 . 개체의 형식\n속성 (Attribute) 개체의 세부 항목들을 의미합니다. 예를 들어, 학생 개체의 속성은 이름, 학번, 생년월일 등이 될 수 있습니다.\n개체 타입(Entity Type) 개체의 형태나 틀을 정의합니다. 예를 들어, 학생 개체 타입은 ’학생’이라는 틀에 맞는 모든 정보를 담습니다.\n개체 집합(Entity Set) 동일한 개체 타입의 여러 인스턴스를 모은 집합. 개별 개체 타입의 집합.\n레코드(엔티티 인스턴스) 개체 타입에 맞춰 실제 값들로 표현된 개체를 레코드라고 합니다. 예를 들어, 특정 학생의 정보는 하나의 레코드로 저장됩니다.\n레코드 타입, 레코드 어커런스.\n식별자(Identifier) 각 개체는 유일하게 식별되며, 이를 통해 다른 개체를 결정하거나 참조할 수 있다.\nER 다이어그램 이름, 학과는 중복되는 경우가 많으므로, 학번을 사용하여 유일하게 식별. 데이터를 더 가치있게 할 수 있으며, 이러한 구현은 어렵지 않다.\n\n물리적 구성 요소\n\n컴퓨터의 물리적 관점에서 모든 데이터는 동일한 방식으로 처리된다. 이는 하드웨어나 물리적인 데이터 저장 구조와 관련이 있다.\n\n구조: 논리적 구조: 우리가 생각하는 데이터의 배치나 정렬 순서에 따라 데이터를 논리적으로 배치하는 것을 의미합니다.\n\n예, 논리적으로 데이터를 오름차순으로 정렬하거나 특정 기준에 맞춰 배열할 수 있다.\n물리적 구조: 실제 컴퓨터가 데이터를 저장하는 방식입니다.\n이는 논리적 배치와 다를 수 있으며, 컴퓨터 시스템이 데이터를 어떻게 물리적으로 저장하고 처리할지에 대한 관점입니다.\n논리적 구조와 물리적 구조는 완전히 다르며, 데이터를 어떻게 논리적으로 설계하느냐와 컴퓨터가 실제로 이를 저장하고 접근하는 방식은 다르게 구현됩니다.\n성능향상을 위해서 필요하며, 그외에는 운영체제에 맡기면 된다.\n\n링크드 리스트 데이터를 연결된 리스트(Linked List) 형태로 배치하여 각 요소가 다음 요소를 가리키는 방식으로 이어집니다. 이는 논리적으로 순차적인 연결을 제공하지만, 물리적으로는 메모리 내에서 연속적으로 저장되지 않을 수 있습니다.\n클러스터링 (Clustering): 데이터를 의도적으로 가까운 위치에 배치하여, 데이터를 빠르게 접근할 수 있도록 하는 기술입니다.\n\n이를 통해 액세스 속도가 빨라집니다.\n특정 특별한 케이스에서 사용되며, 데이터 검색과 접근을 최적화하기 위해 데이터를 물리적으로도 배치하는 경우가 많습니다.\n교제: Fundamentals of Database Systems (7th edition).pdf 데이터베이스실무의 선수과목: 화일처리론"
  },
  {
    "objectID": "r-and-d.html",
    "href": "r-and-d.html",
    "title": "R&D",
    "section": "",
    "text": "개발 일지4\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n개발 일지3\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n개발 일지2\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n개발 일지\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFastAPI\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGit 사용법\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n빅데이터 개요\n\n\n\n빅데이터 정의\n\n\n\n\n\n\n\n\n\nJun 9, 2024\n\n\n통계학: 파이썬을 이용한 분석\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rd/od_02.html",
    "href": "rd/od_02.html",
    "title": "FastAPI",
    "section": "",
    "text": "CHAPTER 3에 대해 다루고자 한다.\n01 소개 https://fastapi.tiangolo.com/#requirements\n02 FastAPI 애플리케이션\n3 - 1 . hello 파일 생성하기 from fastapi import FastAPI #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI() @app.get(“/hi”) def greet(): return “Hello? World?”\n3 - 2 . cmd 실행 uvicorn hello:app –reload\n3 - 3 . 파일에 추가 코드 if name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 4 . 검색창 입력 http://localhost:8000/hi\n3 - 5 . Requests로 /hi 테스트 import requests r = requests.get(“http://localhost:8000/hi”) r.json()\n3 - 6 . Requests와 거의 동일한 HTTPX로 /hi 테스트 # pip install httpx import httpx r = httpx.get(“http://localhost:8000/hi”) r.json()\nHTTPie 설치가 안되어 있을 경우 pip install httpie # 설치 명령어 where http # 설치된 경로 출력 명령어\n3 - 7 . HTTPie로 /hi 테스트 httpie localhost:8000/hi\n3 - 8 . HTTPie로 /hi 를 테스트해 응답 본문만 출력 http -b localhost:8000/hi\n3 - 9 . HTTPie로 /hi를 테스트하고 모든 정보 출력 http -v localhost:8000/hi\n03 HTTP 요청\n3 - 11 . 인사말 경로 변환 from fastapi import FastAPI\napp = FastAPI()\n@app.get(“/hi/{who}”) def greet(who): return f”Hello? {who}”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 12 . 브라우저에서 hi/Mom 테스트 http://localhost:8000/hi/Mom\n3 - 13 . HTTPie로 hi/Mom 테스트 http localhost:8000/hi/Mom\n3 - 14 . Requests로 /hi/Mom 테스트 import requests r = requests.get(“http://localhost:8000/hi/Mom”) r.json()\n3 - 15 . 인사말 쿼리 매개변수 변환 from fastapi import FastAPI #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI()\n@app.get(“/hi”) def greet(who): return f”Hello? {who}”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 16 . 브라우저에서 테스트 http://localhost:8000/hi?who=Mom\n3 - 17 . HTTPie 테스트 http -b localhost:8000/hi?who=Mom\n3 - 18 . HTTPie 및 매개변수 사용 테스트 http -b localhost:8000/hi who==Mom\n3 - 19 . Requests로 테스트 import requests r = requests.get(“http://localhost:8000/hi?who=Mom”) r.json()\n3 - 20 . Requests 및 매개변수 테스트 import requests params = {“who”: “Mom”} r = requests.get(“http://localhost:8000/hi”, params=params) r.json()\n3 - 21 . 인사말 본문 반환 from fastapi import FastAPI, Body #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI()\n@app.post(“/hi”) def greet(who : str = Body(embed=True)): return f”Hello? {who}?”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 22 . HTTPie로 테스트 http -v localhost:8000/hi who=Mom\n3 - 23 . Requests로 테스트 import requests r = requests.post(“http://localhost:8000/hi”, json={“who”: “Mom”}) r.json()\n3 - 24 . Requests로 테스트 from fastapi import FastAPI, Header #pip install fastapi uvicorn #pip show fastapi uvicorn\napp = FastAPI()\n@app.get(“/hi”) def greet(who : str = Header()): return f”Hello? {who}?”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 25 . Requests로 테스트 http -v localhost:8000/hi who:Mom\n3 - 26 . 헤더 반환 from fastapi import FastAPI, Header\napp = FastAPI()\n@app.get(“/agent”) def get_agent(user_agent : str = Header()): return user_agent\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 27 . HTTPie로 User-Agent 헤더 테스트 http -v localhost:8000/agent\n04 HTTP 요청\n3 - 28 . HTTP 상태 코드 지정 from fastapi import FastAPI, Header\napp = FastAPI()\n@app.get(“/happy”) def happy(status_code=200): return “:)”\nif name == “main”: import uvicorn uvicorn.run(“hello:app”, reload=True)\n3 - 29 . HTTP 상태 코드 지정 http localhost:8000/happy\n3 - 30 . HTTP 상태 코드 지정 from fastapi import FastAPI, Response\napp = FastAPI()\n@app.get(“/header/{name}/{value}”) def header(name: str, value: str, response: Response): response.headers[name] = value return “normal body”\n3 - 31 . 응답 HTTP 헤더 테스트 http localhost:8000/header/marco/polo\n파일 저장\n위 코드를 test_json.py 파일로 저장한다.\n3 - 32 . JSON 폭발 방지 # pip install pytest import datetime import pytest from fastapi.encoders import jsonable_encoder import json\n@pytest.fixture def data(): return datetime.datetime.now()\ndef test_json_dump(data): with pytest.raises(TypeError): _ = json.dumps(data)\ndef test_encoder(data): out = jsonable_encoder(data) assert out json_out = json.dumps(out) assert json_out\npytest 실행 pytest test_json.py\n3 - 33 . 모델 변형 from datetime import datetime from pydantic import BaseModel\n\nPydantic 모델 정의\nclass TagIn(BaseModel): tag: str\nclass Tag(BaseModel): tag: str created: datetime secret: str\nclass TagOut(BaseModel): tag: str created: datetime\n\n\n입력 데이터 생성\ninput_data = {“tag”: “fastapi”}\n\n\nTagIn 모델 사용 (데이터 검증)\ntag_in = TagIn(**input_data) print(tag_in)\n\n\nTag 모델 사용\ntag_data = { “tag”: “fastapi”, “created”: datetime.now(), “secret”: “my_secret” } tag = Tag(**tag_data) print(tag)\n\n\nTagOut 모델 사용 (Tag에서 특정 필드 제외)\ntag_out = TagOut(**tag.dict()) # secret 필드는 자동으로 제외됨 print(tag_out)\npytest 실행 python tag.py\n3 - 34 . 모델 변형 from datetime import datetime from model.tag import Tag\ndef create(tag: Tag) -&gt; Tag: “““태그를 생성한다.”“” return tag\ndef get(tag_str: str) -&gt; Tag: “““태그를 반환한다.”“” return Tag(tag=tag_str, created=datetime.utcnow(), secret=““)\n3 - 35 . 모델 변형 from datetime import datetime from model.tag import TagIn, Tag, TagOut import service.tag as service from fastapi import FastAPI\napp = FastAPI()\n@app.post(‘/’) def create(tag_in: TagIn) -&gt; TagIn: tag: Tag = Tag(tag=tag_in.tag, created=datetime.utcnow(), secret=“shhhh”) service.create(tag) return tag_in\n@app.get(‘/{tag_str}’, response_model=TagOut) def get_one(tag_str) -&gt; TagOut: tag: Tag = service.get(tag_str) return tag\n3 - 36 . 실행 uvicorn web.tag:app\n3 - 37 . Tag 가져오기 요청 http -b localhost:8000/GoodTag\n04 자동 문서화\n3 - 1 . 접속하기 http://localhost:8000/docs\n3 - 2 . ㅇ\n3 - 3 . ㅇ\n3 - 4 . ㅇ"
  },
  {
    "objectID": "rd/od_04.html",
    "href": "rd/od_04.html",
    "title": "개발 일지2",
    "section": "",
    "text": "01 VS코드 실행 관리자 권한으로 실행하기.\n1 . uv 설치 pip install uv\n2 . Python 3.12로 가상 환경 생성 uv venv -p 3.12 fastapi-env\n3 . 프로제트 클론하기 djailab/backend를 클론 후 파일 생성 및 폴더에 저장.\ngit clone https://github.com/djailab/backend.git\n4 . 프로젝트 설정 동기화 uv sync\n5 . 파일 실행 명령어 uv run hello.py"
  },
  {
    "objectID": "rd/od_06.html",
    "href": "rd/od_06.html",
    "title": "개발 일지4",
    "section": "",
    "text": "재실 감지 시스템을 개발하기 위한 기초 개념에 대해 다루고자 한다.\n01 기본 개념\n1 . 비동기 프로그래밍 (async/await) FastAPI는 비동기 처리를 지원하므로 기본적인 개념을 이해해야 한다.\n2 . 데이터베이스 개념 SQL 기본 문법 (SELECT, INSERT, UPDATE, DELETE)\n관계형 데이터베이스 (MySQL, PostgreSQL, SQLite 등)\nhttps://www.w3schools.com/sql/\n02 FastAPI 기본 사용법 FastAPI 설치 및 프로젝트 구조\n기본적인 API 엔드포인트 만들기 (@app.get(), @app.post())\nPydantic을 이용한 데이터 검증 (BaseModel)\nFastAPI에서 비동기(async def) 사용법\nfrom fastapi import FastAPI\napp = FastAPI()\n@app.get(“/”) async def home(): return {“message”: “Hello, FastAPI!”}\nFastAPI\nFastAPI framework, high performance, easy to learn, fast to code, ready for production\nfastapi.tiangolo.com\nTutorial - User Guide - FastAPI\nFastAPI framework, high performance, easy to learn, fast to code, ready for production\nfastapi.tiangolo.com\n03 SQLModel을 활용한 데이터베이스 연동 SQLModel 설치 및 기본 사용법\nORM 개념 이해하기 (객체를 데이터베이스 테이블과 매핑)\n데이터베이스 모델 정의 및 CRUD(Create, Read, Update, Delete) 구현\nSQLite로 간단한 DB 실습 후, MySQL/PostgreSQL 연동\nSQLModel\nSQLModel, SQL databases in Python, designed for simplicity, compatibility, and robustness.\nsqlmodel.tiangolo.com\n04 재실 감지 시스템 API 설계 및 구현 RESTful API 설계 방법\nAPI 요청 및 응답 데이터 형식(Pydantic)\nCRUD API 구현\nAPI 테스트(Postman, Curl 활용)\nfrom fastapi import FastAPI, Depends from sqlmodel import Session, select\napp = FastAPI()\n@app.post(“/rooms/{room_id}/status”) async def update_room_status(room_id: int, is_occupied: bool, session: Session = Depends(get_session)): room = session.exec(select(RoomStatus).where(RoomStatus.id == room_id)).first() if room: room.is_occupied = is_occupied session.commit() return {“message”: “Room status updated”} return {“error”: “Room not found”}\n05 실시간 감지 시스템과 연동 (추가 학습) WebSocket을 이용한 실시간 데이터 전송\n센서 데이터 연동 (IoT 장치와 연결)\n백엔드에서 프론트엔드와 통신 (웹 애플리케이션 또는 모바일 앱과 연동)\nWebSockets - FastAPI\nFastAPI framework, high performance, easy to learn, fast to code, ready for production\nfastapi.tiangolo.com\nMQTT - The Standard for IoT Messaging\nWhy MQTT? Lightweight and Efficient MQTT clients are very small, require minimal resources so can be used on small microcontrollers. MQTT message headers are small to optimize network bandwidth. Bi-directional Communications MQTT allows for messaging betwe\nmqtt.org 추가 학습 방향 ✅ Docker와 배포 방법\nDocker로 FastAPI + SQLModel 앱 컨테이너화\n서버에 배포 (AWS, GCP, Heroku 등)\n✅ 보안 및 인증\nJWT 인증 방식 (OAuth2)\nAPI 보안 (CORS, CSRF 방어)\n\nhttps://pypi.org/project/paho-mqtt/"
  },
  {
    "objectID": "da/ida/ida_02.html#예제-1",
    "href": "da/ida/ida_02.html#예제-1",
    "title": "2장: 자료의 요약",
    "section": "1. 예제 1",
    "text": "1. 예제 1\n사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\n\npython\n\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3,\n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3,\n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3,\n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3,\n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2,\n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4,\n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2,\n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2,\n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2,\n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "da/ida/ida_03.html#편차",
    "href": "da/ida/ida_03.html#편차",
    "title": "3장: 연속형 자료",
    "section": "1. 편차",
    "text": "1. 편차\nDeviation 각 관측값과 평균의 차이\n편차의 합은 항상 \\(0\\) 이므로, 편차의 평균도 항상 \\(0\\) 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다.\n퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로, 따라서, 편차에서 부호를 없애는 방법으로 “제곱”을 택한 것이다."
  },
  {
    "objectID": "da/ida/ida_03.html#자유도",
    "href": "da/ida/ida_03.html#자유도",
    "title": "3장: 연속형 자료",
    "section": "2. 자유도",
    "text": "2. 자유도\nDegrees of Freedom 위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n\n모집단의 분산인 경우:\n\n“모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n\n표본의 분산인 경우:\n\n표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단”이다.\n\n\n\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이”가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적”이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성”을 높이기 위해 \\(n\\) 이 아닌 \\(n – 1\\) 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 \\(1\\) 로 설정하지 않는 것은 \\(n – 1\\) 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "da/ida/ida_03.html#사분위수범위-1",
    "href": "da/ida/ida_03.html#사분위수범위-1",
    "title": "3장: 연속형 자료",
    "section": "1. 사분위수범위",
    "text": "1. 사분위수범위\nInterquartile Range, IQR\n\\[\\text{IQR} = Q_3 - Q_1\\]\n\\[\n\\begin{aligned}\nm_1 &= (n+1) \\times 0.25, & Q_1 &= x_{\\lfloor m_1 \\rfloor} + (m_1 - \\lfloor m_1 \\rfloor) \\cdot (x_{\\lceil m_1 \\rceil} - x_{\\lfloor m_1 \\rfloor}) \\\\\nm_3 &= (n+1) \\times 0.75, & Q_3 &= x_{\\lfloor m_3 \\rfloor} + (m_3 - \\lfloor m_3 \\rfloor) \\cdot (x_{\\lceil m_3 \\rceil} - x_{\\lfloor m_3 \\rfloor}) \\\\\n\\text{IQR} &= Q_3 - Q_1\n\\end{aligned}\n\\]\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas모듈의 DataFrame객체 내의 멤버 함수인 describe()함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 \\(std\\) 는 자동적으로 \\(n – 1\\) 로 나누어져서 계산된다."
  },
  {
    "objectID": "rd/bae_01_0.html",
    "href": "rd/bae_01_0.html",
    "title": "빅데이터 개요",
    "section": "",
    "text": "데이터의 정의\n1646년 영국 문헌에 처음 등장. “주어진 것” 이란 의미를 갖는 라틴어의 과거 분사형로 사용.\n1940년대 이전 관념적, 추상적 개념 이후 자연 과학 + 사회 과학 발달 이로 인한 영향으로 기술적, 사실적 개념으로 변화.\n\n추론과 추정의 근거를 이루는 사실.\n현실 세계에서 관찰, 측정하여 수집한 사실.\n\n단순한 객체로도 가치 있음. 다른 객체와의 상호작용 속에서 더 큰 가치를 지님.\n객관적 사실이라는 존재적 특성 지님. 추론 추정 예측 전망을 위한 근거로써 당위성을 가짐.\n정량적 데이터: 주로 숫자로 이뤄진 데이터 정성적 데이터: 텍스트로 구성, 함축적 의미 지님\n정형 데이터: 정해진 형식과 구조로 저장, 연산가능 반정형 데이터: 유연함, 스키마(자료의 구조) 정보를 데이터와 함께 제공, 연산불가 비정형 데이터: 정해진 형식 없음, 연산 불가\n수집과정 =&gt; 재생산 과정. 가역 데이터: 원본 환원 가능, 원본 비가역 데이터: 원본 환원 불가, 새 객체\n암묵지: 개인이 가진 경험, 정리 안됨. 형식지: 암묵지를 정리함.\n형식지로 정리 방법 공통화: 공유. 표출화: 형식지가 됨 연결화: 그걸 재분류 및 체계화 내면화: 전달 받은 형식지 다시 개인화.\n지식의 피라미드 가치 창출 프로세스 데이터 &gt; 정보 &gt; 지식 &gt; 지혜\n\n\n데이터베이스\n1950년대 미군의 군비 상황 용도로 쓰게 됨. 1963년 6월 SDC에서 정의됨\nDB: 소재를 체계적으로 분류한 편집물 DBMS: 응용 프로그램들이 공유되는 소프트웨어 관계형, 객체지향, 네트워크, 계층형 SQL: DB 접근용 언어\n통합: 미중복 저장: 컴퓨터 접근 가능한 저장매체 공용: 여러 사용자와 공유 변화: 지속적 갱신 + 현 데이터 유지\nOLTP: 데이터 갱신 위주 OLAP: 데이터 조회 위주\nDW: 의사 결정 도움 용도 주제지향성: 주요 주제 통합성: 일관형, 전사적인 관점 시계열성: 시점별 비휘발성: 갱신 외에 변경 없음\n구성: 데이터 모델-[ELT, ODS]-DW-[OLAP, …]\n\n\n빅데이터 개요\n일반적 DB 용량을 초과하는 데이터. 작은 용량에선 얻을 수 없었던 새로운 통찰과 가치를 추출\n\n기업: 고객 데이터 학계: 거대 데이터\n데이터 처리 시점: 사전 처리 -&gt; 사후 처리 데이터 처리 범위: 표본조사 -&gt; 전수조사 데이터 가치 판단 기준: 질 -&gt; 양(데이터 지속적 추가) 데이터 분석 방향: 이론적 인과관계 -&gt; 단순한 상하관계\n전세계 정보량 추이 메인 프레임 컴퓨터 &gt; PC 시대 &gt; 인터넷 모바일 시대 &gt; IT\n특징: 규모, 유형, 속도, 품질, 가치 활용: 자원, 기술, 인력\n테크닉: 연관규칙 학습, 유형 분석, 유전 알고리즘, 기계 학습, 회귀 분석, 감성 분석, 소셜 네트워크 분석\n\n\n빅데이터 가치\n\n\n데이터 산업의 이해\n\n\n빅데이터 조직 및 인력"
  },
  {
    "objectID": "da/bae_01_0.html",
    "href": "da/bae_01_0.html",
    "title": "빅데이터 개요",
    "section": "",
    "text": "데이터의 정의\n1646년 영국 문헌에 처음 등장. “주어진 것” 이란 의미를 갖는 라틴어의 과거 분사형로 사용.\n1940년대 이전 관념적, 추상적 개념 이후 자연 과학 + 사회 과학 발달 이로 인한 영향으로 기술적, 사실적 개념으로 변화.\n\n추론과 추정의 근거를 이루는 사실.\n현실 세계에서 관찰, 측정하여 수집한 사실.\n\n단순한 객체로도 가치 있음. 다른 객체와의 상호작용 속에서 더 큰 가치를 지님.\n객관적 사실이라는 존재적 특성 지님. 추론 추정 예측 전망을 위한 근거로써 당위성을 가짐.\n정량적 데이터: 주로 숫자로 이뤄진 데이터 정성적 데이터: 텍스트로 구성, 함축적 의미 지님\n정형 데이터: 정해진 형식과 구조로 저장, 연산가능 반정형 데이터: 유연함, 스키마(자료의 구조) 정보를 데이터와 함께 제공, 연산불가 비정형 데이터: 정해진 형식 없음, 연산 불가\n수집과정 =&gt; 재생산 과정. 가역 데이터: 원본 환원 가능, 원본 비가역 데이터: 원본 환원 불가, 새 객체\n암묵지: 개인이 가진 경험, 정리 안됨. 형식지: 암묵지를 정리함.\n형식지로 정리 방법 공통화: 공유. 표출화: 형식지가 됨 연결화: 그걸 재분류 및 체계화 내면화: 전달 받은 형식지 다시 개인화.\n지식의 피라미드 가치 창출 프로세스 데이터 &gt; 정보 &gt; 지식 &gt; 지혜\n\n\n데이터베이스\n1950년대 미군의 군비 상황 용도로 쓰게 됨. 1963년 6월 SDC에서 정의됨\nDB: 소재를 체계적으로 분류한 편집물 DBMS: 응용 프로그램들이 공유되는 소프트웨어 관계형, 객체지향, 네트워크, 계층형 SQL: DB 접근용 언어\n통합: 미중복 저장: 컴퓨터 접근 가능한 저장매체 공용: 여러 사용자와 공유 변화: 지속적 갱신 + 현 데이터 유지\nOLTP: 데이터 갱신 위주 OLAP: 데이터 조회 위주\nDW: 의사 결정 도움 용도 주제지향성: 주요 주제 통합성: 일관형, 전사적인 관점 시계열성: 시점별 비휘발성: 갱신 외에 변경 없음\n구성: 데이터 모델-[ELT, ODS]-DW-[OLAP, …]\n\n\n빅데이터 개요\n일반적 DB 용량을 초과하는 데이터. 작은 용량에선 얻을 수 없었던 새로운 통찰과 가치를 추출\n\n기업: 고객 데이터 학계: 거대 데이터\n데이터 처리 시점: 사전 처리 -&gt; 사후 처리 데이터 처리 범위: 표본조사 -&gt; 전수조사 데이터 가치 판단 기준: 질 -&gt; 양(데이터 지속적 추가) 데이터 분석 방향: 이론적 인과관계 -&gt; 단순한 상하관계\n전세계 정보량 추이 메인 프레임 컴퓨터 &gt; PC 시대 &gt; 인터넷 모바일 시대 &gt; IT\n특징: 규모, 유형, 속도, 품질, 가치 활용: 자원, 기술, 인력\n테크닉: 연관규칙 학습, 유형 분석, 유전 알고리즘, 기계 학습, 회귀 분석, 감성 분석, 소셜 네트워크 분석\n\n\n빅데이터 가치\n\n\n데이터 산업의 이해\n\n\n빅데이터 조직 및 인력"
  }
]